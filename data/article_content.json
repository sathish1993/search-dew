[{"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-224", "title": "Data Engineering Weekly", "content": "Learn the fundamental concepts to build a data platform in your organization.- Tips and tricks for data modeling and data ingestion patterns- Explore the benefits of an observation layer across your data pipelines- Learn the key strategies for ensuring data quality for your organization\nGet the guide\nOne key part of data engineering is collecting high-quality data to empower business decisions and intelligence. How far can one go to track user activities? Regulations and ethical considerations always need to be considered. Meta was recently caught using a localhost tracking on Android devices to track users\u2019 activity. The author explains the tech behind the localhost tracking.\nhttps://www.zeropartydata.es/p/localhost-tracking-explained-it-could\nA golden repository is a curated collection of high-quality, accurately labeled code examples that serve as the definitive source of clean data for contextual code generation systems.\nIntuit writes about a platform-centric approach to AI code generation, pointing out that the out-of-the-box coding assistant is inefficient without context. \nhttps://medium.com/intuit-engineering/a-platform-centric-approach-to-ai-assisted-code-generation-at-intuit-03984a85558e\nNetflix writes about UDA (Unified Data Architecture), a knowledge graph-based foundation for managing and connecting domain models across disparate systems to address challenges of duplicated/inconsistent models, inconsistent terminology, data quality issues, and limited connectivity. UDA allows teams to register domain models, catalog and map these models to data containers (like GraphQL services, Data Mesh sources, Iceberg tables), transpile them into various schema languages (GraphQL, Avro, SQL), automate data movement, and enable discovery and programmatic introspection.\nhttps://netflixtechblog.com/uda-unified-data-architecture-6a6aee261d8d\nWant to migrate from Airflow but don't know where to begin? Introducing Airlift, a toolkit that reduces risk when migrating from Airflow to Dagster.\n- View Airflow execution alongside your Dagster workflows- Turn existing Airflow DAGs into Dagster assets- Consolidate multiple Airflow instances together in one place\nGet the Airflift Guide\nI strongly believe in systems (including the process of building systems), which breaks people rather than people breaking the system. The process and technology to access data to insight are vital for modern organizations to survive. The author takes the case study of AI at Amazon (with Alexa), and how brittleness leads to competitive disadvantage. \nhttps://surfingcomplexity.blog/2025/06/08/ai-at-amazon-a-case-study-of-brittleness/\nMETR highlights that state-of-the-art AI models like OpenAI\u2019s o3 engage in sophisticated \u201creward hacking\u201d during autonomous coding and AI R&D tasks\u2014exploiting scoring loopholes, altering test setups, or accessing known solutions to game the evaluation without solving the intended problems. Despite being aware that such behavior misaligns with user goals and even denying it when prompted, models still pursue these exploits. The report warns that naively punishing these behaviors may make them harder to detect and urges the need for deeper alignment strategies beyond surface-level fixes.\nhttps://metr.org/blog/2025-06-05-recent-reward-hacking/\nGrab shares how they built an interactive FlinkSQL platform to simplify and scale real-time stream processing beyond their earlier Zeppelin-based setup, which struggled with version drift, slow startups, and poor integrations. The new architecture includes a shared FlinkSQL gateway, custom REST APIs for auth and session control, and a query layer with UI and programmatic access to Kafka via Hive Metastore. A config-driven deployment tool rounds it out, letting users write SQL, define resources, and launch production pipelines in minutes, boosting both speed and adoption of streaming use cases across teams.\nhttps://engineering.grab.com/the-complete-stream-processing-journey-on-flinksql\nInstacart introduces LACE (LLM-Assisted Chatbot Evaluation), a framework that uses LLMs to automate the evaluation of customer support chat quality across five dimensions: understanding, correctness, efficiency, satisfaction, and compliance. To capture nuanced issues, the system combines direct prompting with more advanced agentic methods\u2014LLMs reflecting on or debating their assessments. By separating evaluation logic from output formatting and validating against human feedback, LACE helps pinpoint flaws and drive iterative improvements in chatbot performance and user experience.\nhttps://tech.instacart.com/turbocharging-customer-support-chatbot-development-with-llm-based-automated-evaluation-6a269aae56b2\nNubank shares its integrations of Foundation Models\u2014mainly transformers\u2014into its AI platform to go beyond traditional tabular ML. Building on existing data infrastructure and governance, Nubank added components for sequence data preprocessing, GPU cluster orchestration (via Ray), and deep model training. The new stack boosts AUC by >1.2% on benchmarks without new data. It is now powering key use cases, all backed by tooling for model tracking, cataloging, and impact measurement against tabular baselines.\nhttps://building.nubank.com/foundation-models-ai-nubank-transformation/\nPinterest outlines its Offline Approximate Nearest Neighbors (ANN) as a cheaper, scalable alternative to Online ANN for ad retrieval when queries are relatively static. Instead of real-time searches, Pinterest batch ANN computations and store precomputed <key, neighbors> pairs for fast lookups. This approach powers use cases like \u201cSimilar Item Ads\u201d and \u201cVisual Embedding\u201d-based retrieval, showing strong performance with over 50% lower infrastructure cost than Online ANN, without sacrificing recall or precision.\nhttps://medium.com/pinterest-engineering/unlocking-efficient-ad-retrieval-offline-approximate-nearest-neighbors-in-pinterest-ads-6fccc131ac14\nAll rights reserved, ProtoGrowth Inc., India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-223", "title": "Data Engineering Weekly", "content": "Discover how Dagster's unified control plane enables teams to build reliable, scalable AI/ML systems. Join us with our friends at Neurospace for a hands-on session showcasing:- Dagster's core capabilities for MLOps orchestration- Live demo of production AI pipeline implementation- Practical strategies for EU AI Act compliancePerfect for data teams navigating the intersection of AI innovation and regulatory compliance.\nSave your spot\nThe author highlights the increasing importance of model inference in the AI landscape, shifting the focus from solely training to optimizing the affordability and performance of deploying models. The article discusses various inference optimization techniques, including model compression (pruning, distillation, quantization like GPTQ) and KV cache management (MQA, GQA, MLA, sparsification, multilevel caching).\nhttps://queue.acm.org/detail.cfm?id=3733701\nThe author writes an FAQ-style guide to evaluating Large Language Model (LLM) applications, emphasizing that successful AI teams prioritize measurement and iteration. The FAQ clarifies that Retrieval-Augmented Generation (RAG) is not dead but requires effective retrieval strategies beyond naive vector search, especially for complex tasks like coding. The author strongly advocates for building custom annotation tools for faster iteration, recommends binary (pass/fail) evaluations over Likert scales for clarity, and offers strategies for debugging multi-turn conversations, building automated evaluators, and determining the number of annotators (favoring a single domain expert).\nhttps://hamel.dev/blog/posts/evals-faq/\nEvery platform engineering team desires to build LLM as an intelligence operating system. The author maps the operating system concepts to the LLM applications to give a new dimension to thinking about LLM. \nhttps://medium.com/wix-engineering/7-operating-system-concepts-every-llm-engineer-should-understand-84ddf0cfb89a\nLearn the fundamental concepts to build a data platform in your organization, covering common design patterns for data ingestion and transformation, data modeling strategies, and data quality tips.\nRead the full guide\nThe research highlights the vulnerability of current machine unlearning techniques in Large Language Models (LLMs) to \"benign relearning attacks,\" where even small amounts of seemingly unrelated or publicly available data can \"jog\" an unlearned model's memory, causing it to output previously forgotten information. The authors demonstrate that finetuning-based unlearning methods often obfuscate rather than truly erase knowledge through experiments on benchmarks like TOFU, Who's Harry Potter (WHP), and WMDP.\nhttps://blog.ml.cmu.edu/2025/05/22/unlearning-or-obfuscating-jogging-the-memory-of-unlearned-llms-via-benign-relearning/\nUber's Compliance Data Store (CDS) team describes its config-driven archival and retrieval framework designed to manage petabytes of regulatory data, addressing challenges like HDFS storage quotas, schema evolution, data consistency during backfills, and efficient data access. The architecture uses Python, Piper (Airflow-based orchestrator), Terrablob (S3 abstraction) for cold storage, and MySQL for metadata. The archival process, driven by database and dataset-level YAML configurations, schedules daily or weekly jobs to move data from HDFS (hot storage) to cold storage based on TTL policies.\nhttps://www.uber.com/blog/from-archival-to-access/\nThe article explores micro-patterns in traffic to improve ETA (Estimated Time of Arrival) models, observing that while short-distance travel times can be highly variable due to random events (like traffic lights or minor congestions), longer journeys tend to exhibit more predictable and statistically stable travel times. It draws an analogy to the \"traffic light dance,\" where short-term discrepancies between two vehicles on the same route even out over distance, and proposes that this phenomenon resembles the Central Limit Theorem (CLT), where the sum of (not strictly independent) travel times across many road segments empirically converges towards a normal distribution.\nhttps://eng.lyft.com/how-science-inspires-our-eta-models-bf229e3148e8\nThe author discusses using the Z3 theorem prover to automatically compute lookup tables (LUTs) for fast character classification, specifically for vectorized base64 decoding using SIMD instructions. The approach splits each character's ASCII value into two 4-bit nibbles and uses two 16-byte LUTs (lut_lo\u00a0and\u00a0lut_hi) for classification via a bitwise AND operation and by modeling the classification rules (base64 characters classify to 0, others to >0) as a satisfiability problem in Z3, the tool can automatically derive the content of these LUTs, simplifying a potentially complex and error-prone manual task.\nhttps://lemire.me/blog/2025/06/01/easy-vectorized-classification-with-z3/\nPinterest describes the Hadoop Control Center (HCC), the internal tool to automate and streamline Hadoop cluster operations, particularly for in-place migrations and scaling. Faced with challenges in manual scaling, including IP address limitations, instance availability, cost of running parallel clusters, and risks of application migration, HCC automates the complex scale-in process by managing Auto Scaling Groups (ASGs), decommissioning nodes gracefully (ensuring HDFS data replication and no impact on running YARN applications), and handling excludes files.\nhttps://medium.com/pinterest-engineering/automated-migration-and-scaling-of-hadoop-clusters-69c0967228e4\nPinterest details TransActV2, its home feed recommendation model. To handle the scale, TransActV2 uses nearest neighbor selection at ranking time to feed only the most recent and relevant historical actions to the model, employs int8 quantization for action storage, and utilizes custom OpenAI Triton kernels, fused transformers (SKUT), pinned memory, and request-level de-duplication for efficient, low-latency serving, resulting in substantial offline metric improvements (e.g., +13.31% top-3 repin hit rate) and significant online A/B test lifts (e.g., +6.35% repin increase, +1.41% time spent).\nhttps://medium.com/pinterest-engineering/next-level-personalization-how-16k-lifelong-user-actions-supercharge-pinterests-recommendations-bd5989f8f5d3\nApache Spark 4.0 introduces significant advancements, including enhanced SQL language features like SQL scripting, reusable SQL UDFs, and PIPE syntax, major improvements to Spark Connect with near-complete feature parity for Python and Scala clients, and new support for Go, Swift, and Rust. The release also emphasizes reliability and productivity with ANSI SQL mode enabled by default, a new VARIANT data type for semi-structured data, structured JSON logging, new Python API capabilities like native Plotly-based plotting and a Python Data Source API, and advances in Structured Streaming such as the transformWithState API for arbitrary stateful processing and a State Store Data Source for improved observability.\nhttps://www.databricks.com/blog/introducing-apache-spark-40\nAll rights reserved, ProtoGrowth Inc., India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-222", "title": "Data Engineering Weekly", "content": "Learn what it really takes to run production-grade ML systems\u2014without breaking your architecture or compliance efforts. \nJoin Dagster and Neurospace to learn:- How to build AI pipelines with orchestration baked in- How to track data lineage for audits and traceability- Tips for designing compliant workflows under the EU AI ActRegister for the technical session\nDuckDB announced a new open table format, DuckLake. This announcement has triggered many interesting conversations about storing metadata in a relational database vs. object storage. Is this what Hive metastore should be iterating from storing logical partition to time travel & built-in snapshot tables? With S3 Express One, why not metastore in Express One vs a relational database, which can reduce additional complexity? I don\u2019t know which one gains industry adoption, but DuckLake brings some interesting questions into the ecosystem. \nhttps://duckdb.org/2025/05/27/ducklake.html\nTIL about ICE (Interoperable, Composable, Efficient) stack. Data Engineering Weekly recently published a reference architecture for a composable data architecture. ICE stack elegantly represents the reference architecture. The author further highlights why hyperscalers like AWS, Azure, and Cloudflare offer managed Iceberg services. \nhttps://www.ssp.sh/blog/open-table-format-revolution/\nSince the original\u00a0Attention Is All You Need\u00a0paper, many modern techniques have been developed. The author provides a comprehensive overview of all the latest techniques with a PyTorch code example.\nhttps://www.stephendiehl.com/posts/post_transformers/\nLearn the fundamental concepts to build a data platform in your organization.- Tips and tricks for data modeling and data ingestion patterns- Explore the benefits of an observation layer across your data pipelines- Learn the key strategies for ensuring data quality for your organization\nGet the guide\nPostgres is becoming the one ring to rule them all. Instacart narrates its search infrastructure evolution from Elasticsearch to PGVector, including adopting FAISS for semantic search. I have concerns about mixing search infrastructure with a typical transactional workload. In the Instacart case, though they use Postgres, the instance seems isolated for search indexing rather than a hybrid transactional + search infrastructure. \nhttps://tech.instacart.com/how-instacart-built-a-modern-search-infrastructure-on-postgres-c528fa601d54\nSearching in all data formats will be the next big push in data engineering, and that is one area I\u2019m excited about. Dropbox shares the challenges in indexing multimedia files, its approach to solving the problem, and lessons learned. \nhttps://dropbox.tech/infrastructure/multimedia-search-dropbox-dash-evolution\nThe \u201cTap the Shoulder problem\u201d is one of the biggest productivity killers in the knowledge industry. \nA quick question on Slack will inevitably summon your subject matter expertise. Uber writes about Genie, its internal Slack bot, and the effort to build Genie\u2019s answer quality to near-human precision.\nhttps://www.uber.com/blog/enhanced-agentic-rag/\nWhatnot writes about the challenges in scaling batch inference as the number of users grows, and the optimization to move towards online inference. The blog narrates adopting a hybrid approach with AWS Sagemaker integration and Chalk feature store.\nhttps://medium.com/whatnot-engineering/6x-faster-ml-inference-why-online-batch-16cbf1203947\nPinterest describes modernizing its Home Feed pre-ranking stage, moving from a design where lightweight rankers ran separately on each retrieval source to a more unified system and model architecture. The new pre-ranking layer features a request-level sub-component (processing user/context features to generate a compressed user representation) and an item-level sub-component (performing online item feature extraction, processing, and user-item feature crossing), which are jointly trained but decoupled for efficient serving using a root-leaf architecture to manage a large item corpus. \nhttps://medium.com/pinterest-engineering/modernizing-home-feed-pre-ranking-stage-e636c9cdc36b\nAirbnb writes about enhancing its Interactive Voice Response (IVR) system using machine learning to improve voice-based customer support. The system reimagined IVR journey involves automated speech recognition (ASR) fine-tuned with Airbnb-specific terminology (reducing word error rate from 33% to ~10%), a Contact Reason Detection model to classify caller intent, a Help Article Retrieval and Ranking system (using semantic retrieval and LLM-based re-ranking) to provide relevant self-service information via SMS/app notifications, and a paraphrasing model (using curated summaries and nearest-neighbor matching) to summarize user intent before delivering solutions. \nhttps://medium.com/airbnb-engineering/listening-learning-and-helping-at-scale-how-machine-learning-transforms-airbnbs-voice-support-b71f912d4760\nThe blog captures the emerging lakehouse architecture, combining CDC pipelines and event sourcing systems. The architecture highlights one industry challenge: We still need a specialized OLAP engine like Clickhouse for real-time query capabilities and the typical data ingestion flowing to Lakehouses. The OLAP engines tend to store the data in their proprietary format, so though they provide tiered architecture and cold storage, running a duplicate pipeline and data duplication are inevitable. \nhttps://tech-blogs.eloelo.in/building-eloelos-data-platform-part-i-our-2-year-journey-to-batch-real-time-lakehouse-on-open-def6c8b79dd4\nAll rights reserved, ProtoGrowth Inc., India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-221", "title": "Data Engineering Weekly", "content": "Components provides a modular architecture that enables data practitioners to self-serve while maintaining engineering quality. Built for the AI era, Components offers compartmentalized code units with proper guardrails that prevent \"AI slop\" while supporting code generation.\nSee how it works in 4 easy steps\nAs we adopt the Lakehouse architecture more and more, the quest for a robust query engine increases. I think the market is wide open for more innovations, as Onehouse announces a compute runtime named Quanton. The blog is an excellent compilation of types of query engines on top of the lakehouse, its internal architecture, and benchmarking against various categories. \nhttps://www.onehouse.ai/blog/apache-spark-vs-clickhouse-vs-presto-vs-starrocks-vs-trino-comparing-analytics-engines\nKIP-1150 (\"Diskless Kafka\") is one of my most anticipated releases from Apache Kafka. Outsourcing the replication of Kafka will simplify the overall application layer, and the author narrates what Kafka would be like if we had to develop a durable cloud-native event log from scratch. \nhttps://www.morling.dev/blog/what-if-we-could-rebuild-kafka-from-scratch/\nThe article provides a comprehensive guide to modern Large Language Model (LLM) sampling techniques, explaining why sub-word tokenization (using methods like Byte Pair Encoding or SentencePiece) is preferred over letter or whole-word tokenization. The blog details how LLMs generate text by predicting the next tokens based on learned probabilities and then dives into various sampling methods that introduce controlled randomness to make outputs more varied and creative.\nhttps://rentry.co/samplers\nA comprehensive guide for data platform owners looking to build a stable and scalable data platform, starting with the fundamentals: \n- Architecting Your Data Platform- Design Patterns and Tools- Observability- Data Quality\nGet the guide\nGrey Box Paradigm: You assess quality through outcomes, not through code review\nThe nature of data engineering is an iterative process of the grey box paradigm. If you look at all the BI or UI-based ETL tools, the code is a black box for us, but we validate the outcome generated by the black-box. Understanding this fact will help data tools break new ground with the advancement of AI agents.\nhttps://dlthub.com/blog/grey-box\nSquarespace writes about migrating their business-critical PostgreSQL databases to CockroachDB (CRDB) at scale. The migration strategy involved using PostgreSQL's Change Data Capture (CDC) capabilities (specifically pgoutput and Debezium with Kafka Connect) to stream real-time data changes (inserts, updates, deletes) in Avro format to Kafka topics. Then, a custom Apache Beam consumer processed these events, transforming and writing them to CRDB.\nhttps://engineering.squarespace.com/blog/2025/leveraging-change-data-capture-for-database-migrations-at-scale\nPinterest writes about the Unified Dynamic Framework (UDF), a scalable, resilient solution that has transformed experiment metrics computation. The experimentation pipeline is always the hardest one since it usually runs last after computing critical metrics. Pinterest talks about a unified metric computing framework leveraging Airflow\u2019s dynamic DAG and a metric registry to overcome the inefficiencies in metric computations. \nhttps://medium.com/pinterest-engineering/500x-scalability-of-experiment-metric-computing-with-unified-dynamic-framework-9eb356fee676\nThe article explores the performance challenges of applying filters to Approximate Nearest Neighbor (ANN) vector search, often slowing it down, unlike traditional database filtering, and examines three main strategies: \npre-filtering (filter-then-search, which can degrade to brute force)\npost-filtering (search-then-filter, risking missed results or high latency with restrictive filters)\nIn-algorithm filtering (integrated approaches by Qdrant, Weaviate, and Pinecone). \nThe author highlights benchmark results showing that engines with integrated filtering often improve throughput and maintain low latency with filters. \nhttps://yudhiesh.github.io/2025/05/09/the-achilles-heel-of-vector-search-filters.html\nVimeo outlines its architecture for delivering viewer retention analytics at scale, leveraging ClickHouse and AI to process data from over a billion videos. Vimeo captures granular viewer actions (skips, repeats, scrubs) by logging changes in views per second (+1 for start, -1 for end) and stores these in an AggregatingMergeTree table in ClickHouse, enabling efficient real-time aggregation. To provide AI-driven insights, Vimeo preprocesses retention data (window averaging, Run-Length Encoding) to minimize tokens and uses structured prompt engineering for LLMs (Gemini Flash 2.0 and Lite 2.0) to pinpoint drop-offs and high retention sections. \nhttps://medium.com/vimeo-engineering-blog/behind-viewer-retention-analytics-at-scale-8dbbb5ae7ae2\nA rich metadata model is vital to improve query efficiency. The author did an excellent job of summarizing the levels of metadata collected by Iceberg, how to enable it, and what it is used for. \nhttps://medium.com/@yogevyuval/making-sense-of-apache-iceberg-statistics-5a114d8e90d1\nAll rights reserved, ProtoGrowth Inc., India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-220", "title": "Data Engineering Weekly", "content": "We\u2019re pulling back the curtain. Join us on May 13 for a live deep dive into how Dagster Labs runs Dagster in production. One of our lead data engineers will walk through our real-world implementation, architecture decisions, and the lessons we've learned scaling the platform.\nRegister now\nA free virtual event on open data architectures - Iceberg, Hudi, lakehouses, query engines, and more. Talks from Netflix, dbt Labs, Databricks, Microsoft, Google, Meta, Peloton, and other open data geeks.\nMay 21st, 9 am-3 pm PDT. There will be no fluff. You will experience solid content, good vibes, and a live giveaway!\nRegister Now\nThere is growing support for better interoperability of business assets with LLMs. MCP is the first protocol trying to address this. The author examines the MCP protocol, the adoption of HTTP server-side events vs. the WebSocket approach, and the security implications. \nhttps://raz.sh/blog/2025-05-02_a_critical_look_at_mcp\nWhen I delved further into learning about the MCP specification, Alibaba's blog was a handy guide to understanding the protocol spec's evolution over the last four months. The six security principles of the MCP protocol are interesting to read to understand the upcoming MCP protocol improvements around authentication and authorization.\nhttps://www.alibabacloud.com/blog/a-comprehensive-analysis-and-practical-implementation-of-the-new-features-in-the-mcp-specification_602206\nThe MCP protocol focuses on LLM\u2019s ability to discover and interact with the external world, invoking the external functions/ tools critical to extend the capabilities of Agents. The author gives an excellent overview of function calling with restricting agent actions and guardrails against prompt injections. \nThe blog raised a critical question, which I believe the industry is highly divided on: Can this pattern replace traditional rule engine Saas products? \nhttps://martinfowler.com/articles/function-call-LLM.html\nLearn the fundamental concepts to build a data platform in your organization.\n- Tips and tricks for data modeling and data ingestion patterns- Explore the benefits of an observation layer across your data pipelines- Learn the key strategies for ensuring data quality for your organization\nGet the guide\nDoorDash describes AutoEval, their human-in-the-loop, LLM-powered automated search quality evaluation system, designed to overcome traditional human annotation's scalability, latency, and consistency challenges. AutoEval utilizes LLMs to assess search relevance at scale by sampling user queries, constructing detailed prompts based on internal rating guidelines and structured context, performing LLM inference (using base or fine-tuned models), and aggregating judgments using their custom whole-page relevance (WPR) metric.\nhttps://careersatdoordash.com/blog/doordash-llms-to-evaluate-search-result-pages/\nShopify details their journey in product understanding, evolving from basic classification to a sophisticated system built on Vision Language Models (VLMs) and the Shopify Product Taxonomy (over 10,000 categories, 1,000+ attributes). The blog narrates the adoption of VLMs (like Qwen2VL 7B with FP8 quantization and in-flight batching) for multi-modal understanding, zero-shot learning, and natural language reasoning to classify products and extract attributes within their taxonomy. \nhttps://shopify.engineering/evolution-product-classification\nNetflix writes about the evolution of its ad processing pipeline from third party providers to inhouse systems. The system design consist of a centralized ad event collection system (Ads Event Publisher) to consolidate common operations (decryption, enrichment, hashing) and provide a unified, extensible data contract for various downstream real-time and batch consumers like frequency capping, ads metrics (using Flink and Druid), ad sessionization (Flink), the original Ads Event Handler, and billing/reporting workflows.\nhttps://netflixtechblog.com/behind-the-scenes-building-a-robust-ads-event-processing-pipeline-e4e86caf9249\nLyft writes about its real-time spatial-temporal forecasting system. The blog narrates its forecasting architecture using Apache Beam/Flink, Kafka, Kinesis, DynamoDB, Lyft's ML Platform (Airflow, SageMaker), and ClickHouse for feature generation, online inference (with potential for real-time refitting), and performance monitoring, emphasizing an asynchronous design for scalability.\nhttps://eng.lyft.com/real-time-spatial-temporal-forecasting-lyft-fa90b3f3ec24\nMeta writes about the \"Global Feature Importance\" framework to address challenges in feature exploration and selection for machine learning models, particularly when dealing with thousands of features across numerous models. Their approach involves logging feature importance runs from various models, normalizing these scores (using percentiles to make them comparable), and then aggregating them to generate a global importance score for each feature. \nhttps://medium.com/@AnalyticsAtMeta/collective-wisdom-of-models-advanced-feature-importance-techniques-at-meta-1a7a8d2f9e27\nFlipkart writes about building Plato, an internal analytics platform, to address the challenges of performing data analysis and enabling self-service BI. The architecture includes a user-friendly front-end (Plato Explorer, Model Builder, Data Copilot for NLQ) and a back-end that handles query redirection, optimization, intelligent materialization (cubing), and execution across various storage and processing engines (Spark, Flink, BigQuery, Druid).\nhttps://blog.flipkart.tech/transforming-data-analytics-at-flipkart-self-serve-insights-on-petabytes-scale-data-fa59caf2bc54\nAll rights reserved, ProtoGrowth Inc., India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-219", "title": "Data Engineering Weekly", "content": "Airflow 3 is here and has never been easier or more secure. Spin up a new 3.0 deployment on Astro to test DAG versioning, backfills, event-driven scheduling, and more.\nGet started \u2192\nA free virtual event on open data architectures - Iceberg, Hudi, lakehouses, query engines, and more. Talks from Netflix, dbt Labs, Databricks, Microsoft, Google, Meta, Peloton, and other open data geeks.\nMay 21st, 9 am\u20143 pm PDT. There will be no fluff. You will experience solid content, good vibes, and a live giveaway!\nRegister Now\nSo we beat on, boats against the current, borne back ceaselessly into the past. -The Great Gatsby\nThe author demonstrates that building a reliable data engineering practice is hard, then and now, no matter what terminology or naming we use. The fundamental complexity remains the same, and a deep understanding is essential to build data engineering practices.\nhttps://luminousmen.com/post/data-engineering-now-with-30-more-bullshit\nMany architectural practices, such as documentation generation, test suggestions, and architecture diagramming, are ignored due to time and cost pressure. The blog highlights how emerging AI tools automate otherwise cognitively intensive manual tasks to bring reliability in software engineering. \nhttps://www.infoq.com/news/2025/05/ai-toolkit-unify-workflows/\nAnother interesting article from Uber demonstrates how AI significantly accelerates the reliability effects. Uber writes about FixrLeak, a generative AI-based framework that automates the detection and repair of resource leaks. FixrLeak combines Abstract Syntax Tree (AST) analysis with generative AI (GenAI) to produce accurate, idiomatic fixes while following Java best practices like try-with-resources. \nhttps://www.uber.com/blog/fixrleak-fixing-java-resource-leaks-with-genai/\nMeta describes its data management practices as adopting a \u201cshift-left\u201d approach, integrating data schematization and annotations early in product development. The recommendation engine to find the data flow violation is an interesting design to monitor the data assets at scale. \nhttps://engineering.fb.com/2025/04/28/security/how-meta-understands-data-at-scale/\nPinterest writes about adopting Multi-gate Mixture-of-Experts (MMoE) architecture for its ad engagement models to improve performance beyond traditional deep learning models like DCNv2. Pinterest found MMoE effective, particularly with DCNv2 experts and lightweight gates, optimized serving costs using mixed precision inference (reducing latency by 40%), and addressed the challenge of limited data retention periods by using knowledge distillation (with a pairwise loss) from production models during batch training to transfer knowledge from older, deleted data to new experimental models.\nhttps://medium.com/pinterest-engineering/multi-gate-mixture-of-experts-mmoe-model-architecture-and-knowledge-distillation-in-ads-08ec7f4aa857\nWhatnot describes their transition from a batch prediction system to an online inference framework for ranking, which is shown in their \"For You Feed.\" The initial batch system, which pre-calculated watch/purchase likelihood for all buyer-seller pairs daily, faced O(nm) scaling issues, couldn't utilize real-time show-level features, suffered from data staleness, and had cold-start problems. The blog narrates the new online inference system-generated predictions at request time using real-time features retrieved from their feature store (Chalk) and how it significantly improved feed relevance and drove substantial increases in GMV, orders, and watch time. \nhttps://medium.com/whatnot-engineering/evolving-feed-ranking-at-whatnot-25adb116aeb6\nDropbox describes building Dropbox Dash, a universal search and knowledge management product, highlighting the challenges of business data environments (diversity, fragmentation, modalities) and their solutions using retrieval-augmented generation (RAG) and AI agents. The article also explains their AI agent architecture, which uses a planning stage (LLM generates code in a Python-like DSL) and an execution stage (code validation and execution via a custom minimal interpreter) to handle complex multi-step tasks, emphasizing security, testing, and lessons learned about combining RAG and agents, prompt variability, and model trade-offs.\nhttps://dropbox.tech/machine-learning/building-dash-rag-multi-step-ai-agents-business-users\nThe authors write a comprehensive article to help readers understand the nuances of handling watermarks in Apache Flink. TIL about the idle stream problem. When a Kafka topic has multiple partitions, but one partition doesn't receive any data, it can block watermark progression for the entire operator since the watermark at each stage is the minimum across all source partitions. \nhttps://rmoff.net/2025/04/25/its-time-we-talked-about-time-exploring-watermarks-and-more-in-flink-sql/\nThe author gives an overview of DeepSeek\u2019s file system. It is on my to-do list to read their design notes. I\u2019m looking forward to the author\u2019s future notes on DeepSeek to understand how DeepSeek FS differs from other filesystems. \nhttps://maknee.github.io/blog/2025/3FS-Performance-Journal-1/\nAll rights reserved, ProtoGrowth Inc., India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-218", "title": "Data Engineering Weekly", "content": "Airflow 3 is here and has never been easier to use or more secure. Spin up a new 3.0 deployment on Astro to test DAG versioning, backfills, event-driven scheduling, and more.\nGet started \u2192\nAs AI development becomes mainstream, so does the need to adopt all the best practices in software engineering. The author emphasises the need for evaluation-driven development, inspired by test-driven development. The author walks through three broad categories of evaluation-driven development.\nfunctional correctness\nAI-as-a-judge\ncomparative evaluation\nhttps://queue.acm.org/detail.cfm?id=3722043\nOpenAI publishes a comprehensive guide on building AI Agents. The guide walks through three core components of AI Agents.\nModel\nTools\nInstructions\nhttps://cdn.openai.com/business-guides-and-resources/a-practical-guide-to-building-agents.pdf\nCoding is one of the fastest domains to adopt large language models, and Claude is recognized as one of the most effective models for coding tasks. Claude outlines best practices for agent coding, including connecting with Git and a walkthrough of the explore, plan, code, and commit \u201d  model.\nhttps://www.anthropic.com/engineering/claude-code-best-practices\nThis introduction to Apache Airflow\u00ae 3 series will cover everything you need to know, from feature deep dives to upgrade preparedness tips.\nJoin Airflow experts and contributors to learn: - Tips to improve your productivity with DAG versioning and backfills- How to use new features like data assets and event-driven scheduling- New architectural changes that allow you to run tasks anywhere, any time\nRegister now to join the Astronomer webinar series to learn everything you need to know to take advantage of this release.Register Now \u2192\nThe author explores recent developments in reinforcement learning (RL) for enhancing large language model (LLM) reasoning, with a focus on training-time methods. Proximal and Group Relative Policy Optimization, together with DeepSeek-R1\u2019s RL with Verifiable Rewards, show that well-shaped rewards not only tighten LLM reasoning but also curb \u201ctoo-long-yet-wrong\u201d answers and coax emergent self-verification that transfers to search and other domains.\nhttps://sebastianraschka.com/blog/2025/the-state-of-reinforcement-learning-for-llm-reasoning.html\nDiscord writes about its message-search stack from a monolithic, Redis-queued Elasticsearch cluster to a Kubernetes-based, cell architecture-backed Elasticsearch deployment managed by ECK, pairing Pub/Sub-driven ingestion with an index-aware router, user-sharded DM search, and dedicated multi-shard \u201cBFG\u201d cells for supersized guilds. I find Discord\u2019s approach a persuasive blueprint for any high-growth platform: treat search as a fleet of right-sized, self-healing cells, and you gain resilience, upgrade agility, and capacity headroom without rewriting your query layer.\nhttps://discord.com/blog/how-discord-indexes-trillions-of-messages\nLinkedIn discusses Hoptimator, a system that enables consumer-driven, managed ingestion pipelines, specifically for Apache Pinot. Before Hoptimator, Pinot ingestion often required data producers to create and manage separate, Pinot-specific preprocessing jobs to optimize data, such as re-keying, filtering, and pre-aggregating. With Hoptimator, Pinot itself can dynamically create, control, and optimize these ingestion pipelines via a Subscription API, using Flink SQL jobs orchestrated by Hoptimator to deliver data tailored to Pinot's needs (correct fields, partitioning, etc.), reducing user friction, operator toil, and resource consumption on Pinot servers, while automating pipeline management.\nhttps://www.linkedin.com/blog/engineering/infrastructure/powering-apache-pinot-ingestion-with-hoptimator\nZomato writes about replacing a brittle 150 GB Flink Java job with a lean Flink SQL pipeline, offloading late-event handling to a reconciliation ETL on S3, emitting incremental rather than daily aggregates, and reducing the deduplication TTL from 24 hours to 2 hours. The redesign reduced state size by more than 99%, saved over $3,000 per month, and delivered zero-downtime reliability with far simpler operations. Treating state as a cost to be minimised\u2014not a convenience\u2014turns Flink from a liability into a flexible, low-maintenance growth lever.\nhttps://blog.zomato.com/eliminating-bottlenecks-in-real-time-data-streaming-a-zomato-ads-flink-journey\nThe article details methods for loading and optimizing two types of logs (database change logs [DB logs] and server logs) into Apache Iceberg tables using Apache Flink. The article explores compression strategies (finding zstd level 9 optimal for server logs despite increased CPU usage), partitioning approaches (addressing timezone issues with string-based identity partitioning for server logs), optimization techniques (binpack compaction, partial progress, delayed cleanup), and monitoring methods (using Trino/Spark metadata tables and Prometheus/Grafana) for maintaining Iceberg table health and performance.\nhttps://tech.kakao.com/posts/695\nThe article provides a comprehensive guide to Kafka consumer offsets, explaining their role in tracking consumption progress and the importance of manual offset control for reliability and exactly-once semantics (EOS). The blog outlines the challenges of traditional offset management, including inaccuracies stemming from control records and potential issues with stale metadata during leader changes. It highlights the benefits of committing the leader epoch alongside the offset. Finally, it introduces KIP-1094 (available in Kafka 4.0.0), which addresses these challenges by adding a\u00a0nextOffsets\u00a0method to\u00a0ConsumerRecords, providing consumers with the accurate next offset and leader epoch directly from the\u00a0poll\u00a0response, thus enabling more precise and reliable offset commits.\nhttps://www.confluent.io/blog/guide-to-consumer-offsets/\nDatabricks discusses how Liquid Clustering, combined with row-level concurrency, simplifies managing concurrent writes in Delta Lake tables, thereby eliminating the need for complex partitioning strategies or error-prone retry loops that are often required with traditional approaches. The article explains that row-level concurrency, automatically enabled with Liquid Clustering or deletion vectors, detects conflicts at the row level, allowing multiple writes to succeed even if they modify the same data file, as long as they don't touch the same rows, leveraging deletion vectors and row tracking to reconcile changes efficiently during commit time.\nhttps://www.databricks.com/blog/deep-dive-how-row-level-concurrency-works-out-box\nAll rights reserved, ProtoGrowth Inc., India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/ai-and-data-in-production-insights", "title": "Data Engineering Weekly", "content": "In our latest episode of Data Engineering Weekly, co-hosted by Aswin, we explored the practical realities of AI deployment and data readiness with our distinguished guest, Avinash Narasimha, AI Solutions Leader at Koch Industries. This discussion shed significant light on the maturity, challenges, and potential that generative AI and data preparedness present in contemporary enterprises.\nAvinash Narasimha is a seasoned professional with over two decades of experience in data analytics, machine learning, and artificial intelligence. His focus at Koch Industries involves deploying and scaling various AI solutions, with particular emphasis on operational AI and generative AI. His insights stem from firsthand experience in developing robust AI frameworks that are actively deployed in real-world applications.\nOne key question often encountered in the industry revolves around the maturity of generative AI in actual business scenarios. Addressing this concern directly, Avinash confirmed that generative AI has indeed crossed the pilot threshold and is actively deployed in several production scenarios at Koch Industries. Highlighting their early adoption strategy, Avinash explained that they have been on this journey for over two years, emphasizing an established continuous feedback loop as a critical component in maintaining effective generative AI operations.\nDeployment strategies for AI, particularly for generative models and agents, have undergone significant evolution. Avinash described the systematic approach based on his experience: \nBeginning with rigorous experimentation\nTransitioning smoothly into scalable production environments\nIncorporating robust monitoring and feedback mechanisms. \nThe result is a successful deployment of multiple generative AI solutions, each carefully managed and continuously improved through iterative processes.\nDuring our conversation, we explored the significance of data readiness, a pivotal factor that influences the success of AI deployment. Avinash emphasized data readiness as a fundamental component that significantly impacts the timeline and effectiveness of integrating AI into production systems.\nHe emphasized the following:\n- Data Quality: Consistent and high-quality data is crucial. Poor data quality frequently acts as a bottleneck, restricting the performance and reliability of AI models.\n- Data Infrastructure: A Robust data infrastructure is necessary to support the volume, velocity, and variety of data required by sophisticated AI models.\n- Integration and Accessibility: The ease of integrating and accessing data within the organization significantly accelerates AI adoption and effectiveness.\nAvinash openly discussed challenges that many enterprises face concerning data readiness, including fragmented data ecosystems, legacy systems, and inadequate data governance. He acknowledged that while the journey toward optimal data readiness can be arduous, organizations that systematically address these challenges see substantial improvements in their AI outcomes.\nAvinash also offered actionable insights into overcoming common data-related obstacles:\n- Building Strong Data Governance: A robust governance framework ensures that data remains accurate, secure, and available when needed, directly enhancing AI effectiveness.\n- Leveraging Cloud Capabilities: He noted recent developments in cloud-based infrastructure as significant enablers, providing scalable and sophisticated tools for data management and model deployment.\n- Iterative Improvement: Regular feedback loops and iterative refinement of data processes help gradually enhance data readiness and AI performance.\nLooking ahead, Avinash predicted increased adoption of advanced generative AI tools and emphasized ongoing improvements in model interpretability and accountability. He expects enterprises will increasingly prioritize explainable AI, balancing performance with transparency to maintain trust among stakeholders.\nMoreover, Avinash highlighted the anticipated evolution of data infrastructure to become more flexible and adaptive, catering specifically to the unique demands of generative AI applications. He believes this evolution will significantly streamline the adoption of AI across industries.\n- Generative AI is Ready for Production: Organizations, particularly those that have been proactive in their adoption, have successfully integrated generative AI into production, highlighting its maturity beyond experimental stages.\n- Data Readiness is Crucial: Effective AI deployment is heavily dependent on the quality, accessibility, and governance of data within organizations.\n- Continuous Improvement: Iterative feedback and continuous improvements in data readiness and AI deployment strategies significantly enhance performance and outcomes.\nOur discussion with Avinash Narasimha provided practical insights into the real-world implementation of generative AI and the critical role of data readiness. His experience at Koch Industries illustrates not only the feasibility but also the immense potential generative AI holds for enterprises willing to address data challenges and deploy AI thoughtfully and systematically.\nStay tuned for more insightful discussions on Data Engineering Weekly.\nAll rights reserved, ProtoGrowth Inc., India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-217", "title": "Data Engineering Weekly", "content": "Get a first look at all the new features in Airflow 3.0, such as DAG versioning, backfills, and dark mode, in a live session this Wednesday, April 23. Plus, get your questions answered directly by Airflow experts and contributors.\nRegister now \u2192\nThoughtWorks' technology radar inspired many enterprises to build their internal tech radars, standardizing and suggesting technology, tools, and framework adoption. The blog took out the last edition\u2019s recommendation on AI and summarized the current state of AI adoption in enterprises.\nhttps://www.thoughtworks.com/insights/blog/machine-learning-and-ai/ai-technology-radar-vol-32\nAll aspects of software engineering are rapidly being automated with various coding AI tools, as seen in the AI technology radar. Data engineering is one aspect where I see a few startups starting to disrupt. One of the core challenges of data engineering, as the author put it elegantly,\nThe core difficulty lies in the fact that each step in the process requires specialized domain knowledge. A requirement originating from the business domain must pass through a series of experts, including data analysts, data scientists, data engineers, platform engineers, and infrastructure teams. Each of these groups speaks its own \u201clanguage\u201d and brings its context.\nIt leads to building a sequence of agents, integrates well with the existing workflow, and makes data engineering a more interesting problem to solve. \nhttps://medium.com/@gejing/context-matters-the-vision-of-data-analytics-and-data-science-leveraging-mcp-and-a2a-c66ed0846b59\nI was re-reading Jack Vanlightly's excellent series on understanding the consistency model of various lakehouse formats when I stumbled upon the blog on decomposing transaction systems. The simplistic model expressed in the blog made it easy for me to reason about the transactional system design.\n If you want to understand the lakehouse system designs, I recommend reading these blogs a couple of times.\nAlex Miller: https://transactional.blog/blog/2025-decomposing-transactional-systems\nJack Vanlightly: https://jack-vanlightly.com/analysis-archive\nMarc Brooker: https://brooker.co.za/blog/2025/04/17/decomposing.html\nDownload this free 130+ page eBook for everything a data engineer needs to know to take their DAG writing skills to the next level (+ plenty of example code).\n\u2192 Understand the building blocks DAGs, combine them in complex pipelines, and schedule your DAG to run exactly when you want it to\u2192 Write DAGs that adapt to your data at runtime and set up alerts and notifications\u2192 Scale your Airflow environment\u2192 Systematically test and debug Airflow DAGsBy the end of the DAG guide, you'll know how to create and manage reliable, complex DAGs using advanced Airflow features such as those in the screenshot \ud83d\udcf8.\nGet Ebook \u2192\nWhen the Timely DataFlow paper came out, I tried my best to understand the paper with little to no success :-) It is still the case tbh, and I\u2019m not alone, but I found Chris's blog to be the closest explanation about the incremental view maintenance that reignites my curiosity to learn more about it. With the chatGPTs as your knowledge assistance, I hope to get this time around :-) \nhttps://materializedview.io/p/everything-to-know-incremental-view-maintenance\nWhen I saw the KIP-1150 proposal, I was like, okay, finally it is happening. Kafka is probably the most reliable data infrastructure in the modern data era. The popularity also exposes its Achilles heel, the replication and network bottlenecks. With AWS rapidly slicing the cost of S3 Express, the blog makes a solid argument that disk-based Kafka is 3.7X expensive than diskless Kafka out of S3 Express One. \nhttps://topicpartition.io/blog/kip-1150-diskless-topics-in-apache-kafka\nContinue on the slow but steady impact of S3 Express One in the data infrastructure, the AWS team writes about how adopting Express One to save Flink checkpoints significantly improves its performance. \nIt makes me think, what could the impact of a similar system design be in a Lakehouse architecture? Apache Hudi, for example, introduces an indexing technique to Lakehouse. We all know that data freshness plays a critical role in the performance of Lakehouse. If we can place the metadata, indexing, and recent data files in Express One, we can potentially build a Snowflake-style performant architecture in Lakehouse. \nhttps://aws.amazon.com/blogs/storage/prime-video-improved-stream-analytics-performance-with-amazon-s3-express-one-zone/\nAs Iceberg is getting growing adoption, I also noticed some of its weaknesses popping up around the real-time data ingestion, upsert operations, and incremental data processing. The blog from Xiaomi on adopting Poimon narrates the challenges around Iceberg with equality deletes.\nhttps://www.alibabacloud.com/blog/xiaomis-real-time-lakehouse-implementation-best-practices-with-apache-paimon_602163\nAgoda integrated GPT into its CI/CD pipeline to optimize SQL stored procedures by feeding the model SP code, schema definitions, and performance metrics, then surfacing optimized queries and indexing recommendations as merge requests. This AI\u2011driven workflow has slashed manual review effort, accelerated approvals, and materially improved SP quality, exemplifying a practical way to boost database developer productivity. \nhttps://medium.com/agoda-engineering/how-agoda-uses-gpt-to-optimize-sql-stored-procedures-in-ci-cd-29caf730c46c\nSwiggy writes about transforming Apache Superset into a high\u2011performance monitoring platform by tuning Gunicorn\u2019s worker configuration and migrating from Databricks Classic Warehouse to a Serverless Warehouse to eliminate timeouts and shutdowns. The strategic embedding of dashboards, the development of custom plugins, and the creation of a mobile UI to align analytics with diverse internal workflows is an excellent read to think about building an in-house BI system. \nhttps://bytes.swiggy.com/business-monitoring-at-swiggy-apache-superset-at-scale-b784d0c4012d\nIf you\u2019re looking to build an anomaly detection out of time series data (which is pretty common in the data engineering workflow), this is an interesting read. In the data pipeline, we have many commercial anomaly detection tools available, and often expensive ones. \nI wonder if anyone has tried building a Prometheus exporter for a data pipeline to hook some of the anomaly detection (not the infrastructure, but the data itself)? Please comment if you\u2019ve done so, I would love to discuss the design. \nhttps://medium.com/booking-com-development/anomaly-detection-in-time-series-using-statistical-analysis-cc587b21d008\nAll rights reserved, ProtoGrowth Inc., India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-216", "title": "Data Engineering Weekly", "content": "Be among the first to see Airflow 3.0 in action and get your questions answered directly by the Astronomer team. You won't want to miss this live event on April 23rd!\nSave Your Spot \u2192\nStanford gives an insight into AI adoption in the industry with the AI adoption. The key factors are\nThe smaller models are getting better.\nThe models become cheaper to use\nThe rise of more useful agents\nBoth corporate and venture capital are flowing into AI\nAll the key factors indicate AI is no longer a niche field and is rapidly getting commoditized. \nhttps://hai.stanford.edu/news/ai-index-2025-state-of-ai-in-10-charts\nWith the rapid adoption of AI, it is critical to take time to understand the foundation from an abstract reasoning perspective. Grab writes the same about Self-Attention, Multi-Head Attention, and Masked Attention.\nhttps://medium.com/gojekengineering/how-transformers-understand-language-attention-explained-simply-5ec89c54ae9d\nThe article highlights recent trends in reinforcement learning (RL) and examines OpenAI\u2019s strategic application across products such as the O-series models, Operator agent, Deep Research, and CoPilot. It clarifies misconceptions around DeepSeek R1, emphasizing that DeepSeek leverages RL and latent reasoning models internally to refine thought processes without verbose outputs rather than mere distillation from OpenAI\u2019s o1 model.\nhttps://www.interconnects.ai/p/rl-backlog-openais-many-rls-clarifying\nDownload this free 130+ page eBook for everything a data engineer needs to know to take their DAG writing skills to the next level (+ plenty of example code).\n\u2192 Understand the building blocks DAGs, combine them in complex pipelines, and schedule your DAG to run exactly when you want it to\n\u2192 Write DAGs that adapt to your data at runtime and set up alerts and notifications\n\u2192 Scale your Airflow environment\n\u2192 Systematically test and debug Airflow DAGs\nBy the end of the DAG guide, you'll know how to create and manage reliable, complex DAGs using advanced Airflow features such as those in the screenshot \ud83d\udcf8.\nGet Ebook \u2192\nSwiggy shares best practices for operationalizing machine learning (ML), highlighting four core areas: \nRigorous Exploratory Data Analysis (EDA) for anomaly detection and drift monitoring via statistical techniques like Z-scores and KS tests; \nSensitivity Analysis to assess feature importance, set reliable operational bounds, and mitigate outliers; \nExplainable AI (XAI) leveraging methods such as SHAP to foster transparency and trust in predictions. \nMeticulous Coding Standards, including clean coding practices, collaborative reviews, robust unit testing, and clear documentation. \nhttps://bytes.swiggy.com/building-rock-solid-ml-systems-bb775f8a7126\nDiscord\u2019s methodical, macro-driven customizations significantly elevate collaboration, performance, and reliability\u2014an exemplary demonstration of thoughtful engineering that tackles practical challenges in large-scale data operations.\nDiscord details its innovative approach to scaling dbt for petabyte-scale data management across over 2,500 models by introducing custom environment isolation via macros, performance enhancements with configurable incremental processing and \u201cdbt turbo\u201d strategies, precise data backfills through meta field-driven targeted refreshes, and comprehensive CI/CD guardrails using automated cost and dependency analyses. \nhttps://discord.com/blog/overclocking-dbt-discords-custom-solution-in-processing-petabytes-of-data\nWealthfront introduces an in-house SQL testing library tailored for AWS Athena, emphasizing principles of zero-footprint testing via CTEs, usability through Python integration and existing Avro schemas, dynamic test execution, and clear test feedback. They thoughtfully address practical challenges such as logging, SQL-Python type compatibility using custom Pydantic types, SQL length constraints through temporary views, and adoption friction by automating test generation integrated seamlessly into Airflow and CI/CD pipelines. \nhttps://eng.wealthfront.com/2025/04/07/our-journey-to-building-a-scalable-sql-testing-library-for-athena/\nThe article introduces the \u201cshadow table\u201d strategy, which manages complex data migrations (such as schema refactoring, microservice extraction, or database upgrades) by maintaining synchronized parallel data copies. This strategy typically uses a pattern of creation, backfilling, real-time synchronization via CDC or triggers, verification, and strategic cutover. The shadow table approach is particularly effective, as it balances control, consistency, and operational safety in critical, large-scale migrations.\nhttps://www.infoq.com/articles/shadow-table-strategy-data-migration/\nThe article evaluates Apache Spark\u2019s Dataset versus DataFrame APIs, advocating for Dataset\u2019s compile-time type safety, reduced runtime errors, schema clarity, and maintainability\u2014key for accuracy-focused teams\u2014while acknowledging shared performance optimizations like Catalyst and Tungsten. Although it notes the Dataset\u2019s drawback of needing explicit join conditions, it suggests practical solutions using UDFs and tuple transformations to achieve type-safe joins without sacrificing readability. Despite minor performance trade-offs, Dataset\u2019s benefits significantly enhance correctness, clarity, and long-term maintainability in robust data engineering practices.\nhttps://medium.com/agoda-engineering/reducing-runtime-errors-in-spark-why-we-migrated-from-dataframe-to-dataset-5b8fc5ac7297\nAll rights reserved, ProtoGrowth Inc., India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/towards-composable-data-infrastructure", "title": "Data Engineering Weekly", "content": "The last few years have been an interesting journey for the data infrastructure landscape. The rapid growth of AI has sparked attention from the modern data stack, and we have declared that big data is dead, and so is the modern data stack. We quickly move on to the debate about table formats, the great data table format war, and the catalog war.\nThough these debates seem like a hype cycle, we can\u2019t deny these technologies' advancement in data engineering. As it is evident that a clean, contextual, and scalable data infrastructure is an essential ingredient for an organization to succeed in the AI era, we see increased focus on building better, scalable data infrastructure.\nIn one of our recent Data Engineering Surveys, we found that 91% of respondents said they were either using a lakehouse or looking to adopt it. At the same time, I\u2019ve seen confusion around how to build a scalable and portable architecture. Only 45% of the responders said they have strong expertise in Lakehouse. Portability becomes the primary reason for an organization to adopt Lakehouse to control costs and improve flexibility.\nThough every vendor claims to live and breathe the true spirit of the open standards, this is simply not possible in a growth-driven economy. \nIt leads to hidden vendor locks, which are hard to realize during the architecture design phase. \nAt the same time, I\u2019ve seen cases where full-stack control outperforms open designs, especially when latency or SLA guarantees are non-negotiable. That\u2019s why I think the anti-vendor-lock narrative is too one-dimensional. \nSometimes, a system performs more optimally when it has full control over the data and the process, which yields better system performance. Balancing both is the fine art of data engineering.\nIn this blog, I\u2019m sketching my thought processes around building a portable data lakehouse architecture that balances vendor optimization and ecosystem openness.\nThe foundational layer of the data infrastructure typically looks like this:\nObject Storage houses all raw data.\nFile Format determines how that data is physically laid out (e.g., Parquet).\nTable Format adds transactional consistency, versioning, and other table-level abstractions on top of these files.\nCatalog keeps track of metadata, table schemas, partitions, and discoverability across the organization.\nCompute Engine (Spark, Flink, Trino, etc.) consumes data from the catalog and storage layers to execute transformations and queries.\nI\u2019ve excluded data ingestion, transformation, and orchestration from this design. I consider them the \u201cSupporting Layer\u201d of the data infrastructure. The \u201cConsumption & Discovery Layer,\u201d such as BI tools and notebooks, is also excluded from the scope of this design. Both of them require a separate reference architecture guide.\nThe decision about object storage and file format is more or less straightforward. Your choice of cloud providers determines the choice of the Object storage, and Parquet is a standard file format for storing tabular data in a columnar format. Though there are upcoming file formats like Nimble, Parquet enjoys best-in-class support from the table formats.\nThe choice of Lakehouse is one of the industry's hotly debated and analysed topics. I\u2019m not going much further here, as different companies have different preferences. Some might evaluate the number of vendors supporting a format, some based on the integrated experience to build productivity, or some evaluate specific use cases to see which Lakehouse provides the best performance.\nBuy vs Build is one of the most frequently debated decisions when adopting new technology, and Lakehouse is no different. The complexity around table maintenance is no easy task as your data keeps growing. The choice depends on the business context, but I will always ask to understand the buy vs build decision better.\nIs this a unique problem for us?\nDoes building in-house provide a business competitive advantage?\nHas anyone solved this problem at our scale\nWhat is the integration complexity to our existing infrastructure if we buy\nThe majority of us choose to buy as the complexity grows. The vendor choice for Delta Lake (Databricks) and Apache Hudi (Onehouse) is straightforward. Depending on your selection criteria, this can be a good or bad. The complexity of the architecture mostly comes when you have more than one choice of a system.\nIf you choose Iceberg or you\u2019re looking to adopt a multi-lakehouse format, let's examine the system design more closely with the choice of the catalog design.\nCatalogs are integral to the modern Lakehouse architecture, particularly due to their embedded transaction support and table management capabilities within specifications like Apache Iceberg. This centrality has fostered a diverse ecosystem of catalogs and vendors, underscoring the importance of thoughtful system design to ensure compatibility and performance. A subtle yet critical reality is that, although catalog and query engine technologies often appear distinct in architecture diagrams, their effectiveness hinges on tight integration.\nA few interoperability questions become evident while building the lakehouse.\nIs my data portable to another engine to maintain the table so I won\u2019t be vendor locked?\nCan I bring multiple compute engines to run atop my lakehouse to speed up the innovation?\nThe primary selling point of Lakehouse is that your data is portable and not tied to any closed format native to a proprietary database. It is true, but that doesn\u2019t mean your data is easily portable to another vendor. Having an open data format is one thing, but continuous maintenance of the tables is another part.\nMy version of portability is that you have a user table in Lakehouse format. Catalog A manages the table maintenance. Can you flip the switch and say Catalog B to manage the table without data migration?\nThe Iceberg standardization spec opens many catalogs' implementations, making portability challenging. Switching a catalog runs a complete scan, which causes either data movement or an additional storage API call that adds to the bill. S3Table is another classic example of locking, where it systematically prevents other catalogs from managing the Lakehouse.\nThe symbiotic relationship between query engines and metadata catalogs remains pivotal in modern data processing. Effective query optimization depends fundamentally on robust and detailed metadata, capturing essential aspects like schemas, partition boundaries, file-level statistics, and data distribution patterns. Leveraging this metadata, query planners can make precise, informed decisions\u2014enabling key optimizations such as partition pruning, predicate pushdown, and selective file skipping. These optimizations dramatically reduce unnecessary I/O operations and computational overhead by intelligently narrowing the scope of data scans.\nMoreover, query engines must adeptly handle variations in view definitions and SQL dialects across catalog implementations. Tight integration ensures accurate interpretation and consistent execution across diverse systems. Equally critical, catalogs vary significantly in access control mechanisms, requiring close collaboration to enforce security policies effectively at query time.\nReal-time access to updated statistics, evolving schemas, and contextual security information ensures query plans remain performant and compliant as datasets expand and complexity grows. In large-scale environments, these incremental improvements in efficiency and security accumulate, leading to substantial reductions in query execution times and optimized resource utilization, underscoring the strategic importance of integrating query engines closely with metadata catalogs.\nThe federated architecture separates responsibilities between the write and read paths, significantly enhancing reliability and performance. By centralizing all write operations through a single catalog, organizations establish a unified source of truth for metadata, ensuring consistency, preventing synchronization conflicts, and simplifying critical tasks such as transaction management, schema evolution, and data quality enforcement.\nOn the read side, specialized, distributed read-only catalogs can cater specifically to individual query engines. This targeted approach allows each catalog to optimize metadata translation, effectively handling variations in SQL syntax and view definitions across different engines. Such engine-specific metadata tailoring enables more precise query planning and optimization. Additionally, these read-only catalogs can implement security and access control policies aligned precisely with their corresponding query engines, providing consistent and effective governance.\nThis federated, write-once-read-many approach strikes a strategic balance between consistency and optimization. Delineating write and read responsibilities fosters seamless interoperability among diverse tools and engines, each interacting with data via tailored interfaces. At the same time, the underlying metadata remains centralized, consistent, and robustly managed.\nA purpose-built \"Catalog Replicator\" system is essential to realize this federated architecture's full potential. This specialized tool would serve as the critical infrastructure component that addresses the inherent complexities of maintaining a federated catalog ecosystem:\nIntelligent Metadata Synchronization: The Catalog Replicator would propagate metadata changes from the primary catalog to engine-specific read-only replicas in near real time, implementing sophisticated change detection and efficient distribution mechanisms to minimize staleness.\nTranslation Layer: By incorporating a flexible translation capability, the replicator would transform catalog metadata between different formats and structures, ensuring each query engine receives metadata optimized for its specific requirements and capabilities.\nAccess Control Mapping: The replicator would maintain consistent security policies across catalog boundaries by implementing policy translation rules that map the primary catalog's access controls to their equivalent representations in each target catalog system.\nMonitoring and Reconciliation: Automated verification processes within the replicator would continuously validate consistency between catalogs, detecting and resolving discrepancies to maintain system-wide integrity.\nSchema Evolution Coordination: The replicator would orchestrate complex schema changes across the catalog ecosystem, ensuring that all read-only catalogs appropriately reflect and accommodate evolutionary changes while respecting the capabilities of their associated query engines.\nThis Catalog Replicator represents a critical enabling technology for federated catalog architectures. It transforms what would otherwise be a collection of implementation challenges into a systematic, managed solution that delivers on the promise of true interoperability.\nWe've been down the \"one catalog to rule them all\" path. Hive Metastore emerged as the de facto standard catalog for the Hadoop ecosystem, but its limitations became increasingly apparent as query engines evolved. Despite widespread adoption, Hive Metastore struggled to keep pace with the specialized metadata requirements of modern query engines, forcing uncomfortable compromises between compatibility and optimization.\nThe industry responded by creating specialized catalogs \u2013 but that pendulum swung too far, fragmenting metadata management and creating silos. A federated approach represents the thoughtful middle ground we've been searching for. It acknowledges the need for specialized metadata optimization and its critical importance.\nAs our data systems grow in scale and complexity, architectural approaches that balance specialization with consistency will prove increasingly essential. The federated catalog pattern, enabled by robust replication technology, represents exactly the kind of pragmatic innovation that mature data platforms need \u2013 one that enables continued innovation in query processing while maintaining the interoperability and governance that enterprises require.\nAll rights reserved, ProtoGrowth Inc., India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-215", "title": "Data Engineering Weekly", "content": "Be among the first to see Airflow 3.0 in action and get your questions answered directly by the Astronomer team. You won't want to miss this live event on April 23rd!\nSave Your Spot \u2192\nThat raises an important question: not whether AI becomes foundational infrastructure, but how we prepare for that without getting caught flat-footed.\nThe article summarizes the recent macro trends in AI and data engineering, focusing on Vibe coding, human-in-the-loop system design, and rapid simplification of developer tooling.  \nhttps://www.thoughtworks.com/insights/blog/technology-strategy/macro-trends-tech-industry-april-2025\nAI virtual assistants like Siri and Alexa exemplify large-scale, real-time data systems in action\u2014blending conversational AI, personalization, and IoT integration. This article highlights their growing complexity, from multimodal interaction to enterprise adoption, underscoring the data and infrastructure challenges beneath the surface. As these assistants evolve, they signal a future where scalable, low-latency data pipelines become essential for seamless, intelligent user experiences.\nhttps://www.alibabacloud.com/blog/ai-virtual-assistants-current-trends-challenges-%26-future_602099\nOne reason why all the engineering documentation fails and quickly becomes outdated is that it is always written from the author's perspective. Unlike coding, we never (or rarely) apply a code review process for documentation.\nThe Grab blog delights me since I have tried to do this many times. Writing on Github is not intuitive, and Google Docs has poor version and review management. Kudos to the Grab team for building a docs-as-code system.\nhttps://engineering.grab.com/facilitating-docs-as-code-with-markdown\nWritten by practitioners, for practitioners. Everything you need to know to solve issues with your DAGs:\n\u2705 Identifying issues during development\u2705 Using tools that make debugging more efficient\u2705 Conducting root cause analysis for complex pipelines in production\nGet Ebook \u2192\nUsing a five-level relevance scale, Pinterest built an LLM-based system to enhance search relevance by mapping Pins to user queries. A cross-encoder teacher model, fine-tuned on human-labeled data and enriched Pin metadata, was distilled into a lightweight student model using semi-supervised learning over billions of impressions. The system demonstrated strong improvements in nDCG@20 and fulfillment rates in offline and online tests, with robust generalization across languages.\nhttps://medium.com/pinterest-engineering/improving-pinterest-search-relevance-using-large-language-models-4cd938d4e892\nLinkedIn describes enhancing its Revenue Attribution Report (RAR) system, which analyzes encrypted advertiser CRM data and LinkedIn ad activity stored in Apache Pinot, by replacing AES encryption with Additive Symmetric Homomorphic Encryption (ASHE). This new approach allows aggregate queries (like sum) to be computed directly on the encrypted data within Pinot without decrypting individual rows, significantly reducing network traffic (by over 99%), lowering CPU usage, enabling better use of Pinot's aggregation capabilities, and improving privacy by minimizing plaintext data handling, while maintaining low latency.\nhttps://www.linkedin.com/blog/engineering/data/how-we-used-homomorphic-encryption-to-enhance-privacy-and-cut-network-congestion\nZillow shares an in-depth look into building a real estate Knowledge Graph to unify diverse home-related data sources and enhance user-facing applications like search and personalization. The article outlines a methodical approach\u2014from ontology design to ML-driven entity disambiguation and relationship discovery using SBERT/BERT\u2014that underscores the importance of structured semantics in real-world product impact. I find this a compelling example of operationalizing knowledge graphs at scale, and it highlights the growing convergence of ML, search, and knowledge engineering in building data-driven user experiences.\nhttps://www.zillow.com/tech/leveraging-knowledge-graphs-in-real-estate-search/\nDuolingo shares how it reimagined data modeling through the lens of software engineering, treating modeled datasets like APIs to enhance consistency, reliability, and developer experience. By introducing code linting, automated data diffs, blue-green-style deployments, and focused observability, they built a resilient, company-wide system for working with raw user interaction data. The approach bridges the data and software engineering gap, offering a practical blueprint for scaling trustworthy data systems.\nhttps://blog.duolingo.com/dataset-development/\nThe documentation even reads like it\u2019s better not to define them yourself, as the automatic system will be better and will adjust to dynamic factors. But in a company that does a lot of hourly processing, it feels criminal to me not to have an hour partition field defined on every table referenced by your pipeline.\nIn one of the recent vendor calls, I heard the exact same phrase: the claim was that our system would be very efficient and the user shouldn\u2019t worry about data distribution.\u00a0My response: \u201cThe user knows their data better than anyone.\u201d\u00a0This is one of the traps all the vendors and\u00a0platform teams fall into, which eventually becomes a rigid system. The article reiterates the same. \nhttps://medium.com/gumgum-tech/switching-from-snowpipe-to-data-lake-ingestion-for-simplicity-and-cost-savings-c661d3087c10\nThe tiered-topic approach to handling backoff and DLQ made me think deeply about the pattern. I presume the system design is tuned to process recent data without impacting ordering and causing additional Kafka consumer costs. \nhttps://medium.com/manomano-tech/handle-errors-in-kafka-consumers-like-a-bliss-retries-and-dlt-reporting-for-duty-dc1ec7cbd50f\nAll rights reserved, ProtoGrowth Inc., India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-214", "title": "Data Engineering Weekly", "content": "Be among the first to see Airflow 3.0 in action and get your questions answered directly by the Astronomer team. You won't want to miss this live event on April 23rd!\nSave Your Spot \u2192\nData Council has always been one of my favorite events to connect with and learn from the data engineering community. Data Council 2025 is set for April 22-24 in Oakland, CA. As a special perk for Data Engineering Weekly subscribers, you can use the code dataeng20 for an exclusive 20% discount on tickets!\nhttps://www.datacouncil.ai/bay-2025\nBVP writes about its thesis around Data 3.0 and its challenges and opportunities. A few exciting theses exist around composite data stack, catalogs, and MCP. One thing that stands out to me is\nAs AI-driven data workflows increase in scale and become more complex, modern data stack tools such as drag-and-drop ETL solutions are too brittle, expensive, and inefficient for dealing with the higher volume and scale of pipeline and orchestration approaches.\nhttps://www.bvp.com/atlas/roadmap-data-3-0-in-the-lakehouse-era\nThe article emphasizes that successful AI development depends more on robust evaluation and iterative improvement than merely relying on tools and frameworks. The author highlights several key principles:\n\u2022 Prioritize error analysis to identify improvements with the highest return on investment (ROI).\n\u2022 Invest in simple, customized data viewers to efficiently analyze AI outputs.\n\u2022 Empower domain experts to craft effective prompts directly.\n\u2022 Leverage synthetic data effectively to bootstrap evaluation processes.\n\u2022 Ensure trust in evaluation systems through clearly defined criteria and regular alignment checks.\n\u2022 Structure AI roadmaps around experiments rather than fixed features, enabling continuous learning and adaptation.\nhttps://hamel.dev/blog/posts/field-guide/\nWe all bet on 2025 being the year of Agents. It is no surprise that we see the growth of agent frameworks emerging. The blog is an excellent comparison of all the leading agent frameworks in the market now.\nhttps://langfuse.com/blog/2025-03-19-ai-agent-comparison\nWritten by practitioners, for practitioners. Everything you need to know to solve issues with your DAGs:\n\u2705 Identifying issues during development\u2705 Using tools that make debugging more efficient\u2705 Conducting root cause analysis for complex pipelines in production\nGet Ebook \u2192\nIt goes to the classic saying, \"Everyone wants to build, but no one wants to test.\" Eval plays a critical role in the growth and maturity of LLM-centric systems. The article provides an excellent overview of evaluating an LLM system. \nhttps://www.thoughtworks.com/insights/blog/generative-ai/how-to-evaluate-an-LLM-system\nInteracting with the data in natural language is a long-standing quest for data engineers. The paper critically examines the Text2SQL task, highlighting that limitations go beyond model performance to encompass the entire solution pipeline and evaluation process. The study identifies significant data-quality issues and biases in current benchmarks (such as the widely-used Spider dataset), noting that these problems hinder accurate evaluation and real-world application.\nPaper: https://arxiv.org/pdf/2501.18197\nhttps://machinelearning.apple.com/research/evaluating-text2sql-solutions\nWe can\u2019t deny the advancement of LLM and AI coding tools bought last year. Coding will never be the same again, but a few fundamental skills can\u2019t be disrupted and will be proven valuable in the age of AI. Debugging and troubleshooting are the most valuable skills to develop, and the author has written an exciting article on them. \nhttps://autodidacts.io/troubleshooting/\nMeta discusses the use and benefits of asymmetric experiments (different test and control group sizes) compared to traditional symmetric experiments. The article explains the mathematical trade-off: reducing the test group size while increasing the control group size can maintain the same confidence interval width, which is beneficial when recruitment is cheap but the test intervention is expensive or potentially risky, illustrating this with Meta's use of asymmetric designs for \"holdout\" experiments to measure the long-term impact of positive product changes while minimizing the number of users held back from the improvement.\nhttps://medium.com/@AnalyticsAtMeta/the-power-of-asymmetric-experiments-meta-8a8030d68c31\nAirbnb writes about its framework for estimating listing lifetime value (LTV) on its platform, which differs from traditional single-seller models. Airbnb defines and estimates three types of LTV:\nBaseline (total bookings over 365 days, predicted using machine learning),\nIncremental (baseline LTV adjusted for cannibalization from other listings, estimated via a production function)\nMarketing-induced incremental (additional LTV generated by Airbnb initiatives).\nThe article discusses challenges, including accurate baseline LTV prediction amidst market shocks (like COVID-19), measuring incrementality without direct labels, and handling uncertainty by dynamically updating LTV estimates based on accrued bookings and updated features.\nhttps://medium.com/airbnb-engineering/how-airbnb-measures-listing-lifetime-value-a603bf05142c\nLyka, a direct-to-consumer dog food company, describes migrating its data analytics platform from Google BigQuery to an AWS-based lakehouse architecture. The new architecture integrates tools like S3, Iceberg, Glue Catalog, Snowflake, Athena, dbt, Airflow, and Omni.\nThe article outlines key steps in the migration process:\n1. Setting up the storage layer:\nEstablished Iceberg tables on Amazon S3.\n2. Translating data models:\nConverted approximately 400 dbt models from BigQuery SQL to Snowflake SQL, using Claude 3.5 Sonnet for assistance.\n3. Adapting to Snowflake\u2019s cost model:\nOptimized workflows to align with Snowflake\u2019s pricing and operational model.\n4. Implementing comprehensive testing:\nConducted extensive testing at multiple levels, including:\ndbt tests\nRow-level tests\nAggregate tests\n5.\u00a0Migrating Dashboards:\nMigrated approximately 90 dashboards created in Omni.\n6. Stakeholder management and change control:\nManaged the migration process effectively by emphasizing clear communication, stakeholder engagement, and robust control measures.\nhttps://medium.com/@coreycheung/we-built-a-data-lakehouse-to-help-sell-dog-food-a94f6ea9c648\nAll rights reserved, ProtoGrowth Inc., India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-213", "title": "Data Engineering Weekly", "content": "Data Council has always been one of my favorite events to connect with and learn from the data engineering community. Data Council 2025 is set for April 22-24 in Oakland, CA. As a special perk for Data Engineering Weekly subscribers, you can use the code dataeng20 for an exclusive 20% discount on tickets!\nhttps://www.datacouncil.ai/bay-2025\nI think it will be hard to compare data engineering in 2024 and data engineering in 2028 and say those are the same things.\nInterestingly, I recently shared a similar phrase with a data team I advise in my spare time. I\u2019m curious to observe how the industry raises its level of abstraction as teams integrate AI tooling into their workflows.\nhttps://www.getdbt.com/blog/how-ai-will-disrupt-data-engineering\nWhat should I prefer for 2028, or how can I break into data engineering? These are common LinkedIn requests. I honestly don\u2019t have a solid answer, but this blog is an excellent overview of upskilling. The author emphasizes the importance of mastering state management, understanding \"local first\" data processing (prioritizing single-node solutions before distributed systems), and leveraging an asset graph approach for data pipelines. The author stresses clear communication with stakeholders, continuous learning, and practical experience. \nhttps://georgheiler.com/post/learning-data-engineering\nAre we truly making progress towards AGI? The author presents a \"bear case\" regarding AI advancement, predicting that current methods, such as scaling large language models (LLMs) and employing techniques like reinforcement learning (RL) and chain-of-thought (CoT), will not lead to Artificial General Intelligence (AGI). The author concludes that while LLMs will become useful tools, a different approach will likely be necessary for AGI, potentially in the 2030s.\nhttps://www.lesswrong.com/posts/oKAFFvaouKKEhbBPm/a-bear-case-my-predictions-regarding-ai-progress\nThis is an interesting series about reasoning models. The author discusses inference-time compute scaling methods and categorizes approaches into inference-time compute scaling, pure reinforcement learning, reinforcement learning with supervised fine-tuning, and supervised fine-tuning with distillation. The article highlights recent research papers that explore techniques like \"wait\" tokens, test-time preference optimization, thought-switching penalties, adversarial robustness, and various search strategies. \nhttps://magazine.sebastianraschka.com/p/state-of-llm-reasoning-and-inference-scaling\nRecently, I had an intriguing conversation with a friend who explained how surveys systematically undermine society. The article resonated with me when I read it. The author contends that depending solely on data to shape business strategy is a fallacy, akin to how rote memorization in mathematics fails to lead to the discovery of new theorems. Data professionals should enhance strategy through deep engagement with organizational realities and puzzling facts by providing context and understanding, complementing the need for creative and context-sensitive thinking. \nhttps://locallyoptimistic.com/post/the-fallacy-of-data-driven-strategy/\nNetflix discusses developing a foundation model for personalized recommendations, inspired by large language models (LLMs), to centralize member preference learning and streamline their recommender system. The article covers tokenizing user interactions, incorporating both request-time and post-action features, adapting model objectives (multi-token prediction, auxiliary objectives), addressing unique challenges like entity cold-starting (using incremental training, combining ID-based and metadata-based embeddings), and outlining downstream applications (direct prediction, embedding utilization, fine-tuning). \nhttps://netflixtechblog.medium.com/foundation-model-for-personalized-recommendation-1a0bd8e02d39\nLinkedIn writes about the evolution of Nuage, its internal control plane framework for managing data infrastructure resources. Initially a self-service platform (Nuage 1.0), it transitioned to a decentralized model (Nuage 2.0) and then to Nuage 3.0, which features centralized management, decoupled logic, enhanced security, improved performance, and simplified onboarding. The article highlights Nuage 3.0's architecture, key capabilities (discoverability, access control, resource management, monitoring), client interfaces (UI, APIs, CLIs), benefits (agility, ownership, performance, security), and future considerations like self-serve onboarding, infrastructure as code, and an AI assistant.\nhttps://www.linkedin.com/blog/engineering/infrastructure/journey-of-next-generation-control-plane-for-data-systems\nAirbnb writes about building an Embedding-Based Retrieval (EBR) system for its search, designed to efficiently narrow down a large pool of potential listings into a smaller, more relevant set for further ranking. The article details constructing training data using contrastive learning with positive and negative listing pairs based on user trips, a two-tower model architecture that separates listing and query features for offline and online processing, and an online serving strategy using an inverted file index (IVF) with Euclidean distance for efficient retrieval and balanced clustering, and highlights the significant improvement in booking.\nhttps://medium.com/airbnb-engineering/embedding-based-retrieval-for-airbnb-search-aabebfc85839\nGrab writes about Hugo, its data ingestion platform's pipeline monitoring, diagnosis, and resolution to improve stability and address on-call challenges. Grab narrates how it built a system with modules for signal collection (failures, SLA misses, data quality issues), diagnosis (identifying root causes and assignees), an RCA table, auto-resolution (using custom handlers and retry mechanisms), a data health API (for external access), and a Data Health Workbench (a dashboard for visualization and manual intervention), leading to improved data visibility, reduced downtime, and a lighter on-call workload.\nhttps://engineering.grab.com/improving-hugo-stability\nCan you utilize Trino for running your ETL pipeline or simply for ad-hoc analytics? The author concludes that Trino performs best when the ETL is designed to accommodate some of Trino\u2019s limitations (such as keeping ETL queries short to facilitate easy failure recovery) and when a reliable external system like Apache Airflow manages retries and state controls. \nhttps://engineering.salesforce.com/how-to-etl-at-petabyte-scale-with-trino-5fe8ac134e36/\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-212", "title": "Data Engineering Weekly", "content": "DataOps on Apache Airflow\u00ae is powering the future of business \u2013 this report reviews responses from 5,000+ data practitioners to reveal how and what\u2019s coming next.\nGet the report \u2192\nData Council has always been one of my favorite events to connect with and learn from the data engineering community. Data Council 2025 is set for April 22-24 in Oakland, CA. As a special perk for Data Engineering Weekly subscribers, you can use the code dataeng20 for an exclusive 20% discount on tickets!\nhttps://www.datacouncil.ai/bay-2025\nThe poll results show that while 45% of respondents feel confident in their Lakehouse expertise, a majority (55%) recognize gaps in knowledge or need support. The poll indicates that while adoption is growing, there is still a significant need for learning, collaboration, and assistance in effectively leveraging Lakehouse technology. The key takeaway is that Lakehouse is gaining traction, but many teams are still in the learning phase.\nThe most valuable AI applications are not standalone but integrate deeply with existing workflows and data sources. The author emphasizes funding the crucial underlying infrastructure for AI to function effectively. The true value lies in companies building data pipelines, model adaptation tools, and integration frameworks rather than user interfaces.\nhttps://www.linkedin.com/pulse/infrastructure-behind-ais-app-layer-shruti-gandhi-h1jgc/\nAnother LLM productivity success story. Airbnb describes migrating nearly 3,500 React component test files from Enzyme to React Testing Library (RTL) using a combination of large language models (LLMs) and automation. The blog narrates the process involving a step-based state machine with automated validation and LLM-driven refactoring, retry loops with dynamic prompting, providing extensive context to the LLM (including related code, examples, and guidelines), and a \"sample, tune, sweep\" strategy for iterative improvement, ultimately achieving 97% automated migration in six weeks, significantly faster and more cost-effective than the initially estimated 1.5 years of manual effort!!!.\nhttps://medium.com/airbnb-engineering/accelerating-large-scale-test-migration-with-llms-9565c208023b\nSquare writes about using a RoBERTa-based machine learning model to improve merchant categorization accuracy, crucial for personalized product experiences, business strategy, growth, product eligibility, and accurate interchange fees. The model leverages high-quality training data, the RoBERTa architecture, and post-onboarding signals; it significantly outperforms previous methods (with a ~30% absolute accuracy improvement), and the article describes the data preprocessing, model creation with Databricks and Hugging Face, inference optimization using multiple GPUs, PySpark, batch size optimization, and incremental predictions, and finally presents accuracy improvements across various business categories.\nhttps://developer.squareup.com/blog/roberta-model-for-merchant-categorization-at-square/\nWe asked 5,000+ data engineers how they use Airflow. What we learned? Airflow has never been more critical for data operations\u2013 or more important to their careers.Check out the full report for more insights and DataOps trends!\nhttps://www.astronomer.io/airflow/state-of-airflow\nIntuit writes about using a dual-loop system to build a GenAI-powered pipeline to improve knowledge discovery in its technical documentation. The inner loop enhances document quality and structure through GenAI plugins (analyzer, improver, style guide, discoverability, augmentation). The outer loop improves information retrieval and answer synthesis via embedding, search, and answer plugins. This approach is interesting, and I wonder how data catalogs can use it. \nhttps://medium.com/intuit-engineering/revolutionizing-knowledge-discovery-with-genai-to-transform-document-management-0cdf4385c11c\nFor real-time ML applications, the most recent data often holds the most predictive power. Feature freshness, defined as \"the time between when new data becomes available and when a model can use it for prediction,\" is crucial for capturing and acting on these recent signals. Stale or delayed features can lead to missed fraud detection, irrelevant recommendations, and poor decision-making. Tecton writes one such case study of how HomeToGo evolves its architecture from batch inference to real-time. \nhttps://www.tecton.ai/blog/understanding-the-feature-freshness-problem-in-real-time-ml/\nEverything in AI eventually comes down to the quality and completeness of your internal data. The article provides an introduction to preparing datasets for Large Language Model (LLM) training, covering data preprocessing (extracting text from various formats like HTML, PDF, and Office documents, and filtering low-quality content), deduplication (using techniques like CCNet, MinHash, and Locality Sensitive Hashing), and creating datasets for fine-tuning. It discusses dataset considerations (relevance, annotation quality, size, ethics, data cutoffs, modalities, synthetic data), formats for instruction and preference tuning, synthetic data creation (Self-Instruct), data labeling approaches (human, LLM-assisted, cohort-based, RLHF-based), and data processing architectures using Amazon Web Services.\nhttps://aws.amazon.com/blogs/machine-learning/an-introduction-to-preparing-your-own-dataset-for-llm-training/\nData exchange is critical when discussing AI and the need for data quality. Apache Arrow continuously makes a big impact on data exchange. The blog narrates how Apache Arrow offers better data serialization efficiency and avoids design pitfalls from the past.\nhttps://arrow.apache.org/blog/2025/02/28/data-wants-to-be-free/\nDue to its non-deterministic nature, a well-designed evaluation process for Generative AI is critical. The blog stresses the need for granular, structured feedback, especially from experts, and outlines key considerations for evaluation design. These include defining clear success metrics, combining automated and human-in-the-loop evaluations, incorporating explicit and implicit feedback, designing relevant feedback mechanisms, utilizing offline and online evaluation, addressing subjectivity and variability, ensuring privacy, and conducting early and frequent user testing.\nhttps://medium.com/data-science-at-microsoft/beyond-thumbs-up-and-thumbs-down-a-human-centered-approach-to-evaluation-design-for-llm-products-d2df5c821da5\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-211", "title": "Data Engineering Weekly", "content": "Data Council has always been one of my favorite events to connect with and learn from the data engineering community. Data Council 2025 is set for April 22-24 in Oakland, CA. As a special perk for Data Engineering Weekly subscribers, you can use the code dataeng20 for an exclusive 20% discount on tickets!\nhttps://www.datacouncil.ai/bay-2025\nIn the last two episodes of Data Engineering Weekly, we discussed the current state of Lakehouse. I\u2019m curious to know your organizational strategy for Lakehouse.\nDeepSeek's Fire-Flyer (FF) is a classic example of resource constraint. It combines system design innovations with cost-effective, high-performance AI-HPC infrastructure for large-scale AI model training and inference. The blog details DeepSeek's motivations for building FF (addressing cost, scalability, and AI-specific optimization), outlines FF's limitations (PCIe bandwidth, NVLink issues), and discusses implications for AI teams (democratization, cost-efficiency, hardware-software integration). \nhttps://gradientflow.com/deepseek-fire-flyer/\nThe article details the author's journey in developing AI agents, starting with early observations about LLMs' problem-solving capabilities and the challenges non-experts face in using them. The blog highlights the iterative process, including prototyping an agent with a visual trajectory explorer, open-sourcing the BeeAI framework, and learning from user feedback. The article emphasizes the importance of developer experience, flexible agent architectures, innovative interaction modalities, and rigorous evaluation. \nhttps://medium.com/@mayamurad/hard-earned-lessons-from-a-year-of-building-ai-agents-945d90c78707\nAgent building is one of the fast-moving software engineering disciplines. Alibaba writes about the evolution and development trends of AI Agents, emphasizing the shift from single-agent to multi-agent systems and the importance of data-centric platforms. The blog highlights Alibaba's approach to building AI agent competitiveness through models, data, and scenarios. It focuses on private data and high-frequency, structured use cases, introduces a \"data flywheel\" for continuous improvement, and presents a data-centric intelligent agent architecture. \nhttps://www.alibabacloud.com/blog/development-trends-and-open-source-technology-practices-of-ai-agents_602037\nOne of DEW\u2019s predictions is the lakehouse systems will absorb typical database functionalities as they grow. We called it LakeDB in the prediction. It is exciting to see Apache Hudi increasingly moving towards it with secondary index support, built-in table services, etc. Apache Hudi\u2019s adoption is reassuring for the LakeDB prediction. \n https://hudi.apache.org/blog/2025/03/05/hudi-21-unique-differentiators/\nGojek's Ads engineering team writes about its telemetry event collection, focusing on accurate impression counting and data reliability. The blog highlights the implementation of HyperLogLog (HLL) for efficient and precise counting of unique ad impressions, reducing load and improving memory efficiency by ~50%. It also introduces a parallel telemetry system using Gojek's Courier (MQTT-based) to verify and identify gaps in their existing system. \nhttps://medium.com/gojekengineering/every-impression-counts-how-the-ads-team-supercharged-gojeks-telemetry-event-collection-6a450a4476b6\nGrab describes enhancing their Spark observability tool, Iris, by migrating from a Telegraf/InfluxDB/Grafana (TIG) stack to a StarRocks-based architecture. The system design provides a unified platform for real-time and historical data, streamlines data ingestion directly from Kafka, uses materialized views for faster query performance, and offers a custom web application (Iris UI) for improved user experience, enabling them to make better-informed decisions. The article also details the data model, ingestion process, query optimization strategies, and plans.\nhttps://engineering.grab.com/building-a-spark-observability\nHalodoc writes about its in-house schema change management framework using Apache Hudi, PySpark, and AWS Glue Data Catalog. Glue Data Catalog limitations and a need for more control, Halodoc writes a framework to handle schema changes (column additions, deletions, and data type changes) by comparing schemas, updating configurations, and processing data, saving up to 15 hours per month by automation.\nhttps://blogs.halodoc.io/schema-change-management-at-halodoc/\nEnriching a model context with enterprise data without compromising security and privacy will be an interesting system design challenge. Claude recently released MCP to do the same. Slack publishes a similar system design that extends the Slack app ecosystem to access enterprise data in a secure, zero-copy design. \nhttps://slack.engineering/how-we-built-enterprise-search-to-be-secure-and-private/\nKafka replication is the most reliable protocol implementation; however, it also saturates the consumer throughput. Adding and removing a broker is a challenge in itself. Traditional Kafka brokers store data locally on attached storage volumes, which can lead to availability and resiliency issues. MSK Express brokers, however, offer fully managed and highly available Regional Kafka storage, decoupling compute and storage resources. I like this design better than the S3-based Kafka protocol.  \nhttps://aws.amazon.com/blogs/big-data/express-brokers-for-amazon-msk-turbo-charged-kafka-scaling-with-up-to-20-times-faster-performance/\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/is-apache-iceberg-the-new-hadoop", "title": "Data Engineering Weekly", "content": "The modern data stack constantly evolves, with new technologies promising to solve age-old problems like scalability, cost, and data silos. Apache Iceberg, an open table format, has recently generated significant buzz. But is it truly revolutionary, or is it destined to repeat the pitfalls of past solutions like Hadoop?\nIn a recent episode of the Data Engineering Weekly podcast, we delved into this question with Daniel Palma, Head of Marketing at Estuary and a seasoned data engineer with over a decade of experience. Danny authored a thought-provoking article comparing Iceberg to Hadoop, not on a purely technical level, but in terms of their hype cycles, implementation challenges, and the surrounding ecosystems. This blog post expands on that insightful conversation, offering a critical look at Iceberg's potential and the hurdles organizations face when adopting it.\nFor those unfamiliar with Hadoop's trajectory, it's crucial to understand the context. In the mid-2000s, Hadoop emerged as a groundbreaking solution for processing massive datasets. It promised to address key pain points:\nScaling: Handling ever-increasing data volumes.\nCost: Reducing storage and processing expenses.\nSpeed: Accelerating data insights.\nData Silos: Breaking down barriers between data sources.\nHadoop achieved this through distributed processing and storage, using a framework called MapReduce and the Hadoop Distributed File System (HDFS). However, while the promise was alluring, the reality proved complex. Many organizations struggled with Hadoop's operational overhead, leading to high failure rates (Gartner famously estimated that 80% of Hadoop projects failed). The complexity stemmed from managing distributed clusters, tuning configurations, and dealing with issues like the \"small file problem.\"\nApache Iceberg enters the scene as a modern table format designed for massive analytic datasets. Like Hadoop, it aims to tackle scalability, cost, speed, and data silos. However, Iceberg focuses specifically on the table format layer, offering features like:\nSchema Evolution: Adapting to changing data structures without rewriting tables.\nTime Travel: Querying data as it existed at a specific time.\nACID Transactions: Ensuring data consistency and reliability.\nPartition Evolution: Changing data partitioning without breaking existing queries.\nIceberg's design addresses Hadoop's shortcomings, particularly data consistency and schema evolution. But, as Danny emphasizes, an open table format alone isn't enough.\nIceberg, by itself, is not a complete solution. It requires a surrounding ecosystem to function effectively. This ecosystem includes:\nCatalogs: Services that manage metadata about Iceberg tables (e.g., table schemas, partitions, and file locations).\nCompute Engines: Tools that query and process data stored in Iceberg tables (e.g., Trino, Spark, Snowflake, DuckDB).\nMaintenance Processes: Operations that optimize Iceberg tables, such as compacting small files and managing metadata.\nThe ecosystem is where the comparison to Hadoop becomes particularly relevant. Hadoop also had a vast ecosystem (Hive, Pig, HBase, etc.), and managing this ecosystem was a significant source of complexity. Iceberg faces a similar challenge.\nDanny highlights operational complexity as a major hurdle for Iceberg adoption. While the Iceberg itself simplifies some aspects of data management, the surrounding ecosystem introduces new challenges:\nSmall File Problem (Revisited): Like Hadoop, Iceberg can suffer from small file problems. Data ingestion tools often create numerous small files, which can degrade performance during query execution. Iceberg addresses this through table maintenance, specifically compaction (merging small files into larger ones). However, many data ingestion tools don't natively support compaction, requiring manual intervention or dedicated Spark clusters.\nMetadata Overhead: Iceberg relies heavily on metadata to track table changes and enable features like time travel. If not handled correctly, managing this metadata can become a bottleneck. Organizations need automated processes for metadata cleanup and compaction.\nCatalog Wars: The catalog choice is critical, and the market is fragmented. Major data warehouse providers (Snowflake, Databricks) have released their flavors of REST catalogs, leading to compatibility issues and potential vendor lock-in. The dream of a truly interoperable catalog layer, where you can seamlessly switch between providers, remains elusive.\nInfrastructure Management: Setting up and maintaining an Iceberg-based data lakehouse requires expertise in infrastructure-as-code, monitoring, observability, and data governance. The maintenance demands a level of operational maturity that many organizations lack.\nIf your organization is considering Iceberg, Danny stresses the importance of careful planning and evaluation:\nDefine Your Use Case: Clearly articulate your specific needs. Are you prioritizing performance, cost, or both? What are your data governance and security requirements? Your answers will influence your choices for storage, computing, and cataloging.\nEvaluate Compatibility: Ensure your existing infrastructure and tools (query engines, data ingestion pipelines) are compatible with Iceberg and your chosen catalog.\nConsider Cloud Vendor Lock-in: Be mindful of potential lock-in, especially with catalogs. While Iceberg is open, cloud providers have tightly coupled implementation specific to their ecosystem.\nBuild vs. Buy: Decide whether you have the resources to build and maintain your Iceberg infrastructure or if a managed service is better. Many organizations prefer to outsource table maintenance and catalog management to avoid operational overhead.\nTalent and Expertise: Do you have the in-house expertise to manage Spark clusters (for compaction), configure query engines, and manage metadata? If not, consider partnering with consultants or investing in training.\nStart the Data Governance Process: Don't wait until the last minute to build the data governance framework. You must create the framework and processes before jumping into adoption.\nThe role of the catalog is evolving. Initially, catalogs focused on managing metadata for structured data in Iceberg tables. However, the vision is expanding to encompass unstructured data (images, videos, audio) and AI models. This \"catalog of catalogs\" or \"uber catalog\" approach aims to provide a unified interface for accessing all data types.\nThe benefits of a unified catalog are clear: simplified data access, consistent semantics, and easier integration across different systems. However, building such a catalog is complex, and the industry is still grappling with the best approach.\nAmazon's recent announcement of S3 Tables raised eyebrows. These tables combine object storage with a table format, offering a highly managed solution. However, they are currently limited in terms of interoperability. They don't support external catalogs, making integrating them into existing Iceberg-based data stacks difficult. The jury is still unsure whether S3 Tables will become a significant player in the open table format landscape.\nChoosing the right query engine is crucial for performance and cost optimization. While some engines like Snowflake boast excellent performance with Iceberg tables (with minimal overhead compared to native tables), others may lag. Factors to consider include:\nPerformance: Benchmark different engines with your specific workloads.\nCost: Evaluate the cost of running queries on different engines.\nScalability: Ensure the engine can handle your anticipated data volumes and query complexity.\nCompatibility: Verify compatibility with your chosen catalog and storage layer.\nUse Case: Different engines excel at different tasks. Trino is popular for ad-hoc queries, while DuckDB is gaining traction for smaller-scale analytics.\nThe ultimate question is whether the benefits of Iceberg outweigh the complexities. For many organizations, especially those with limited engineering resources, fully managed solutions like Snowflake or Redshift might be a more practical starting point. These platforms handle the operational overhead, allowing teams to focus on data analysis rather than infrastructure management.\nHowever, Iceberg can be a compelling option for organizations with specific requirements (e.g., strict data residency rules, a need for a completely open-source stack, or a desire to avoid vendor lock-in). The key is approaching adoption strategically, clearly understanding the challenges, and a plan to address them.\nDanny predicts consolidation in the table format space. Managed service providers will likely bundle table maintenance and catalog management with their Iceberg offerings, simplifying the developer experience. The next step will be managing the compute layer, providing a fully end-to-end data lakehouse solution.\nInitiatives like Apache XTable aim to provide a standardized interface on top of different table formats (Iceberg, Hudi, Delta Lake). However, whether such abstraction layers will gain widespread adoption remains to be seen. Some argue that standardizing on a single table format is a simpler approach.\nBeyond traditional analytics, Iceberg has the potential to contribute significantly to event-driven architectures and machine learning. Its features, such as time travel, ACID transactions, and data versioning, make it a suitable backend for streaming systems and change data capture (CDC) pipelines.\nSeveral challenges remain in the open table format landscape:\nSimplified Data Ingestion: Writing data into Iceberg is still unnecessarily complex, often requiring Spark clusters. Simplifying this process is crucial for broader adoption.\nCatalog Standardization: The lack of a standardized catalog interface hinders interoperability and increases the risk of vendor lock-in.\nDeveloper-Friendly Tools: The ecosystem needs more developer-friendly tools for managing table maintenance, metadata, and query optimization.\nApache Iceberg offers a powerful approach to building modern data lakehouses. It addresses many limitations of previous solutions like Hadoop, but it's not a silver bullet. Organizations must carefully evaluate their needs, resources, and operational capabilities before embarking on an Iceberg journey.\nStart small, test thoroughly, automate aggressively, and prioritize data governance. Organizations can unlock their potential by approaching Iceberg adoption cautiously and clearly while avoiding the pitfalls plaguing earlier data platform initiatives. The future of the data lakehouse is open, but the path to get there requires careful navigation.\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-210", "title": "Data Engineering Weekly", "content": "DataOps on Apache Airflow\u00ae is powering the future of business \u2013 this report reviews responses from 5,000+ data practitioners to reveal how and what\u2019s coming next.\nGet the report \u2192\nData Council has always been one of my favorite events to connect with and learn from the data engineering community. Data Council 2025 is set for April 22-24 in Oakland, CA. As a special perk for Data Engineering Weekly subscribers, you can use the code dataeng20 for an exclusive 20% discount on tickets!\nhttps://www.datacouncil.ai/bay-2025\nThere is a growing concern about AI's impact on the knowledge workforce. Understanding which skills are in growing demand and the need for upskilling as the software abstraction changes is critical. I found the blog to be a fresh take on the skill in demand by layoff datasets. \nhttps://semaphore.io/blog/tech-layoffs\nDeepSeek continues to impact the Data and AI landscape with its recent open-source tools, such as Fire-Flyer File System (3FS) and smallpond. The blog provides an excellent analysis of smallpond compared to Spark and Daft. \nOur internal benchmark of the NYC dataset shows a 48% performance gain of smallpond over Spark!!\nhttps://mehdio.substack.com/p/duckdb-goes-distributed-deepseeks\nDeepSeek\u2019s Fire-Flyer File System (3FS) re-triggers the importance of an optimized file system for efficient data processing. The industry relies more or less on S3 as a de facto data storage, and I found the experimentation on optimizing the S3 read optimization to be an excellent reference. \nhttps://medium.com/tr-labs-ml-engineering-blog/slow-reads-for-s3-files-in-pandas-how-to-optimize-it-c3bfdb947a70\nDatasets and data-driven scheduling are one of the most adopted features by Airflow users, 48% of the respondents in the recent Airflow survey said they are already using this feature, and nearly 30% are asking for an expansion of it! Whether you use Datasets already or want to get started, we've got you covered!\nSave Your Spot \u2192\nThe blog explores prompt engineering as a bridge between natural language and AI tasks, contrasting it with traditional programming. While prompt engineering\u2019s lower learning curve and accessibility make it a valuable complement, it falls short in precision, reliability, and scalability. The conclusion is that prompt engineering will enhance rather than replace traditional programming long-term.\nhttps://www.infoq.com/articles/prompt-engineering/\nThe blog explores various strategies for table compaction in data engineering, focusing on Delta Lake, Hudi, and Iceberg. It evaluates methods including no compaction, pre-write optimized writes, scheduled compaction, and automatic compaction, ultimately recommending automatic compaction for its simplicity and consistent performance. The author highlights that while scheduled or manual compaction may occasionally still be necessary for larger datasets, enabling automatic compaction reduces complexity and ensures stable read and write performance over time.\nhttps://milescole.dev/data-engineering/2025/02/26/The-Art-and-Science-of-Table-Compaction.html\nData is the Key\nOptimization starts with collecting data and asking the right questions. Netflix writes an excellent article describing its approach to cloud efficiency, starting with data collection to questioning the business process. \nhttps://netflixtechblog.com/cloud-efficiency-at-netflix-f2a142955f83\nOne of DEW\u2019s 2025 predictions is that we will see increased adoption of the data Mesh principles. Adevinta writes about transforming its data infrastructure from a lakehouse architecture to a data mesh, leveraging Databricks and initiatives like data contracts and data product frameworks. Key highlights include\nUsing data contracts for source-aligned data products (bronze layer).\nCreating \"one big table\" for domain-aggregated data products (silver layer)\nImplementing a \"consuming suite\" for consumer-aligned data products and prototyping (gold layer) automates governance and enables scalable, decentralized data product creation.\nhttps://medium.com/adevinta-tech-blog/from-lakehouse-architecture-to-data-mesh-c532c91f7b61\nCloudflare writes about how it manages and extracts value from a massive data pipeline ingesting over 700 million events per second, using controlled downsampling (via techniques like \"bottomless buffers\" and adaptive sampling) to handle potential data loss. The blog explains how the Horvitz-Thompson estimator is used to derive accurate analytics and confidence intervals from sampled data, illustrates a real-world example of how incorrect sampling can lead to biased results, and describes how these techniques are exposed in its analytics APIs.\nhttps://blog.cloudflare.com/how-we-make-sense-of-too-much-data/\nState Farm writes about strategies for handling large Apache Spark execution plans caused by extensive data transformations. The blog compares caching, checkpointing, local checkpointing, temporary writes, and rebuilding from RDDs, emphasizing their impact on performance, fault tolerance, and memory usage. The analysis shows that while local checkpointing is often the most efficient, checkpointing or temporary writes offer more reliability. \nhttps://engineering.statefarm.com/when-the-spark-execution-plan-gets-too-big-eb658872d603\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/the-state-of-lakehouse-architecture", "title": "Data Engineering Weekly", "content": "Lakehouse architecture represents a major evolution in data engineering. It combines data lakes' flexibility with data warehouses' structured reliability, providing a unified platform for diverse data workloads ranging from traditional business intelligence to advanced analytics and machine learning. Roy Hassan, a product leader at Upsolver, now Qlik, offers a comprehensive reality check on Lakehouse implementations, shedding light on their maturity, challenges, and future directions.\nA Lakehouse is not a specific product, tool, or service but an architectural framework. This distinction is critical because it allows organizations to tailor implementations to their needs and technological environments. For instance, Databricks users inherently adopt a Lakehouse approach by storing data in object storage, managing it with the Delta Lake format, and analyzing it directly on the data lake.\nThe adoption and maturity of Lakehouse implementations vary across cloud platforms and ecosystems:\nDatabricks: Many organizations have built mature Lakehouse implementations using Databricks, leveraging its robust capabilities to handle diverse workloads.\nAmazon Web Services (AWS): While AWS provides services like Athena, Glue, Redshift, and EMR to access and process data in object storage, many users still rely on traditional data lakes built on Parquet files. However, a growing number are adopting Lakehouse architectures with open table formats such as Iceberg, which has gained traction within the AWS ecosystem.\nAzure Fabric: Built on the Delta Lake format, Azure Fabric offers a vertically integrated Lakehouse experience, seamlessly combining storage, cataloging, and computing resources.\nSnowflake: Organizations increasingly use Snowflake in a Lakehouse-oriented manner, storing data in S3 and managing it with Iceberg. While new workloads favor Iceberg, most existing data remains within Snowflake\u2019s internal storage.\nGoogle BigQuery: The Lakehouse ecosystem in Google Cloud is still evolving. Many users prefer to keep their workloads within BigQuery due to its simplicity and integrated storage.\nDespite these differences in maturity, the industry-wide adoption of Lakehouse architectures continues to expand, and their implementation is becoming increasingly sophisticated.\nDiscussions about open table formats often spark debate, but each format offers unique strengths and is backed by a dedicated engineering community:\nIceberg and Delta Lake share many similarities, with ongoing discussions about potential standardization.\nHudi specializes in streaming use cases and optimizing real-time data ingestion and processing. [Listen to The Future of Data Lakehouses: A Fireside Chat with Vinoth Chandar - Founder CEO Onehouse & PMC Chair of Apache Hudi]\nMost modern query engines support Delta Lake and Iceberg, reinforcing their prominence in the Lakehouse ecosystem. While Hudi and Paimon have smaller adoption, broader query engine support for all major formats is expected over time.\nApache XTable aims to improve interoperability between different table formats. While the concept is practical, its long-term relevance remains uncertain. As the industry consolidates around fewer preferred formats, converting between them may introduce unnecessary complexity, latency, and potential points of failure\u2014especially at scale.\nOne common criticism of Lakehouse architecture is its lower abstraction level than traditional databases. Developers often need to understand the underlying file system, whereas databases provide a more seamless experience by abstracting storage management. The challenge is to balance Lakehouse's flexibility and traditional databases' ease of use.\nA successful Lakehouse implementation starts with a well-defined strategy that aligns with business objectives. Organizations should:\n\u2022 Establish a clear vision and end goals.\n\u2022 Design a scalable and efficient architecture from the outset.\n\u2022 Select the right open table format based on workload requirements.\nShared storage is a foundational principle of Lakehouse architecture. Organizations can analyze data using multiple tools and platforms by storing it in a single location and transforming it once. This approach reduces costs, simplifies data management, and enhances agility by allowing teams to choose the most suitable tool for each task.\nCatalogs are crucial in Lakehouse implementations as metadata repositories describing data assets. These catalogs fall into two categories:\nTechnical catalogs, which focus on data management and organization.\nBusiness catalogs, which provide a business-friendly view of the data landscape.\nA growing trend in the industry is the convergence of technical and business catalogs to offer a unified view of data across the organization. Innovations like the Iceberg REST catalog specification have advanced catalog management by enabling a decoupled and standardized approach.\nIn the coming years, AI and machine learning will drive the evolution of data catalogs. Automated data discovery, governance, and optimization will become more prevalent, allowing organizations to unlock new AI-powered insights and streamline data management processes.\nThe rise of AI is transforming the role of data engineers. Traditional responsibilities like building data pipelines are shifting towards platform engineering and enabling AI-driven data capabilities. Moving forward, data engineers will focus on:\n\u2022 Designing and maintaining AI-ready data infrastructure.\n\u2022 Developing tools that empower software engineers to leverage data more effectively.\nLakehouse architecture is rapidly evolving, with growing adoption across cloud ecosystems and advancements in open table formats, cataloging, and AI integration. While challenges remain\u2014particularly around abstraction and complexity\u2014the benefits of flexibility, cost efficiency, and scalability make it a compelling approach for modern data workloads.\nOrganizations investing in a Lakehouse strategy should prioritize best practices, stay informed about emerging trends, and build architectures that support current and future data needs.\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-209", "title": "Data Engineering Weekly", "content": "Streamline code deployment, enhance collaboration, and ensure DevOps best practices with Astro's robust CI/CD capabilities.\nTry Astro Free \u2192\nData Council has always been one of my favorite events to connect with and learn from the data engineering community. Data Council 2025 is set for April 22-24 in Oakland, CA. As a special perk for Data Engineering Weekly subscribers, you can use the code dataeng20 for an exclusive 20% discount on tickets! \nhttps://www.datacouncil.ai/bay-2025\nThis article comprehensively overviews the 2025 open-source data engineering landscape, highlighting key trends, active projects, and emerging technologies. It covers nine categories: storage systems, data lake platforms, processing, integration, orchestration, infrastructure, ML/AI, metadata management, and analytics. Key highlights include the rise of DuckDB, zero-disk architectures, the consolidation of open table formats around Apache Iceberg, the growth of single-node processing, the expansion of stream processing engines, the \"Catalog War,\" and the emergence of composable BI stacks and LLMOps.\nhttps://medium.com/@ApacheDolphinScheduler/open-source-data-engineering-landscape-2025-db53ce18d53d\nI often get requests seeking advice on breaking into data engineering and how to keep learning new skills. I found the blog to be a comprehensive roadmap for data engineering in 2025.\nhttps://blog.det.life/a-non-beginner-data-engineering-roadmap-2025-edition-2b39d865dd0b\nThis article explores table virtualization enabled by Open Table Formats (OTFs) like Apache Iceberg, Delta Lake, and Apache Hudi. It allows different data platforms to access and share the same underlying data without copying, treating OTFs as a storage-layer abstraction. The author highlights integrating this concept with stream-to-table materialization (like Confluent's Tableflow), enabling a composable data architecture across the operational and analytical infrastructure.\nhttps://jack-vanlightly.com/blog/2025/2/17/towards-composable-data-platforms\nWe asked 5,000+ data engineers how Airflow is shaping the modern DataOps landscape. The results? \n\ud83d\udcb0 Airflow has evolved beyond internal analytics to power business-critical and revenue-generating solutions \ud83e\udd16 Experienced Airflow users are deploying AI into production faster than the competition \u2705 Data engineers have selected Airflow as the de facto DataOps tool of choiceLearn more insights from the largest data engineering survey to date and how the upcoming release of Airflow 3.0 will shape the future of DataOps. Save Your Spot \u2192\nThe article presents a case against the Medallion architecture, contrasting it with a Data Product approach. The authors note that Medallion's Bronze-Silver-Gold tiered structure creates a \"pull\" mechanism, which leads to increased latency, unnecessary data movement, compounded quality issues, and a lack of business context in upstream layers. Conversely, Data Products promote a \"push\" mechanism, prioritizing business context from the outset, enabling leaner data movement, improving data quality, and enhancing consumption flexibility, ultimately advocating for a model-driven, context-led data foundation.\nhttps://medium.com/@community_md101/data-products-a-case-against-medallion-architecture-139096ceea08\nWhat is the ROI of the data team? It is a hard push from the executive team towards the data team. The author highlights that quantifying the ROI of a data team is challenging and often ineffective. Instead, the author suggests measuring the data team's value through stakeholder satisfaction, similar to a Net Promoter Score (NPS). The author emphasizes that data teams are service organizations that support other departments, and their success depends on driving action through insights and gaining advocacy from stakeholders who can articulate the team's value.\nhttps://hex.tech/blog/myth-of-data-team-roi/\nGrab writes about its AI Gateway, a centralized platform designed to streamline access to multiple Generative AI (GenAI) providers like OpenAI, Azure, AWS, and Google for Grab employees. The blog narrates the gateway's purpose: simplifying access, enabling experimentation, achieving cost-efficiency, and providing auditing and platformization benefits. The article details the architecture, user journey, features (like exploration keys, unified API, and dynamic routing), challenges faced, current use cases, plans for a model catalog, out-of-the-box governance, and smarter rate limits.\nhttps://engineering.grab.com/grab-ai-gateway\nYelp writes about its journey to automate revenue recognition by building a revenue data pipeline. The blog analyzes the ambiguity in translating ambiguous accounting requirements into engineering-friendly specifications, performing data gap analysis, and evaluating different system design options, ultimately choosing a Data Lake + Spark ETL approach. \n\nhttps://engineeringblog.yelp.com/2025/02/revenue-automation-series-building-revenue-data-pipeline.html\nGusto writes about using token log probabilities in large language models (LLMs) like GPT to predict and mitigate AI hallucinations. The blog narrates how the LLM confidence, measured as the average log probability of generated tokens (Seq-Logprob), correlates with output quality, and by monitoring and setting thresholds for this confidence score, developers can reject low-quality responses, trigger a human review, or gather more information to improve LLM accuracy. \nhttps://engineering.gusto.com/tackling-ai-hallucinations-in-llm-apps-6d46692f8cac\nOne exciting weekend read for me was the KIP-932 proposal to add queue guarantees to Apache Kafka. The proposal discusses how Kafka will implement queue functionality similar to SQS and RabbitMQ. RabbitMQ also implemented many parts of stream processing, and I believe at this point, both the queues and event stream processing are merging into the same system offerings. \nI wonder if these systems expand more capabilities that eventually fall on their own weight. Let me know in the comments. \nhttps://cwiki.apache.org/confluence/display/KAFKA/KIP-932%3A+Queues+for+Kafka\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/beyond-kafka-conversation-with-jark", "title": "Data Engineering Weekly", "content": "Fluss is a compelling new project in the realm of real-time data processing.  I spoke with Jark Wu, who leads the Fluss and Flink SQL team at Alibaba Cloud, to understand its origins and potential. Jark is a key figure in the Apache Flink community, known for his work in building Flink SQL from the ground up and creating Flink CDC and Fluss.\nYou can read the Q&A version of the conversation here, and don\u2019t forget to listen to the podcast. \nFluss is a streaming storage specifically designed for real-time analytics. It addresses many of Kafka's challenges in analytical infrastructure. The combination of Kafka and Flink is not a perfect fit for real-time analytics; the integration of Kafka and Lakehouse is very shallow. Fluss is an analytical Kafka that builds on top of Lakehouse and integrates seamlessly with Flink to reduce costs, achieve better performance, and unlock new use cases for real-time analytics.\nFluss and Kafka differ fundamentally in design principles. Kafka is designed for streaming events, but Fluss is designed for streaming analytics.\nThe first difference is the Data Model. Kafka is designed to be a black box to collect all kinds of data, so Kafka doesn't have built-in schema and schema enforcement; this is the biggest problem when integrating with schematized systems like Lakehouse. In contrast, Fluss adopts a Lakehouse-native design with structured tables, explicit schemas, and support for all kinds of data types; it directly mirrors the Lakehouse paradigm. Instead of Kafka's topics, Fluss organizes data into database tables with partitions and buckets. This Lakehouse-first approach eliminates the friction of using Lakehouse as a deep storage for Fluss.\nThe second difference is the\u00a0Storage Model. Fluss introduces Apache Arrow as its columnar log storage model for efficient analytical queries, whereas Kafka persists data as unstructured and row-oriented logs for efficient sequence scans. Analytics requires strong data-skipping ability in storage, so sequence scanning is not common; columnar pruning and filter pushdown are basic functionalities of analytical storage. Among the 20,000 Flink SQL jobs at Alibaba, only 49% of columns of Kafka data are read on average.\nThe third difference is Data Mutability: Fluss natively supports real-time updates (e.g., row-level modifications) through LSM tree mechanisms and provides read-your-writes consistency with milli-second latency and high throughput. While Kafka primarily handles append-only streams, the Kafka compacted topic only provides a weak update semantic that compact will keep at least one value for a key, not only the latest.\nThe fourth difference is the Lakehouse Architecture. Fluss embraces the Lakehouse Architecture. Fluss uses Lakehouse as a tiered storage, and data will be converted and tiered into data lakes periodically; Fluss only retains a small portion of recent data. So you only need to store one copy of data for your streaming and Lakehouse. But the true power of this architecture is it provides a union view of Streaming and Lakehouse, so whether it is a Kafka client or a query engine on Lakehouse, they all can visit the streaming data and Lakehouse data as a union view as a single table. It brings powerful analytics to streaming data users.\nOn the other hand, it provides second-level data insights for Lakehouse users. Most importantly, you only need to store one copy of data for your streaming and Lakehouse, which reduces costs. In contrast, Kafka's tiered storage only stores Kafka log segments in remote storage; it is only a storage cost optimization for Kafka and has nothing to do with Lakehouse.\nThe Lakehouse storage serves as the historical data layer for the streaming storage, which is optimized for storing long-term data with minute-level latencies. On the other hand, streaming storage serves as the real-time data layer for Lakehouse storage, which is optimized for storing short-term data with millisecond-level latencies. The data is shared and is exposed as a single table. For streaming queries on the table, it firstly uses the Lakehouse storage as historical data to have efficient catch-up read performance and then seamlessly transitions to the streaming storage for real-time data, ensuring no duplicate data is read. For batch queries on the table, streaming storage supplements real-time data for Lakehouse storage, enabling second-level freshness for Lakehouse analytics. This capability, termed Union Read, allows both layers to work in tandem for highly efficient and accurate data access.\nConfluent Tableflow can bridge Kafka and Iceberg data, but that is just a data movement that data integration tools like Fivetran or Airbyte can also achieve. Tableflow is a Lambda Architecture that uses two separate systems (streaming and batch), leading to challenges like data inconsistency, dual storage costs, and complex governance. On the other hand, Fluss is a Kappa Architecture; it stores one copy of data and presents it as a stream or a table, depending on the use case. Benefits:\nCost and Time Efficiency: no longer need to move data between system\nData Consistency: reduces the occurrence of similar-yet-different datasets, leading to fewer data pipelines and simpler data management.\nAnalytics on Stream\nFreshness on Lakehouse\nKafka is a general-purpose distributed event streaming platform optimized for high-throughput messaging and event sourcing. It excels in event-driven architectures and data pipelines. Fluss is tailored for real-time analytics. It works with streaming processing like Flink and Lakehouse formats like Iceberg and Paimon.\nArchitecture: Pinot is an OLAP database that supports storing offline and real-time data and supports low-latency analytical queries. In contrast, Fluss is a storage to store real-time streaming data but doesn't provide OLAP abilities; it utilizes external query engines to process/analyze data, such as Flink and StarRocks/Spark/Trino (on the roadmap). Therefore, Pinot has additional query servers for OLAP serving, and Fluss has fewer components.\nPinot is a monolithic architecture that provides complete capabilities from storage to computation. Fluss is used in a composable architecture that can plug multiple engines into different scenarios. The rise of Iceberg and Lakehouse has proven the power of composable architecture. Users use Parquet as the file format and Iceberg as the table format, Fluss on top of Iceberg as the real-time data layer, Flink for streaming processing, and StarRocks/Trino for OLAP queries. Fluss in the architecture can augment the existing Lakehouse with mill-second-level fresh data insights.\nAPI: The API of Fluss is RPC protocols like Kafka, which provides an SDK library, and query engines like Flink provide SQL API. Pinot provides SQL for OLAP queries and BI tool integrations.\nStreaming reads and writes: Fluss provides comprehensive streaming reads and writes like Kafka, but Pinot doesn't natively support them. Pinot connects to external streaming systems to ingest data using a pull-based mechanism and doesn't support a push-based mechanism.\nIf you want to build streaming analytics streaming pipelines, use Fluss (and usually Flink together). If you want to build OLAP systems for low-latency complex queries, use Pinot. If you want to augment your Lakehouse with streaming data, use Fluss.\nFluss focuses on storing streaming data and does not offer streaming processing capabilities. On the other hand, Flink is the de facto standard for streaming processing. Fluss aims to be the best storage for Flink and real-time analytics. The vision behind the integration is to provide users with a seamless streaming warehouse or streaming database experience. This requires seamless integration and in-depth optimization from storage to computation. For instance, Fluss already supports all of Flink's connector interfaces, including catalog, source, sink, lookup, and pushdown interfaces.\nIn contrast, Kafka can only implement the source and sink interfaces. Our team is the community's core contributor to Flink SQL; we have the most committers and PMC members. We are committed to advancing the deep integration and optimization of Flink SQL and Fluss.\nA Fluss cluster consists of two main processes: the CoordinatorServer and the TabletServer. The CoordinatorServer is the central control and management component. It maintains metadata, manages tablet allocation, lists nodes, and handles permissions. The TabletServer stores data and provides I/O services directly to users. The Fluss architecture is similar to the Kafka broker and uses the same durability and leader-based replication mechanism.\nConsistency:\u00a0A table creation will request CoordinatorServer, which creates the metadata and assigns replicas to TabeltServers (three replicas by default), one of which is the leader. The replica leader writes the incoming logs and replica followers fetch logs from the replica leader. Once all replicas replicate the log, the log write response will be successfully returned.\nFault Tolerance: If the TabletServer fails, CoordinatorServer will assign a new leader from the replica list, and it becomes the new leader to accept new read/write requests. Once a failed TabeltServer comes back, it catches up with the logs from the new leader.\nScalability: Fluss can scale up linearly by adding TabletServers.\nLet\u2019s start with why we need columnar storage for streaming data. Fluss is designed for real-time analytics. In analytical queries, it's common that only a portion of the columns are read, and a filter condition can prune a significant amount of data. This applies to streaming analytics, such as a Flink SQL query on Kafka data. For example, among the 20,000 Flink SQL jobs at Alibaba, only 49% of the columns of Kafka data are read on average. Still, you must read 100% of the data and deserialize all the columns.\nWe introduced Apache Arrow as our underlying log storage format. Apache Arrow is a columnar format that arranges data in columns. In the implementation, clients send Arrow batches to the Fluss server, and the Fluss server continuously appends the arrow batches into log files. When a read requests to read specific columns, the server returns the necessary column vectors to users, thus reducing networking costs and improving performance. In our benchmark, if you read only 10% columns, you will have a 10x increase in read throughput.\nFluss has 2 table types:\nLog Table\nPrimary Key Table\nLog table only supports appending data, just like Kafka topics. The primary key table has the primary key definition and thus supports real-time updates on the primary key. Log Table uses LogStore to store data in Arrow format in the storage model. Primary Key Table uses LogStore to store changelogs and KvStore to store the materialized view of the changelog. KvStore leverages RocksDB to support real-time updates. RocksDB is a key-value embedded storage engine based on the LSM tree; the key is the primary key, and the value is the row.\nWrite path:\u00a0when an update request is to the TabletServer, it first looks KvStore for the previous row of the key, combines the previous row and new row as the changelog, writes the changelog to LogStore, and uses it as a WAL for KvStore recovery, then it writes the new row into KvStore. Flink can consume changelogs from the table's LogStore to process streams.\nPartial updates: Look up the previous row of the key, merge the previous row and the new row on the update columns, and write back the merged row to KvStore.\nFluss achieves high throughput and low latency through a combination of innovative techniques. It utilizes end-to-end zero-copy operations, transferring data directly from the producer, through the network, to the server and filesystem, and back to the consumer without unnecessary data duplication. Data is processed in batches (defaulting to 1 MB), making the system latency-insensitive. Further efficiency is gained through zstd level 3 compression, reducing data size. Asynchronous writes allow multiple batches in transit simultaneously, eliminating delays in waiting for write confirmations. Finally, columnar pruning minimizes the amount of data transferred by only sending the necessary columns for a given query.\nWe utilize the same approach with Kafka, the synchronous replication, and the ISR (in-sync-replicas) strategy.\nRecovery time: like Kafka, within seconds. But for the primary key table, it may take minutes as it has to download snapshots of RocksDB from remote storage.\nThe Fluss cluster can scale linearly by adding TabletServers. A table can scale up throughput by adding more buckets (the partition concept in Kafka). We don't support data rebalancing across multiple nodes, but this is a work in progress.\nFluss is undergoing significant lakehouse refactoring to enhance its capabilities and flexibility. This includes making the data lake format pluggable and expanding beyond the current Paimon support to incorporate formats like Iceberg and Hudi through collaborations with companies like Bytedance and Onehouse. Support for additional query engines is also being developed, with Spark integration currently in progress and StarRocks planned for the near future. Finally, to ensure seamless integration with existing infrastructure, Fluss is being made compatible with Kafka, allowing Kafka clients and tools to interact directly with the platform.\nhttps://www.alibabacloud.com/blog/why-fluss-top-4-challenges-of-using-kafka-for-real-time-analytics_601879\nhttps://www.alibabacloud.com/blog/introducing-fluss-streaming-storage-for-real-time-analytics_601921\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-208", "title": "Data Engineering Weekly", "content": "Streamline code deployment, enhance collaboration, and ensure DevOps best practices with Astro's robust CI/CD capabilities.\nTry Astro Free \u2192\nThe reasoning capabilities of LLM open up building learning agents. This article discusses reasoning models, a specialization of LLMs for complex tasks requiring multi-step generation. The author outlines four key approaches to building these models: inference-time scaling, pure reinforcement learning, supervised finetuning with reinforcement learning, and distillation via supervised finetuning. The article also highlights DeepSeek R1 as a milestone in open-weight reasoning models and emphasizes that effective, budget-friendly strategies, like distillation and journey learning, enable smaller-scale research.\nhttps://magazine.sebastianraschka.com/p/understanding-reasoning-llms\nThis article provides another exciting explanation of reasoning capabilities in LLM. It explores reasoning LLMs and highlights the shift from scaling train-time compute to test-time compute for improved performance. The author visually explains techniques like Chain-of-Thought, search against verifiers, and modifying proposal distributions, using DeepSeek-R1 as a key example. The article also emphasizes DeepSeek-R1's training pipeline focused on reinforcement learning and touches upon the distillation of smaller models and even unsuccessful attempts.\nhttps://newsletter.maartengrootendorst.com/p/a-visual-guide-to-reasoning-llms\nEnterprises are increasingly trying to build application capabilities to leverage Gen-AI capabilities. This author outlines common pitfalls in building generative AI applications, including unnecessarily using generative AI, mistaking product issues for AI flaws, starting with overly complex solutions, and overestimating early success. The blog highlights the over-reliance on AI for evaluation instead of human input and crowdsourcing use cases without a comprehensive strategy.\nhttps://huyenchip.com/2025/01/16/ai-engineering-pitfalls.html\nWant to automate key parts of your Apache Airflow pipeline development lifecycle?\nIn this session, Marc Lamberti and Kenten Danas will cover everything you need to know about using CI/CD to manage your Airflow DAGs, including:\u2192 The basics of using CI/CD with Airflow\u2192 How to leverage Astro\u2019s built-in Github integration and other CI/CD features\u2192 Strategies for choosing and implementing the best deployment options\nListen to the Webinar \u2192\nPrompt engineering is a fundamental aspect of leveraging LLMs, representing a significant shift in how we interact with technology. However, developing customer-ready features requires a custom setup that integrates smoothly with the development environment and its requirements. LinkedIn writes about how it built the prompt playground using Jupyter Notebook to set the baseline model.\nhttps://www.linkedin.com/blog/engineering/product-design/building-collaborative-prompt-engineering-playgrounds-using-jupyter-notebook\nWorkflow orchestration is a core component in a business, ranging from business process automation, data pipeline, and AI/ML workload. It is interesting to see a strong trend to use YAML as a syntax for describing the graph of tasks in the workflow DSL.\nhttps://mlops.community/a-survey-of-workflow-orchestration-systems/\nHigh-quality activity tracking is vital for a data-driven organization. Netflix writes about its impression tracking system, which captures user interactions with content previews to enhance personalization. The blog describes the system's architecture, including collecting and processing raw events via Apache Kafka and Apache Flink, enriching them, and storing them in Apache Iceberg. The article also highlights their data quality measures. \nhttps://netflixtechblog.com/introducing-impressions-at-netflix-e2b67c88c9fb\nPayPal writes about using \"Delta CV\" (Delta Customer Value) to measure the incremental lift in customer profit margin after adopting a new product or completing an action. The blog discusses causal inference and synthetic control methodology, comparing adopters (treatment group) to a matched group of non-adopters (control group) based on pre-adoption features. The article also highlights the interpretations, caveats, and non-additive nature of Delta CV while emphasizing its role in decision-making at PayPal.\nhttps://medium.com/paypal-tech/estimating-incremental-lift-in-customer-value-delta-cv-using-synthetic-control-522be5e3da3a\nOne of the core features of LakeHouse formats is the support of concurrency and ACID guarantees. The author discusses the differences between pessimistic concurrency control, optimistic concurrency control, and multi-version concurrency control by comparing all three table formats (Hudi, DeltaLake & Iceberg) concurrency implementations. \nhttps://hudi.apache.org/blog/2025/01/28/concurrency-control/\nThe continuous impact of Apache Arrow in data engineering is undeniable. The author highlights the same by demonstrating the efficiency of adopting Streaming Arrow RecordBatches to build a zero-copy streaming pipeline, eliminating serialization overhead and enabling direct, columnar, high-throughput data movement between databases and processing engines. \nhttps://medium.com/@mcgeehan/redefining-data-engineering-with-go-and-apache-arrow-df9059ddf55c\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/the-ascending-arc-of-ai-agents", "title": "Data Engineering Weekly", "content": "Artificial Intelligence (AI) is at a turning point. For decades, conversations about Artificial General Intelligence (AGI) have been met with skepticism. Yet, recent breakthroughs in model architectures, memory management, and continual learning suggest that our machines are becoming ever more capable. This article traces a timeline of key innovations, illustrating how we have moved from simple language model reasoning to interactive, context-rich, and self-improving AI agents.\nThe story begins with large language models (LLMs) that introduce the idea of chain-of-thought reasoning. Models like GPT-3 showcased remarkable language capabilities, yet they were limited by an inability to consult external sources on the fly. They would generate a chain of reasoning internally\u2014often hidden from users\u2014and produce answers that could sound convincing but were prone to factual hallucinations. While these LLMs represented a quantum leap from previous \u201cbag-of-words\u201d or purely statistical models, it quickly became clear that reasoning in isolation could lead to cascading errors and illusions of certainty.\nProblem: The community realized that halting the cascade of hallucinations required more than just bigger models; it demanded a way for AI to test and ground its internal reasoning against external sources.\nThe first major pivot toward solving these issues came with the ReAct framework, which stands for Reason + Act. Instead of keeping reasoning locked away, ReAct interleaved reasoning steps with explicit actions, such as querying external APIs or searching a knowledge base. This meant the model could plan a series of steps (\u201cI need to search for X, then interpret Y\u201d), act on that plan, and incorporate any retrieved data back into its subsequent reasoning.\n\u2022 Grounding Knowledge: By stepping into the \u201cexternal world,\u201d ReAct mitigated hallucinations. If the model\u2019s initial assumption was off, the feedback loop allowed it to detect inconsistencies and correct course.\n\u2022 Translucent Thought Process: This approach made AI more transparent; each reasoning step was directly tied to an observable action, offering new opportunities for human oversight and fine-grained control.\nOutcome: ReAct established a cornerstone principle: reasoning should not be an isolated monologue but a collaborative dialogue with the world.\nOnce models began interacting with the world, the next challenge was continuous adaptation. Traditional AI training, where a static dataset is used to teach a model once, proved insufficient for dynamic, evolving tasks. Enter VOYAGER, a system that thrives in Minecraft's open-ended environment.\n\u2022 Automatic Curriculum: Instead of a single, rigid training objective, VOYAGER gradually introduced challenges scaled to its evolving competence, reminiscent of how students progress from easier to more difficult homework.\n\u2022 Skill Library: Newly mastered behaviors were stored in a reusable library of code snippets, enabling cumulative knowledge. VOYAGER built a foundation for tackling more complex tasks with each skill learned.\n\u2022 Iterative Prompting: Failures weren\u2019t the end but a signal to refine. Whenever the agent faltered, it leveraged feedback from the environment to adjust how it generated or executed future code.\nOutcome: VOYAGER demonstrated that lifelong learning was not just a theoretical aspiration but a practical framework: by structuring tasks, storing skills, and iterating, an AI agent could continually enrich its capabilities without restarting from scratch.\nAs agent-based methods matured, a new bottleneck became evident: context windows. Even the most advanced LLMs could only handle a limited amount of text simultaneously\u2014problematic for tasks involving long dialogues, extended documents, or multi-session problem-solving. That\u2019s where MemGPT emerged, borrowing concepts from operating systems to implement a paging-like mechanism for AI memory.\n\u2022 Memory Hierarchy: MemGPT distinguishes between active \u201con-chip\u201d memory (the context window) and \u201cpersistent\u201d long-term storage. Like an OS swaps pages of data in and out of RAM, MemGPT fetches relevant information and swaps out irrelevant data.\n\u2022 Infinite Context: By cleverly scheduling these memory \u201cpages,\u201d the system can effectively manage an unbounded repository of knowledge\u2014responding intelligently even in multi-hour or multi-day conversations.\n\u2022 Complementary to Lifelong Learning: MemGPT\u2019s architecture pairs naturally with VOYAGER-like skills, allowing an agent to recall specialized knowledge when needed.\nOutcome: The dream of an \u201cinfinite context\u201d agent started to look tangible, enabling continuous dialogue and knowledge accumulation without forgetting essential details.\nWith models now capable of reasoning and acting (ReAct), learning over time (VOYAGER), and managing vast or ongoing context (MemGPT), the next step was to see how they performed on real-world tasks. Enter SWE-bench, a benchmark that tests AI\u2019s ability to navigate and modify complex codebases:\n\u2022 Multi-File Dependencies: SWE-bench examples require changes across multiple files, demanding a holistic understanding of the software\u2019s architecture.\n\u2022 Context-Intensive Solutions: Without something akin to MemGPT\u2019s extended context, LLMs often produce patchwork fixes that fail to address deeper code dependencies.\n\u2022 Actionable Feedback: The SWE bench environment highlights where AI agents stumble\u2014whether it\u2019s missed edge cases, incomplete refactoring, or oversimplified patches.\nOutcome: These real-world exercises revealed critical shortcomings, helping researchers pinpoint how to refine their approaches. At the same time, it validated that combining ReAct, VOYAGER, and MemGPT features could dramatically improve success rates on complex coding tasks.\nEven as these agent frameworks advanced, the broader question of how knowledge moves from one AI generation to the next persisted. Model distillation offered an elegant solution\u2014mirroring how textbooks distill expert wisdom for students:\n\u2022 Teacher \u2192 Student: Large, comprehensive models act as \u201cteachers,\u201d encoding a wealth of information. Distilling that knowledge creates smaller, specialized \u201cstudents\u201d who can still perform at a high level.\n\u2022 Iterative Refinement: Textbooks undergo new editions, and so do distilled models. Each iteration integrates fresh insights from ReAct-like grounded reasoning, VOYAGER\u2019s skill libraries, or MemGPT\u2019s memory management.\n\u2022 Beyond Simple Compression: Effective distillation isn\u2019t a mere downscaling exercise. It involves contextualizing knowledge, providing structured examples, and sometimes adding new insights from real-world tasks (like SWE-bench).\nOutcome: This cyclical teacher-student paradigm ensures that breakthroughs in one generation are systematically transferred and evolved in the next, accelerating the collective progress of AI systems.\nThe journey from pure LLM reasoning to grounded, interactive, lifelong-learning, context-adept agents signals a broader paradigm shift in AI. By addressing\u2014one after another\u2014the challenges of hallucination, forgetfulness, rigid task definitions, and real-world complexity, these innovations collectively pave the way for systems that behave less like static chatbots and more like adaptive co-pilots in problem-solving.\n1. Reasoning + Acting (ReAct): No more sealed \u201cthought bubbles\u201d\u2014agents can test their logic in the real world.\n2. Lifelong Learning (VOYAGER): AI grows over time, amassing and reusing a library of skills with minimal human intervention.\n3. Infinite Memory (MemGPT): Context no longer needs to be a bottleneck, enabling extended interactions and robust recall.\n4. Real-World Challenges (SWE-bench): Benchmarks push agents to handle professional-grade tasks, revealing their promise and limitations.\n5. Model Distillation: Knowledge seamlessly passes from one generation to the next, forging an ever more capable AI lineage.\nMulti-agent systems present significant challenges, including an inherent tendency for complexity to accumulate over time, which can ultimately lead to system failures. The associated costs go beyond mere downtime, including the significant human overhead needed for debugging and maintenance. \nWhile open-loop designs are especially prone to unbounded error accumulation, closed-loop approaches demand robust feedback mechanisms that may add further layers of complexity. Ultimately, these feedback loops can become exponentially complex, compounding the difficulties of scaling and managing agent-based architectures. The key to the continuous adoption of agents is how the multi-agent system becomes more deterministic or how a business learns to operate in a non-deterministic system. \nWe stand at the threshold of a new AI epoch. Learning from the limitations of static chain-of-thought models, researchers have developed ReAct to ground reasoning in external reality, VOYAGER to ensure continual growth, MemGPT to enable near-infinite recall, and SWE-bench to validate performance on real, messy tasks. Model distillation wraps these pillars together, ensuring that each generation of AI can stand on the shoulders of its predecessors.\nWhile full-fledged Artificial General Intelligence remains an ambitious target, the cumulative impact of these advances cannot be overstated. They illuminate a path where AI agents evolve from mere tools into collaborative problem-solvers, capable of navigating the complexities of our world and, in many ways, exceeding the imagination of their original creators.\nReAct: Synergizing Reasoning and Acting in Language Models [https://arxiv.org/abs/2210.03629]\nVoyager: An Open-Ended Embodied Agent with Large Language Models [https://arxiv.org/abs/2305.16291]\nSWE-bench: Can Language Models Resolve Real-World GitHub Issues? [https://arxiv.org/abs/2310.06770]\nMemGPT: Towards LLMs as Operating Systems [https://arxiv.org/abs/2310.08560]\nChain-of-Verification Reduces Hallucination in Large Language Models [https://arxiv.org/abs/2309.11495]\nThe agent economy [https://www.felicis.com/insight/the-agent-economy]\n\n"}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-207", "title": "Data Engineering Weekly", "content": "Streamline code deployment, enhance collaboration, and ensure DevOps best practices with Astro's robust CI/CD capabilities.\nTry Astro Free \u2192\nThe mixture of Experts (MoEs) are transformer models efficiently gaining traction in the open AI community. MoEs necessitate less compute for pre-training compared to dense models, facilitating the scaling of model and dataset size within similar computational budgets. The article further details techniques, such as load balancing, expert capacity, and parallelism strategies, to enhance MoE training and inference efficiency.\nhttps://huggingface.co/blog/moe\nUnstructured data processing is a top priority for enterprises that want to harness the power of GenAI. It brings challenges in data processing and quality, but what data quality means in unstructured data is a top question for every organization. I found the product blog from QuantumBlack gives a view of data quality in unstructured data. \nhttps://medium.com/quantumblack/solving-data-quality-for-gen-ai-applications-11cbec4cbe72\nPinterest writes about its embedding-based retrieval system enhancements for Homefeed personalization and engagement. The system improves user interactions by integrating MaskNet and DHEN feature crossing, scaling embeddings via contrastive learning, and refining the serving corpus with time-decayed scoring mechanisms. The blog post highlights the industry trend of search engines transitioning towards embedding-based systems, moving beyond traditional IDF models.\nhttps://medium.com/pinterest-engineering/advancements-in-embedding-based-retrieval-at-pinterest-homefeed-d7d7971a409e\nWant to automate key parts of your Apache Airflow pipeline development lifecycle? \nIn this session, Marc Lamberti and Kenten Danas will cover everything you need to know about using CI/CD to manage your Airflow DAGs, including: \u2192 The basics of using CI/CD with Airflow\u2192 How to leverage Astro\u2019s built-in Github integration and other CI/CD features\u2192 Strategies for choosing and implementing the best deployment options\nSave Your Spot \u2192\nAirbnb writes about the challenge of ranking search results on maps, which is distinct from list-based rankings due to uniform user attention across map pins. Airbnb restricted the range of booking probabilities for map pins, which led to significant booking improvements. Further iterations included tiered map pins and a map re-centering algorithm based on booking probabilities. The system design is an excellent reminder of thinking from a user's perspective. \nhttps://medium.com/airbnb-engineering/improving-search-ranking-for-maps-13b03f2c2cca\nYelp's blog discusses integrating large language models (LLMs) to enhance its understanding of millions of daily search queries.  The blog narrates a multi-stage process, from ideation to production rollout, leveraging LLMs for tasks like query segmentation and review highlighting, significantly improving user experience.  The blog emphasizes a strategic approach involving proof-of-concept testing, fine-tuning smaller models for cost-effectiveness, and iterative prompt engineering to achieve scalable and impactful results in real-world search applications.\nhttps://engineeringblog.yelp.com/2025/02/search-query-understanding-with-LLMs.html\nMeta writes about its access tool's system design, which helps export individual users\u2019 access logs. The blog highlights the challenges of Hive-style large data warehouses not fitting for finding the needle-in-the-haystack problem and the importance of batching such requests.\n https://engineering.fb.com/2025/02/04/security/data-logs-the-latest-evolution-in-metas-access-tools/\nData Quality in a real-time streaming system is always challenging. Though sophisticated row-based quality frameworks like CEL (Common Expression Language) provide efficient checking, the challenge is always measuring data quality over a period of time. GetInData writes an excellent summary of adding data quality checks in a Flink streaming pipeline. \nhttps://medium.com/@getindatatechteam/data-quality-in-streaming-a-deep-dive-into-apache-flink-03a908ad6c8f\nOne of the biggest challenges in SQL is the unit testing. The author highlights three key challenges in SQL.\nIntractability of Testing: Even simpler queries require a larger, complex object graph of test data\nLake of reusable business logic: CTE & Views are there, but not as efficient as functions in high-level languages.\nLack of composability: SQL queries operate on concrete table names, making it difficult to build reusable query fragments. \nThe author proposes a Functor-style programming pattern for SQL, which is similar to the Table-Value parameter in SQL Server (T-SQL)\nhttps://borretti.me/article/composable-sql\nProtobuf (Protocol Buffers) can serialize data into a compact binary format, making it smaller and faster to transmit over the network than human-readable formats like Json & csv. The blog narrates how Protobuf serialization converts structured data into a compact binary format by encoding each field with a tag (field number and wire type) and a value, using efficient methods like variable-length integers and length-prefixed strings. \nhttps://victoriametrics.com/blog/go-protobuf/index.html\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-206", "title": "Data Engineering Weekly", "content": "Streamline code deployment, enhance collaboration, and ensure DevOps best practices with Astro's robust CI/CD capabilities.\nTry Astro Free \u2192\nDeepSeek triggered quite a conversation and had an economic impact last week. Many articles explain how DeepSeek works, and I found the illustrated example much simpler to understand. DeepSeek development involves a unique training recipe that generates a large dataset of long chain-of-thought reasoning examples, utilizes an interim high-quality reasoning model, and employs large-scale reinforcement learning (RL). Notably, the process includes an RL step to create a specialized reasoning model (R1-Zero) capable of excelling in reasoning tasks without labeled SFT data, highlighting advancements in training methodologies for AI models.\n\nhttps://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1\nThe article discusses the challenges of transitioning Generative AI products to production and introduces patterns to address these issues, focusing on Direct Prompting, Evals, and embedding. Direct Prompting involves sending user prompts directly to an LLM, which is useful but limited by the model's training data and potential for misuse. Evals are introduced to evaluate LLM responses through various techniques, including self-evaluation, using another LLM as a judge, or human evaluation to ensure the system's behavior aligns with intentions. \nhttps://martinfowler.com/articles/gen-ai-patterns/\nPinterest discusses an embedding-based retrieval system that enhances its recommendation capabilities. It employs a two-tower model approach to learn query and item embeddings from user engagement data. Pinterest implements this system for home feed and notification services, utilizing a combination of long-term user engagement, user profiles, and contextual information to improve user engagement and efficiency significantly. The article covers the implementation details, including an auto-retraining workflow to maintain model freshness and ensure synchronization between the user and item embedding models during deployment.\nhttps://medium.com/pinterest-engineering/establishing-a-large-scale-learned-retrieval-system-at-pinterest-eb0eaf7b92c5\nGitHub Copilot shares its approach to evaluating AI models. It focuses on automated offline evaluations to assess performance, quality, and safety before integrating new models like Anthropic's Claude 3.5 Sonnet and Google's Gemini 1.5 Pro. The evaluation process includes over 4,000 automated tests, measuring the percentage of passing unit tests, similarity to known passing states, and token usage. It also includes manual and automated assessments of chat capabilities using a separate LLM for quality assurance. All these efforts aim to maintain high standards for code generation and assistance.\nhttps://github.blog/ai-and-ml/generative-ai/how-we-evaluate-models-for-github-copilot/\nThe article discusses bias and fairness in AI and NLP applications, emphasizing how societal biases encoded in training data can lead to unfair model predictions. The author highlights methods to identify and measure these biases, such as using counterfactual examples and analyzing gender-specific associations in word embeddings. The article advocates for a three-pronged approach to mitigate biases\u2014pre-processing, post-processing, and in-processing methods\u2014and stresses the importance of defining fairness in specific application contexts, measuring bias, and implementing appropriate mitigation strategies to ensure responsible and equitable NLP system deployment.\nhttps://medium.com/tr-labs-ml-engineering-blog/bias-and-fairness-in-natural-language-processing-7663a6d33932\nI\u2019m always a bit uncomfortable with medallion architecture since it is a glorified term for the traditional ETL process. I finally found a good critique that discusses its flaws, such as multi-hop architecture, inefficiencies, high costs, and difficulties maintaining data quality and reusability.\nThe article advocates for a \"shift left\" approach to data processing, improving data accessibility, quality, and efficiency for operational and analytical use cases. Shifting left involves moving data processing upstream, closer to the source, enabling broader access to high-quality data through well-defined data products and contracts, thus reducing duplication, enhancing data integrity, and bridging the gap between operational and analytical data domains.\nhttps://www.infoq.com/articles/rethinking-medallion-architecture/\nGetYourGuide discusses migrating its Business Intelligence (BI) data source from Snowflake to Databricks, achieving a 20% cost reduction. The article highlights the use of Looker for BI analysis and the centralization of its data warehouse to streamline infrastructure. The migration featured an incremental rollout, automated validations, and performance optimizations, leading to 98% of queries functioning correctly and 72% executing in under 10 seconds.\nhttps://www.getyourguide.careers/posts/from-snowflake-to-databricks-our-cost-effective-journey-to-a-unified-data-warehouse\nThe article introduces Lithium, an ETL++ platform developed by Atlassian for dynamic and ephemeral data pipelines, addressing unique needs like user-initiated migrations and scheduled backups. Lithium uses a Bring Your Own Host (BYOH) model, allowing developers to integrate custom processors within their services and ensuring data proximity and tenant isolation. Key features include workplan auctioning for resource allocation, in-progress remediation for handling data validation failures, and integration with external Kafka topics, achieving a throughput of 1.2 million entities per second in production.\nhttps://www.atlassian.com/blog/atlassian-engineering/lithium\nAffirm migrated from daily MySQL snapshots to Change Data Capture (CDC) replay using Apache Iceberg for its data lake, improving data integrity and governance. The CDC approach addresses challenges like time travel, data validation, performance, and cost by replicating operational data to an AWS S3-based Iceberg Data Lake. The new system automates validation, reduces operational costs by 6x, decreases data storage needs by 1024x, and improves data pipeline performance by 40%.\nhttps://tech.affirm.com/expressive-time-travel-and-data-validation-for-financial-workloads-c8b8cc8d12f4\nThe article highlights the growing popularity of single-node processing frameworks like DuckDB, Apache DataFusion, and Polars in 2024, challenging the distributed-first mindset of the \"big data\" era. It argues that most companies lack true \"big data\" and that single-node solutions offer a more efficient, cost-effective approach for their analytical needs, especially considering data aging effects and the 90/10 rule of analytical workloads. Moreover, advancements in hardware and the economics of cloud pricing further support the case for single-node processing, offering simplified architecture, better resource utilization, and seamless integration with modern data workflows.\nhttps://alirezasadeghi1.medium.com/the-rise-of-single-node-processing-challenging-the-distributed-first-mindset-111333162b83\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-205", "title": "Data Engineering Weekly", "content": "Streamline code deployment, enhance collaboration, and ensure DevOps best practices with Astro's robust CI/CD capabilities.\nTry Astro Free \u2192\nDeepseek certainly dispels the myth that building foundation models is expensive. The industry's next wave will focus on strengthening reasoning and instruction-following capabilities. The paper runs the multi-step and constraint function calling and publishes results on various leading foundation models. \nhttps://huggingface.co/papers/2501.10132\nThe RAG technique is likely one of the first adopted for building Gen AI applications. The author offers a comprehensive guide to enhancing Retrieval-Augmented Generation (RAG) applications, stressing the importance of systematic measurement and iteration rather than random adjustments.\nThe article discusses common pitfalls such as absence bias and intervention bias while advocating for a user-centric approach that emphasizes evaluating retrieval accuracy through precision and recall, focusing on recall. It also examines techniques like using synthetic data to initiate evaluation, segmenting queries for in-depth analysis, utilizing structured extraction and multimodality for various data types, implementing query routing for specialized indices, fine-tuning embeddings and re-rankers based on user feedback, and improving user experience through feedback loops and strategic UI elements.\nhttps://jxnl.co/writing/2025/01/24/systematically-improving-rag-applications/#how-to-systematically-improve-rag-applications\nMeta writes about data lineage as a crucial part of its Privacy-Aware Infrastructure (PAI) initiative to track data flows and implement privacy controls, such as purpose limitation. The data lineage system leverages static code analysis, runtime instrumentation, and input/output data matching to collect data flow signals across various technology stacks, creating a comprehensive lineage graph. \nhttps://engineering.fb.com/2025/01/22/security/how-meta-discovers-data-flows-via-lineage-at-scale/\n130+ pages of beginner \u2192 advanced DAG writing features with plenty of example code.\nGet Ebook \u2192\nThe semantic layer is a highly debated architectural style in late 2023.  The author captures the recent development in the BI layer from a monolithic BI system \u2192 bottomless BI tools \u2192 universal semantic layer.\nhttps://alirezasadeghi1.medium.com/the-evolution-of-business-intelligence-from-monolithic-to-composable-architecture-7a46d42374e9\nUnderstanding the customer journey and a data-driven attribution model is vital for successful marketing operations. The blog narrates the difference between rule-based and data-driven attribution models and how LinkedIn is leveraging Multi-Touch Attribution (MTA) and Marketing Mix Modeling (MMM).\nhttps://www.linkedin.com/blog/engineering/marketing/buyer-journey-insights-with-data-driven-attribution\nThe Strava Geo team developed Rain, a new service that acts as a key-value store for large, immutable datasets generated in Spark, to address challenges related to efficiently serving these datasets. Rain, which behaves like a distributed cache, allows for distributed writes from Spark, immutable data hot-swapping, and cost savings using an LRU Redis cache and a single distributed datastore. This system enables faster iteration, reduced memory footprint, significant cost savings, and the ability to serve terabyte-scale datasets at low latency.\nhttps://medium.com/strava-engineering/rain-a-key-value-store-for-stravas-scale-7f580f5b4848\nGlovo writes about using Apache Airflow as a central component in its Data Mesh architecture to orchestrate data product computations. Glovo customized Airflow\u2019s components, such as DagFactory, to simplify DAG creation and custom operators, like CheckpointerOperator and CheckpointSensor, to manage data product dependency. Furthermore, the article highlights Glovo's evolution towards a declarative approach to defining data products. \nhttps://medium.com/glovo-engineering/using-airflow-in-glovo-6754a2fe79a5\nBlaBlaCar writes about its data pipeline architecture. It follows the typical ELT (Extract, Load, Transform) model orchestrated by Airflow, utilizing tools like Google Dataflow, Rivery, custom Python scripts for ingestion, and dbt for transformations. It is interesting to see more Data Mesh approaches, organizing data into domains with dedicated teams and implementing a robust software development lifecycle for its data pipelines, including version control, code reviews, and automated testing. \nhttps://medium.com/blablacar/data-pipelines-architecture-at-blablacar-3ca43403cb39\nC4 diagram is predominant in defining the software architecture, and never thought through C4 for data teams. The author explores the idea of C4 modeling for the data teams. \nhttps://blog.datatraininglab.com/c4-modelling-for-data-teams-from-chaos-to-clarity-a9f499007e20\nWith Polaris, Unity Catalog, and Glue Catalog, I\u2019m excited about the potential of having table metadata, such as summary statistics, stored for efficient and quick look-up to accelerate query performance. The author describes how\u00a0pg_mooncake, a Postgres extension, surpasses DuckDB in certain analytical workloads while using DuckDB internally and storing data in Parquet files. Pg_mooncake utilizes Postgres to store table and detailed Parquet metadata, including column statistics, which allows for effective filtering of data files and row groups during query execution.\nhttps://www.mooncake.dev/blog/duckdb-parquet\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/envisioning-lakedb-the-next-evolution", "title": "Data Engineering Weekly", "content": "The world of data management is undergoing a rapid transformation. The rise of cloud storage, coupled with the increasing demand for real-time analytics, has led to the emergence of the Data Lakehouse. This paradigm combines the flexibility of data lakes with the performance and reliability of data warehouses. Apache Iceberg, Apache Hudi, and Delta Lake have been at the forefront of this revolution, bringing essential capabilities like schema evolution, ACID transactions, and efficient updates to the Lakehouse architecture. However, as described in Google's internal data management system, Google\u2019s Napa presents a compelling vision that suggests the potential for a next-generation architecture. \nWe call this class next-generation Lakehouse LakeDB and hope the Lakehouse community collectively takes the logical next step toward it.\nThis article delves into Napa's core concepts, compares it with Iceberg, Hudi, and Delta Lake, and argues that Napa's design principles, combined with innovations from systems like Apache Pinot, pave the way for LakeDB, a more integrated, performant, and flexible approach to data management.\nApache Iceberg, Apache Hudi, and Delta Lake have significantly advanced the Lakehouse concept.\nApache Iceberg provides a robust table format that enables schema evolution, time travel, and efficient query planning through detailed metadata management. It excels at managing large analytical datasets and ensuring data consistency.\nApache Hudi focuses on bringing database-like capabilities to the data lake, such as upserts, deletes, and change data capture (CDC). It leverages log-structured storage and indexing to facilitate efficient data mutations.\nDelta Lake is an open-source storage layer that brings ACID transactions to Apache Spark and big data workloads. It provides schema enforcement, time travel (data versioning), and unified batch and streaming processing. Delta Lake's Delta Live Tables feature offers native support for declarative pipeline development and materialized views.\nAll three have become essential components in modern data architectures. Still, they primarily address specific aspects of the Lakehouse: Iceberg focuses on the table format and metadata management, Hudi focuses on data mutability and incremental processing, and Delta Lake focuses on enabling ACID properties and unified processing.\nGoogle's Napa presents a different perspective. It's not just a table format or a data ingestion framework; it's a comprehensive analytical data management system designed for massive scale, low-latency queries and continuous data ingestion. Napa's key features include:\nLog-Structured Merge-Tree (LSM) based Ingestion: Napa uses an LSM-tree approach, optimized for high write throughput, to handle continuous data ingestion. It efficiently merges incoming data with existing data through a series of compactions.\nMaterialized Views for Query Acceleration: Napa heavily relies on materialized views, which are automatically maintained and updated to deliver exceptional query performance. These views are indexed and optimized for fast lookups and range scans.\nQueryable Timestamp (QT): Napa's QT mechanism provides a global view of data consistency and allows users to balance data freshness with query performance. Unlike traditional time-travel features, QT offers a more granular and consistent view of data across the system.\nF1 Query Integration: Napa leverages Google's F1 Query engine for query optimization and execution, enabling efficient handling of complex analytical queries. While F1 Query is a key component, Napa's architecture is flexible and could integrate with other query engines.\nConfigurability: Napa offers extensive configuration options, allowing users to tune the system to meet their needs regarding data freshness, query performance, and cost.\nThe following table summarizes the key differences between Napa, Iceberg, Hudi, and Delta Lake:\nNapa's design philosophy, combined with the strengths of existing Lakehouse technologies and innovations from systems like Apache Pinot, suggests a paradigm shift \u2013 a more integrated and powerful approach we call LakeDB.\nLakeDB envisions a unified data management system that empowers users to define their desired trade-offs for freshness, cost, correctness, and indexes while the system handles these requirements seamlessly. It is characterized by:\nA single, cohesive system that seamlessly integrates all critical data management functions: storage, ingestion, metadata management, and query processing. Users can specify their needs for data freshness and correctness, and LakeDB will automatically optimize the underlying processes to meet these requirements.\nGap Analysis: Current Lakehouses often involve a fragmented collection of tools, leading to complexity, performance bottlenecks, and metadata inconsistencies. For example:\nApache Iceberg excels at metadata management but requires separate tools for ingestion (e.g., Apache Spark) and query processing (e.g., Trino or Spark).\nDelta Lake integrates well with Spark but lacks native real-time ingestion and advanced query optimization support.\nApache Hudi provides real-time ingestion but relies on external systems for query execution and metadata management. This fragmentation forces users to combine multiple tools manually, increasing operational overhead and the risk of inconsistencies.\nDynamically chooses between Copy-on-Write (CoW) and Merge-on-Read (MoR) strategies based on workload characteristics and user-defined preferences for cost and performance. LakeDB automatically manages snapshots for optimal query performance and intelligently creates, maintains, and optimizes materialized views to meet user-defined freshness and correctness requirements.\nGap Analysis: Existing systems often require manual selection of CoW/MoR and lack automated, adaptive optimization of materialized views and snapshots based on access patterns. For example:\nApache Hudi requires users to choose between CoW and MoR at table creation, which can lead to suboptimal performance if workload patterns change.\nDelta Lake supports materialized views through Delta Live Tables but lacks automated optimization for dynamic workloads.\nApache Iceberg does not natively support materialized views, requiring users to implement and manage them manually. These limitations result in higher operational complexity and suboptimal resource utilization.\nIncorporates a mechanism like Napa's Queryable Timestamp (QT) to provide fine-grained control over data freshness. Users can specify their desired freshness levels, and LakeDB will balance these requirements with performance and cost considerations, ensuring that queries return results within the defined freshness constraints.\nGap Analysis: Current Lakehouses offer limited and often coarse-grained control over data freshness, making it difficult to optimize for different workloads. For example:\nApache Iceberg and Delta Lake rely on periodic snapshots, which can delay data availability for real-time use cases.\nApache Hudi supports near real-time ingestion but lacks a mechanism like QT to balance freshness with query performance.\nMost systems require users to manually manage data ingestion pipelines to achieve desired freshness levels, which can be error-prone and resource-intensive.\nIt supports various index types, similar to Apache Pinot, where you can select the index types you want for each column. Users can define their indexing needs, and LakeDB will automatically manage and optimize these indexes to meet query performance requirements while minimizing storage costs.\nGap Analysis: Current Lakehouse formats lack comprehensive indexing support, particularly for advanced index types like Star-Tree indexes, leading to performance limitations on analytical workloads, especially those involving high-cardinality dimensions. For example:\nApache Iceberg and Delta Lake rely on basic partitioning and data skipping, which are insufficient for high-cardinality dimensions or complex queries.\nApache Hudi supports indexing but lacks advanced indexing techniques like Apache Pinot, which are critical for real-time analytics.\nThis system offers extensive configuration options on the same table by multiple consumers, allowing users to tune it to their specific needs regarding performance, cost, data freshness, and storage optimization. Users can define their desired trade-offs, and LakeDB will automatically configure itself to meet these requirements, reducing the need for manual intervention.\nGap Analysis: Current Lakehouse components often offer limited configurability, hindering optimization for specific use cases and workloads. For example:\nDelta Lake provides some configurability through Spark settings but lacks fine-grained control over data freshness and cost.\nApache Iceberg allows the configuration of metadata and table layout but does not provide options for dynamic workload optimization.\nApache Hudi offers configurability for table types (CoW/MoR) but requires manual tuning to balance performance and cost\u2014this lack of configurability forces users to compromise or invest significant effort in manual tuning.\nLakeDB Vision: Provides strong consistency guarantees (ACID properties) and robust support for schema evolution, building on the foundations of Iceberg and Delta Lake. Users can define their correctness requirements, and LakeDB will ensure that these requirements are met, even as schemas evolve.\nGap Analysis: While existing Lakehouse formats offer varying degrees of ACID support and schema evolution capabilities, challenges remain in ensuring data consistency and handling schema changes gracefully. For example:\nDelta Lake and Apache Iceberg support ACID transactions but can struggle with concurrent schema changes and data consistency in distributed environments.\nApache Hudi provides ACID guarantees but requires careful table versions and metadata management.\nUsers must often manually handle schema evolution and data consistency, which can be complex and error-prone.\nBuilt on open standards, it supports open data formats, interoperates with various query engines, and allows custom extensions. Users can define their desired freshness, cost, correctness, and index trade-offs. LakeDB will handle these requirements out of the box while providing flexibility to extend the system as needed.\nGap Analysis: Some Lakehouse solutions can lead to vendor lock-in and limited interoperability, hindering innovation and flexibility. For example:\nDelta Lake is tightly integrated with Databricks, which can limit interoperability with other platforms.\nApache Iceberg and Apache Hudi are more open but require significant effort to integrate with different query engines and tools.\nUsers often face challenges extending these systems to meet specific use cases, limiting their innovation ability.\nPerformance: By tightly integrating all data management functions, leveraging advanced indexing and materialized views, and dynamically optimizing data layout, LakeDB can achieve significantly better query performance than current Lakehouse architectures, approaching the speed of specialized OLAP systems. Users can define their performance requirements, and LakeDB will automatically optimize to meet these needs.\nSimplicity: A unified system is inherently simpler to manage and operate than a collection of disparate tools. Users can define their desired trade-offs, and LakeDB will handle the rest, reducing the need for manual intervention and configuration.\nEfficiency: LakeDB's integrated approach and intelligent automation can optimize resource utilization and reduce operational overhead. Users can define their cost requirements, and LakeDB will automatically optimize resource usage to meet these constraints.\nFlexibility: Granular control over data freshness and extensive configuration options empower users to adapt the system to diverse workloads. Users can define their desired trade-offs, and LakeDB will automatically adjust to meet these requirements.\nReal-Time Capabilities: Advanced indexing, particularly techniques from Apache Pinot, enables LakeDB to support real-time analytical workloads with low latency and high throughput. Users can define their real-time requirements, and LakeDB will automatically optimize to meet these needs.\nApache Iceberg, Apache Hudi, and Delta Lake have been instrumental in shaping the data Lakehouse paradigm. However, Google's Napa presents a compelling vision for the future \u2013 a vision further enhanced by incorporating ideas from systems like Apache Pinot. This vision culminates in LakeDB, a more integrated, performant, flexible, and intelligent data management system.\nNapa: Powering Scalable Data Warehousing with Robust Query Performance at Google\nLink: Google Research - Napa Paper\nSummary: This paper details the design and implementation of Napa, Google's analytical data management system. The system emphasizes materialized views, robust query performance, and flexibility in balancing freshness, cost, and correctness.\nF1 Query: Declarative Querying at Scale\nLink: F1 Query Paper\nSummary: This paper describes F1 Query, Google's federated query processing platform integrated with Napa for query optimization and execution.\nMesa: Geo-Replicated, Near Real-Time, Scalable Data Warehousing\nLink: Mesa Paper\nSummary: Mesa is Google's distributed data warehousing system. It is similar to Napa in terms of scalability and real-time data ingestion.\nStarTree Indexes in Apache Pinot\nLink: StarTree Indexes in Apache Pinot\nSummary: This blog series explains how StarTree indexes in Apache Pinot provide intelligent materialized views, significantly improving query performance for high-cardinality data.\nChange Data Capture with Apache Pinot\nLink: CDC with Apache Pinot\nSummary: This article discusses how Apache Pinot handles Change Data Capture (CDC) and integrates it with real-time data streams, enabling efficient updates and querying.\nApache Hudi vs. Delta Lake vs. Apache Iceberg\nLink: Hudi vs. Delta vs Iceberg\nSummary: This article compares the three leading data Lakehouse formats, highlighting their strengths and use cases.\nUnderstanding Apache Hudi\u2019s consistency model\nLink: Hudi consistency Model\nSummary: This article discusses the Hudi consistency model. The blog contains consistency models for all table formats.\nDelta Lake: Unpacking the Transaction Log\nLink: Delta Lake Transaction Log\nSummary: This resource explains how Delta Lake's transaction log works, enabling ACID transactions and time travel.\nApache Iceberg: High-Performance Table Format\nLink: Iceberg Documentation\nSummary: This is the official documentation for Apache Iceberg, detailing its features, such as schema evolution, partitioning, and integration with query engines.\nOptimizing Queries Using Materialized Views: A Practical, Scalable Solution\nLink: Materialized Views Paper\nSummary: This foundational paper discusses algorithms for query optimization using materialized views, which are central to Napa's design.\nCalcite: Materialized Views\nLink: Calcite Materialized Views\nSummary: This documentation explains how Apache Calcite implements materialized views and query rewriting, which is relevant to LakeDB's vision of intelligent optimization.\nDebezium: Change Data Capture\nLink: Debezium Documentation\nSummary: Debezium is a CDC tool that integrates with systems like Apache Pinot, enabling real-time data ingestion and updates.\nApache Pinot: Real-Time OLAP\nLink: Apache Pinot Documentation\nSummary: The official documentation for Apache Pinot covers its architecture, indexing, and real-time analytics capabilities.\nProgressive Partitioning for Parallelized Query Execution in Google's Napa\nLink: Progressive Partitioning Paper\nSummary: This paper discusses Napa's partitioning strategies for efficient query execution.\nKrypton: Real-Time Serving and Analytical SQL Engine at ByteDance\nLink: Krypton Paper\nSummary: Krypton is a cloud-native HSAP system that offers real-time analytics, similar to Napa and LakeDB.\nDelta Lake Example: Setup and Initialization\nLink: Delta Lake Example\nSummary: A practical guide to setting up and using Delta Lake for ACID transactions and data versioning.\nApache Iceberg Example: Setup and Initialization\nLink: Iceberg Example\nSummary: A step-by-step guide to creating and querying Iceberg tables.\nApache Hudi Example: Setup and Initialization\nLink: Hudi Example\nSummary: A tutorial on setting up Apache Hudi for upserts, deletes, and incremental processing.\nCalcite Materialized Views Explained\nLink: Calcite Materialized Views Blog\nSummary: A detailed blog post explaining Calcite's materialized view implementation and query rewriting algorithms.\nStarTree Indexes in Apache Pinot: Part 1\nLink: StarTree Indexes Blog\nSummary: A comprehensive blog series on StarTree indexes and their impact on query performance."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-204", "title": "Data Engineering Weekly", "content": "Astro is the fully-managed DataOps platform powered by Apache Airflow. With Astro, you can build, run, and observe your data pipelines in one place, ensuring your mission critical data is delivered on time.\nTry Astro Free \u2192\nThe combination of reasoning, logic, and access to external information that are all connected to a Generative AI model invokes the concept of an agent.\nThe white paper explores the definition of agents, various components in agents, and an example code.\nhttps://www.kaggle.com/whitepaper-agents\nAI Agent is a fast-moving discipline, and as with any rapid development discipline, it is hard to keep track of the progress. The author did an excellent job summarizing the AI Agents from various recent blogs. \nhttps://jack-vanlightly.com/blog/2025/1/16/ai-agents-in-2025\nWith structured data, we try to understand business and predict its trajectory. The LLM and growing focus on processing unstructured data allow businesses to automate operations with data. \nThe author captures the current landscape of processing unstructured data landscape.\nhttps://www.generativevalue.com/p/the-unstructured-data-landscape\nDownload this free 130+ page eBook for everything a data engineer needs to know to take their DAG writing skills to the next level (+ plenty of example code).\u2192 Understand the building blocks DAGs, combine them in complex pipelines, and schedule your DAG to run exactly when you want it to\u2192 Write DAGs that adapt to your data at runtime and set up alerts and notifications\u2192 Scale your Airflow environment\u2192 Systematically test and debug Airflow DAGsBy the end of the DAG guide, you'll know how to create and manage reliable, complex DAGs using advanced Airflow features such as those in the screenshot \ud83d\udcf8.\nhttps://www.astronomer.io/ebooks/dags-definitive-guide/\nOne of the significance of the foundation model is that it makes a few traditional machine learning functions much simpler. Ramp writes about adopting RAG to classify the internal industrial code to NAICS codes.\nhttps://engineering.ramp.com/industry_classification\neBay writes about the first hybrid foundation model usage case with an in-house hosted Llama model. There are two reasons why eBay went with Llama, which is more or less true for many infrastructure components at scale.\nThese services come with considerable costs, making them impractical for businesses like eBay that need fine-tuned, scalable, and cost-effective solutions. \nAdditionally, relying on third-party models introduces data security risks and limits fine-tuning capabilities based on proprietary data.\nhttps://innovation.ebayinc.com/tech/features/scaling-large-language-models-for-e-commerce-the-development-of-a-llama-based-customized-llm-for-e-commerce/\nSearching over encrypted data is vital when handling sensitive data. I\u2019ve worked previously on the Bring-Your-Own-Key model search engine. The system design for encrypted search is something new and exciting to read. \nhttps://www.wix.engineering/post/the-art-of-secure-search-how-wix-mastered-pii-data-in-vespa-search-engine\nLinkedIn writes about its unique use case for ownership change in asset management and challenges with the pipeline's consistency and visibility. One additional detail that would be helpful is whether the pipeline runs a bulk data movement of these ownership changes or ownership changes via the mapping table. \nhttps://www.linkedin.com/blog/engineering/data-streaming-processing/improving-recruiting-efficiency-with-hybrid-bulk-data-processing-framework\nThe article discusses the challenges of managing increasingly complex machine learning (ML) pipelines and compares three popular orchestration tools: ZenML, Flyte, and Metaflow. The article highlights the benefits of using ML workflow and pipeline orchestration tools, such as task automation, scalability, dependency management, pipeline monitoring, reproducibility, and consistency across environments. \nEach tool is analyzed based on its architecture, key features, and community feedback. The author recommends ZenML for its modularity and extensibility, Flyte for its scalability and reliability, and Metaflow for its simplicity and ease of use. \nhttps://mlops.community/zenml-vs-flyte-vs-metaflow/\nThe article discusses the often-overlooked bottleneck in query processing: the inefficient transfer of results from the source to the client, particularly due to serialization and deserialization (ser/de) overheads. It highlights how the Apache Arrow format addresses this issue through five key attributes: \ncolumnar nature\nself-describing and type-safe design\nzero-copy capability\nstreaming support\nuniversality across various programming languages and platforms \nhttps://arrow.apache.org/blog/2025/01/10/arrow-result-transfer/\nUnderstanding time (handling time from a technical standpoint \ud83d\ude00) is critical in distributed system design and data pipeline management. The author's five-part series highlights time's evolution from a simple ordering mechanism to a tool for coordination and performance optimization. \nThe author emphasizes that synchronized time serves as an alignment mechanism, enabling consistent snapshots, conflict detection, and fencing to prevent stale operations. The article also discusses the increasing use of time-based speculation for performance gains, the importance of monotonic clocks and hybrid logical clocks (HLCs) for correctness, and emerging trends such as the growing adoption of synchronized clocks and time-based speculation in distributed databases.\nhttps://muratbuffalo.blogspot.com/2025/01/use-of-time-in-distributed-databases_14.html\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/the-emerging-role-of-ai-data-engineers", "title": "Data Engineering Weekly", "content": "How does a chatbot seamlessly interpret your questions? How does a self-driving car understand a chaotic street scene? The answer lies in unstructured data processing\u2014a field that powers modern artificial intelligence (AI) systems. Unlike neatly organized rows and columns in spreadsheets, unstructured data\u2014such as text, images, videos, and audio\u2014requires advanced processing techniques to derive meaningful insights.\nAs large language models (LLMs) and AI agents become indispensable in everything from customer service to autonomous vehicles, the ability to manage, analyze, and optimize unstructured data has become a strategic imperative. To address these challenges, AI Data Engineers have emerged as key players, designing scalable data workflows that fuel the next generation of AI systems. Their role is not just important; it is essential.\nUnstructured data, by its very nature, lacks a predefined structure or format, making it one of the most complex forms of data to manage. Social media posts, scanned legal documents, sensor data from IoT devices, and video recordings are all examples of unstructured data that require specialized techniques to process and analyze effectively.\nEach type of unstructured data\u2014text, images, videos, or audio\u2014presents unique challenges. For example:\nText Data: Natural Language Processing (NLP) techniques are required to handle the subtleties of human language, such as slang, abbreviations, or incomplete sentences.\nImages and Videos: Computer vision algorithms must analyze visual content and deal with noisy, blurry, or mislabeled datasets.\nAudio: Processing speech or environmental sounds requires speech recognition tools and audio analysis techniques.\nAdding to this complexity is the sheer volume of data generated daily. Billions of social media posts, hours of video content, and terabytes of sensor data are produced daily. Traditional data systems cannot keep pace with this scale, necessitating distributed and scalable frameworks capable of handling high-performance data workflows.\nExtracting actionable insights from unstructured data is computationally expensive. Tasks like Optical Character Recognition (OCR), which converts text in images to machine-readable formats, and natural language processing (NLP), which enables AI to understand and generate human language, require significant hardware resources, such as GPUs or TPUs. Adopting an intelligent scheduling engine to process data using GPU and CPU, adjusting to the intensity of the workload to balance cost and efficiency, presents a unique challenge.\nUnstructured data often contains sensitive information, such as personal details in emails or facial data in surveillance footage. Mishandling this data exposes organizations to significant risks, including regulatory fines and reputational damage. To safeguard sensitive information, compliance with frameworks like GDPR and HIPAA requires encryption, access control, and anonymization techniques.\nAI Data Engineers play a pivotal role in bridging the gap between traditional data engineering and the specialized needs of AI workflows. They are responsible for designing, implementing, and maintaining robust, scalable data pipelines that transform raw unstructured data\u2014text, images, videos, and more\u2014into high-quality, AI-ready datasets.\nTheir expertise lies in enabling seamless data integration into machine learning models, ensuring AI systems perform efficiently and effectively. Beyond technical tasks, AI Data Engineers uphold ethical standards and privacy requirements, making their contributions vital to building trustworthy AI systems.\nTo understand the significance of the role, let\u2019s break down the responsibilities of AI Data Engineers into key categories:\nDesign and implement pipelines to preprocess diverse data types, including text, images, videos, and tabular data.\nUse tools like Python, Apache Spark, and Ray to handle tasks like tokenization, normalization, feature extraction, and embedding generation.\nAddress challenges like noisy data, incomplete records, and mislabeled inputs to ensure high-quality datasets.\nLeverage generative AI models to create synthetic data, augmenting existing datasets for improved model training.\nDevelop data augmentation strategies to introduce variations that enhance the robustness and accuracy of AI models.\nValidate synthetic data to ensure it is representative, diverse, and suitable for the intended AI applications.\nImplement techniques to detect and resolve data integrity issues such as missing values, outliers, or duplicates.\nIdentify and mitigate biases within datasets, ensuring fair and ethical AI outcomes.\nBuild distributed data workflows to handle large-scale datasets using tools like Apache Spark and Ray.\nOptimize real-time and batch processing pipelines, ensuring efficiency and minimizing latency.\nAlign data workflows with legal and regulatory requirements such as GDPR, HIPAA, and CCPA.\nEmploy privacy-preserving techniques like data masking, encryption, and pseudonymization to protect sensitive information.\nAdvocate for ethical practices in synthetic data generation and AI application development.\nSeamlessly integrate preprocessed data into machine learning frameworks such as TensorFlow, PyTorch, or Hugging Face.\nDevelop modular, reusable components for end-to-end AI pipelines.\nEstablish monitoring solutions to ensure consistent data pipeline performance.\nProactively identify and resolve bottlenecks or inefficiencies in the pipeline to maintain reliability.\nPerforming the above responsibilities requires a multifaceted skill set that blends technical expertise, analytical thinking, and ethical awareness. Key skills include:\nProficiency in Python, SQL, and data engineering frameworks like Airflow, Spark, and Ray.\nExperience with vector databases (e.g., FAISS, Milvus) and embedding libraries for AI workflows.\nStrong knowledge of AI/ML frameworks like TensorFlow, PyTorch, and Hugging Face.\nFamiliarity with generative models like GPT-4, GANs, diffusion models, and synthetic data techniques.\nDeep understanding of ETL processes, distributed data systems, and pipeline optimization.\nExperience preprocessing multimodal data for AI applications, including text (NLP), images (computer vision), and video.\nAbility to assess and address preprocessing needs tailored to specific AI applications.\nExpertise in identifying inefficiencies and implementing solutions for high-performance workflows.\nFamiliarity with data privacy laws and compliance requirements (e.g., GDPR, HIPAA).\nCommitment to promoting fairness and transparency in AI data workflows.\nAs organizations increasingly rely on AI-driven technologies, the role of AI Data Engineers has evolved into a critical enabler of innovation and efficiency. From addressing the challenges of unstructured data to ensuring ethical and scalable workflows, these professionals are the architects of robust, intelligent systems. By hiring skilled AI Data Engineers, companies can unlock the full potential of their data, driving competitive advantage in a technology-driven world.\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-203", "title": "Data Engineering Weekly", "content": "Astro is the fully-managed DataOps platform powered by Apache Airflow. With Astro, you can build, run, and observe your data pipelines in one place, ensuring your mission critical data is delivered on time.\nTry Astro Free \u2192\nJetBrains published its annual developer survey, and there is tons of insight on the developer adoption of various programming languages. TypeScript, Python, and Rust are the fastest-growing programming languages, whereas others hold their position as it is. \nhttps://www.jetbrains.com/lp/devecosystem-2024/\n\u201cAgents all the way\u201d is a popular prediction for 2025. This blog captures the current state of Agent adoption, emerging software engineering roles, and the use case category.\nhttps://yougot.us/news/2024-12-28-AI-Agents-Survey-Results\nA comprehensive overview of what an agent is, a description of the environment and the tools, the type of tools and their impact, and so on. If you are starting agent development, this is a must-read article. \nhttps://huyenchip.com//2025/01/07/agents.html\nThe scalability of Airflow is why data teams at companies like Uber, Ford, and LinkedIn choose it to power their data ops. Learn practical strategies to optimize Airflow performance and streamline operations:- Fine-tune configurations to enhance workflow efficiency- Automate Airflow deployments and manage users seamlessly- Monitor system health with advanced observability tools and alertsJoin this live session and learn how to scale Airflow efficiently.\nSave Your Spot \u2192\nThe paper comes at an interesting time when agents took center stage. It argues that agents alone are insufficient for widespread success due to limited value generation, a lack of adaptable personalization, trustworthiness concerns, social unacceptability, and a lack of standardization. To address these shortcomings, the authors propose a new ecosystem that includes Sims (user representations) and Assistants (user-facing programs) alongside agents, facilitating enhanced privacy, personalization, and interaction. \nhttps://arxiv.org/pdf/2412.16241\nThe conferences are a great way to interact and explore new ideas. I always like a good overview of the conference's learning. I wish to attend QCon, but the price is too high. I\u2019m glad to see an excellent overview of 2024 QCon. \nhttps://www.infoq.com/news/2024/12/takeaways-qcon-infoq-dev-summits/\nNetflix publishes the third edition of its internal analytical summit talks. The third part focuses on Dashboard Design Tips, Learnings from Deploying an Analytics API at Netflix, and the guest talk from Benn Stancil. You can read Part 1 here and Part 2 here. \nhttps://netflixtechblog.com/part-3-a-survey-of-analytics-engineering-work-at-netflix-e67f0aa82183\nUber writes about a hybrid Spark and Ray system to optimize the budget allocation of its ride-sharing business. Facing performance bottlenecks with their existing Spark-based system, Uber leveraged Ray's Python parallel processing capabilities for significant speed improvements (up to 40x) in their optimization algorithms. \nhttps://www.uber.com/blog/how-uber-uses-ray-to-optimize-the-rides-business/\nCanva writes about its migration from AWS Data Firehose to Snowpipe Streaming, driven by the need to reduce costs, which consume nearly 50% of its data platform budget. Despite implementation challenges with Kinesis integration, the switch proved successful, resulting in a 45% reduction in cloud spending, enhanced query performance, and eliminated intermediary S3 storage requirements.\nhttps://www.canva.dev/blog/engineering/snowpipe-streaming/\ndata processing pipelines haven't kept pace with the rapid advancement of AI models\nThe article highlights the growing importance of preprocessing data pipelines, but the pipeline processing techniques do not match the demand. Generative AI demands the processing of vast amounts of diverse, unstructured data (e.g., meeting recordings and videos), which contrasts with traditional SQL-centric systems for structured data. The fundamental shift from traditional SQL-centric to AI-centric data processing further widened the efficiency gap.\nhttps://gradientflow.substack.com/p/paradigm-shifts-in-data-processing\nLast week, I had an\u00a0interesting conversation\u00a0with Vinoth Chandar, founder and CEO of Onehouse. Vinoth made excellent points on the challenges of adopting an open table format and the need for the format to move towards a LakeDB. Apache XTable and Delta Lake UniForm are attempting to provide interoperability among table formats, but is that a reality? The author rightly questions if cross-publishing has a long-term benefit, as mere metadata copying won\u2019t provide the exact performance.\u00a0\nhttps://jack-vanlightly.com/blog/2024/9/26/table-format-interoperability-future-or-fantasy\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/the-future-of-data-lakehouses-a-fireside", "title": "Data Engineering Weekly", "content": "What if your data lake could do more than just store information\u2014what if it could think like a database? As data lakehouses evolve, they transform how enterprises manage, store, and analyze their data. To explore this future, I recently sat down with Vinoth Chandar, founder of Onehouse and creator of Apache Hudi, for a fireside chat about the trends shaping the data landscape. Together, we discussed how Hudi drives innovation, the state of open standards, and what lies ahead for data lakehouses in 2025 and beyond.\nDrawing from his experience at industry giants like Uber and LinkedIn, Vinoth shared how Apache Hudi has redefined data management by introducing database-like capabilities into data lakes. This foundational concept addresses a key challenge for enterprises: building scalable, high-performing data platforms that can support the complexity of modern data ecosystems.\nHudi bridges the gap between traditional databases and data lakes by enabling transactional updates, data versioning, and time travel. This hybrid approach empowers enterprises to efficiently handle massive datasets while maintaining flexibility and reducing operational overhead.\nThe release of Apache Hudi 1.0 represented a significant leap forward in data lakehouse technology. Vinoth walked us through some of the version's most impactful features, including:\n\u2022 Partial Update Encoding: This innovation allows incremental updates to data without rewriting entire datasets, significantly boosting efficiency.\n\u2022 Enhanced Indexing: Improved indexing mechanisms streamline query performance, making data retrieval faster and more cost-effective.\n\u2022 Incremental Pipelines: Hudi's ability to process only new or updated data enables real-time analytics and reduces business processing overhead.\nThese advancements address enterprises' real-world challenges, such as maintaining fresh, up-to-date datasets and optimizing for high-throughput scenarios.\nThe industry is abuzz with discussions about open table formats and the so-called \"table format wars\" involving Apache Iceberg, Delta Lake, and Apache Hudi. While some argue that specific formats have gained the upper hand, Vinoth emphasized the importance of evaluating technologies based on specific use cases rather than following trends.\nApache Hudi's unique differentiators, such as its ability to handle complex data operations asynchronously, set it apart. For example, Hudi excels in scenarios requiring large-scale data ingestion with transactional guarantees, a feature critical for the finance, healthcare, and retail industries.\nVinoth also stressed the need for solutions that ensure longevity and adaptability. \"What works today may not solve tomorrow's challenges,\" he noted, underscoring the importance of flexibility in selecting tools.\nA recurring theme in our discussion was the significance of open standards in fostering innovation and avoiding vendor lock-in. While open table formats claim to provide flexibility, Vinoth encouraged a deeper examination of how \"open\" some implementations truly are.\nVinoth pointed out that balancing openness with proprietary advancements remains a complex challenge. Hudi's approach has prioritized community-driven development while staying nimble enough to address enterprise-specific needs. This balance ensures Hudi remains a trusted choice for businesses seeking innovation and stability in their data platforms.\nLooking ahead, Vinoth shared an optimistic vision for the future of data lakehouses. He foresees greater collaboration within the open-source community, leading to simpler, more user-friendly data lakehouse solutions.\nVinoth explained that one of today's biggest challenges is managing data across multiple engines while keeping operational overheads low. He expressed hope for advancements that abstract away these complexities, empowering organizations to focus on deriving insights rather than wrestling with infrastructure.\nHudi, with its robust community and technical innovation, is well-positioned to lead this charge. By continuing to refine its core capabilities and foster industry-wide collaboration, it aims to shape the future of data management.\nOur conversation with Vinoth Chandar offered valuable insights into the dynamic trajectory of data lakehouses and the technologies driving this evolution. Apache Hudi's innovations stand at the forefront of this movement, addressing real-world enterprise challenges and setting new standards for scalability, efficiency, and flexibility.\nAs we move into 2025, one thing is clear: the path forward will require collaboration, innovation, and a commitment to openness. We invite you to join the conversation\u2014what challenges or opportunities do you see in the world of data lakehouses? Please share your thoughts in the comments or connect with us on social media.\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-202", "title": "Data Engineering Weekly", "content": "Run Airflow without the hassle and management complexity. Take Astro (the fully managed Airflow solution) for a test drive today and unlock a suite of features designed to simplify, optimize, and scale your data pipelines. For a limited time, new signups will receive a complimentary Airflow Fundamentals Certification exam (normally $150).\nTry For Free \u2192\nHappy New Year 2025 to all our readers. We are back after a short end-of-year break. We published a series of blogs about the\u00a0state of data engineering in 2024\u00a0and our predictions for 2025 in data\u00a0here\u00a0and\u00a0here.\u00a0\nOne of DEW\u2019s predictions is LakeDB, where we predicted the LakeHouse Formats would move from mere standards to fully functional databases. I\u2019m excited to start this year with a conversation with Vinoth Chandar - Founder and CEO of OneHouse and PMC chair of Apache Hudi. I\u2019m curious to know his thought process in an environment where everyone declared Apache Iceberg win the future. \nJoin me in a conversation with Vinoth on the future of LakeHouses. \nhttps://www.onehouse.ai/webinar/bridging-the-gap-a-database-experience-on-the-data-lake\nBut VCs want that money back and their trap full, so these companies turn out a hosted service for their DBMSs on the cloud. But the cloud makes open-source DBMSs a tricky business. If a system becomes too popular, then a cloud vendor (like Amazon) slaps it up as a service and makes more money than the company paying for the development of the software.\nThis is an excellent summarization of innovators\u2019 dilemma in the tech industry for all the systems built on top of the cloud providers. S3Table is the latest example of cloud vendors offering popular services. \nhttps://www.cs.cmu.edu/~pavlo/blog/2025/01/2024-databases-retrospective.html\nIf you are looking to explore AI Engineering in 2025, the author compiled an excellent list of papers to read through to understand Frontier LLMs, Benchmarks and Evals, Prompting, ICL & Chain of Thought, Retrieval Augmented Generation, Agents, Code Generation, Vision, Voice, Image/Video Diffusion and Finetuning.\nhttps://www.latent.space/p/2025-papers\nAgents, all the way, is one of the hot predictions of 2025. Anthropic provides an excellent overview of building effective agents and has published a cookbook on agent-building patterns on\u00a0Github.\n https://www.anthropic.com/research/building-effective-agents\nDownload this eBook for 7 full example DAG patterns for different ETL and ETL scenarios.\nIncluding:\u2192 ETL pattern DAG using Dynamic Task Mapping\u2192 ELT pattern DAG using explicit external storage\u2192 ETL DAG using XCom\ud83d\udd17 Access the GitHub repository containing a fully functional Airflow project with all 7 DAGs configured to run out-of-the-box\nGet Ebook \u2192\nEffective prompts are often labor-intensive and domain-specific, requiring significant expertise and time. PromptWizard aims to automate and simplify this process by introducing a framework that utilizes a self-evolving, self-adaptive mechanism. Microsoft Research narrates how PromptWizard optimizes instructions and in-context learning examples, enabling continuous improvement through feedback and synthesis for tailored task optimization.\nGithub: https://github.com/microsoft/PromptWizard\nhttps://www.microsoft.com/en-us/research/blog/promptwizard-the-future-of-prompt-optimization-through-feedback-driven-self-evolving-prompts/\nTraditionally, assessing search relevance required extensive human judgment, which was both time-consuming and resource-intensive. LinkedIn attempts to solve this by curating a golden dataset for LLM to train on search relevance assessment using typeahead functionality. The case study is a classic example of AI Engineering becoming a basic skill similar to SQL or Data Structures in the industry. \nhttps://www.linkedin.com/blog/engineering/ai/automated-genai-driven-search-quality-evaluation\nNetflix wrote a two-part summary of its internal analytical engineering summit. The blogs give a sneak peek into Netflix's internal analytical engineering practices, including creating its semantic layer data junction, democratizing analytics with a Slack Bot analytical framework, foundation data platform, and analytical models to measure signups, user journey mapping, and user acquisition. \nPart 1: https://netflixtechblog.com/part-1-a-survey-of-analytics-engineering-work-at-netflix-d761cfd551ee\nPart 2: https://netflixtechblog.com/part-2-a-survey-of-analytics-engineering-work-at-netflix-4f1f53b4ab0f\nZillow introduces Zkafka, a Go library designed to simplify and enhance Kafka message consumption. Traditional Kafka processing faces challenges like head-of-line blocking and inefficient scaling tied to physical partitions. Zkafka addresses these with virtual partitions, which enable concurrent processing within a single Kafka partition while preserving message ordering. By leveraging lightweight goroutines for virtual partitions and implementing key-based routing and sequential offset commits, Zkafka ensures both scalability and reliability. \nhttps://www.zillow.com/tech/zkafka-by-zillow-go-library-for-simplifying-kafka-consumption/\nKafka is undeniably one of the most robust open-source systems. Its wide adoption powers many mission-critical systems. In many system design interviews, candidates mention Kafka as a default component. \nIs Kafka the appropriate component for an analytical engineer? Real-time analytics often requires an OLAP system or stream processing engines like Apache Flink. Fluss is an exciting open-source system I\u2019m looking forward to in 2025 that is trying to address these challenges from Apache Kafka. \nhttps://www.alibabacloud.com/blog/why-fluss-top-4-challenges-of-using-kafka-for-real-time-analytics_601879\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/agents-of-change-navigating-2025", "title": "Data Engineering Weekly", "content": "As we approach the new year, it's time to gaze into the crystal ball and ponder the future. In this post, we delve into predictions for 2025, focusing on the transformative role of AI agents, workforce dynamics, and data platforms. Join Ananth Packkildurai, Ashwin Ashish, and Rajesh as they unravel the future and guide us through the fascinating changes ahead.\nThe Rise of AI Agents\n\"Agents all the way,\" as Rajesh aptly puts it, will likely be the anthem for 2025. The evolution from rudimentary AI models to sophisticated agents capable of decision-making and task execution is remarkable. These agents are not merely about knowledge retrieval but are evolving to include action, planning, and reasoning, fundamentally altering how tasks are approached in an enterprise setting.\nThe key to leveraging these agents is starting small but thinking big. Enterprises are encouraged to experiment with AI, build numerous small-scale agents, learn from each, and expand their agent infrastructure over time. Investment in an Agent Management System (AMS) is crucial, as it offers a framework for scaling, monitoring, and refining AI agents.\nWorkforce Dynamics in an Agent-Driven World\nThe introduction of AI agents naturally raises questions about the future workforce. How will roles change in a world where agents are omnipresent? Ananth shares his journey, highlighting how AI tools have reshaped his approach to coding. In this new era, AI increases productivity but demands a shift in skillsets.\nFor professionals across domains\u2014data engineers, AI engineers, and data scientists\u2014the message is clear: adapt or become obsolete. AI engineers, in particular, will find their skills in high demand as they navigate managing and optimizing agents to ensure reliability within enterprise systems.\nData engineers, too, face an evolving landscape with a heightened focus on unstructured data. The challenge lies in harnessing this data to drive new insights and efficiencies. Despite fears of automation replacing human roles, there is a consensus that humans will remain integral, particularly in guiding the development and application of AI systems.\nData Platforms: The New Frontier\n2024 witnessed significant advancements in data platforms, setting the stage for even more exciting developments in 2025. The shift towards intelligent data platforms will continue, with enterprises seeking to seamlessly integrate structured and unstructured data, ensuring quality, governance, and trustworthiness.\nThe debate around table formats and Lakehouse architectures continues, but the focus is on unifying data ecosystems to enable AI-driven insights. Data quality and privacy remain at the forefront, especially as AI applications demand fresh and accurate data.\nMoreover, we anticipate a growing emphasis on intelligent data platforms that unify data and metadata, further supported by efforts to enhance data cataloging and lineage tracking. These platforms are instrumental in building the robust data infrastructure necessary to support the burgeoning field of AI agents.\nConclusion: Embrace, Adapt, and Innovate\nLooking ahead to 2025, enterprises and professionals must embrace the opportunities presented by AI agents, adapt their skill sets to align with new technologies, and innovate to stay ahead in a competitive landscape. As Ashwin wisely advises, focus on building a solid foundation; it's the bedrock upon which future successes will stand.\nAs we close this chapter and open another, it's a thrilling time to be part of the data and technology space\u2014to redefine what is possible and propel us toward a future teeming with promise and potential.\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/the-future-of-data-engineering-dews", "title": "Data Engineering Weekly", "content": "DEW published The State of Data Engineering in 2024: Key Insights and Trends, highlighting the key advancements in the data space in 2024. We witnessed the explosive growth of Generative AI, the maturing of data governance practices, and a renewed focus on efficiency and real-time processing. But what does 2025 hold? After analyzing emerging patterns and vibrant conversations within the data community, DEW presents our top five predictions for the trends shaping the data world in the coming year.\nNVIDIA is possibly once in a generation company to see the market cap as it becomes the largest company in the world in terms of its market cap. Google recently announced its breakthrough innovation in its quantum computing with Willow. AI-embedded PCs and devices with Neural Processing Units (NPUs) enable offline AI operations and improve data privacy. Meanwhile, innovations like Google\u2019s Edge TPU will accelerate the shift toward energy-efficient edge computing, reducing dependency on centralized cloud infrastructures. Companies such as Amazon, Google, and Microsoft are intensifying competition in the custom AI chip market, with examples like Amazon\u2019s Trainium2 chip showcasing a focus on specialized efficiency. These trends mark a decisive move towards hybrid and energy-efficient computing architectures, bridging the gap between performance, cost, and privacy in AI applications.\nAt the cutting edge, neuromorphic and quantum computing are improving significantly, opening new horizons for AI capabilities. Inspired by the human brain, Neuromorphic chips promise unparalleled energy efficiency and the ability to process unstructured data locally on devices. The advancement in computing will expand AI\u2019s role in autonomous systems and robotics. Together, these advancements in AI hardware will power breakthroughs in natural language processing, computer vision, robotics, and healthcare in 2025 and beyond.\nDomain-specific language models (LLMs) will evolve the application of AI across industries. These models, trained on sector-specific datasets, will deliver unparalleled accuracy and relevance. Industries such as healthcare, finance, legal, and manufacturing will embrace these models to tackle complex, context-rich challenges with precision. By tailoring AI capabilities to meet the nuanced demands of individual sectors, domain-specific LLMs will pave the way for a new wave of innovation, transforming workflows and decision-making processes across the enterprise landscape.\nAlongside the rise of domain language models, small language models (SLMs) will gain prominence for their cost-effectiveness and adaptability. Fine-tuned on task-specific datasets, SLMs will offer highly efficient solutions that often outperform their larger counterparts in focused applications. With reduced computational requirements and enhanced ease of deployment, SLMs will democratize access to AI, enabling organizations of all sizes to implement sophisticated language capabilities without the overhead of managing resource-intensive systems.\nAs enterprises adopt a growing number of specialized AI agents, AI orchestrators will emerge as the backbone of the AI-native data stack. These orchestrators will serve as intelligent control planes, dynamically routing tasks to the most suitable agents, synthesizing their outputs, and recommending actionable insights. Equipped with deep content understanding, multilingual processing capabilities, and support for diverse data types, they will seamlessly integrate multiple AI agents into cohesive workflows.\nSimultaneously, AI models will advance to tackle complex problems using multistep reasoning, a critical evolution beyond simple question-and-answer paradigms. These models will enhance their accurate and insightful analysis capacity by breaking intricate tasks into smaller, sequential steps. This capability will enable AI agents to perform long-tail automation jobs in coding, medicine, law, and other industries. Together, AI orchestrators and multistep reasoning will mark a new era in AI, amplifying its impact on problem-solving and decision-making across domains.\nThe demand for data insights drives a fundamental shift in how organizations approach data engineering. In 2025, we will witness the rise of a new breed of integrated development environments (IDEs) specifically designed to democratize data access and manipulation effectively. Tools like lakebyte.ai are the beginning of such a revolution.\nThese New Age Data IDEs will be characterized by:\nSeamless Integration: They will seamlessly integrate the entire data lifecycle, from data ingestion and transformation to analysis, visualization, and deployment, all within a unified environment.\nAI-Powered Assistance: They will be infused with AI capabilities, offering intelligent code completion, automated data cleaning, and smart suggestions for pipeline optimization. Imagine an IDE that not only helps you write code but also understands the semantics of your data and can suggest the best way to transform it.\nLow-Code/No-Code Interfaces: Visual, drag-and-drop interfaces will allow users with limited coding experience to build and manage data pipelines while still providing the flexibility for advanced users to write custom code when needed.\nCollaboration Features: These IDEs will facilitate seamless collaboration between data engineers, data scientists, analysts, and business users, enabling them to work together on data projects within a shared environment.\nBuilt-in Data Governance: Data quality checks, CI/ CD pipeline, the ability to run integration testing before pushing into production, access controls, and lineage tracking will be integrated directly into the development workflow, ensuring that data governance is not an afterthought.\nSupport for Multiple Data Sources and Formats: They will offer native connectors to a wide range of data sources, including databases, data lakes, streaming platforms, and cloud storage, and support various data formats, including structured, semi-structured, and unstructured data.\nCloud-Native and Scalable: These IDEs will be designed to run in the cloud, leveraging the scalability and elasticity of cloud infrastructure.\nThis democratization, facilitated by powerful and intuitive IDEs, will empower \"Citizen Data Engineers\"\u2014individuals with domain expertise who may not be traditional programmers but can now build and manage data workflows. It will break down the barriers between technical and non-technical teams, accelerating data-driven innovation. In 2025, Prompt Wrangling will become the most important skill for data engineers.\nThe lines between data lakes, data warehouses, and databases are blurring. In 2025, we predict the rise of a new paradigm: the LakeDB. This evolution of the LakeHouse concept will directly bring more robust database-like functionality to data lakes, combining the scalability and flexibility of object storage with the performance and ease of use of a traditional database. Critically, a true LakeDB will move beyond simply querying data on object storage & a table format; it will natively manage buffering, caching, indexes, and write operations, offering performance and efficiency currently found in LakeHouse.\nToday's LakeHouses often rely on external processing frameworks like Spark or Flink for data ingestion, transformation, and write operations. It introduces complexity, latency, and interoperability issues with non-uniform performance across the implementations. LakeDBs will internalize these functions, providing:\nNative Write Capabilities: LakeDBs will offer optimized write paths directly to the underlying object storage, eliminating the need for external processing engines for many common operations. The recent S3 conditional write indicates that the cloud object storage will support the LakeDB write path.\nIntelligent Buffering and Caching: LakeDBs will intelligently manage data buffering and caching, optimizing for both read and write performance.\nBuilt-in Transaction Management: LakeDBs will offer robust transaction management capabilities, leveraging technologies like S3 conditional writes and advanced metadata management to ensure data consistency and integrity.\nIntelligent query performance: LakeDB improves query efficiency through advanced indexing, query optimization, and the integration of in-process OLAP engines like DuckDB, enabling small data processing to be more efficient. The users don\u2019t need to build a query strategy for small data vs big data, where LakeDB picks the best possible strategy.\nAutomated data management features like data tiering, compaction, and other optimizations to simplify operations and reduce costs.\nSupport for Vector Search & Beyond: Built-in support for vector databases and similarity search. It will allow the pick and choose the indexing technique to apply in each column to optimize the write and read performance. The LakeHouse formats have already started this with Hudi\u2019s secondary index support and Delta\u2019s variant data type.\nWhile the LakeDB concept is still nascent, we expect to see significant innovation in this space in 2025. The existing LakeHouse format may evolve to incorporate more LakeDB-like features, and new solutions built from the ground up with this vision may emerge.\nThough there is much skepticism about data contracts and mesh, given the trend, we predict that more companies will embrace data mesh architecture, especially for cases where intra-company data exchange is required. Zero ETL and federated query architectures mostly drive this shift.\nZero ETL represents the movement toward minimizing data movement and duplication. Technologies like data virtualization, federated query engines, and data-sharing protocols will enable organizations to access and analyze data in place without the need for complex and time-consuming ETL processes.\nData sharing will become a first-class concern. Secure and efficient data-sharing protocols and platforms will enable organizations to collaborate with partners, customers, and competitors, unlocking innovation and value-creation opportunities. Expect greater adoption of standards like Delta Sharing and continued evolution.\nThese trends, combined with the Data Mesh philosophy, will empower domain teams to own their data pipelines, create and manage data products, and share data seamlessly across organizational boundaries. Data sharing becomes critical as companies increase their adoption of training LLM with their data; the data sharing model will lead to greater agility, faster time to insight, and a more decentralized and scalable approach to data management.\nThe rise of AI, the democratization of data through new IDEs, the evolution of the data engineer role, the emergence of LakeDB, and the mainstream adoption of Data Mesh principles fueled by Zero ETL and federated architectures will reshape the landscape, creating both exciting opportunities and new challenges. As we navigate this dynamic environment, one thing remains certain: the role of the data engineer has never been more critical. By embracing these trends and adapting to the evolving demands of the data-driven world, data engineers will continue to be the architects of insight, the guardians of data quality, and the driving force behind innovation. The future of data is bright, and we at Data Engineering Weekly are thrilled to be your guide on this exhilarating journey.\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/the-state-of-data-engineering-in", "title": "Data Engineering Weekly", "content": "As we reflect on 2024, the data engineering landscape has undergone significant transformations driven by technological advancements, changing business needs, and the meteoric rise of artificial intelligence. This comprehensive analysis examines the key trends and patterns that shaped data engineering practices throughout the year.\nIntegrating Generative AI (GenAI) and Large Language Models (LLMs) into data platforms emerged as the most transformative trend 2024. Organizations across industries moved beyond experimental phases to implement production-ready GenAI solutions within their data infrastructure.\nCompanies like Uber, Pinterest, and Intuit adopted sophisticated text-to-SQL interfaces, democratizing data access across their organizations. Tools like Uber\u2019s QueryGPT and Pinterest\u2019s text-to-SQL solution bridge the gap between business users and data by allowing natural language queries. These solutions go beyond basic query generation to prioritize accuracy, security, and compliance.\nLLMs are reshaping governance practices. Grab\u2019s Metasense, Uber\u2019s DataK9, and Meta\u2019s classification systems use AI to automatically categorize vast data sets, reducing manual efforts and improving accuracy. Beyond classification, organizations now use AI for automated metadata generation and data lineage tracking, creating more intelligent data infrastructures.\nStructured frameworks have become essential to ensure effective GenAI implementation. Companies like Uber and Grab have developed toolkits like the Prompt Engineering Toolkit and LLM-Kit, which focus on:\nPrompt management and version control\nSecurity and compliance guardrails\nPerformance monitoring and cost optimization\nThese frameworks address critical challenges, standardizing how LLMs integrate into data workflows while mitigating risks like high costs or compliance breaches.\nThe data lake ecosystem has matured significantly in 2024, particularly in table formats and storage technologies.\nAWS\u2019s introduction of S3 Tables marked a pivotal shift, enabling faster queries and easier management. Building on Apache Iceberg\u2019s foundation, S3 Tables integrates storage and compute layers, yielding up to 3x performance improvements through optimized query planning and compaction strategies.\nDelta Lake, Apache Hudi, and Apache Iceberg have competed fiercely in 2024, each offering unique strengths:\nDelta Lake: ACID compliance and cloud optimization\nApache Hudi: Real-time ingestion and upsert capabilities\nApache Iceberg: A wide vendor ecosystem and scalable warehouse design\nOrganizations like Flipkart and Grab have shared implementation insights, helping others navigate these options and make informed adoption decisions.\n2024 witnessed intense competition in the catalog space, highlighting the strategic importance of metadata management in modern data architectures. Databricks' acquisition of Tabular and the subsequent open-sourcing of Unity Catalog, followed by Snowflake's release of the open-source Polaris Catalog, marked a significant shift in the industry's data governance and discovery approach. Despite their \"open-source\" nature, these catalogs often remain tightly coupled with their respective commercial platforms, challenging the fundamental promise of open table formats. While vendors strive to provide optimal integrated experiences for their customers, this fragmentation increases operational complexity and business costs, making it harder to scale operations or adopt new technologies.\nIn 2024, organizations redefined search technology by adopting hybrid architectures that combine traditional keyword-based methods with advanced vector-based approaches. These systems address the increasing complexity of search queries, blending semantic understanding with precise ranking processes to deliver highly relevant results. LinkedIn, for example, implemented a two-layer search engine that integrates a retrieval layer capable of selecting thousands of candidate posts from billions of options with a multi-stage ranking layer that scores results with remarkable precision. This architecture incorporates real-time query processing and semantic search capabilities, enabling faster and more accurate content discovery. Similarly, Instacart engineered a hybrid retrieval system leveraging pgvector within PostgreSQL, striking a balance between precision and query coverage. This system enhances product discovery by combining the strengths of traditional keyword searches and vector-based techniques, optimizing search relevance for customer queries.\nOther organizations, like Grab and Figma, have pushed search technology boundaries, focusing on performance and scalability. Grab\u2019s search infrastructure employs a two-step vector similarity approach enhanced with large language model (LLM)-based ranking and natural language understanding, enabling superior accuracy and relevance for user queries. Meanwhile, Figma designed a scalable vector search infrastructure to manage high-dimensional data while minimizing computing costs. This system prioritizes real-time processing, ensuring seamless interaction for users navigating complex design workflows. Across industries, these advancements reflect a growing trend toward hybrid systems emphasizing precision, real-time capabilities, and cost-efficient scalability\u2014making them indispensable in an era of increasing data complexity.\nProcessing unstructured data at scale remains one of the biggest challenges for modern organizations, prompting innovative solutions in 2024 that blend efficiency, scalability, and accuracy. Companies like Thomson Reuters Labs have revolutionized document processing by leveraging modern formats like Parquet and Arrow, which optimize data storage and retrieval for high volumes of unstructured information. These innovations enable faster document parsing and reduce processing overhead, ensuring pipelines can scale without performance bottlenecks. Thomson Reuters Labs also adopted advanced parallel processing techniques to streamline workflows, drastically improving system efficiency. By focusing on scalable document understanding pipelines, the company demonstrated how to manage large-scale unstructured data efficiently while reducing the computational burden.\nIn addition to file format optimizations, organizations have increasingly adopted machine learning and vision-language models to bypass traditional methods like OCR (optical character recognition). Tools like ColPali now leverage these models to analyze visual documents directly, improving document similarity search accuracy and enabling faster processing of data that combines text and images. These advancements are particularly transformative for industries dealing with large volumes of unstructured visual content, such as legal documents, receipts, and research papers. Together, these innovations reflect a growing industry-wide focus on tools and frameworks that process unstructured data more intelligently and cost-effectively, opening new possibilities for analyzing complex, unformatted datasets at unprecedented scales.\nIn 2024, the data quality and governance landscape has transformed as organizations prioritize automation, decentralization, and proactive frameworks to ensure data reliability and compliance. While some have expressed less optimistic views about data mesh and data contracts, many real-world success stories emerge that demonstrate their value. This evolution reflects a broader shift toward scalability, agility, and enhanced governance across data ecosystems.\nExpedia has developed a Service-Level Objective (SLO) platform powered by Kafka for event streaming and PostgreSQL for efficient data storage, delivering near real-time insights through APIs and leveraging DataDog integration to identify data quality issues and minimize disruptions proactively. Similarly, Adevinta has introduced its decentralized Artemis system, which enables individual teams to define custom data quality checks, establish automated alerting mechanisms, and resolve issues proactively. This approach fosters team autonomy while maintaining high-quality standards.\nSwiggy ensures consistent mobile application event collection through its automated event verification framework, which validates data at the source, automates contract validation workflows, reduces pipeline errors early, and establishes a reliable foundation for data processing. Meanwhile, Yelp has extended dbt\u2019s generic test framework to standardize quality checks for data marts, implement domain-specific validation rules, and automate quality assurance processes, ensuring consistent testing and validation across datasets.\nMiro exemplifies the shift toward metadata-driven workflows by transitioning from Airflow code to DataHub YAML specifications. The YAML definition enables the establishment of explicit data contracts that clarify stakeholder responsibilities and simplify contract management. Uber demonstrates the transformative potential of Data Mesh. It decentralizes data ownership while maintaining governance standards through self-serve infrastructure, standardized data contracts, and automated governance processes.\nSimilarly, Notion underscores the critical role of metadata management through its data catalog initiative, which integrates metadata seamlessly into workflows by leveraging a strong data platform foundation, clear ownership models, and automated metadata collection. Meanwhile, Next Insurance strengthens governance and standardization with its DAGLint implementation, a tool that enforces best practices, optimizes workflow structures, and ensures quality assurance across development patterns.\nEmerging Best Practices in Data Governance\nAs organizations embrace innovation in data quality and governance, several best practices have emerged as industry standards:\nShift from reactive to proactive quality management\nStandardization of quality assurance processes\nAutomation of validation and monitoring\nImplementation of clear data contracts\nEnhanced metadata management\nDecentralized ownership with centralized governance\nThe relentless growth of data volumes and cloud computing costs has made cost optimization and performance tuning critical priorities in 2024. Organizations have implemented sophisticated strategies to balance performance requirements with cost efficiency.\nBy optimizing their most expensive pipelines, Medium's engineering team demonstrated significant cost savings in their Snowflake environment. Their approach focused on:\nIdentifying and eliminating redundant data processing\nOptimizing JOIN operations and table structures\nImplementing efficient incremental processing patterns\nReducing unnecessary data movements\nGreyBeam's deep dive into Snowflake's query cost attribution revealed the importance of granular cost monitoring and optimization. Their analysis provided frameworks for measuring per-query costs, enabling teams to identify and optimize expensive operations proactively.\nPayPal achieved remarkable results by leveraging Spark 3 and NVIDIA's GPUs, reducing cloud costs by up to 70% for their big data pipelines. Their implementation demonstrated how hardware acceleration could dramatically improve performance and cost efficiency.\nDoorDash's implementation of Kafka multi-tenancy showcases how architectural decisions can significantly impact infrastructure costs. Their approach includes:\nResource sharing across multiple applications\nEfficient capacity planning\nAutomated resource management\nOptimized storage utilization\nSeveral key patterns have emerged in 2024 with cost & performance optimization:\nMove from reactive to proactive cost management\nImplement granular monitoring and attribution\nConsider cost implications during architectural design\nBalance performance requirements with cost efficiency\n2024 has been a transformative year for data engineering, with AI technologies becoming mainstream, data lake solutions maturing, and efficiency and governance taking center stage. Organizations have moved beyond theory to implement real-world solutions that address complex challenges.\nWhat is ahead of us in 2025? Later this week, we will publish DEW's prediction for 2025 and beyond. Stay Tuned. \nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-201", "title": "Data Engineering Weekly", "content": "Run Airflow without the hassle and management complexity. Take Astro (the fully managed Airflow solution) for a test drive today and unlock a suite of features designed to simplify, optimize, and scale your data pipelines. For a limited time, new sign-ups will receive a complimentary Airflow Fundamentals Certification exam (normally $150).\nTry For Free \u2192\nIt is almost 10 years since the introduction of type hinting in Python. Meta published the state of the type hint usage of Python. Python is undeniably becoming the de facto language for data practitioners.\n 88% of respondents \u201cAlways\u201d or \u201cOften\u201d use Types in their Python code. The blog further gives insight into IDE usage and documentation access. \nhttps://engineering.fb.com/2024/12/09/developer-tools/typed-python-2024-survey-meta/\nDataframe is the fundamental component of many data analysis libraries. Every DataFrame implementation has its pros and cons. The coiled team published the TPC-H benchmark study on various dataframe and recommendations on when to use what. \nhttps://docs.coiled.io/blog/tpch.html\nWe can\u2019t deny the tug-of-war in the industry over Apache Iceberg. The comment on Iceber, a Hadoop of the modern data stack, surprises me. However, I 100% agree with the complex stack to maintain. Iceberg has not reduced the complexity of the data stack, and all the legacy Hadoop complexity still exists on top of Apache Iceberg. \nhttps://blog.det.life/apache-iceberg-the-hadoop-of-the-modern-data-stack-c83f63a4ebb9\nIf you're reading this... this guide is for you!\nYour comprehensive 44-page guide to one of the most common data engineering use cases on the top open-source orchestrator.\nGet Ebook \u2192\nLinkedIn writes about its internal SQL Bot, an AI-powered assistant that translates natural language into SQL queries, enabling employees to access data insights independently. The tool leverages a multi-agent system built on LangChain and LangGraph, incorporating strategies like quality table metadata, personalized retrieval, knowledge graphs, and Large Language Models (LLMs) for accurate query generation. These features, combined with a user-friendly interface and options for customization, resulted in high adoption rates and positive feedback, with 95% of users rating SQL Bot's accuracy as \"Passes\" or above.\nhttps://www.linkedin.com/blog/engineering/ai/practical-text-to-sql-for-data-analytics\nIf you control the data source's origin, always choose ProtoBuf or Avro formats rather than JSON. While widely used for data interchange, JSON presents several challenges that can impact system compatibility and efficiency. The article demonstrates various challenges in adopting JSON formats.\nNumber Handling: JSON's lack of specific number types can cause problems with large integers and special values like Infinity and NaN.\nText Encoding: Allowing unpaired surrogates in strings can lead to compatibility issues between systems.\nLack of Byte String Support: It is difficult to handle binary data efficiently.\nStreaming Limitations: JSON doesn't natively support streaming, causing issues with large datasets.\nCanonicalization and Data Loss: Canonicalizing JSON for digital signatures can lead to data loss.\nParser Diversity: The varying behavior of JSON parsers can cause interoperability problems.\nhttps://mcyoung.xyz/2024/12/10/json-sucks/\nThe author argues against storing event times solely in UTC, advocating instead for recording the user's intended time and location to avoid issues stemming from user error or timezone changes. The article highlights the complexities of storing event times due to incorrect user timezone selection and unexpected changes in international timezone rules, as exemplified by the Lebanon DST dispute and the Microsoft Exchange DST update 2007. The author recommends storing the user's intended time alongside the event's location and timezone, allowing for accurate representation and updates. \nhttps://simonwillison.net/2024/Nov/27/storing-times-for-human-events/\nEvaluating Large Language Models (LLMs) in the legal field requires a multifaceted approach, considering jurisdictional accuracy, legal terminology, contractual precision, compliance, and the currency of legal knowledge. The article discusses various methodologies, including traditional metrics like Precision, Recall, F1 Score, and BLEU, but emphasizes their limitations in capturing the nuances of legal language and context. It highlights the emerging trend of using LLMs as evaluators, multi-agent debates for nuanced assessments, and the indispensable role of subject matter experts (SMEs) for contextual understanding and risk assessment while also introducing tools like Scorecard and DeepEval to streamline the evaluation process.\nhttps://medium.com/tr-labs-ml-engineering-blog/evaluating-quality-in-large-language-models-a-comprehensive-approach-using-the-legal-industry-as-a-a014459a9454\nThe article discusses the challenge of determining confidence scores for Generative AI (GenAI) responses in financial applications, particularly in automating invoice parsing at Spotify. The authors evaluated three methods: calibrator models, logarithmic probabilities (logprobs), and majority voting, finding that majority voting, an ensemble method selecting the most common response from multiple models, demonstrated the strongest positive correlation with accuracy. Despite its simplicity, implementing majority voting required careful consideration of factors such as the number of models, weight assignment, and score calibration, ultimately leading to a reliable solution for generating confidence scores in their GenAI application.\nhttps://engineering.atspotify.com/2024/12/building-confidence-a-case-study-in-how-to-create-confidence-scores-for-genai-applications/\nDropbox writes about its semantic search implementation to enhance its functionality by understanding the meaning behind user queries and document content rather than relying on exact keyword matches. The semantic search implementation, which includes cross-lingual search capabilities, resulted in a 17% reduction in zero-results rate and a 2% increase in qualified click-through rate. Dropbox narrates the extensive evaluation process to choose the multilingual-e5-large model for its speed and quality.\nhttps://dropbox.tech/machine-learning/selecting-model-semantic-search-dropbox-ai\nIf you\u2019ve not adopted the WAP (Write-Audit-Publish) pattern in your data pipeline, I highly recommend taking a deeper look at it. At Data Engineering Weekly, we published a comprehensive guide on An Engineering Guide to Data Quality - A Data Contract Perspective. Along the line, AWS writes about implementing the WAP pattern leveraging Iceberg\u2019s branching feature. \nhttps://aws.amazon.com/blogs/big-data/build-write-audit-publish-pattern-with-apache-iceberg-branching-and-aws-glue-data-quality/\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-200", "title": "Data Engineering Weekly", "content": "Run Airflow without the hassle and management complexity. Take Astro (the fully managed Airflow solution) for a test drive today and unlock a suite of features designed to simplify, optimize, and scale your data pipelines. For a limited time, new sign-ups will receive a complimentary Airflow Fundamentals Certification exam (normally $150).\nTry For Free \u2192\nAs we publish the 200th edition of Data Engineering Weekly, I want to take a moment to express my deepest gratitude to each of you\u2014our readers, contributors, and supporters. What began as a fun initiative to share insights and learnings has grown into a thriving community of global data practitioners.\nYour engagement, feedback, and passion for data engineering have been the cornerstone of this journey. Thank you for your continued support and for making this milestone possible. Here\u2019s to the next 200 editions of learning and growing together!\n2024 marks the year that generative AI became a mission-critical imperative for the enterprise. The Menlo Ventures state of Gen AI highlights the growing impact of Gen AI in the enterprise landscape. \nhttps://menlovc.com/2024-the-state-of-generative-ai-in-the-enterprise\nAWS's announcement of the S3 Table is the biggest news in the data industry. The blog is an excellent first look at S3 tables. The idea of object storage supporting the table format is excellent progress and gives a path to many interesting innovations like this. I'm excited about the S3 table and its potential. Remember, without S3, there would be no Iceberg.  \nhttps://meltware.com/2024/12/04/s3-tables\nAWS wrote the first tech blog highlighting the S3 Table's 3X performance by improving compaction. The blog compares the performance of compacted tables vs. uncompacted tables. However, the blog does not explain the S3 table compaction strategy in depth. This talk gives an excellent summary of how AWS Glue handles Iceberg compaction.\nTalk: AWS Glue's Iceberg Optimizations: Compaction, Snapshot Expiration & Future Plans\nhttps://aws.amazon.com/blogs/storage/how-amazon-s3-tables-use-compaction-to-improve-query-performance-by-up-to-3-times/\nNo matter how well you know Airflow, DAG errors are inevitable.Don't miss:\u2192 Helpful Airflow features for debugging your DAGs\u2192 How to avoid common issues\u2192 Unit testing \u2192 Automating tests as part of a CICD workflow\nSave Your Spot \u2192\nGojek writes about GoSage, a Graph Neural Network (GNN)- based solution that detects complex fraud patterns by analyzing relationships between entities on its platform. GoSage utilizes a hierarchical attention mechanism to prioritize critical connections, capture higher-order interactions, and identify sophisticated collusion schemes. The approach helped Gojek significantly improve fraud detection capabilities, reducing false positives and enhancing platform security.\nhttps://medium.com/gojekengineering/gosage-how-we-detect-fraud-syndicates-at-gojek-with-graph-neural-networks-d4d0f4890de1\nGrab describes migrating a backend service's stream processing functionality to a new service, ensuring zero data loss or duplication while handling 20,000 reads per second. The team implemented a custom time-based switchover logic in shared code, scheduling precise cutover times for each stream to coordinate the transition between services. This strategy and thorough testing and monitoring enabled a seamless migration completed in three weeks without downtime or data inconsistencies.\nhttps://engineering.grab.com/seamless-migration\nThere is increased activity in the data industry in in-process/single-node OLAP engines like DuckDB. The article discusses the decline of in-memory database systems, which initially gained popularity in the early 2010s due to decreasing memory prices and increasing hardware reliability. Despite their initial promise of speed and simplicity, the stagnation of RAM price drops and the continued growth of data, coupled with the rise of affordable and fast SSDs, led to their diminished adoption.\nhttps://cedardb.com/blog/in_memory_dbms/\nThe industry perceives that moving to a LakeHouse format will be inexpensive compared to other preparatory databases. Though this is true to an extent, inefficient data modeling will consume more computing costs. Expedia writes about one such optimization with storage-partitioned join. \nhttps://medium.com/expedia-group-tech/turbocharge-efficiency-slash-costs-mastering-spark-iceberg-joins-with-storage-partitioned-join-03fdc1ff75c0\nTwitch describes leveraging Views with its Data Lake to enhance data agility, minimize downtime, and streamline data reprocessing. Views provide a consistent and adaptable interface, enabling actions like atomic swaps for dataset promotion, seamless column renames, and VARCHAR resizing without data restatement. This approach allows Twitch to respond to data quality issues quickly, implement two-way door changes, and improve developer workflows, ultimately supporting over 100 petabytes of data and 8,000 datasets.\nhttps://blog.twitch.tv/en/2024/12/05/views-pwn-tables-as-data-interfaces/\nMost often, we run Spark jobs on data that is small enough. The blog is an excellent read as a reminder that having this hammer for every problem is okay, except you know how to tune it. The author suggests a set of configurations to tune while running the Spark on a single node.\nhttps://luminousmen.com/post/how-to-speed-up-spark-jobs-on-small-test-datasets\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/the-chaos-of-catalogs", "title": "Data Engineering Weekly", "content": "If you\u2019ve spent time in data engineering, you\u2019ve likely found yourself staring at a convoluted architecture diagram and thinking, OMG, the catalogs! What started as a relatively simple solution for managing metadata with Apache Hive has evolved into a fragmented ecosystem of competing solutions. The proliferation of data catalogs has brought innovation and chaos to the modern data landscape.\nIn the early days of Hadoop, Hive offered an SQL interface on top of MapReduce, allowing users to write queries instead of low-level code. Hive also introduced a catalog for managing metadata\u2014storing table schemas, partition information, and data locations. The Hive metastore quickly became the backbone of Hadoop-based data systems, powering tools like HiveQL and later serving as a foundation for query engines like Presto and Spark SQL.\nThe ecosystem was relatively simple back then. The Hive Metastore was the default choice, and most tools operated on top of Hadoop-compatible storage systems. This setup allowed programmers to focus on choosing programming models rather than worrying about which catalog to choose.\nHowever, as data volumes grew and use cases became more complex, Hive Metastore\u2019s limitations became increasingly apparent.\nScalability Struggles: The reliance on relational databases like MySQL or PostgreSQL means the Hive Metastore often falters as the metadata volume scales, leading to slow query responses for growing data lakes.\nConcurrency Limits: Large-scale deployments with numerous services frequently hit connection limits, disrupting metadata access.\nOverhead in Metadata Retrieval: Retrieving metadata for highly partitioned tables introduces latency, particularly in complex schemas.\nSchema Evolution Pain Points: Adjusting schemas, particularly for massive datasets, is cumbersome and slows down iterative development cycles.\nSingle Point of Failure: Downtime in the metastore can paralyze critical data processing pipelines.\nHigh Maintenance Overhead: Managing schema migrations and version upgrades adds a considerable operational burden.\nWhile foundational, the Hive Metastore\u2019s centralized role presents challenges that can hinder the agility and scalability modern data ecosystems demand. Addressing these limitations is crucial for efficient data management and query performance.\nAs the limitations of Hive Metastore became apparent, new systems emerged to fill the gaps. Lakehouse systems like Apache Hudi, Iceberg, and Delta Lake solved many challenges with Hadoop-centric data lakes. Many enterprise companies rapidly adopt the lakehouse formats, migrating from the proprietary data warehouses.\nThe success of lakehouse formats has driven a rapid proliferation of data catalogs. Each vendor started to build their proprietary implementation of the open(?) catalogs. What was once a unified ecosystem built around the Hive Metastore has splintered into a fragmented web of incompatible systems.\nToday, in addition to the three open lakehouse formats, we have at least six or seven major catalog implementations to specific table formats or vendor ecosystems. Some vendors tightly couple their catalogs with commercial platforms, while others design them for open-source frameworks. This fragmentation poses significant challenges for companies adopting the lakehouse formats.\nThe vendor-specific implementation of the Iceberg catalog is understandable since every tool will try to provide the best-integrated experience for its customers. However, the integration will break the interoperability issues that break the very own promise of the \u201cOpen\u201d in Open table formats.\nThe fragmentation of catalogs has implications that extend far beyond the technical realm. It increases the cost and complexity of managing data for businesses, making it harder to scale operations or adopt new technologies. It represents a missed opportunity for the industry to create a cohesive and interoperable ecosystem.\nAt this point, two viable path forward options are available for companies adopting the lakehouse architecture.\nI don\u2019t think I need to write anything about this approach. We all know how it will end up.\nAdopting a federated catalog model is a more realistic and scalable approach. This strategy acknowledges that multiple catalogs will coexist in the ecosystem and focuses on enabling seamless integration. Tools like Apache XTable exemplify this approach by providing bidirectional synchronization between table formats and various catalogs, including Hive Metastore, AWS Glue, and Unity Catalog. This model ensures that businesses can leverage the strengths of different catalog solutions while maintaining interoperability and reducing operational friction.\nThe federated model also aligns with the principles of open data ecosystems. Promoting synchronization rather than rigid centralization allows companies to adopt best-in-class tools without locking themselves into a single vendor\u2019s proprietary ecosystem.\nIn our pursuit of better scalability, interoperability, and flexibility, we\u2019ve built an ecosystem teeming with endless paths, none of which lead to a unified destination. It\u2019s the chaos we\u2019ve created and it\u2019s the chaos we must now endure.\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-199", "title": "Data Engineering Weekly", "content": "\u201cAI has increased as a topic of national interest for countries across the globe, and correspondingly narratives about which countries lead in AI have become more prominent than ever\u201d\nIn the next few years, we will see countries actively trying to capture the leadership in AI, which is envisioned as the next big industrialization. It is an exciting time to be a data practitioner. \nhttps://hai.stanford.edu/news/global-ai-power-rankings-stanford-hai-tool-ranks-36-countries-ai\nWe started to see the pattern, like a software starter pack or framework to build applications quickly, and prompt engineering started to have its starter pack. Uber writes about a prompt engineering toolkit integrating essential components like prompt template management, revision control, and evaluation pipelines, enabling teams to create, iterate, and deploy well-crafted prompts with built-in safety measures and performance tracking.\nhttps://www.uber.com/blog/introducing-the-prompt-engineering-toolkit/\nIt is almost like Grab and Uber engineering shadow each other; Grab writes about its LLM kit. Grab's LLM-Kit focuses on providing a pre-configured structure and integrated tech stack specifically designed to accelerate the development of production-ready Generative AI applications. The blog emphasizes using a \"cookbook\" within the organization to provide developers with practical resources.\nhttps://engineering.grab.com/supercharging-llm-application-development-with-llm-kit\nLinkedIn rightly pointed out that the adoption of LLM is rapidly increasing among enterprises.\n(LinkedIn moved from) simple \u201cprompt in, string out\u201d solutions to crafting assistive agent experiences that offered multi-turn conversation capabilities supported by advanced contextual memory. \nAs the adoption and functionality increase, we can\u2019t deny the complexity also increases. The LinkedIn GenAI platform story is a precursor for many enterprise adoptions of Gen AI.\nhttps://www.linkedin.com/blog/engineering/generative-ai/behind-the-platform-the-journey-to-create-the-linkedin-genai-application-tech-stack\nFor years, many tried to build abstractions or alternates on top of SQL, but not much with success. Will Gen AI be the closest abstract that is potentially suited as an alternative to SQL? Maybe; I\u2019m not sure yet. It is great to see more companies write about the adoption of text2sql. Fiverr writes about such adoption and publishes the accuracy of different models.\nhttps://medium.com/fiverr-engineering/democratize-data-and-information-with-text-to-code-models-text2sql-cb6beff6f820\nOne of my favorite articles this week is Airbnb, which discusses how it segments Airbnb\u2019s supply. Segmentation brings a unique challenge as diverse datasets bring different facts at different lifecycle points. The blog also demonstrates the value of segmentation & data engineering in a business operation when it is done right. \nhttps://medium.com/airbnb-engineering/from-data-to-insights-segmenting-airbnbs-supply-c88aa2bb9399\nEvent sourcing is the core of data engineering, and Kafka is a standard tool for data pipelines. But is Kafka the ideal solution for event sourcing? The question has haunted me for almost 5 years now, and all my architecture designs always have a workaround for Kafka limitation, where it can\u2019t efficiently handle multi-tenant, head-of-the-line blocking issues. I stumbled upon this blog where the author reflected some of my thoughts on Kafka. \nhttps://dcassisi.com/2023/05/06/why-is-kafka-not-ideal-for-event-sourcing/\nThis is an excellent summarization of Netflix's usage of the WAP pattern on Apache Iceberg to ensure data quality. At DEW, we wrote extensively about\u00a0WAP patterns and data quality management, which tightly coupled the data contract concept. The author narrates how Iceberg's branching and tag feature helps build a zero-copy WAP pattern.\nhttps://blog.det.life/how-does-netflix-ensure-the-data-quality-for-thousands-of-apache-iceberg-tables-76d3ef545085\nNever use count(distinct), use approax_count_distinct as a default\nThe data team should emphasize adopting a probabilistic data structure to improve the cost efficiency and speed of the query. The author walks through how the HyperLogLog approach of approx count gives better performance at a lower cost.\nhttps://engineering.doit.com/bigquery-hll-how-we-cut-count-distinct-query-costs-by-93-using-hyperloglog-74fc369b6092\nSampling is another efficient approach to improve speed and lower costs to produce statistically significant results. The blog is elegant and well-written, discussing the sampling algorithms and their efficiency. \nhttps://blog.moertel.com/posts/2024-08-23-sampling-with-sql.html\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-198", "title": "Data Engineering Weekly", "content": "I can\u2019t believe DEW will reach almost its 200th edition soon. What I started as a fun hobby has become one of the top-rated newsletters in the data engineering industry. All credit goes to the incredible data engineering community, where people are constantly writing and sharing their knowledge with the community. DEW is one of the mediums that curates all the incredible work done by many talented data practitioners across the globe. \nWe are planning many exciting product lines to trial and launch in 2025. One of the common themes I encounter when I meet data engineering leaders worldwide is their single most common question: How can we upskill? With the high demand, We are actively working on launching Data & Gen-AI in Jan-Feb 2025\nCourse #1: Practical Generative AI: A Step-by-Step Guide\nCourse #2: Applying Functional Principles in Data Pipeline\nYou can pre-signup for the courses here\nhttps://forms.gle/wzQFmgbPPBLUvEPo8\nThe Amazon Ads team writes about their Iceberg optimization usage, which helps them accelerate the Spark workload. The blog highlights how moving from 6-character base-64 to 20-digit base-2 file distribution brings more distribution in S3 and reduces request failures. \nhttps://aws.amazon.com/blogs/storage/how-amazon-ads-uses-iceberg-optimizations-to-accelerate-their-spark-workload-on-amazon-s3/\nNetflix writes about scalable Distributed Counter abstractions for accurately counting events across its global services with millisecond latency. The system leverages a combination of an event-based storage model in its TimeSeries Abstraction and continuous background aggregation to calculate counts across millions of counters efficiently. The service offers configurable counter types optimized for various use cases with a unified Control Plane configuration.\nhttps://netflixtechblog.com/netflixs-distributed-counter-abstraction-8d0c45eb66b2\nThe modern data warehouses are good at running at scale, given the cost is not a constraint. However, cost is indeed a constraint for many; hence, understanding the efficiency of the query engine is more important than ever before. The blog is a good summary of how to use Snowflake QUERY_TAG to measure and monitor query performance. \nhttps://medium.com/snowflake/best-practices-for-using-query-tag-in-snowflake-32bfb8d4efba\nCanva leverages Snowflake extensively across the organization for data warehousing and analytics. Due to the platform's diverse user base and workloads, Canva faced challenges maintaining visibility into Snowflake usage and costs. Canva writes about its custom solution using dbt and metadata capturing to attribute costs, monitor performance, and enable data-driven decision-making, significantly enhancing its Snowflake environment management.\nhttps://www.canva.dev/blog/engineering/our-journey-to-snowflake-monitoring-mastery/\nOur quest to simplify SQL is always an adventure. BigQuery's pipe syntax seems exciting to watch, and it is an interesting approach to how it gets adopted. The blog narrates a few examples of Pipe Syntax in comparison with the SQL queries.\nhttps://medium.com/@josip.bartulovic3/write-manageable-queries-with-the-bigquery-pipe-syntax-4263efd67487\nBooking.com writes about evolving its recommendation service into a scalable, self-serve platform that empowers teams to create personalized user experiences efficiently. The Recommendation Platform (RecP) leverages a structured pipeline approach to standardize the resolution of machine learning challenges, allowing for component reusability across various use cases and enabling customers to define complex recommendation logic. \nhttps://medium.com/booking-com-development/self-serve-platform-for-scalable-ml-recommendations-358caf217a2e\nGrab has enhanced its LLM-powered data classification system, Metasense, to improve accuracy and minimize manual workload. By utilizing post-rollout data and implementing prompt engineering techniques, Grab has addressed weaknesses in the previous model, such as identifying PII data in large, mixed datasets. Integrating LangChain and LangSmith frameworks has streamlined prompt optimization and fostered collaboration among data scientists and engineers, resulting in transparent performance metrics and a more efficient solution.\nhttps://engineering.grab.com/metasense-v2\nNext Insurance writes about its internal tool, DAGLint, which uses Python's Abstract Syntax Tree (AST) to enforce consistent structures and best practices across their Airflow DAGs. By creating custom linting rules tailored to their team's needs, Next Insurance has improved its data workflows' maintainability, scalability, and quality, making it easier for engineers to collaborate and debug issues. The company has also integrated DAGLint into its CI/CD pipelines and created a monitoring dashboard to ensure ongoing compliance with its standards.\nhttps://medium.com/@snir.isl/mastering-airflow-dag-standardization-with-pythons-ast-a-deep-dive-into-linting-at-scale-1396771a9b90\nApache DataFusion recently announced that it is the fastest query engine for querying Apache Parquet format on a single machine. The blog post made me curious to understand DataFusion's internals. The blog made me wonder about the datawarehouse query pattern, which always writes once and reads many; perhaps if we can pre-load parquet metadata into the DuckDB/SQLite table, the reads will be much faster. \nI\u2019ve seen a similar work by Ben E. C. Boyter on Bloom Filters and SQLite.\nhttps://blog.haoxp.xyz/posts/parquet-to-arrow/\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-197", "title": "Data Engineering Weekly", "content": "In our most recent internal developer survey, 65% of respondents said they prefer an IDE as the primary platform to engage with generative-AI for development tasks.\nCompanies rapidly adopt Gen-AI into their developer workflow and internal knowledge management to improve productivity. Slack provides an excellent overview of this process. I predict Gen-AI will power more specialized IDE to improve developer productivity. \nhttps://slack.engineering/empowering-engineers-with-ai/\nthe author writes an excellent overview of understanding multimodel LLMs. The blog outlines two main approaches for building these models: the Unified Embedding Decoder Architecture and the Cross-Modality Attention Architecture. The article then reviews recent research papers on multimodal LLMs, including models like Llama 3.2, Molmo, NVLM, Qwen2-VL, Pixtral, MM1.5, Aria, Baichuan-Omni, Emu3, and Janus, highlighting their approaches and unique characteristics.\nhttps://sebastianraschka.com/blog/2024/understanding-multimodal-llms.html\nThe author writes a comprehensive guide on creating an effective large language model (LLM) as a judge for evaluating AI products, focusing on the process of \"Critique Shadowing.\" The author outlines a seven-step process, starting with identifying a Principal Domain Expert, generating diverse datasets, and having the expert provide detailed critiques alongside pass/fail judgments. The blog highlights the importance of iterating on this process, continuously refining the LLM judge by learning from the expert's insights, and ensuring alignment with business goals.\nCreating a LLM-as-a-Judge That Drives Business Results\nIf you haven't registered for the IMPACT Summit yet, now's the perfect time \ud83d\udd08Here\u2019s what we\u2019ve got in store:- A half-day virtual event created to elevate your 2025 data strategy- Sessions jam-packed with industry experts sharing how they're driving data and AI adoption- Practical tips and best practices from Monte Carlo customers- Opportunities to connect and network with other data professionals- Giveaways and raffles for attendees, including three All-Access subscriptions to DataExpert.io!- And more!What are you waiting for? Register for IMPACT today!\nimpactdatasummit.com\nI am always happy to read the tech history behind a tech evaluation. The blog is an excellent overview of Lakehouse formats, initiated by the Apache Hudi project from Uber, and walks through the evolution of features such as transactional guarantees and the latest unified table format.\nhttps://alirezasadeghi1.medium.com/the-history-and-evolution-of-open-table-formats-0f1b9ea10e1e\nUber is possibly one of the largest Presto clusters in operations. Uber writes about express queries, such as any Presto query, that can be finished within 2 minutes. To identify express queries, we developed a method using historical data to predict whether an upcoming query is an express query. \nhttps://www.uber.com/blog/presto-express/\nIn another optimization story, LinkedIn writes about right-sizing the spark executor memory. TIL about Deadline-aware Preemptive Job Scheduling. The blog narrates how LinkedIn achieved\nClose the gap between allocated and utilized executor memory.\nMinimize execution failures due to executor OOM errors. \nhttps://www.linkedin.com/blog/engineering/infrastructure/right-sizing-spark-executor-memory\nThe incremental data processing from dbt triggers some interesting conversations. Thanks to the dbt community, the incremental model is getting the required attention. In the second part of the blog from Atheon Analytics, the author describes some of the challenges with the dbt incremental materialization and the internal optimization framework that helped to halve the execution time.\nhttps://medium.com/@AtheonAnalytics/supercharging-dbt-vol-2-how-we-modified-dbts-incremental-materialisation-to-more-than-halve-f5def3ecbe3f\nThe story from Harness is possibly the first migration I noticed from dbt to SQLMesh. The author highlights the critical features in SQLMesh that motivate them to migrate.\ndbt\u2019s Jinja templating and non-deterministic macros\nElimination of YAML\nPlan-Apply workflow\nhttps://www.harness.io/blog/from-dbt-to-sqlmesh\nAirflow is increasing support for data-aware scheduling as it takes a more prominent role in the upcoming releases. The blog is an excellent overview of building dynamic data pipelines with Airflow datasets and pub/sub. \nhttps://medium.astrafy.io/dynamic-data-pipelines-with-airflow-datasets-and-pub-sub-d91c81d75f51\nIt is good to see a blog about Redshift unrelated to migrating. \nWe primarily used Spark jobs to read S3 data and publish it to our in-house Kafka-based Data Pipeline (which you can read more about here) to get data into both Data Lake and Redshift. \ud83e\udd14\ud83e\udd14\ud83e\udd14\ud83e\udd14\ud83e\udd14\nhttps://engineeringblog.yelp.com/2024/11/loading-data-into-redshift-with-dbt.html\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-196", "title": "Data Engineering Weekly", "content": "software is no longer simply a tool for organizing work; software becomes the worker itself, capable of understanding, executing, and improving upon traditionally human-delivered services.\nThe author narrates that multiple agents working together achieve better results than one. It is certainly an exciting phase in software development. \nhttps://foundationcapital.com/system-of-agents/\nEvery workflow software tries to incorporate Gen AI in some form. However, it is important to know which part of the workflow of the Gen AI will be powerful and where it is not. Airbnb discusses the trade-off between traditional and AI-driven workflow and narrates how it incorporated Gen AI to improve conversational experience. \nhttps://medium.com/airbnb-engineering/automation-platform-v2-improving-conversational-ai-at-airbnb-d86c9386e0cb\nOne of the industry's obvious questions is how one can evaluate chatbots using the non-deterministic nature of LLM. \nThe author narrates seven key areas to measure the performance of a chatbot.\nSearch Performance for RAG-based Chatbots\nResponse Quality\nUser Engagement Metrics\nLatency and Performance\nError Handling and Robustness\nScalability and Resource Utilization\nSecurity and Privacy Compliance\nhttps://medium.com/data-science-at-microsoft/evaluating-llm-based-chatbots-a-comprehensive-guide-to-performance-metrics-9c2388556d3e\nIf you haven't registered for the IMPACT Summit yet, now's the perfect time \ud83d\udd08Here\u2019s what we\u2019ve got in store:- A half-day virtual event created to elevate your 2025 data strategy- Sessions jam-packed with industry experts sharing how they're driving data and AI adoption- Practical tips and best practices from Monte Carlo customers- Opportunities to connect and network with other data professionals- Giveaways and raffles for attendees, including three All-Access subscriptions to DataExpert.io!- And more!What are you waiting for? Register for IMPACT today!\nimpactdatasummit.com\nThumbtack shares valuable insights from building its ML infrastructure team. The blog emphasizes the importance of starting with a clear client focus to avoid over-engineering and ensure user-centric development. Strategic pauses and adaptability are crucial for delivering timely, relevant solutions, while transparent communication builds stakeholder trust. Finally, a dedicated, flexible team drives momentum and expertise, balancing efficiency with selective commitments.\nhttps://medium.com/thumbtack-engineering/what-we-learned-building-an-ml-infrastructure-team-at-thumbtack-0a5687cf3364\nCapitalOne writes about its experience building Serverless ML on top of AWS Lambda. The challenges around memory, data size, and runtime are exciting to read. Sampling is an obvious strategy for data size, but the layered approach and dynamic inclusion of dependencies are some key techniques I learned with the case study. \nhttps://medium.com/capital-one-tech/serverless-ml-lessons-from-capital-one-4b262f848e25\nThe Gen AI Summit, consisting of a wider group of 20,000 Engineers, AI entrepreneurs, and Scientists, will host 1,000 AI teams in Austin, TX, November 7-8. Join for two days of sessions, socials, case studies, and workshop tutorials. Passes include app-brain-date networking, birds of a feature, post-event parties, etc. 60+ speakers from LinkedIn, Shopify, Amazon, Lyft, Grammarly, Mistral, et al.\nData Engineering Weekly readers get 15% discount by registering the following link,\nhttps://www.eventbrite.ca/e/5th-ann-mlops-world-and-generative-ai-world-conference-tickets-755502136227?discount=DataEngineeringWeekly\nEvery code-first data workflow grew into a UI-based or Yaml-based workflow. \nI believe this is the hard truth, as we have seen repeatedly in the industry with several internal tools from various companies. The author writes about the yaml abstraction to build Airflow DAG to simplify the DAG builder experience. \nhttps://medium.com/quintoandar-tech-blog/building-data-pipelines-effortlessly-with-a-dag-builder-for-apache-airflow-2f5f307fb781\nThe blog is an excellent summary of the path we crossed with the outbox pattern and the challenges ahead. Though the outbox pattern provides many benefits in integrating event-driven architecture, the system's complexity is undeniable. It's good to know about Dapr and restate.dev.\nhttps://www.decodable.co/blog/revisiting-the-outbox-pattern\nUber writes about the implementation details of Pinot\u2019s upsert operation, with the newer deletion support and the challenges in maintaining the in-memory hashmap that maps Record-Primary keys to Record locations. Pinot internally maintains a state of Primary Key \u2192 distinct-segment-count in the redesign. This new feature tracks how many segments a record for a given primary key exists in. This count helps to ensure data consistency when deleting and compacting segments.\nFor example, if the count is less than or equal to 1, Pinot allows the deletion of metadata on the record. Pinot then marks the validDocId as invalid, allowing for the compaction of the deleted record and ensuring the removal of records in other segments.\nhttps://www.uber.com/blog/enabling-infinite-retention-for-upsert-tables/\nClickHouse discusses the challenges of efficiently storing and processing semi-structured JSON data in a column-oriented database and introduces two key building blocks: the Variable and Dynamic types. The article details how these building blocks are used to implement the JSON type, which provides support for dynamically changing data, high-performance storage, scalability, and tuning options. It also showcases the advantages of the JSON type in terms of data compression and query performance and outlines the roadmap for future enhancements.\nhttps://clickhouse.com/blog/a-new-powerful-json-data-type-for-clickhouse\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-195", "title": "Data Engineering Weekly", "content": "LLMs and vector databases significantly improved the ability to process and understand unstructured data. I never thought of PDF as a self-contained document database, but that seems a reality that we can\u2019t deny. The blog is an excellent summary of the existing unstructured data landscape.\nhttps://www.felicis.com/insight/unstructured-data-stack\nFigma writes about challenges in building vector search at scale. The learning mostly involves understanding the data's nature, frequency of data processing, and awareness of the computing cost. It is exciting to read probably the first blog on building a vector search infrastructure at scale. \nhttps://www.figma.com/blog/the-infrastructure-behind-ai-search-in-figma/\nI spent quality time earlier this year assisting with India\u2019s DPDP law requirements,  which spiked my curiosity about building privacy-preserving computing. We recently published a comprehensive engineering guide to build a privacy-first design. The blog from Meta discusses how it designed a privacy-preserving storage. \nhttps://engineering.fb.com/2024/10/22/security/ipls-privacy-preserving-storage-for-your-whatsapp-contacts/\nIf you haven't registered for the IMPACT Summit yet, now's the perfect time \ud83d\udd08Here\u2019s what we\u2019ve got in store:- A half-day virtual event created to elevate your 2025 data strategy- Sessions jam-packed with industry experts sharing how they're driving data and AI adoption- Practical tips and best practices from Monte Carlo customers- Opportunities to connect and network with other data professionals- Giveaways and raffles for attendees, including three All-Access subscriptions to DataExpert.io!- And more!What are you waiting for? Register for IMPACT today!\nimpactdatasummit.com\nPossibly one of the complicated pipelines to build is the Financial reconciliation engine. At the last DEWCon summit, Flipkart, India\u2019s leading e-commerce company,\u00a0talked about its reconciliation pipeline. On a similar line, Uber writes about its comprehensive settlement accounting system designed to handle the immense volume of transactions processed each month efficiently.\nhttps://www.uber.com/blog/ubers-advanced-settlement-accounting-system/\nApache Yarn has limitations, including a lack of application isolation, high engineering effort to upgrade, and a lack of feature compatibility with the capacity and fair scheduler. Pinterest writes about adopting Apache YuniKorn, a yarn alternative with a Kubernetes-compatible resource scheduler for container orchestrator systems.\nhttps://medium.com/pinterest-engineering/resource-management-with-apache-yunikorn-for-apache-spark-on-aws-eks-at-pinterest-0dba3afb4609\nThe Gen AI Summit, consisting of a wider group of 20,000 Engineers, AI entrepreneurs, and Scientists, will host 1,000 AI teams in Austin, TX, November 7-8. Join for two days of sessions, socials, case studies, and workshop tutorials. Passes include app-brain-date networking, birds of a feature, post-event parties, etc. 60+ speakers from LinkedIn, Shopify, Amazon, Lyft, Grammarly, Mistral, et al.\nData Engineering Weekly readers get 15% discount by registering the following link,\nhttps://www.eventbrite.ca/e/5th-ann-mlops-world-and-generative-ai-world-conference-tickets-755502136227?discount=DataEngineeringWeekly\nBringing data engineering development close to software engineering practices is a dream we all strive for. The blog argues that separating software development and data analytics teams within organizations is harmful, citing Conway\u2019s Law to illustrate how this organizational structure negatively impacts software design. The author proposes three trends: data engineering as a software engineering discipline, data contracts and data products, and Shift Left as ways to address this problem. \nhttps://jack-vanlightly.com/blog/2024/10/21/the-curse-of-conway-and-the-data-space\nIn last week\u2019s newsletter, we highlighted the problems with Flink\u2019s state management. \nEphemeral local storage is a blessing, but persistent local storage is a curse. \nLuckily, the Flink community is actively innovating on this. Alibaba's blog gives an in-depth overview of Flink\u2019s state management and what it takes to build a storage-compute separation architecture. \nhttps://www.alibabacloud.com/blog/evolution-of-flink-2-0-state-management-storage-computing-separation-architecture_601133\nWix writes about implementing AWS SageMaker Batch Transform to enhance the efficiency of their machine learning model operations. Wix's system utilizes over 200 models daily, necessitating a scalable and robust solution. The implementation includes a sophisticated retry mechanism that addresses failed input files by isolating problematic rows and rerunning the Batch Transform job with an optimized configuration. \nhttps://medium.com/wix-engineering/sagemaker-batch-transform-unleashed-my-journey-at-wix-to-achieve-scalable-ml-9c6004196d84\nGrab writes about integrating large language models (LLMs) with vector similarity search to improve the accuracy and relevance of search results. The authors propose a two-step approach: initially, a vector similarity search is performed to narrow down potential matches, followed by an LLM-based ranking process that leverages natural language understanding to refine the results. The benchmark demonstrates that this LLM-assisted search outperforms traditional vector similarity searches in handling complex and nuanced queries.\nhttps://engineering.grab.com/llm-assisted-vector-similarity-search.\nExpedia writes about the \"GenAI Toolkit\" for implementing and controlling access to generative AI models within enterprise environments. It highlights the challenges of using platforms like ChatGPT and Azure AI Service, which lack sufficient control over account usage and cost tracking across multiple applications. The GenAI Toolkit, which comprises the GenerativeAI Proxy (GAP) and EG-Guardrails service, provides a secure and flexible solution by addressing data protection, content filtering, authentication, and resource management issues. \nhttps://medium.com/expedia-group-tech/gateways-guardrails-and-genai-models-aa606379164d\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-194", "title": "Data Engineering Weekly", "content": "Notion writes about its journey in adopting data catalogs and describes how a vanilla data catalog solution will only be effective if it adopts a strong data platform foundation. Adopting Typescript rather than the specialized IDL languages is a good strategy, although I wonder how it works in cross-language systems like Android & iOS. I presume the typescript-to-js conversion helps here.\nhttps://www.notion.so/blog/a-brief-history-of-notions-data-catalog\nAn excellent overview of the potential impact of GenAI in data engineering workflow that can change the pipeline authoring, shipping pipeline, and maintaining the pipeline. Suppose you think of data pipeline building as a manufacturing process that goes from sourcing the raw material to delivering a finished (data) product. There are tons of optimizations we can do to improve the efficiency. The problem statement is something I\u2019m excited about, and I write about it here as something I\u2019m working on.\nhttps://blog.dataengineer.io/p/how-genai-will-impact-data-engineering\nUber\u2019s latest blog post dives into how the company trains large language models (LLMs) by blending open-source models with domain-specific expertise. Uber leverages open-source tools and infrastructure to streamline the training process while fine-tuning and optimizing the models with in-house techniques. Uber\u2019s post outlines its infrastructure stack and training pipeline, emphasizing the balance of open-source and custom solutions to drive generative AI advancements.\nhttps://www.uber.com/blog/open-source-and-in-house-how-uber-optimizes-llm-training/\nIf you haven't registered for the IMPACT Summit yet, now's the perfect time \ud83d\udd08Here\u2019s what we\u2019ve got in store:- A half-day virtual event created to elevate your 2025 data strategy- Sessions jam-packed with industry experts sharing how they're driving data and AI adoption- Practical tips and best practices from Monte Carlo customers- Opportunities to connect and network with other data professionals- Giveaways and raffles for attendees, including three All-Access subscriptions to DataExpert.io!- And more!What are you waiting for? Register for IMPACT today!\nimpactdatasummit.com\nThe most important reason streaming is difficult is to embed the state (RocksDB) in the stream processor. The blog narrates the drawbacks, including increased downtime due to state rebuilds, inflexibility in scaling compute and storage resources, limited visibility into the state, and a lack of advanced functionality like time-to-live (TTL) management and easy state inspection and patching. \nLife is too short to scale storage and computing together, and this space requires a fundamental rethinking.\nhttps://www.responsive.dev/blog/stop-embedding-rocksdb-in-kafka-streams\nThe search quality team at Pinterest saw over a 30x decrease in annual cost for one of their inference jobs after migrating it from Spark(\u2122) to Ray(\u2122).\nThere is an increased desire to find a computing engine that is less costly but robust than Apache Spark. Pinterest's three-part series about Ray batch inference is an exciting read about running batch jobs in Ray.\nhttps://medium.com/pinterest-engineering/last-mile-data-processing-with-ray-629affbf34ff\nhttps://medium.com/pinterest-engineering/ray-infrastructure-at-pinterest-0248efe4fd52\nhttps://medium.com/pinterest-engineering/ray-batch-inference-at-pinterest-part-3-4faeb652e385\nConfluent writes about the challenges of handling bad data in event streams, specifically focusing on the differences in dealing with bad data in batch processing compared to event streaming. State events, which represent the entire state of an entity, are much easier to correct as they can be compacted, allowing for the deletion of older versions and the propagation of updated data. Delta events, which describe changes or actions, are significantly harder to fix due to their immutable nature. \nThe blog concludes with a final strategy, \"rewind, rebuild, and retry,\" highlighting the importance of preventative measures like schemas, data quality checks, and robust testing to avoid such costly interventions.\nhttps://www.confluent.io/blog/shift-left-bad-data-in-event-streams-part-2/\nThe article is an excellent narration of the recent dbt support for microbatch incremental workload. I\u2019m happy about the support coming in dbt, as I worked out a similar incremental batch processing strategy that the author explains here in dbt-core with Airflow. I still remember a veteran data engineer at work telling me how dbt got away with it til now without this feature :-)  I\u2019m sure we are not alone here. Many companies would have done something similar, so I\u2019m glad it is finally coming. \nhttps://medium.com/@kayrnt/microbatch-on-dbt-93d600ced394\nWe marketers sometimes think of data as a boolean: either we have it or don\u2019t. However, data problems come in many shapes and forms.\nThe blog is an excellent summary of data problems. The author breaks down \u201cdata problems\u201d into five distinct categories: measurement, quality, accessibility, literacy, and activation. The author further narrates the symptoms of each and possible remedies.\nhttps://www.021newsletter.com/p/you-dont-have-a-data-problem\nShopify writes about its real-time pipeline design for building the image embedding engine to improve consumer search experience. The blog describes the trade-off between parallel data processing and memory usage while processing images. \nhttps://shopify.engineering/how-shopify-improved-consumer-search-intent-with-real-time-ml\nInternal tools, especially data tools, should address performance and user experience. Netflix writes about how the data platform team investigated the UI latency issue in Jupyter Notebook. What I like most about the blog is not only the investigation but also the authors' explanation of how they use LLM in their debug workflow to understand the issue in more depth.\nMoreover, after several rounds of discussion with ChatGPT, we learned more about the architecture and realized that, in theory, the usage of pystan and nest_asyncio should not cause the slowness in handling the UI WebSocket\nhttps://netflixtechblog.com/investigation-of-a-workbench-ui-latency-issue-faa017b4653d\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-193", "title": "Data Engineering Weekly", "content": "The LLM, known as the token economy, is emerging as an agent-driven workflow tool across the industry. The author narrates why tokens are the new currency in the LLM economy. \nI recently took a survey from a productivity tool I paid for, asking if I\u2019m willing to pay double the subscription cost to use the LLM feature. I was like, hell no. From a consumer perspective, I want to pay the same but expect a much better experience. I\u2019m still on the edge of the LLM economy, but I'm optimistic the tools will be LLM-driven. \nhttps://medium.com/wix-engineering/the-emerging-economy-of-llms-883f2ab13067\nCan LLM develop mathematical reasoning capabilities? The paper from Apple evaluated the current leading LLMs and said no, it can\u2019t. Key reasons for them are,\nLLM relies on probabilistic pattern matching; hence, instead of understanding the underlying mathematical concepts, LLMs might simply replicate patterns they observed in their training data.\nSmall changes in input tokens can significantly alter model outputs, revealing token bias and fragility.\nLLM performance deteriorates with increased complexity.\nhttps://machinelearning.apple.com/research/gsm-symbolic\nJoe Reis provides a great overview of what is happening in the data industry. LLM is entering the PoC phase; people are still confused but in the upskilling phase. A key highlight for this is \ud83d\udc47\ud83c\udffc\nData\u2019s still a mess. Most data initiatives fail. Data teams are seen as a cost center and not getting the support they deserve. Same as it ever was.\nIf you haven't registered for the IMPACT Summit yet, now's the perfect time \ud83d\udd08 Here\u2019s what we\u2019ve got in store:- A half-day virtual event created to elevate your 2025 data strategy- Sessions jam-packed with industry experts sharing how they're driving data and AI adoption- Practical tips and best practices from Monte Carlo customers- Opportunities to connect and network with other data professionals- Giveaways and raffles for attendees, including three All-Access subscriptions to DataExpert.io!- And more!What are you waiting for? Register for IMPACT today!\nimpactdatasummit.com\nInternal support is a disrupting but essential part of building a successful platform. Uber writes about Genie, a Gen AI on-call Copilot. Genie addresses these challenges by providing quick and accurate answers to questions, retrieving relevant information from internal knowledge bases, and reducing the need for constant interaction with on-call engineers.\nhttps://www.uber.com/blog/genie-ubers-gen-ai-on-call-copilot/\nGrab writes about Data-Arks, an internal platform that houses frequently used SQL queries and Python functions. Data-Arks serves as a vital component in integrating Large Language Models (LLMs) into the analytics workflow, streamlining processes like generating regular metric reports and conducting fraud investigations\nhttps://engineering.grab.com/transforming-the-analytics-landscape-with-RAG-powered-LLM.\nIncremental data processing is vital for an efficient and cost-effective data infrastructure. The author categorizes these queries into four types: append-only, upsert, min-delta (CDC), and full-delta (CDC). The article explores how each table format handles these queries, analyzing their strengths and limitations.\nhttps://jack-vanlightly.com/blog/2024/9/19/table-format-comparisons-change-queries-and-cdc\nIf I understand correctly, the gist of the article is where you position the common data model/ metrics that can be used across the organization. I think these layers are a guiding principle instead of a strict framework. The common data models are considered the \u201ccore\u201d domain, which is itself a kind of data mart. The article is a good reminder to focus on the \u201csharable core domain\u201d in data modeling regardless of whether or not to expand the medallion architecture.\nhttps://lakshmanok.medium.com/what-goes-into-bronze-silver-and-gold-layers-of-a-medallion-data-architecture-4b6fdfb405fc\nExpedia Group Technology designed a new SLO platform to enhance data reliability, leveraging Kafka for event streaming, PostgreSQL for data storage, and APIs for querying. The platform efficiently ingests and enriches data from multiple sources with internal metadata, providing near real-time access and seamless integration with DataDog for proactive monitoring and real-time alerting.\nhttps://medium.com/expedia-group-tech/enhancing-data-reliability-with-an-slo-platform-de00249756f6\nGumGum\u2019s data engineering team optimized batch scoring by integrating BigQuery ML with ONNX, streamlining a previously complex workflow. Moving scoring directly into BigQuery eliminated the need for external Python-based containers, reducing both time and costs. This solution leverages Scikit-Learn models in ONNX format, allowing efficient, SQL-based batch scoring directly in BigQuery, significantly improving scoring performance on large datasets.\nhttps://medium.com/gumgum-tech/boosting-batch-scoring-efficiency-with-bigquery-ml-and-onnx-85a114265c35\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/the-dawn-of-the-ai-native-data-stack", "title": "Data Engineering Weekly", "content": "The data world is abuzz with speculation about the future of data engineering and the successor to the celebrated modern data stack. While the modern data stack has undeniably revolutionized data management with its cloud-native approach, its complexities and limitations are becoming increasingly apparent. As we grapple with these, another seismic shift is upon us\u2014the rise of Large Language Models (LLMs).\nAgent systems powered by LLMs are already transforming how we code and interact with data. As an avid user of tools like cursor.ai, I've experienced firsthand the productivity gains they offer. I converted a Java streaming platform into Rust, completing the task faster and gaining valuable insights into Rust's intricacies. We can\u2019t deny the coding assistance of LLMs and the improved productivity.\u00a0\nWith the rapid advancement of LLMs and their integration into cloud-native environments, we stand at the cusp of a new era in data engineering. This next phase, the AI-Native Data Stack, will fundamentally alter how we build, maintain, and scale data systems. To understand this evolution, let's draw parallels from a seemingly unrelated field\u2014manufacturing\u2014and its historical transformation.\nThink of iconic structures like the Empire State Building, the Golden Gate Bridge, and the Hoover Dam. What do they have in common? Bethlehem Steel, a titan of American industry, produced the steel that binds these marvels. In the early 20th century, centralized manufacturing plants dominated production with imposing factories and regimented assembly lines. This approach offered economies of scale but was inherently rigid, inflexible, and vulnerable to disruptions.\nThis centralized model mirrors early monolithic data warehouse systems like Teradata, Oracle Exadata, and IBM Netezza. These systems provided centralized data storage and processing at the cost of agility. They were powerful for their time but ultimately struggled to adapt to modern businesses' diverse and rapidly evolving needs.\nBoth industries eventually faced a reckoning. Centralized factories and monolithic data systems became too rigid and expensive to scale, unable to cope with the increasing complexity of manufacturing and the explosion of diverse, unstructured data in the digital age.\n[Supply Chain of Nutella - https://econlife.com/2014/01/nutellas-global-supply-chain/]\nGlobalization and advancements in transportation revolutionized manufacturing in the latter half of the 20th century. Monolithic factories gave way to modular production through intricate supply chains, with specialized providers handling each step: this increased efficiency, reduced costs, and enhanced flexibility.\nData engineering followed a similar path. With the advent of cloud infrastructure, monolithic data warehouses were replaced by the Modern Data Stack\u2014systems like Snowflake, Redshift, and BigQuery. These cloud-native platforms allowed companies to decompose the data pipeline into specialized services optimized for specific functions like storage, processing, or transformation.\nHowever, the modern data stack presents challenges like manufacturing's global supply chains. While modular and specialized, integrating multiple cloud-native tools can lead to fragmentation and complexity. Navigating this intricate web of services can increase operational costs and create inefficiencies.\nSeeking greater efficiency, the manufacturing industry turned to automation in the late 20th century. Robots began to transform factories, enabling faster and more consistent production. Initial resistance due to concerns about job displacement, cost, and reliability gradually faded as robotic systems became more sophisticated and indispensable. Combining modular supply chains and automation created a more flexible and scalable industry.\nToday, we witness a parallel shift in data engineering with the rise of LLMs. Just as robots revolutionized manufacturing, LLMs are poised to reshape how we code, analyze, and interact with data systems. Tools like cursor.ai empower developers with unprecedented productivity by automating repetitive tasks, providing intelligent insights, and facilitating seamless cross-language coding.\nThink of LLMs as the \"robots\" of the data world, automating and optimizing tasks that were once time-consuming and manual. LLMs can automatically generate code for data transformation, optimize queries for performance, identify and rectify data quality issues, and even predict future data trends. This automation frees up data engineers to focus on higher-level tasks like system design and strategic data analysis.\nAs this transformation unfolds, organizations must prepare for the rise of the AI-Native Data Stack. LLMs and AI-driven agent systems are already integrating into data infrastructure, making this shift a reality, not just a theory. Businesses that adapt early will position themselves to fully harness these technologies, unlocking new efficiency, scalability, and adaptability levels.\nIn the next part of this series, we'll explore the AI-Native Data Stack's core components, benefits, and challenges organizations might face during its implementation. If you\u2019re interested in how the AI-Native Data Stack can revolutionize your organization\u2019s approach to data engineering, I\u2019d love to continue the conversation. Let\u2019s explore how this emerging technology can help your business stay ahead in an increasingly AI-driven world.\nPlease feel free to connect with me at https://www.linkedin.com/in/ananthdurai/ or schedule a time to chat through https://calendly.com/apackkildurai.\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-192", "title": "Data Engineering Weekly", "content": "PyTorch plays a significant role in the latest development of AI in the industry. The article nicely summarizes the key takeaways from the conference. All the talks are now available on YouTube. If you are new to PyTorch and want to learn, the tutorial from freecodecamp is the best. \nhttps://pytorch.org/blog/pytorch-conference-2024-recap/\nA blog about real-world learning is always a delight to read. The author narrates the learning from optimizing the most expensive Snowflake pipelines. The learning goes back to the fundamentals of pipeline design principles.\nRegularly review if pipelines are still required.\nMinimize the data used in pipelines, aka do incremental data pipeline design.\nOptimize pipeline schedules. Ask if it should run hourly? can it be daily or weekly? \nFilter data effectively to make sure the query uses partition pruning. \nUse time windows for analysis. Don\u2019t query two years of data.\nhttps://medium.engineering/learnings-from-optimising-22-of-our-most-expensive-snowflake-pipelines-5ea6fcf57356\nThe author discusses evaluating a product's performance from a digital product manager\u2019s perspective. The matrix goes beyond traditional customer-focused metrics and incorporates business costs and benefits to help product managers make well-rounded decisions. It updates the popular Pirate metrics (AARRR) framework by adding concepts like Attention and Adoption. It emphasizes balancing customer impact with business objectives such as ROI, brand value, and operational costs.\u00a0\nhttps://rachelfkm.medium.com/customer-business-metrics-matrix-for-digital-product-managers-d1da7da3450e\nYou want to deliver reliable business insights to your organization, but that probably means you\u2019re spending a whole lotta time writing a whole lotta of rules. Data quality rules, that is. But, what if you could automate them instead \u2013 and spend more time delivering value for your stakeholders? Join us on October 10th at 9am PT | 12pm ET as I walk through how data observability platforms like Monte Carlo can help data analytics teams reduce time spent on data quality by 80% or more.\nmontecarlodata.com\nUber writes about redesigning its Experimentation platform to reduce the latency of experiment evaluations by 100x. The new architecture moved from a remote evaluation model, where clients made requests to a central service, to a local evaluation model, where clients compute evaluations locally. The authors discuss the challenges and learnings in implementing this change, including ensuring correctness at scale, addressing log volume concerns, and managing decentralized evaluations.\nhttps://www.uber.com/blog/making-ubers-experiment-evaluation-engine-100x-faster/\nGrab writes about the evolution of its model-serving platform, Catwalk. The article describes four distinct phases of Catwalk's development, highlighting the challenges faced and solutions implemented at each stage. \nPhase 0 outlines the need for a dedicated platform due to the inefficiencies of ad-hoc approaches. \nPhase 1 introduced a managed platform for TensorFlow Serving models but faced scalability limitations. \nPhase 2 transitioned to a self-serving approach, empowering data scientists with greater control over their models. \nPhase 3 replaced Helm charts with Kubernetes CRDs for enhanced deployment control and flexibility. \nPhase 4 introduced the Catwalk Orchestrator, a high-code orchestration framework that enables users to write custom business logic.\nhttps://engineering.grab.com/catwalk-evolution\nLinkedIn writes about how the company has improved its Feed ranking system by using dwell time, or the amount of time a user spends on a piece of content, as a signal of user interest. The authors describe how they have tackled technical challenges such as noisy dwell time signals, the need for an adaptive threshold, and introducing biases through static thresholds to develop an innovative model that predicts \"long dwell\" behavior.\nhttps://www.linkedin.com/blog/engineering/feed/leveraging-dwell-time-to-improve-member-experiences-on-the-linkedin-feed\nPrevention is better than cure. The author highlights the key benefits of thinking and designing systems to support quality and compliance upfront (shift-left) rather than later. The key benefits are\nImproved data quality,\nEnhanced data governance\nIncreased security\nCost efficiency\nhttps://medium.com/@nydas/4-key-benefits-of-shift-left-ff0e4bb74a3f\nApache Superset is the most popular open-source BI tool in the industry. The goal of the monitoring framework is to treat every dashboard as a data product, and by ingesting the Superset\u2019s metadata into the Data Warehouse, the HomeToGo data team is trying to lifecycle management and governance more automatedly. Though the blog talks about Superset, every team should strive to adopt a similar framework to manage the lifecycle of their dashboards. \nhttps://engineering.hometogo.com/how-hometogo-improved-our-superset-monitoring-framework-60eb98e1a650\nIn the new era of LLM, data engineers strive to improve our interaction with the vast data storage in the data warehouse. \u201ctext-to-SQL\u201d and \u201ctext-to-insight.\u201d are the dreams we started to chase after, and I\u2019m sure we are not far away from the dream. Microsoft IDEAS team writes about one such system design and the challenges to overcome.  \nhttps://medium.com/data-science-at-microsoft/leveraging-ai-for-next-level-data-productivity-in-ideas-part-3-c19390079fe7\nThe blog provides an excellent comparison of text-to-SQL conversion accuracy, speed, and cost. While GPT-4o and Claude 3.5 Sonnet consistently lead in handling complex queries with high accuracy, emerging models like Mistral Large V2 and the Llama family show promise in certain areas.\nhttps://medium.com/querymind/text-to-sqls-power-players-comparing-claude-3-5-sonnet-gpt-4o-mistral-large-2-llama-3-1-d4530a3d4407\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-191", "title": "Data Engineering Weekly", "content": "Product ideas powered by data and AI must go through rapid iteration on shareable, lightweight live prototypes instead of static proposals. However, hosting an internal application for fast prototyping is always a challenging platform to build and maintain. Airbnb writes about Sandcastle, an Airbnb-internal prototyping platform that enables data scientists, engineers, and product managers to bring data/AI ideas to life. \nhttps://medium.com/airbnb-engineering/sandcastle-data-ai-apps-for-everyone-439f3b78b223\nGrab confirms my observation that users abandoned their searches in 18% of sessions without clicking on any dataset. This suggested that the search results were not meeting their needs, which led to redesigning the system to focus on search accessible where data consumers hang out the most: Slack.\nThe LLM and vector search will significantly lead to headless data catalogs or merge into technical catalogs such as Unity Catalogs. I doubt that data catalogs, as standalone tools, will have their user experience as part of it. \nhttps://engineering.grab.com/hubble-data-discovery\nNgrok shares its experience building a real-time data platform with an event-driven architecture, ensuring scalability and efficient data management. The focus on handling high throughput while maintaining real-time analytics offers valuable insights into modern data platform design, complementing ideas we've explored in past discussions on scaling data platforms.\nhttps://ngrok.com/blog-post/how-we-built-ngroks-data-platform\nReliable data is the backbone of useful business insights\u2014and keeping bad data out of production is step one.For most analysts, delivering reliable insights means writing data quality rules \u2013and lots of them \u2013 to monitor for issues and data quality dimensions. But, every second you spend writing and managing a data quality rule is another second you can\u2019t spend delivering new value for stakeholders. But what if\u2014instead of writing all those rules\u2014you could automate them instead? How might that impact your ability to deliver trusted business insights?Join us as we explore the challenges of data quality management and how AI-powered data quality features can accelerate incident detection workflows to provide more visibility into the health of your data products.\nmontecarlodata.com\nQuery analytics is essential in data platform engineering to understand the popular tables, index utilization, types of queries, and predicate windows of the queries. Uber writes about Preon, a query analytical infrastructure for analyzing Presto Queries.\nhttps://www.uber.com/blog/preon/\nThe recent Presto technical blog goes one step further on query analytics, using it to optimize queries with Historical-Based optimization frameworks. The blog narrates the query planning caching on Redis on historical queries to compare the current execution plan and the historical plan to derive the optimal query execution plan.\nhttps://prestodb.io/blog/2024/09/26/query-optimization-with-historical-based-optimization-framework-in-presto/\nIt was a good weekend read about the proposed pipe syntax in SQL, which is more similar to Unix pipes in terms of its core concept\u2014sequential data flow and transformation. Unix pipes typically represent a physical flow of data between processes. While visually suggesting a sequence, Pipe SQL maintains SQL's declarative nature. I like the addition of experimental debug operators such as DEBUG and ASSERT. What do you think about Pipe syntax in SQL?\nhttps://research.google/pubs/sql-has-problems-we-can-fix-them-pipe-syntax-in-sql/\nYelp\u2019s latest technical blog explores enhancing ML pipeline efficiency by enabling direct Cassandra ingestion from Spark. The article details how bypassing intermediate storage steps reduces latency and improves data processing speed. The approach highlights the importance of streamlining data workflows for faster machine learning model training and deployment.\nhttps://engineeringblog.yelp.com/2024/09/boosting-ml-pipeline-efficiency-direct-cassandra-ingestion-from-spark.html\nPinterest\u2019s blog discusses improving recommender systems' performance through feature caching using CacheLib. By caching frequently used features, Pinterest reduces latency and boosts system efficiency. The method underscores the value of optimizing data retrieval in real-time applications, especially for large-scale recommendation systems.\nhttps://medium.com/pinterest-engineering/feature-caching-for-recommender-systems-w-cachelib-8fb7bacc2762\nColpali is gaining popularity because it significantly improves document retrieval efficiency and accuracy by leveraging vision-language models and bypassing the need for OCR, making it ideal for handling large and visually rich documents. The author narrates an in-depth overview of document similarity search with ColPali.\nhttps://huggingface.co/blog/fsommers/document-similarity-colpali\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/engineering-privacy-a-technical-overview", "title": "Data Engineering Weekly", "content": "Once again, I want to thank the Data Heros community. Last Friday, we discussed the challenges in bulk discovery and anonymization processes in data warehouses. The collective design choices and ideas lead to a comprehensive overview of thinking about designing data infrastructure with a privacy-first approach.\nPrivacy and access management within data infrastructure is not just a best practice; it's a necessity. Regulations like GDPR (General Data Protection Regulation), focusing on data subject rights and cross-border data flows, have significant implications for data handling practices. Robust privacy and access management protocols are crucial for GDPR compliance, protecting sensitive information, and maintaining user trust.\u00a0\nSimultaneously, The proliferation of AI and language models has led to surging regulatory requirements. Let's briefly tour the AI regulations shaping the global landscape.\nBy prioritizing these measures, you mitigate legal and financial risks and demonstrate your commitment to data privacy, which can be a significant differentiator in the marketplace and positively impact your business.\u00a0\nWhen it comes to privacy in data infrastructure, there are three key components: enforcing least privilege access, encrypting sensitive data, and effectively managing data deletion. Let's examine them in more detail.\u00a0\nThe least privileged access model gives users the minimal level of access they need to do their jobs\u2014nothing more. This approach isn\u2019t just a good practice; it\u2019s a requirement in many privacy regulations. For example, GDPR and HIPAA require strict access controls to protect sensitive data. Using Role-Based Access Control (RBAC) or Attribute-Based Access Control (ABAC) in LakeHouse architectures helps enforce this model. Regularly auditing and adjusting access roles ensures that only the right people can access the right data at the right time. Not only does this reduce the risk of breaches, but it also checks a key box in regulatory compliance.\nData anonymization is your best line of defense against unauthorized access. Encrypting data at rest and in transit ensures that sensitive information stays safe, even if someone gains access. Regulations like GDPR, HIPAA, and PCI-DSS make encryption a must-have. Most cloud providers offer built-in encryption options and key management systems (KMS), making it easier to stay compliant without sacrificing security. Encryption is one of the easiest ways to demonstrate that you're taking privacy seriously, especially in a data breach where encrypted data is often exempt from notification requirements.\nData deletion is not just a nice-to-have\u2014it\u2019s a regulatory obligation. Laws like GDPR's 'Right to be Forgotten' and CCPA\u2019s data retention rules require organizations to delete personal data when it\u2019s no longer needed or a user requests it. Implementing automated lifecycle policies to delete data after a specific period is crucial for compliance and reducing the risk of unnecessary data retention.\nA fundamental principle of privacy-conscious data management is treating data based on sensitivity. The Medallion Architecture, also known as the \"Bronze, Silver, Gold\" architecture, is a robust approach that segregates data into layers of increasing refinement and accessibility:\nBronze Layer: Various sources ingest raw, unprocessed data into this landing zone, where privacy-sensitive data exists. The data team should always tightly lock and have restricted access to the bronze layer. \nSilver Layer: In this zone, data undergoes cleaning, transformation, and enrichment, becoming suitable for analytics and reporting. Access expands to data analysts and scientists, though sensitive elements should remain masked or anonymized.\nGold Layer: This layer contains highly curated, aggregated data, often optimized for specific business use cases. It's accessible to a wider audience, including business users and BI tools.\nThe flow of sensitive data through this architecture ensures that it's progressively desensitized as it moves from Bronze to Gold. A conscious design choice to reduce the sensitive data as it flows through the right minimizes the risk of exposure while still enabling valuable insights to be extracted.\nProtecting data requires fine-grained control over who can access what. Two prevalent models are:\nRole-Based Access Control (RBAC): This system assigns permissions based on predefined organizational roles (e.g., data analyst, marketing manager). It's relatively simple to implement and manage but can become rigid in dynamic environments.\nAttribute-Based Access Control (ABAC): Grants or denies access based on attributes of both the user (e.g., department, clearance level) and the data (e.g., sensitivity, project). It offers greater flexibility and granularity but can be more complex to configure and maintain.\nThe choice between RBAC and ABAC depends on your organization's specific needs. Smaller organizations or those with stable roles may find RBAC sufficient, while larger enterprises or those with complex data-sharing requirements benefit from ABAC's adaptability. Grab\u2019s blog on migrating from RBAC to ABAC is an excellent reference design.\nA privacy engine acts as a watchdog within your data warehouse, ensuring compliance with data classification policies and enabling automated data protection actions. Key functions include:\nAuditing data classification policy enforcement: The engine continuously monitors data access patterns and flags any violations of classification rules. This helps identify unauthorized access attempts or data misclassification.\nBulk deleting and anonymization: When data reaches its end-of-life or needs to be desensitized, the engine can perform bulk deletion or anonymization jobs. Anonymization techniques like masking, pseudonymization, and generalization render data unidentifiable while preserving its utility for analysis.\nEnsuring consistent enforcement of policies across diverse data stores.\nManaging secret keys for the anonymization algorithms.\nMutation is an expensive operation in LakeHouse and modern data warehouses.\u00a0\nBYOC is another architectural style to consider in the anonymization process. With BYOC, all the data anonymization for a client happens via a customer-supplied secret key, aka BYOC(Bring Your Own Key). Clients can remove their keys when they no longer require the service, which makes the data non-readable. Slack writes an in-depth article on how it manages enterprise keys as a reference design.\nTraditional data governance models focus on applying privacy controls after collecting data. However, a \"shift-left\" approach advocates integrating data classification and anonymization into the data publishing phase. This has several advantages:\nPrivacy is baked in from the start: Sensitive data is protected as soon as it enters the system, reducing the risk of accidental exposure.\nReduced downstream complexity: Classifying and anonymizing data early reduces the effort required to manage and enforce policies in the data warehouse.\nImproved data quality: Early classification enables better data lineage and more accurate data cataloging.\nPrivacy by design is often an afterthought in an organization. However, streamlining the medallion architecture from a privacy perspective can give enough time to strengthen with anonymization and access control. If you\u2019re in the middle of or thinking about the privacy architecture in your organization, please feel free to schedule a time to chat here.\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-190", "title": "Data Engineering Weekly", "content": "Our mission is to empower data professionals and organizations to make informed, data-driven decisions by providing a comprehensive buyer's guide and comparison matrix for selecting the best data tools. We have already published a comparison matrix for CDC and Data Observability. Next, we will publish a comparison matrix for LakeHouse. \nAll the comparison matrix builds upon collective interactive sessions with the Data Heros community. Please watch the community LinkedIn page, where we will post the link for the discussion. \nWe won't record this conversation in the true spirit of community and open knowledge-sharing.\nCompanies are increasingly adopting the natural language interface to interact with the enterprise data. \nI\u2019m fairly confident that a framework that supports a deterministic way of building a data pipeline and a conversational way of building business logic is the future of data pipeline engineering. \nUber writes about its journey in building the natural query interface for the data warehouse, including the lessons learned from the first iteration and the adoption of the multi-agent approach to tuning accuracy. \nhttps://www.uber.com/blog/query-gpt/\nAs the adoption of LLM increases, prompt injection, hallucination, and other security & compliance guards are required to secure the application. Dropbox writes a case study on integrating Lakera Guard into their prompt engineering to secure the LLM infrastructure.\nhttps://dropbox.tech/security/how-we-use-lakera-guard-to-secure-our-llms\nFlipkart is leveraging NVIDIA's NeMo Guardrails to strengthen AI safety in its e-commerce platform, ensuring compliance and safeguarding customer interactions. This move demonstrates the growing importance of AI governance in real-world applications, especially as data-driven businesses like Flipkart scale their machine-learning operations. This highlights a crucial trend for data engineers: AI safety mechanisms are becoming an integral part of ML pipelines, reinforcing the need for robust monitoring and control frameworks in the data ecosystem.\nhttps://blog.flipkart.tech/flipkart-enhances-ai-safety-in-e-commerce-implementing-nvidia-nemo-guardrails-cb2f293b29c0\nThis year's core theme is Trusted Data & AI, and two core breakout tracks will feature industry leaders in the data space!\nThe Leading with Trusted Data & AI track is designed for innovative data leaders in the age of AI. It features sessions from visionaries pioneering the future of data at Pepsi, ZoomInfo, Drata, Roche, and more. The Pioneering Data Observability track is comprised of sessions led by data practitioners from leading data teams at Payoneer, Earnest, SurveyMonkey, Grammarly, and DraftKings, who will share their tech, frameworks, and strategies for scaling data observability to build trusted data, systems, and code.No matter where you sit in your organization, you won't miss the opportunity to learn from industry leaders and gain valuable insights into what's in store for the future of trusted data and AI. Register today: \nimpactdatasummit.com\nThomson Reuters enhances customer support by using Retrieval-Augmented Generation (RAG), which integrates large language models with real-time information retrieval. The combination allows customer service teams to provide accurate and context-aware responses by pulling up-to-date information from company resources. The key takeaway is that RAG adoption is increasing, and that shows in the improved quality and relevance of automated responses, making customer interactions more efficient and effective.\nhttps://medium.com/tr-labs-ml-engineering-blog/better-customer-support-using-retrieval-augmented-generation-rag-at-thomson-reuters-4d140a6044c3\nGrab has implemented a powerful solution for data classification by leveraging large language models (LLMs) to tag and categorize their data automatically. Traditionally, this task was labor-intensive and prone to inconsistencies. By integrating LLMs, Grab streamlined the classification process, allowing the system to efficiently generate column-level tags for sensitive data and business metrics. The LLM-powered approach improves accuracy and speed, providing a more scalable way to manage metadata for their diverse and rapidly growing data.\nhttps://engineering.grab.com/llm-powered-data-classification\nThe logical next step for enterprises to adopt LLM is to train the model with domain-specific data. Wix writes about customizing large language models (LLMs) for enterprise data using domain adaptation techniques. This process allows them to fine-tune LLMs to better understand and respond to their specific data, enhancing performance for tasks like customer service and internal operations. \nhttps://www.wix.engineering/post/customizing-llms-for-enterprise-data-using-domain-adaptation-the-wix-journey\nDecoupling the storage from the compute/ brokers significantly reduces the total cost of ownership. We see this trend in stream processing systems. Pinterest writes about its adoption of tiered storage for Apache Kafka. The system handles large-scale data by offloading older, less frequently accessed data to cheaper storage tiers while keeping active data on high-performance storage.\nhttps://medium.com/pinterest-engineering/pinterest-tiered-storage-for-apache-kafka-%EF%B8%8F-a-broker-decoupled-approach-c33c69e9958b\nCan AI replace human data analysts? The author shares a perspective on how human analysts can potentially use AI. The author emphasizes that human intuition, experience, and contextual understanding are key to making sense of complex data beyond what automated tools can achieve but states the importance of combining human expertise with AI data tools to foster better collaboration between business teams and data analysts.  \nhttps://datafordoers.substack.com/p/lessons-from-human-data-analysts \nYuno writes about transforming its data infrastructure by implementing Apache Hudi, optimizing data lake performance, and reducing costs by 70%. Hudi's features, such as time travel, indexing, and automated file management, enabled real-time data insights and improved efficiency. Yuno utilized Hudi\u2019s flexibility across different use cases and integrated it with AWS Glue and Airflow for orchestration. \nhttps://www.y.uno/post/how-apache-hudi-transformed-yunos-data-lake\nCan you measure the effectiveness of your data ecosystem? The HomeToGo team writes about a set of \"North Star Metrics\" to align its data domain efforts with business objectives. The metrics are a guiding measure to track the performance and impact of data initiatives across the organization. The key takeaway is that having a clear, measurable objective allows teams to maintain focus and ensure their work contributes directly to the company's overall goals.\nhttps://engineering.hometogo.com/hometogos-north-star-metric-for-our-data-domain-7f0f5fb96e30\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/evaluating-data-observability-tools", "title": "Data Engineering Weekly", "content": "https://bit.ly/data-observability-buyers-guide \nThe Buyer Guide for Data Observability is out. Please feel free to make a copy or comment to add more criteria.\nI want to extend my gratitude to the Data Heroes Community for their valuable insights and discussions, which served as the foundation for this piece. The points and thoughts shared here are largely drawn from the community's collective knowledge and contributions. Aswin & I have merely composed and organized their ideas to present the buyer\u2019s guide. \nData observability has become increasingly critical as companies seek greater visibility into their data processes. This growing demand has found a natural synergy with the rise of the data lake. In the pre-data-lake era, traditional data warehouses were limited by disk space and processing power. Storage and computing were tightly coupled within a single, large system, making running additional processes, such as observability tools, difficult without straining the system\u2019s resources. As a result, monitoring data in real time was often an afterthought.\nThe advent of data lakes has changed the landscape of data infrastructure in two fundamental ways:\n1. Decoupling of Storage and Compute: Data lakes allow observability tools to run alongside core data pipelines without competing for resources by separating storage from compute resources.\n2. Comprehensive Data Management: Data lakes enable storing both structured and unstructured data in a single repository, providing a more holistic view of an organization\u2019s data. This opens up new possibilities for monitoring and diagnosing data issues across various sources.\nThese innovations have driven the emergence of data observability tools, which can now operate efficiently without burdening the core infrastructure. Organizations can track, troubleshoot, and optimize their data pipelines in real-time, ensuring smoother operations and better insights.\nHowever, as with any advanced tool, data observability comes with costs and complexities. The buyer\u2019s guide aims to help data professionals make an informed decision when choosing the data observability tool. \nIn today\u2019s data-driven world, organizations rely on massive amounts of data to make critical business decisions. However, data pipelines are often complex and distributed, making them prone to errors such as data loss, schema drift, and quality issues. A single failure can disrupt business operations, lead to compliance violations, and reduce decision-making effectiveness.\nData observability acts as the nervous system of your data platform, continuously monitoring data pipelines for issues like data quality degradation, freshness, and anomalies. It ensures you can trust the data flowing through your systems and react quickly to any issues. Observability provides full visibility into the health and performance of data systems, allowing teams to prevent outages and fix data issues proactively before they affect downstream processes.\nsource: https://www.acceldata.io/why-data-observability\nWithout data observability, organizations risk making decisions based on incomplete, inaccurate, or outdated data, which can have serious consequences.\nData testing and data observability are both critical to ensuring data quality but serve different purposes:\nThis involves writing specific test cases to verify that data meets the expected criteria at various points in the pipeline. Data tests check for missing values, schema mismatches, and incorrect data types. Testing is a proactive way to catch known issues, but it only works for pre-defined cases. \n   Data testing can be categorized into:\n   - Unit tests: Check individual components of the data pipeline.\n   - Integration tests: Ensure smooth interaction between systems.\n   - End-to-end tests: Validate the entire pipeline from source to target.\n   While necessary, data testing can only catch issues that have been explicitly anticipated. \nIn contrast, observability is a continuous, real-time monitoring approach that offers holistic insight into data systems. It goes beyond pre-defined tests to detect unknown or unforeseen problems across the entire data pipeline, such as data drift, anomalies, or delayed data.\nSource: MonteCarlo Data\n   - Data testing is proactive, catching known problems.\n   - Data observability is reactive, monitoring for unknown issues in real-time.\nData testing checks for rule-based validations, while observability ensures overall pipeline health, tracking aspects like latency, freshness, and lineage.\nIn short, While a test can check if a dataset has 10,000 rows, observability ensures the data arriving continuously through the pipeline matches historical behavior, identifies trends, and flags anomalies.\nWhen selecting a data observability tool, assessing both functionality and how well it integrates into your existing data stack is important. Based on our evaluation criteria, here\u2019s how to choose the right tool:\nThe tool should seamlessly connect with your data sources, whether SaaS applications, databases, or cloud platforms.\nEvaluate whether the tool can continuously monitor essential metrics such as:\n   - Schema drift detection: Automatically alerts you when schema changes occur.\n   - Anomaly detection: Identifies unexpected changes in your data.\n   - Data freshness checks: Ensures that data is timely and up to date.\n   - Custom data quality rules: Ability to set custom rules for your specific data quality needs.\n   A strong observability tool will not only detect these issues but also provide automated suggestions or alerts to fix them before they become larger problems. \nThe tool should provide full visibility across the entire pipeline, including data ingestion, transformation, and consumption. It should offer:\n   - Data lineage tracking: Understand the flow and transformations of data through various systems.\n   - Pipeline health monitoring: Track data health at every stage, preventing bottlenecks and data loss.\nReference: At Grab, engineers use observability to track every data point in their pipeline, ensuring reliable and scalable data infrastructure.\nLook for tools that provide real-time alerts to flag data issues as they happen. These alerts should be customizable based on thresholds or specific business rules, and the system should integrate with incident management platforms like PagerDuty or Opsgenie for immediate resolution.\nReference: Drift Monitoring Architecture explains how real-time data monitoring can prevent schema drift and other common issues.\nA good tool must offer robust RCA capabilities, enabling data engineers to quickly diagnose the cause of issues in the data pipeline. Visualization tools that map out pipeline dependencies are particularly helpful here.\nYour observability tool should scale with your data volume and be able to monitor both small and enterprise-scale datasets. It should also support streaming data observability, ensuring that real-time data flows are continuously monitored.\nReference: Checkout.com demonstrates how to monitor large-scale data systems focusing on performance.\nAdvanced tools leverage machine learning to predict anomalies and suggest remediation steps. Automation of common tasks like reconciling data or fixing issues is essential for a tool to be efficient at scale.\nReference: DBT Test Options discusses how automation in testing and monitoring can increase efficiency.\nEnsure the tool helps you comply with regulations such as GDPR and HIPAA by providing features like audit trails, role-based access control, and data sovereignty features.\nEvaluate the total cost of ownership of the tool, considering the initial investment and operational costs. The tool should offer a balance between features and pricing.\nLook for customizable dashboards and collaboration features that make it easier for various teams (data engineers, analysts, etc.) to work together. Tools should also offer self-service capabilities, empowering users to monitor data quality independently.\nData observability has become a crucial component of a reliable data infrastructure in an era where data is the backbone of decision-making. While data testing is essential for checking known issues, it is not enough to maintain overall data health. Data observability takes this a step further, providing continuous, real-time insights into your data pipelines, enabling organizations to prevent costly outages and ensure data reliability at scale.\nWhen choosing a data observability tool, keep in mind the comprehensive evaluation criteria discussed in this article. By selecting the right tool, you can ensure that your data pipelines are always healthy and deliver trustworthy and accurate data.\n- [Facebook Watch](https://m.facebook.com/watch/9445547199/490224945331402)\n- [Grab Engineering](https://engineering.grab.com/data-observability)\n- [Drift Monitoring Architecture](https://medium.com/@linghuang_76674/drift-monitoring-architecture-aa57fc26a19c)\n- [DBT Test Options](https://datacoves.com/post/dbt-test-options)\n- [Checkout.com Tech Blog](https://medium.com/checkout-com-techblog/testing-monitoring-the-data-platform-at-scale-e22d9cf433e8)\n- [Policygenius Tech Blog](https://medium.com/policygenius-stories/data-warehouse-testing-strategies-for-better-data-quality-d5514f6a0dc9)\n- [Monte Carlo](https://www.montecarlodata.com/blog-data-observability-in-practice-using-sql-1/)"}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-189", "title": "Data Engineering Weekly", "content": "Many companies are slowly adopting DataMesh, and Uber writes about adopting the data mesh principle. Whether or not Data Mesh is a separate product is debatable, but it is certainly an impactful framework for scaling data platforms. I expect more and more frameworks to be built on top of the existing compute and orchestration layer. \nhttps://www.uber.com/blog/datamesh/\nMiro writes about its adoption of Data Contracts. The approach talks about moving the data contract coded approach from Airflow code to DataHub yaml specification to provide more visibility to the stakeholders. Miro and Uber case studies emphasize framework simplification and the need for simplified tooling around the data mesh concept to scale a data platform. \nhttps://miro.com/careers/life-at-miro/tech/data-products-reliability-the-power-of-metadata/\ndbt publishes a whitepaper about the Analytical Development Lifecycle aligning with the software development lifecycle. It is a good summarization of various phases of the data asset creation. The whitepaper does not discuss the implementation of this lifecycle; it would be nice to have a follow-up with implementation references similar to AWS's well-architected framework.\nhttps://www.getdbt.com/resources/guides/the-analytics-development-lifecycle\nWe know high-quality data is powerful. But can it predict presidential elections? \nAt IMPACT 2024, we\u2019re thrilled to host Allan Lichtman, American historian and the mastermind behind the Keys to the White House, a model that has successfully predicted U.S. presidential winners for decades, as our keynote speaker! During his keynote, he\u2019ll share his insights into how data and pattern recognition shape the world, build trust, and\u2014sometimes\u2014even help us predict the future.\nimpactdatasummit.com\nMicrosoft IDEAS (Insights, Data, Engineering, Analytics, Systems) team writes about the evolution of the data platform. The blog narrates how the lack of neutral guidance or standards leads to ambiguity and the formation of a central team to bring trust in data. I hope to see more in-depth technical articles about the DataCop and Nitro orchestration engine.\nhttps://medium.com/data-science-at-microsoft/the-evolution-of-a-data-platform-the-story-of-the-microsoft-ideas-team-f1c215173da2\nAirbnb Engineering's Riverbed project addresses the challenges of optimizing real-time data access in hybrid cloud environments by implementing a \"data hydration\" layer. This approach efficiently fetches and caches data for services, reducing latency and cloud costs. The article explains how data hydration can balance performance and cost in modern data platforms.\nhttps://medium.com/airbnb-engineering/riverbed-data-hydration-part-1-e7011d62d946\nInstacart optimizes search relevance by combining traditional keyword-based and vector searches in a hybrid retrieval system. This approach improves search results by balancing precision and coverage for customer queries. The article provides a great overview of how Instacart enhances the shopping experience through search relevance. The growing adoption of pgvector is exciting as it greatly simplifies the overall architecture complexity.\nhttps://tech.instacart.com/optimizing-search-relevance-at-instacart-using-hybrid-retrieval-88cb579b959c\nThe widespread adoption of vector search increases the migration from Elasticsearch to a more simplified design. Like Instacart, Vinted writes about its adoption story of hybrid search with Vespa. I suppose the migration triggers Elasticsearch Open Source again? \nThe article talks about the migration challenges and the hybrid search testing strategies. \nhttps://vinted.engineering/2024/09/05/goodbye-elasticsearch-hello-vespa/\nHaving a proprietary query engine that charges the users by query execution time with no visibility on how optimal the technology is a suboptimal solution. I\u2019m happy that Snowflake provides visibility to per-query execution cost with the query_attribution_history model. Is it sufficient? The article walkthrough is insufficient and provides a framework to measure the cost more accurately. Kudos to the GreyBeam team for posting the full SQL to measure the cost.  \nhttps://blog.greybeam.ai/a-deep-dive-into-snowflakes-query-cost-attribution-finding-cost-per-query/\nNapkin Math is a collection of mental models and calculations designed for software engineers to estimate system behaviors without needing detailed measurements. It covers latency, throughput, and scaling, helping engineers make informed decisions quickly. \nNapkin Math is a valuable resource for data platform engineers because it equips them with quick, intuitive calculations for estimating system performance and resource requirements. This can be crucial when making decisions about scaling data pipelines, optimizing query performance, or estimating the cost of infrastructure changes without detailed simulations. \nhttps://github.com/sirupsen/napkin-math\nThe blog talks about the Apache Beam pipeline's engineering approach to unit testing. Though the blog talks specifically to Beam, every pipeline can adopt the theme.\nDon\u2019t spend more time on framework integration since they are mostly well-tested already.\n Separate the business logic from the pipeline code to be well-tested outside the pipeline.\nRun the entire pipeline with a sample dataset to verify the chain of tasks executed as expected. \nhttps://beam.incubator.apache.org/blog/unit-testing-in-beam/\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-188", "title": "Data Engineering Weekly", "content": "Run Airflow without the hassle and management complexity. Take Astro (the fully managed Airflow solution) for a test drive today and unlock a suite of features designed to simplify, optimize, and scale your data pipelines. For a limited time, new sign-ups will receive a complimentary Airflow Fundamentals Certification exam (normally $150).\nTry For Free \u2192\nThe future of analysis isn't about replacement; it's about augmentation. \nI saw someone quoted in X that using LLM to code is like using Google Maps to drive. At some point, we would wonder how people coded without code agents. The author explores what the future holds for the analyst and their relationship with GenAI.\nhttps://gradientflow.substack.com/p/rethinking-analyst-roles-in-the-age\nIn April 2024, Nikkei(the parent company of FT) issued a\u00a0press release\u00a0announcing internally built and trained Nikkei Language Models (NiLM). NiLM is a series of LLMs specializing in economic and financial information, trained on 40 years\u2019 content of Nikkei group titles and among the largest and highest-quality LLMs in Japanese. The blog,\u00a0like the previous one, explores the role of Gen AI in the publishing industry. \nhttps://medium.com/ft-product-technology/putting-the-gen-ai-puzzle-together-9470c3aaef7a\nThe previous two blogs show a glimpse of the next wave in Gen-AI: the ability to train internal business/ content with LLM and its business impact. The LLM pre-training and post-training paradigms are rolling out the foundational future to enable organizations to build with internal knowledge. The author explores how leading language models in the market support pre and post-training paradigms. \nhttps://magazine.sebastianraschka.com/p/new-llm-pre-training-and-post-training\nInterested in building Generative AI applications but not sure where to get started? Astronomer's latest academy module, Introduction to GenAI with Airflow, covers just this! \ud83c\udf93 \nThis FREE module will cover how Apache Airflow helps power GenAI apps, then move into a hands-on project where you'll build a RAG pipeline that produces a content generation application. You\u2019ll also use some of the most in-demand technologies, including Streamlit, OpenAI, LangChain, and Weaviate!\nStart Learning \u2192\nIt takes a village to engineer data. It requires balanced teamwork with multiple organizational personalities to cultivate a data-driven culture. Spotify writes an excellent article highlighting the key Data Science persona.\nhttps://engineering.atspotify.com/2024/09/are-you-a-dalia-how-we-created-data-science-personas-for-spotifys-analytics-platform/\nHow do we know all the published data products met the production quality? The obvious answer is running the fitness function. A fitness function is a test to evaluate how close a given implementation is to its stated design objectives. The blog narrates key design objectives of data products, such as accuracy, completeness, and timeliness, and explains how to run the data product fitness functions with data catalogs. \nhttps://martinfowler.com/articles/fitness-functions-data-products.html\nData quality is a core pillar of any data governance strategy, but how do you build a data quality program, let alone operationalize it? \nJoin Monte Carlo for a deep dive session into how data governance leaders can take their data quality strategies to the next level with end-to-end data observability. Learn how data observability platforms like Monte Carlo automate detection and incident management workflows.\nmontecarlodata.com\nThe traditional ticket routing algorithm often yields a complex rule engine with a combination of customer-miscategorized topics. This inefficiency often results in poor customer experience. RazorPay writes about how it enhances the customer experience with an AI-powered ticket category classification system.\nhttps://engineering.razorpay.com/enhancing-customer-support-through-ai-powered-ticket-issue-categorization-a1795d9f0f3c\nWix shares the same case study as RazorPay about enriching the routing system to improve the customer experience. The blog narrates the usage of Reinforcement learning-based solutions for customer care routing. It is interesting to see how the trade-off between a higher resolution rate and wait time impacts each other. Wix concluded that a higher resolution rate at the cost of wait time yields more customer satisfaction. \nhttps://www.wix.engineering/post/ai-for-revolutionizing-customer-care-routing-system-at-wix\nIt is very rare to see open and honest learning about using technology, and this time, the author shares the development of Flink SQL. The author\u2019s key learnings from working with Flink SQL include optimizing join and aggregation performance using lookup joins and two-phase aggregation to resolve data skew and minimize state overhead. Additionally, handling distinct aggregations more efficiently through Partial/Final aggregation was crucial, as was leveraging Flink's stream-batch integration for unified real-time and batch processing\nhttps://www.alibabacloud.com/blog/flink-sql-development-experience-sharing_601569\nWe often discussed the cost of the data warehouse but missed the elephant in the room: Kafka. The fundamental function of Kafka handling millions of events per second, in combination with replication and tight coupling of the producer and the consumers to the cluster, puts more pressure on network bandwidth. All that leads to operating Kafka being a complex and expensive process. The StackOverflow blog narrates best design practices for running cost-efficient Kafka clusters. \nhttps://stackoverflow.blog/2024/09/04/best-practices-for-cost-efficient-kafka-clusters\nSampling is one of the most powerful tools for extracting meaning from large datasets. TABLESAMPLE is my fav, but it has its cons. The author explores various other sampling techniques to implement in SQL. \n1. Weighted Sampling Without Replacement (A-ES Algorithm): This technique assigns each row a random key and selects the top rows based on these keys, ensuring fair sampling in proportion to row weights.\n2. Deterministic Sampling: Uses pseudorandom functions to generate repeatable random samples.\n3. Sampling With Replacement: This variation of the A-ES algorithm allows multiple selections of the same row by considering all possible arrivals from a Poisson process.\nhttps://blog.moertel.com/posts/2024-08-23-sampling-with-sql.html\nRust is gaining popularity in data infrastructure, with more new systems built using it. The blog holds the index of popular rust tools in the data infrastructure, and I believe the trend continues to grow from here. \nhttps://xuanwo.io/2024/07-rewrite-bigdata-in-rust/\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-187", "title": "Data Engineering Weekly", "content": "Run Airflow without the hassle and management complexity. Take Astro (the fully managed Airflow solution) for a test drive today and unlock a suite of features designed to simplify, optimize, and scale your data pipelines. For a limited time, new sign-ups will receive a complimentary Airflow Fundamentals Certification exam (normally $150).\nTry For Free \u2192\nThis is a virtual conference at the intersection of Data and AI. It is not a conference for the hype. It\u2019s real users talking about real experiences.\nYou will not hear the words AGI. Instead, you will see engineers, product managers, and founders talking about the data engineering practices of AI/ML. It will be fun and educational.\n- 40+ speakers\n- 12th September 2024\n- Three simultaneous virtual tracks\n- Panels, Workshops, Lighting Talks, Keynotes, Fireside Chats and Entertainment.\nhttps://home.mlops.community/public/events/dataengforai\nThere is no shortage of criticism of dashboards. :-) Dashboards are not going anywhere. Dashboards create a shared sense of reality and a standardized interpretation of the associated data. Spotify writes an excellent overview of its dashboard standardization process, including when to use Tableau over Looker and the internal search portal to find the dashboards.\nhttps://engineering.atspotify.com/2024/08/unlocking-insights-with-high-quality-dashboards-at-scale/\nMany companies are trying to build smarter AI Engineers, but I do think there is more repeatable and manual work that we do that can be potentially automated. In a way, augmenting engineers is more appealing than creating a clone. The blog provides an excellent narrative and opportunity in the SRE space, which is true for data ops, too.  \nhttps://foundationcapital.com/goodbye-aiops-welcome-agentsres-the-next-100b-opportunity/\nUber writes about the offline table creation process with Apache Pinot. The blog narrates the offline data modeling support and integration with frameworks like Apache Spark. \nOne of Apache Pinot's highlights is creating a Pinot index offline and simply uploading it via calling /segment API. The design plays a major role when we reindex/ backfill the production clusters. \nhttps://www.uber.com/blog/pinot-for-low-latency/\nIf you're looking to build, maintain, and more efficiently manage your data pipelines, check out our comprehensive guide, Data Pipelines with Apache Airflow. This ebook covers practical use cases, and provides an overview of key concepts and best practices, including how to set up Airflow in production environments, along with best practices for building, testing, and deploying Airflow DAGs.\nGet Ebook \u2192\nThe simplicity of ClickHouse with one click install and the developer experience stands out among the other OLAP engines. The author evaluates ClickHouse's advantages, challenges in the design, and the ongoing innovations on top of Postgres to become the OLAP system. \nWe often think of OLAP as a storage engine for immutable events but fail to notice the mutable dimensions required to produce an analytical view. While evaluating the OLAP engines, I encountered one challenge with ClickHouse: its inefficiency in handling upsert operations. I don\u2019t know the current state, so please point out any articles that discuss ClickHouse upsert ops. \nhttps://materializedview.io/p/unpacking-the-buzz-around-clickhouse\nIn this post, I\u2019ll focus on merge-on-read as copy-on-write is just a bad idea for any streaming ingest workload. \ud83c\udfaf\nThis is an excellent blog comparing the streaming ingest of row-level operation comparison among the LakeHouse formats. We can broadly classify the support into two categories.\nNative primary key support -  LSM-like design - [Hudi, Paimon)\nNo native primary key support - offload the row-level ops to the compute engines (Iceberg, Delta)\nhttps://jack-vanlightly.com/blog/2024/8/22/table-format-comparisons-streaming-ingest-of-row-level-operations\nThis one-day event will feature the world\u2019s top data leaders, architects, and practitioners coming together to discuss best practices and strategies for designing, building, and scaling trusted data and AI systems that drive real growth and business value. \nTrust me, you don't want to miss it. Register here: \nimpactdatasummit.com\nClassi discusses adopting an event-driven ingestion pipeline design connecting AWS and Google Cloud. I see a pattern where the data warehouse is in Google BigQuery, and the core infrastructure is in AWS. Is it a failure in Redshift? Could it be a time for AWS to consider buying Snowflake?\nhttps://manhnguyen98.medium.com/how-we-migrated-the-current-log-pipeline-to-a-near-real-time-streaming-system-0b054ad53725\nS3 support for conditional writing certainly sparks a lot of excitement. In a nutshell, the S3 PutObject operation now supports an optional If-None-Match header. When specified, the call will only succeed when no file with the same key exists in the target bucket; otherwise, you\u2019ll get a 412 Precondition Failed response. The blog narrates how one could build a leader election with S3; otherwise, it requires specialized infrastructure like Zookeeper.\nhttps://www.morling.dev/blog/leader-election-with-s3-conditional-writes/\nNetflix writes about the reward function in a recommendation engine that focuses on long-term member satisfaction out of implicit feedback like click or skip. It is an insightful read emphasizing the reward function aligned with the business objective.\nhttps://netflixtechblog.com/recommending-for-long-term-member-satisfaction-at-netflix-ac15cada49ef\nInterleaving design involves alternating between two options within the same user experience to observe and compare user reactions in real time. This method ensures a fair and immediate comparison by minimizing external factors and focusing on how users respond to each option as they encounter it. \nDoorDash writes about how it adopted interleaving designs and its advancement in the sensitivity gain of experimentation.\nhttps://doordash.engineering/2024/08/27/doordash-experimentation-with-interleaving-designs/\nEvery business has this at the top of its mind: What kinds of potential users should we give an incentive to, and for how much? Instacart writes about measuring the customers' iLTV (Incremental Lifetime Value) after applying incentives. These articles about surrogate indexes, contextual bandits, etc., have provided many follow-up reads for me. \nhttps://tech.instacart.com/instacarts-economics-team-using-surrogate-indices-to-estimate-long-run-heterogeneous-treatment-0bf7bc96c6e6\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-186", "title": "Data Engineering Weekly", "content": "Run Airflow without the hassle and management complexity. Take Astro (the fully managed Airflow solution) for a test drive today and unlock a suite of features designed to simplify, optimize, and scale your data pipelines. For a limited time, new sign-ups will receive a complimentary Airflow Fundamentals Certification exam (normally $150).\nTry For Free \u2192\nThis is a virtual conference at the intersection of Data and AI. It is not a conference for the hype. It\u2019s real users talking about real experiences.\nYou will not hear the words AGI. Instead, you will see engineers, product managers, and founders talking about the data engineering practices of AI/ML. It will be fun and educational.\n- 40+ speakers\n- 12th September 2024\n- Three simultaneous virtual tracks\n- Panels, Workshops, Lighting Talks, Keynotes, Fireside Chats and Entertainment.\nhttps://home.mlops.community/public/events/dataengforai\nOne of the educational reads for me this week is about how Google search ranking works. The author summarizes the learning from the Google Search Engine paper leak and various public hearing documents from antitrust cases. Google\u2019s search ranking system is a complex, multi-step process that begins with indexing new content, assigning it a unique DocID, and calculating its relevance based on keyword presence. It then passes through various ranking systems like Mustang, Superroot, and NavBoost, which refine the results to the top 10 based on factors like content quality, user behavior, and link analysis.\nhttps://searchengineland.com/how-google-search-ranking-works-445141\nWhen building their data platform, companies face a critical decision: adopt an all-in-one solution from vendors like Databricks, Snowflake, or AWS or compose a custom platform using tools from different providers. The blog is a good overview of various components in a typical data stack. \nI think an all-in-one solution or best-of-breed will be a big decision in the industry in the coming years. The Data Engineering Weekly is trying to address this issue with our buyer\u2019s guide, starting with CDC. \nhttps://location.foursquare.com/resources/blog/leadership/modern-data-platform-an-unbundling-of-a-traditional-data-warehouse/\nStructuring the analytical organization to align with the business goal is always challenging. Several companies have written about centralized vs. decentralized organization structures, and RazorPay has made solid recommendations on approaching the problem. \nhttps://engineering.razorpay.com/structuring-the-analytics-team-distributed-vs-centralized-approaches-ad91f4da9f89\nWhen it comes to success with Generative AI, it\u2019s all about the data \u2014 specifically, your data. Powerful deep learning models are becoming smarter, more accessible and cost-effective. However, it\u2019s only by combining these with rich proprietary datasets and operational data streams that organizations can find true differentiation. This guide discusses the criticality of orchestrated data pipelines for the most common use cases in Generative AI and covers how to build differentiated, resilient, and governed AI applications.\nGet Guide \u2192\nSince its introduction, EBS has come a long way, and we prefer EBS over instance stores in many use cases. The author narrates a fascinating perspective on the journey of EBS, queueing theory with an amazing analogy, and the importance of comprehensive instrumentation in improving the systems. This is a must-read for this week. \nhttps://allthingsdistributed.com/2024/08/continuous-reinvention-a-brief-history-of-block-storage-at-aws.html\nThe separation of storage and computing certainly brings a lot of flexibility in operating data stores. We witnessed the uprising of serverless/S3-dependent message queues and Postgres engines. The author writes an overview of the performance implication of disaggregated systems compared to traditional monolithic databases.\nhttps://muratbuffalo.blogspot.com/2024/07/understanding-performance-implications.html\nMany systems\u2014everything from relational databases, time-series databases, message queues, data warehouses, and services for application metrics\u2014use object storage as a core part of their architecture. \nI often wonder if we are building a pyramid infrastructure scheme on top of the object storage. The author gave a different perspective with the one-way-door vs. two-way-door analogy, stating that object storage has been around for 20+ years now. The prediction around the innovation of the programming model is an interesting read.  \nhttps://blog.colinbreck.com/predicting-the-future-of-distributed-systems/\n\u00b5Wheel is an event-driven aggregate management system for ingesting, indexing, and querying stream aggregates. The author writes about integrating uwheel with DataFusion (a query engine for building high-quality data-centric systems in Rust, using the Apache Arrow in-memory format). The temporal aggregation and pruning using custom indices, which drastically reduce query execution times, is exciting, and I look forward to trying this out soon. \nhttps://uwheel.rs/post/datafusion/\nhttps://uwheel.rs/post/datafusion_uwheel/\nThe blog discusses the key design principles and best practices for collaborative development using protobuf. More than that, I included this blog to remind you of the importance of event governance and a structured eventing approach, which is critical for an organization to build a strong data foundation.\nTreat Events as a first-class citizen, and remember that it is always the upstream that causes the failure. \nhttps://eng.lyft.com/protocol-buffer-design-principles-and-practices-for-collaborative-development-8f5aa7e6ed85\nAirbnb writes about the criticality of data classification and the impact on user trust if we fail to do so. The blog narrates the shift-left approach in data governance with three critical principles.\nShift data classification from data to schema\nShift classification from offline to online\nShift from Data Steward to Data Owner\nhttps://medium.com/airbnb-engineering/personal-data-classification-2d816d8ea516\nApache Parquet becomes the defacto columnar storage for LakeHouse formats, and it is a must for data engineers to know its internals. The author did an amazing job of describing how Parquet stores the data and compression and metadata strategies.  \nhttps://medium.com/@vutrinh274/i-spent-8-hours-learning-parquet-heres-what-i-discovered-97add13fb28f\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-185", "title": "Data Engineering Weekly", "content": "Run Airflow without the hassle and management complexity. Take Astro (the fully managed Airflow solution) for a test drive today and unlock a suite of features designed to simplify, optimize, and scale your data pipelines. For a limited time, new sign-ups will receive a complimentary Airflow Fundamentals Certification exam (normally $150).\nTry For Free \u2192\nIn last week's edition, we saw a couple of developer stories on how developers use Gen-AI. [How I Use \"AI\" & Automating away the boring parts of my job]. The recent Stack Overflow survey echoes the statement that the usage of AI tools is gaining popularity in the development process. \nhttps://survey.stackoverflow.co/2024/ai/\nThe blog highlights the current state of agent infrastructure. The next phase of AI innovation will revolve around automating the boring stuff and skills essential to human life, but expertise is hard to obtain. Agents perfectly fit into that, where it expands the scope of LLMs' capabilities by pairing foundation models with customer-specific data, multi-step reasoning, and the ability to take action on users\u2019 behalf. \nhttps://www.felicis.com/insight/the-agentic-web\nThis latest version of Apache Airflow brings greater flexibility and expansion of some of the most widely used features \u2728 Join this webinar for an in-depth look at the features that make Apache Airflow even better:\n\ud83d\udcab New data-aware scheduling capabilities\u2705 Hybrid execution (allows you to use multiple executors in the same Airflow instance)\ud83d\udda5\ufe0f Updates to the UI, including the widely anticipated dark modeWherever you are on your data orchestration journey, don\u2019t miss this chance to learn from the experts and have your questions answered \ud83d\ude4b\nSave Your Spot \u2192\nLinkedIn writes about its semantic search capabilities in content search. The content search engine has two layers: a retrieval layer and a multi-stage ranking layer. When a query is received, the retrieval layer selects a few thousand candidate posts from the overall pool of billions of posts. Then, the multi-stage ranking layer scores these candidate posts in two stages and returns a ranked list of posts.\nhttps://www.linkedin.com/blog/engineering/search/introducing-semantic-capability-in-linkedins-content-search-engine\nEtsy writes about its machine learning-based content moderation system, which leverages supervised learning to detect policy violations on its platform. The system utilizes multimodal signals from text and images to train models predicting whether listings violate Etsy\u2019s policies. The architecture includes BERT-based text encoders and EfficientNet image encoders, with a final softmax layer for classification.\u00a0\nhttps://www.etsy.com/codeascraft/machine-learning-in-content-moderation-at-etsy\nThe industry is also moving towards writing ETL jobs similarly to any other software development practice, with features such as modular ETL, test-driven development, data quality checks, observability, version control, etc\nI can\u2019t agree with this enough. The need to adopt software development practices in the ETL process is much higher, as the success of AI-driven applications depends on data quality. The tooling in this area is still not well developed and has a long way to go. Uber writes about Sparkle, a modular ETL framework that brings the best software engineering practices into the ETL process. \nhttps://www.uber.com/blog/sparkle-modular-etl/\nEvery infrastructure will require automation and standardization to accelerate the adoption after reaching a critical mass. DoorDash writes about its Kafka Self-Serve platform, which enables engineers to manage Kafka resources, such as topics and user access, with minimal reliance on infrastructure teams. It simplifies configurations, enforces best practices, and offers a user-friendly interface via its internal DevConsole.  \nhttps://doordash.engineering/2024/08/13/doordash-engineers-with-kafka-self-serve/\nThis is another insightful blog from the author on the LakeHouse format. It compares how the table formats represent the canonical set of files. The approach essentially falls into two categories.\nThe log of deltas approach (Hudi and Delta Lake)\nThe log of snapshots approach (Iceberg and Paimon)\nThe key highlights are,\nDelta Lake periodically writes a checkpoint to the log, which rolls up all deltas to make a snapshot as a Parquet file.\nHudi maintains the current snapshot in the metadata table.\nIceberg and Paimon use a log of snapshots but register the changes made in each snapshot.\nhttps://jack-vanlightly.com/blog/2024/8/7/table-format-comparisons-how-do-the-table-formats-represent-the-canonical-set-of-files\nIn the spirit of learning how the LakeHouse systems represent canonical files, the blog about how Postgres stores data gives me an excellent comparison read. \nhttps://drew.silcock.dev/blog/how-postgres-stores-data-on-disk/\nStaying on LakeHouse formats, Flipkart writes about benchmarking results on Iceberg and Hudi. It is good to see Flipkart publish the configurations and benchmarking results. \nFlipkart concludes that Iceberg performs better on load profiles and upserts, whereas Hudi performs better in query performance, especially at larger data sizes.\nhttps://blog.flipkart.tech/iceberg-vs-hudi-benchmarking-tableformats-dffe6f81f26e\nYelp writes about extending dbt\u2019s generic test framework by demonstrating the importance of building a standard set of quality checks for their user session data mart. The approach certainly triggered my thoughts on the possibility of building a standard set of domain-specific quality tests in finance, marketing, and so on. \nhttps://engineeringblog.yelp.com/2024/08/dbt-Generic-Tests-in-Sessions-Validation-at-Yelp.html\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-184", "title": "Data Engineering Weekly", "content": "Run Airflow without the hassle and management complexity. Take Astro (the fully managed Airflow solution) for a test drive today and unlock a suite of features designed to simplify, optimize, and scale your data pipelines. For a limited time, new sign-ups will receive a complimentary Airflow Fundamentals Certification exam (normally $150).\nTry For Free \u2192\nI don't think that \"AI\" models (by which I mean large language models) are over-hyped. \nBut the reason I think that the recent advances we've made aren't just hype is that, over the past year, I have spent at least a few hours every week interacting with various large language models and have been consistently impressed by their ability to solve increasingly difficult tasks I give them.\nI second the author\u2019s thoughts here, and I\u2019m sure many share similar experiences. The author wrote a comprehensive article highlighting how LLM helped with tasks that were otherwise much harder. \nhttps://nicholas.carlini.com/writing/2024/how-i-use-ai.html\nFollowing people sharing how LLM improves their quality of life by automating repetitive tasks, the author highlights the use of LLM from a different perspective. The product feedback analysis from Github issues and analyzing user experience from video/ audio are very helpful in enriching knowledge on certain technologies, too. \nhttps://medium.com/@webpaige/automating-away-the-boring-parts-of-my-job-with-gemini-1-5-pro-long-context-windows-6d5a1d9a6f38\nThis event will be packed with practical and insightful sessions around everything data orchestration, GenAI, ETL Orchestration, the future of Airflow and more. Check out the sessions and speakers here, and use discount code 30DISC_ASTRONOMER for 30% off your ticket! \ud83c\udf9f\ufe0f\nGet Your Tickets \u2192\nEmbedding and chunking are critical natural language processing (NLP) and information retrieval techniques. Embedding transforms the text into dense vector representations that capture semantic meaning, enabling more effective similarity searches, clustering, and classification.\u00a0On the other hand, chunking refers to breaking down large texts or data into smaller, manageable units, often for more efficient processing or to preserve context. The author writes a comprehensive overview of Embedding and Chunking. \nhttps://malaikannan.github.io//2024/08/05/Chunking/\nhttps://malaikannan.github.io//2024/07/31/Embeddings/\nAI coding assistant is one of the widely used applications of LLM. What would be the logical next step once you know about embedding & chunking? Well, build your own AI code assistant. The author writes a comprehensive guide on building AI code assistance, and the best part is the code is open source. \nhttps://www.thenile.dev/blog/building_code_assistant\nAs data volumes continue growing, Storing and processing the data that powers these metrics comes with significant costs related to storage, computational resources, and processing power. Meta narrates how it builds deterministic sampling to reduce the event volume and how it helps in testing and QA environments.\nhttps://medium.com/@AnalyticsAtMeta/scaling-analytics-instagram-the-power-of-deterministic-sampling-8ee7332d77ae\nUber writes about securing a Hadoop-based data lake on Google Cloud Platform (GCP) by replacing HDFS with Google Cloud Storage (GCS) while maintaining existing security models like Kerberos-based authentication. The blog narrates the development of a layered access model and built a Storage Access Service (SAS) to bridge the security differences between HDFS and GCS, with the implementation of a multi-layer caching strategy to scale the system, enabling the handling of high request volumes with low latency.\nhttps://www.uber.com/blog/securing-hadoop-on-gcp/\nJack Vanlightly\u2019s article delves into Delta Lake\u2019s consistency model, highlighting its support for ACID transactional guarantees with a focus on consistency and isolation. Delta Lake uses a write-ahead log (the delta log) to maintain atomicity, and it achieves snapshot isolation for reads through multi-version concurrency control (MVCC). The article also explains the role of optimistic concurrency control in handling multiple concurrent writers, ensuring data integrity, and avoiding conflicts during transactions on Delta Lake.\nhttps://jack-vanlightly.com/analyses/2024/4/29/understanding-delta-lakes-consistency-model\nMiles Cole\u2019s article explores the compatibility issues between Delta Lake implementations in Databricks and Microsoft Fabric, highlighting key differences and challenges. The article provides a detailed compatibility matrix and offers guidance on managing features like Liquid Clustering and V2 Checkpoints to ensure interoperability between different Delta Lake environments. It is a fast-moving field, so I suppose this comparison will get outdated pretty soon. \nhttps://milescole.dev/integration/2024/03/22/Decoding-Delta-Lake-Compatibility-Between-Fabric-and-Databricks.html\nOnce the data pipeline reaches a critical adoption, the abstract pipeline brings much-needed standardization to improve developer productivity and reduce cost. Walmart wrote about how it saved millions of dollars with unified configuration-driven data pipelines. \nhttps://medium.com/walmartglobaltech/achieve-million-dollar-savings-with-unified-code-and-configuration-driven-data-pipelines-ee938c2d12a6\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/evaluating-change-data-capture-tools", "title": "Data Engineering Weekly", "content": "TL;DR Aswin and I are thrilled to announce the release of the first version of our comprehensive guide for evaluating Change Data Capture.\nCDC Evaluation Guide Google Sheet Link: https://bit.ly/cdc-evaluation-guide\nCDC Evaluation Guide Github Link: https://github.com/EvalBench/cdc\nChange Data Capture (CDC) is a powerful technology in data engineering that allows for continuously capturing changes (inserts, updates, and deletes) made to source systems. CDC tools fuel analytical apps and mission-critical data feeds in banking and regulated industries, with use cases ranging from data synchronization, managing risk, and preventing fraud to driving personalization.\nAs we advance into the Gen AI era, Change Data Capture (CDC) systems are emerging as crucial components of the ever-evolving data architecture. The ability to capture and propagate data changes in real time opens up numerous opportunities and addresses several contemporary business and technical challenges. Here are some key reasons why CDC is becoming more relevant than ever.\u00a0\nLakehouses have become a standard pattern in data infrastructure, combining the best features of data lakes and warehouses. Unlike data lakes, which are predominantly append-only, lakehouses support data mutation natively. The support for the mutation feature means they can handle updates and deletions seamlessly, an ability that enhances real-time synchronization through CDC (Change Data Capture) tools. As a result, lakehouses support more dynamic and flexible data architectures, catering to a broader range of analytics and operational workloads. For instance, in a fast-paced retail environment, lakehouses can ensure that inventory data remains up-to-date and accurate in the data warehouse, optimizing supply chain efficiency.\nMany businesses still rely on legacy systems like Mainframes and SAP for their core operations. However, these businesses must unlock the value within these systems and leverage it in modern, cloud-based analytical environments. CDC tools facilitate nonintrusive data extraction and real-time integration from these legacy systems into modern platforms without disrupting ongoing transactions. This capability is crucial as it allows businesses to modernize their data infrastructure incrementally, ensuring a smooth transition and continuous value extraction from legacy investments.\nIn today\u2019s fast-paced business environment, responding to real-time events is a competitive advantage. Whether it\u2019s launching targeted marketing campaigns, optimizing supply chains, or personalizing customer experiences, businesses are increasingly demanding real-time insights and actions. CDC enables real-time data flows from operational systems to analytics platforms, ensuring decision-makers can access the most current data. This immediacy is critical for applications that require up-to-the-minute accuracy and responsiveness.\nAnother significant trend is the shift towards warehouse-native product analytics. Instead of relying on separate analytical databases, many organizations embed analytics directly within their data warehouses. This approach simplifies data architecture and enhances performance by reducing data movement and latency. CDC is integral to this model, ensuring that any changes in transactional data are instantly available for analysis within the data warehouse. This seamless integration supports more sophisticated and timely product analytics, driving better business outcomes.\nThe adoption of microservices architecture has transformed how applications are developed and deployed. However, managing data consistency across microservices can be challenging. A common solution is the outbox pattern, where changes are written to an outbox table and then asynchronously processed. CDC systems can efficiently capture and propagate these outbox changes to other services or analytics platforms. This approach ensures data consistency and supports the decoupling of microservices, enabling more scalable and resilient architectures.\nAdvancements in Artificial Intelligence (AI) and large language models (LLMs) are driving the need for context-rich applications like Retrieval-Augmented Generation (RAG) and Knowledge Graphs. These applications require comprehensive and up-to-date contextual information to function effectively. CDC provides real-time data feeds to populate and update these knowledge bases continuously. By capturing and integrating data changes from diverse sources, CDC helps build and maintain the context-rich environments that modern AI applications demand.\nTimestamp-based CDC relies on a timestamp column that tracks the last modification time of a record. It identifies changed records by comparing timestamps, making it less invasive and easy to implement for batch feeds.\nModify the database schema to include timestamp columns. Most packaged software backends lack this, requiring alternative extraction methods.\nMissed changes occur if the timestamp is not updated correctly.\nUnsuitable for databases without native modification timestamp support.\nImpacts source system performance during query execution.\nDoes not handle hard deletes.\nThe Schema evolution management pushes to the pipeline code to manage.\u00a0\nTable-diff CDC compares the current state of the data with a previous snapshot to identify changes. This method is often a last resort when other CDC techniques are infeasible.\nHighly resource-intensive and slow for large datasets.\nRequires significant storage for maintaining snapshots.\nComplex to implement and manage.\nTrigger-based CDC uses database triggers to capture changes. Triggers automatically execute specified actions when events like inserts, updates, or deletes occur. This method is straightforward to implement and compatible with many databases.\nIt can introduce significant overhead on the database.\nComplex to manage and debug, especially in high-transaction environments.\nPotential performance degradation if not carefully optimized.\nLog-based CDC reads the database transaction log to capture changes. It is highly efficient with minimal impact on the source database, and can handle high volumes of changes with low latency. This technique focuses on most real-time use cases and the evaluation document. However, access to the transaction log is required, which may not always be feasible depending on the database system.\nRequires deep integration with the database system.\nMay face issues with proprietary log formats.\nLog retention policies need careful handling to avoid data loss.\nEvaluating a Change Data Capture (CDC) tool requires a comprehensive understanding of various critical categories. These categories ensure the CDC tool is efficient, reliable, and suitable for the organization's needs. Below are the essential evaluation criteria:\nBootstrap capabilities are crucial for initializing the CDC process by capturing the initial data state before incremental changes. An effective CDC tool should:\n- Support full initial data loads seamlessly.\n- Provide mechanisms for handling large datasets efficiently.\n- Ensure minimal impact on source systems during the bootstrap phase.\nThe source connector ecosystem determines the tool's ability to integrate with various data sources. Key considerations include:\n- Wide range of supported databases and applications.\n- Compatibility with both on-premise and cloud-based sources.\n- Regular updates and support for new data sources.\nThis category evaluates the CDC tool's ability to deliver data to various targets. An optimal CDC tool should:\n- Support various target systems, including databases, data warehouses, and data lakes.\n- Ensure compatibility with cloud and on-premise environments.\n- Facilitate seamless data delivery to real-time analytics platforms and other endpoints.\nThe core functionalities that make the CDC tool robust and effective include:\n- Low-latency data capture and delivery.\n- Reliable change data capture mechanisms (e.g., log-based, trigger-based).\n- Support for schema evolution and data type transformations.\nEffective data management and transformation capabilities are essential for maintaining quality and usability. The CDC tool should:\n- Provide data filtering, enrichment, and transformation features.\n- Ensure data consistency and integrity during capture and delivery.\n- Support complex transformation logic and integration with external ETL processes.\nA CDC tool's ability to integrate with existing systems and extend its functionalities is crucial. Considerations include:\n- APIs and SDKs for custom integrations.\n- Compatibility with data integration and orchestration tools.\n- Support for extensibility through plugins and custom scripts.\nDeployment flexibility ensures the CDC tool fits within the organization's infrastructure. Key aspects include:\n- Support for various deployment models (e.g., on-premise, cloud, hybrid).\n- Ease of installation and configuration.\n- Scalability to handle growing data volumes and increased change frequencies.\nStrong vendor support and an active community are vital for ongoing success. Important factors are:\n- Professional support and training services are available.\n- Comprehensive documentation and knowledge base.\n- Active user community for peer support and knowledge sharing.\nEnsuring data security and robust monitoring capabilities are non-negotiable. The CDC tool should:\n- Implement strong authentication and authorization mechanisms.\n- Provide encryption for data in transit and at rest.\n- Offer comprehensive monitoring and alerting features for operational visibility.\nA user-friendly interface and ease of use significantly enhance productivity. Key elements include:\n- Intuitive user interface and user experience design.\n- Clear and comprehensive reporting and dashboards.\n- Streamlined workflows for common tasks and troubleshooting.\nEvaluating CDC tools against these categories ensures a holistic assessment, helping organizations select tools that align with their technical requirements and strategic objectives. This structured approach facilitates informed decision-making, enabling seamless data integration and real-time analytics capabilities.\nIn this series of data tool evaluations, we aim to bring sharable intelligence to our community. Historically, benchmark studies have dominated software evaluation, often showing vendor bias or lacking practical value. We aim to establish standard evaluation criteria to help our community build educated data infrastructure tailored to their business needs. By combining our collective learning, we can develop efficient data engineering practices.\nWe invite your support and contribution to this evaluation series. To facilitate open collaboration, we have released the evaluation criteria under the MIT license on GitHub. You can review, contribute to, and refine the criteria to ensure they meet the highest standards. Together, we can create a robust and adaptable framework for evaluating data tools that benefit everyone.\nPlease contribute additional evaluation criteria that we should include in the evaluation matrix.\nShare your experience of evaluating the CDC tool. It can be a blog link or a short note in README.\u00a0\nIf you want to share your evaluation experience over a podcast, please fill out this form.\nPlease contribute additional evaluation criteria that we should include in the evaluation matrix.\nShare your customer case study using your CDC tool. It can be a blog link or a short note in README.\u00a0\nIf you want to share your evaluation experience over a podcast, please comment below.\nhttps://shopify.engineering/capturing-every-change-shopify-sharded-monolith\nhttps://medium.com/brexeng/change-data-capture-at-brex-c71263616dd7\nhttps://medium.com/capital-one-tech/the-journey-from-batch-to-real-time-with-change-data-capture-c598e56146be\nhttps://www.wix.engineering/post/change-data-capture-at-deviantart\nhttps://bytes.swiggy.com/architecture-of-cdc-system-a975a081691f\nhttps://www.confluent.io/blog/how-bolt-adopted-cdc-with-confluent-for-real-time-data-and-analytics/\nhttps://www.coinbase.com/blog/soon-for-near-real-time-data-at-coinbase-part-1\nhttps://medium.com/@wyaddow/planning-your-tests-for-change-data-capture-cdc-e80c462330e1"}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-183", "title": "Data Engineering Weekly", "content": "Run Airflow without the hassle and management complexity. Take Astro (the fully managed Airflow solution) for a test drive today and unlock a suite of features designed to simplify, optimize, and scale your data pipelines. For a limited time, new sign-ups will receive a complimentary Airflow Fundamentals Certification exam (normally $150).\nTry For Free \u2192\nFor the last two weeks, we met several CDC experts and multiple iterations of preparing the evaluation guide for CDC tools. The CDC evaluation guide will be an open license to distribute and contribute freely. Thank you, everyone, for contributing to the evaluation guide. We will be releasing the CDC guide next Wednesday!! Stay Tuned. \nAs the next edition of the evaluation series, we are looking to take\u00a0Data Observability. We would like to hear from you if you have an opinion on data observability evaluation or have done any recent evaluations. Please DM us through LinkedIn; Let\u2019s connect!!!\nIt is finally here. Snowflake opensource Polaris catalog, an Apache Iceberg\u2019s REST catalog specification. The Iceberg\u2019s REST catalog support naturally extends to supporting Doris\u2122, Apache Flink\u2122, Apache Spark\u2122, Daft, DuckDB, Presto, SingleStore, Snowflake, Starburst, Trino, Upsolver, and more, and I\u2019m sure more to follow. \nFrom this vantage point, it seems data lakehouse vendors have accidentally stumbled upon the wedge that traditional data catalogs have been struggling to find. - Chris Riccomini https://twitter.com/criccomini/status/1808554909375344719\nIt will be interesting to observe the convergence of technical and governance catalogs within the emerging trend of open-source data catalog implementations, as these catalogs have historically been separate entities. \nhttps://www.snowflake.com/blog/polaris-catalog-open-source/\nIf you ask any executives in an organization, they will say they are data-driven. It is often an expression of desire rather than reality. Thus, the data team has more responsibility than just ingesting and building the data pipeline. The blog makes an excellent point on what makes data illiterate. I\u2019m curious how data platforms can automate such things to make these mistakes disappear. \nhttps://jxnl.co/writing/2024/06/02/10-ways-to-be-data-illiterate-and-how-to-avoid-them/\nPinterest writes about its adoption of StarRocks and migration from Apache Druid. The migration yields Pinterest a 50% reduction in p90 latency and a 3-fold reduction in cost. \nThe Archmage thrift proxy layer to hide the infrastructure and enable easier version upgrades is a good engineering practice that I would recommend for any platform team. \nhttps://medium.com/pinterest-engineering/delivering-faster-analytics-at-pinterest-a639cdfad374\nModern, full-stack orchestration unifies observability across all layers of your data environment, enhancing data product reliability, increasing development speed, and lowering costs. This guide provides key insights into developing a unified data orchestration framework that enhances data reliability, business agility, and operational efficiency.\nGet Your Copy \u2192\nUber writes about using Apache Pinot to serve subsecond latency analytics at scale. The blog narrates the challenges of implementing Apache Pinot at scale and the optimization techniques in Pinot to yield high performance. The p99 read latency of ~1 second is impressive, considering Uber\u2019s scale. \nhttps://www.uber.com/en-GB/blog/job-counting-at-scale/\nContinuing our case studies with low latency, user-facing analytics stories from Pinterest and Uber, Dune writes about its usage of DuckDB. I am interested in the emerging pattern around DuckDB. \nDune writes about its re-architecture of introducing DuckDB as a caching layer with Trino to make interactive queries run faster.  \nhttps://dune.com/blog/how-weve-improved-dune-api-using-duckdb\nConsistency models are crucial in Lakehouse formats because they define the rules governing how and when data updates are visible to different users and processes. \nThe blog is an excellent overview of the consistency model in Apache Hudi. The author provides a comprehensive guide on other Lakehouse consistency models, which we will cover in later editions. \nhttps://jack-vanlightly.com/analyses/2024/4/24/understanding-apache-hudi-consistency-model-part-1\nhttps://jack-vanlightly.com/analyses/2024/4/24/understanding-apache-hudi-consistency-model-part-2\nhttps://jack-vanlightly.com/analyses/2024/4/25/understanding-apache-hudi-consistency-model-part-3\nData exploration for data science and machine learning typically requires a sample data set to be available for faster iteration. Hudi-rs project is an interesting one that provides an abstraction to bring part of Hudi data to DuckDB for interactive analytics. \nIf you want to expose Apache Hudi table to data scientists or sub-second latency user-facing analytics, Hudi-rs fits perfectly into the bill. \nhttps://dipankar-tnt.medium.com/hudi-rs-with-duckdb-polars-daft-datafusion-single-node-lakehouse-347ee1a45371\nMost LLM chat & embedding APIs charge the users by the number of tokens per request model. I won\u2019t be surprised to hear the term \u201cToken Tax\u201d soon, as AI companies want to be profitable. The article analyzes methods to optimize token usage, particularly in creating JSON and YAML formats. It also introduces innovative constrained generation techniques that promise to revolutionize how we approach structured data generation.\nhttps://medium.com/data-science-at-microsoft/token-efficiency-with-structured-output-from-language-models-be2e51d3d9d5\nWith over 400 million SKUs, Walmart writes about using Gen AI to improve product categorization. By incorporating domain-specific features and human-labeled data, Walmart avoids reliance on noisy customer engagement data, thus improving the categorization accuracy for both common and uncommon products on the platform.\nhttps://medium.com/walmartglobaltech/using-predictive-and-gen-ai-to-improve-product-categorization-at-walmart-dc9821c6a481\nData processing efficiency has taken center stage as we starkly moved away from the big data and massively parallel computing era. We look at efficiency in data processing within a single node, which increases the momentum for systems like DuckDB, Arrow, and Polaris. Rust is becoming a defacto tool for data toolings with excellent Python binding. Echoing a similar trend, Thomson Reuters writes about challenges in processing documents at scale and how it started utilizing Parquet, Arrow, and Polaris to improve efficiency. \nhttps://medium.com/tr-labs-ml-engineering-blog/performance-engineering-in-document-understanding-161b59afc9f0\nhttps://medium.com/tr-labs-ml-engineering-blog/performance-engineering-in-document-understanding-b54642ced03c\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-182", "title": "Data Engineering Weekly", "content": "Probability one of the hottest announcements this week is Llama 3.1 release - the first-ever open-sourced frontier AI model competitive with leading foundation models across a range of tasks, including GPT-4, GPT-4o, and Claude 3.5 Sonnet.\nThe\u00a0Llama3 herd of models\u00a0is an insightful paper that helps one deeply understand the foundational model. \nhttps://ai.meta.com/blog/meta-llama-3-1/\nWe can\u2019t deny that Gen-AI is becoming an integral part of product strategy, pushing the need for platform engineering. The blog is an excellent summarization of the common patterns emerging in GenAI platforms.\nhttps://huyenchip.com/2024/07/25/genai-platform.html\nAmazon\u2019s migration from Apache Spark to Ray is possibly the most fascinating read of recent times. Amazon discusses their CDC use cases and the challenges with Apache Spark in running copy-on-write at scale.  Switching from Apache Spark to Ray improves compact 12X larger datasets than Apache Spark, improves cost efficiency by 91%, and processes 13X more data per hour.\nhttps://aws.amazon.com/blogs/opensource/amazons-exabyte-scale-migration-from-apache-spark-to-ray-on-amazon-ec2/\nInstacart writes about integrating LLM in their interview process and how it helps them identify the right candidates. I like testing people on their practical knowledge rather than artificial coding challenges. \nAdopting LLM in SQL-centric workflow is particularly interesting since companies increasingly try text-2-SQL to boost data usage. Swiggy recently wrote about its internal platform,\u00a0Hermes, a text-to-SQL solution. The blog Prompt Engineering for a Better SQL Code Generation With LLMs is a pretty good guide on applying prompt engineering to improve productivity. \nhttps://tech.instacart.com/data-science-spotlight-cracking-the-sql-interview-at-instacart-llm-edition-52d04bde474c\nWhat would the correct ratio of the number of data engineers corresponding to the overall people count in an organization? What are the different roles inside the data engineering functions, and what should their ratio be? The author provides insights into how the companies are structured and how the data engineering population compares to the overall employee population in an organization.\n \nhttps://medium.com/@mikldd/how-top-data-teams-are-structured-48d46a64b990\nNetflix open-sources its workflow orchestration engine, Maestro, which is already close to 2000+ likes on Github. JSON workflow definition gives flexibility to build DSL on higher-level languages like Python & Java. Though I understand the motivation behind a custom SEL (Safe Expression Language), it might increase the learning curve. \nA key highlight for me is the following features from Maestro.\nRollup support and the fact they wrote about it.\nPipeline breakpoint feature.\nhttps://netflixtechblog.com/maestro-netflixs-workflow-orchestrator-ee13a06f9c78\nThe effectiveness of Kafka Tiered-Storage is a widely discussed topic. It can always lead to unexpected side effects, as noted in the recent Honeycomb blog Investigating Mysterious Kafka Broker I/O When Using Confluent Tiered Storage. \nUber writes a comprehensive guide on Kafka\u2019s tiered storage and explains the interface design, which helps you write your tiered storage support. \nhttps://www.uber.com/blog/kafka-tiered-storage/\nBooking.com writes about its ranking platform, which is pivotal in its wider search platform. The blog highlights key challenges with building a system with a subsecond latency and three 9\u2019s availability. \nhttps://medium.com/booking-com-development/the-engineering-behind-booking-coms-ranking-platform-a-system-overview-2fb222003ca6\nIt is always exciting to read how you internally use a tool you build and sell to your customers. Databricks shares one such story: adopting Unity Catalog and its migration journey from the Hive Meta Store. \nThe phase-by-phase approach is an excellent case study for migrating your data catalogs to the unity catalog. \nhttps://www.databricks.com/blog/databricks-databricks-kicking-journey-governance-unity-catalog\nServerless of anything (Postgres, Kafka, Redis) is the hot trend in infrastructure development. Decoupling computing from storage allows for independent and elastic scaling of computing and storage resources. The blog highlights the 2024 Sigmod paper Understanding the Performance Implications of the Design Principles in Storage-Disaggregated Databases.\nThe author summarizes the key design principles of disaggregated databases, including:\nSoftware-level Disaggregation (P1): Decoupling the storage engine from the compute engine.\nLog-as-the-Database (P2): Sending only write-ahead logs to the storage side upon transaction commit.\nShared-Storage Design (P3): Allowing multiple compute nodes to share the same storage.\nPerformance is always a key question when storage and computing are separated, and the author highlights that buffering reduces the performance gap between disaggregated and non-disaggregated databases.\nhttps://muratbuffalo.blogspot.com/2024/07/understanding-performance-implications.html\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-181", "title": "Data Engineering Weekly", "content": "There are plenty of data tools and vendors in the industry. But how can we choose a tool for the specific need? The traditional evaluation of running PoC on all the selected vendor tools is time-consuming and practically unviable for growth-driven companies.\nData Engineering Weekly is launching a new series on software evaluation focused on data engineering to better guide data engineering leaders in evaluating data tools. We are starting with Change Data Capture tools and expanding to other categories. Stay tuned!!!\nA strong foundational analytical function is vital to making informed decisions, operational efficiency, and competitive differentiation. The author highlights key lessons learning building an analytical org from scratch with key highlights,\n1. Focus on Needs Over Nomenclature: Define the outcomes you want from your analytics team instead of getting caught up in the semantics of terms like analytics, data science, and business intelligence.\n2. The Three C\u2019s of Analytics: Emphasize data creation, curation, and consumption. Build reliable data, maintain usable data models, and ensure the data is interpreted correctly for decision-making.\n3. Hiring the Right Team: Start with generalists who possess both technical and soft skills. As the organization grows, bring specialists to fill specific gaps and ensure a strong, adaptable team.\nhttps://review.firstround.com/starting-an-analytics-org-from-scratch-lessons-from-a-decade-at-doordash/\nGen AI is a past moving that often overwhelms you if you start exploring it. What initially everyone thought of as simple prompts in English has more of an engineering effort than it appears to be. The blog is an excellent summary of what one needs to know about Gen-AI to start.\nhttps://www.thenile.dev/blog/all-about-ai\nThe article \"Extrinsic Hallucinations in LLMs\" explores the causes and solutions for hallucinations in large language models (LLMs). It addresses pre-training data issues and the challenges of fine-tuning with new knowledge, presenting methods for detecting hallucinations, such as retrieval-augmented evaluation and sampling-based detection. Additionally, the article discusses anti-hallucination techniques like RAG, fine-tuning for factuality, and retrieval-augmented generation.\nKey insights emphasize the importance of grounding model outputs in factual information and employing retrieval methods to enhance model accuracy and reduce hallucinations.\nhttps://lilianweng.github.io/posts/2024-07-07-hallucination/\n80% of enterprise data exists in difficult-to-use formats like HTML, PDF, CSV, PNG, PPTX, and more. The current extraction models from unstructured data often rely on OCR and layout detection to process text and images separately, leading to potential indexing errors and inefficiencies.\nColPali\u2019s approach integrates visual and textual cues more effectively, reducing indexing complexity and enhancing retrieval performance. It's an interesting method to keep track of it.\nhttps://huggingface.co/blog/manu/colpali\nOne of the most significant impacts of large language models (LLMs) is their ability to make information retrieval from multimedia content more accessible. Vimeo discusses its Retrieval-Augmented Generation (RAG) system design for building a knowledge management system. Their bottom-up approach to sentence chunking and entity detection in video conversations is particularly exciting and worth reading about.\nhttps://medium.com/vimeo-engineering-blog/unlocking-knowledge-sharing-for-videos-with-rag-810ab496ae59\nIs serverless computing optimal for all workloads? What is the cost-fit function with serverless vs managed infrastructure? \nThe Sync computing experiments concluded that serverless computing on Databricks is not always cost-effective. It is ideal for short/ad-hoc jobs due to no spin-up time, but it offers limited control and poses migration challenges.\nPersonally, I like the flexibility of not worrying about upgrades and infrastructure. What is your experience with Databricks serverless compute? Please comment. \nhttps://medium.com/sync-computing/top-9-lessons-learned-about-databricks-jobs-serverless-41a43e99ded5\nAdopting the devops principle in the analytical pipeline is every data team's quest. The Klaviyo team writes about its continuous integration with dbt using Dbt cloud\u2019s Slim CI. \nPart 1: https://klaviyo.tech/continuous-integration-with-dbt-c0746d62271c\nPart 2: https://klaviyo.tech/continuous-integration-with-dbt-part-2-47c093a0548e\nThe article How to Create CI/CD Pipelines for dbt Core explains in-depth how the Slim CI works.\nArtemis is Adevinta's innovative system that enhances data quality monitoring by granting teams greater autonomy. Artemis promotes proactive data issue management by allowing teams to define their own data quality checks and set up alerts. It is exciting to see more focused frameworks for data quality, but the success lies in how well these systems integrate with the workflow orchestration engines.\nhttps://medium.com/adevinta-tech-blog/artemis-fostering-autonomy-in-monitoring-data-quality-df59c0d86ef7\nI found this very interesting: a javascript engine on top of a database!! The blog compares the advantages of Javascript-stored programs vs. SQL-stored programs. \nIn the data warehouse, the programming abstraction standard is around SQL and dataframes. The last attempt with something different is Apache Pig, which quickly fades out of popularity. The closest I can think of similar to PL/SQL is dbt, a Python wrapper on top of SQL. We have seen a lot of standardization on table formats; will we see standardization for dbt-like systems?  \nhttps://blogs.oracle.com/mysql/post/a-quick-introduction-to-javascript-stored-programs-in-mysql\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-180", "title": "Data Engineering Weekly", "content": "Canva writes about its event collection infrastructure capabilities, handling 25 billion events per day (800 billion events per month) with 99.999% uptime. \nAt our team\u2019s inception, a key decision we made, one we still believe to be a big part of our success, was that every collected event must have a machine-readable, well-documented schema. \nThe principle is the key motivation for me to write Schemata. If you want to adopt similar principles that Canva follows out of the box, you can use Schemata. \nhttps://www.canva.dev/blog/engineering/product-analytics-event-collection/\nDiscord writes about its migration journey from a homegrown orchestration engine to Dagster. The blog highlights the reasoning behind selecting dbt and Dagster and some of the key improvements while adopting them, such as handling race conditions in dbt incremental update and bulk backfilling with Dagster. \nhttps://discord.com/blog/how-discord-uses-open-source-tools-for-scalable-data-orchestration-transformation\nIt is one of the most fascinating reads about using Graphs as a structure to add knowledge to GenAI. The author highlights that there are two ways to represent knowledge.\nVectors\nGraphs\nThe vector representation is an array of numbers. In a RAG context, it is useful when you want to identify how similar one handful of words is to another. The author is making a case where if you want to make sense of what\u2019s inside of a vector, understand what\u2019s around it, get a handle on the things represented in your text, or understand how any of these fit into a larger context, then vectors as a representation just aren\u2019t able to do that. The graph is an appropriate model to represent knowledge.\nhttps://neo4j.com/blog/graphrag-manifesto/\nStreamline & scale data integration to and from Amazon Bedrock for generative AI applications. Feat. Sathya B. (Senior Solutions Architect at AWS)\nLearn about: \nEfficient methods to feed unstructured data into Amazon Bedrock without intermediary services like S3. \nTechniques for turning text data and documents into vector embeddings and structured data. \nPractical insights into scaling data integration for generative AI with Nexla and Amazon Bedrock. \nReal-world applications of Nexla\u2019s RAG data flow capabilities in enhancing AI deployment.\nhttps://nexla.com/resource/how-to-scale-data-integration-with-nexla-and-amazon-bedrock-for-generative-ai/\nAny system that involves moving money or counting money is always complex to get it correct. I used to joke the entire Walstreet runs on unknown SQL codes that no one understands. The author highlights the engineering best practices such as,\nImmutability and Durability of data\nData should be represented at the smallest grain\nThe code should be Idempotent\nI\u2019m a bit confused by the\u00a0recommendation to use preferred integers to represent financial amounts\u00a0since precision is critical in all financial computing. It would be helpful if the author added more context to the recommendation. \nhttps://substack.wasteman.codes/p/engineering-principles-and-best-practices\nMemory management is one of the hardest parts of building in-memory database engines while supporting large-scale data processing. The author writes an in-depth overview of key parts of DuckDB memory optimization techniques.\nStreaming execution to process a small chunk of data at a time.\nIntermediate spilling to disk while computing aggregations.\nThe buffer manager caches as many pages as possible.\nhttps://duckdb.org/2024/07/09/memory-management\nIbis is an open-source dataframe framework to interact with multiple database engines. Ibis published a 1TB benchmark processing MacBook Pro with 96 GiB of RAM comparing Ibis, Pandas & variations of Polaris.\nhttps://ibis-project.org/posts/1tbc/\nFinding your champion product advocate within your user base is critical in enterprise selling. Metabase writes about why they build a continuous enrichment pipeline for customer contacts pipeline and its system design. The blog provided a nice comparison summary of various 3rd-party data providers in this space and their capabilities. \nhttps://metabase.notion.site/Contact-Organization-Enrichment-dc974a4092674d2dab4da1fc01e57458\nMeta writes about its approach to machine learning prediction robustness and the challenges of ensuring reliable ML predictions. The authors identify several factors that make this difficult, including the stochastic nature of ML models and the constant updates to models and features. Meta\u2019s approach to addressing these challenges involves a systematic framework incorporating preventative measures, fundamental understanding, and technical fortifications. \nhttps://engineering.fb.com/2024/07/10/data-infrastructure/machine-learning-ml-prediction-robustness-meta/\nDropbox writes about building AI-powered features such as Q&A and summaries on unstructured data. The blog highlights optimization techniques to build embedding to enable such features. \nThe lessons learned section highlights the key factors to consider while building, especially around clustering & segmentation, chunk priority calculation, and cached embeddings. \nhttps://dropbox.tech/machine-learning/bringing-ai-powered-answers-and-summaries-to-file-previews-on-the-web\nPinterest writes about Canvas, its homegrown text-to-image foundation model designed to generate images from textual descriptions. \nThe blog details the process of generating high-quality training datasets to build the base model and fine-tune the model further to create personalized experiences.\nhttps://medium.com/pinterest-engineering/building-pinterest-canvas-a-text-to-image-foundation-model-aa34965e84d9\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/a-brief-history-of-modern-data-stack", "title": "Data Engineering Weekly", "content": "The origin of the modern data stack is a topic of intense debate, shrouded in uncertainty and mystery. Some attribute its incubation to Snowflake, Redshift, or Airflow, while others propose different theories. Rather than being the result of a single event, the term \"modern data stack\" emerged from a series of innovations and industry shifts, adding to the intrigue of its history.\nThe foundation of the modern data stack was laid by the creation of Apache Yarn, initially a subproject of Hadoop, later promoted as an Apache top-level project. This might come as a surprise, but by the time Apache Yarn was incubated, S3 as a data store had already gained popularity, leading to the separation of storage and computing. This context is crucial for understanding the evolution of the modern data stack.\nDespite significant advancements, the Big Data project failure rate remains alarmingly high, with over 85% of projects failing to meet their objectives. The industry is grappling with the key reasons behind Hadoop's diminishing promise\u2014a struggle that resonates with many. \nThe MapReduce programming model, for instance, is not SQL-like and requires skilled engineers to write algorithms, making it time-consuming and complex.\nSystems like Apache Hive and Presto started to bridge this gap, but the overall infrastructure remained expensive, challenging companies to justify the cost. Eventually, Hadoop was declared dead. However, Presto demonstrated the possibility of querying massive datasets with scalable storage and computing, paving the way for systems like Redshift, Snowflake, and Databricks to address these pain points and simplify data processing. The rise of cloud data warehouses has given birth to a suite of tools now collectively known as the Modern Data Stack.\nSource: https://medium.com/vertexventures/thinking-data-the-modern-data-stack-d7d59e81e8c6\nEverything has become a SaaS offering in the modern data stack world, particularly attractive to companies struggling with Big Data infrastructure. The modern data stack filled the gap for companies eager to leverage their data without the prohibitive costs of maintaining Hadoop-based infrastructure. It brought large-scale data processing power with the flexibility to scale as needed.\nA wide array of tools emerged, allowing companies to build, buy, and manage data without the burden of maintaining infrastructure. This shift enabled companies to focus on extracting value from data\u2014a dream sold by the modern data stack. \nThe key reasons for its appeal included reduced infrastructure maintenance, high scalability, and improved user experience, which enhanced developer productivity. Adopting default abstractions on top of SQL and Pandas made hiring traditional data warehouse talent easier.\nThe explosion of data tools, each targeting specific functions of data engineering, combined with substantial venture funding, significantly accelerated the growth of the modern data stack. The modern data stack ecosystem began to resemble a giant mural of logos, each representing a different tool or platform.\nThe modern data stack had an amazing impact on data engineering, bringing a developer-first user experience for the first time. However, its very success contributed to its eventual downfall. As companies increasingly adopted these tools, they encountered three major problems: high costs, difficulty integrating, and confusion among themselves.\nInitially, the simplicity and scalability of the modern data stack were appealing. Single-click scaling and ease of use made it a company's go-to solution. However, these advantages led to uncontrollable operational costs. As interest rates rose, companies started to prioritize cost optimization. This was a poor fit for the modern data stack, as the ecosystem encouraged using multiple tools, resulting in high data team budgets. The \"Modern Data Stack High-Speed Tax\" became a term used to describe the cost burden associated with the modern data stack, as highlighted in Instacart\u2019s widely debated S1 filing.\nThe following two Twitter threads summarize the integration issue with Modern Data Stack. In a growth economy, everyone wants to be the control plane for everyone else, resulting in high integration costs and poor overall experience.\nEventually, we declared the modern data stack is dead \ud83d\ude42here, here & here.\nThe sequence of events shows that nothing is accidental; one event leads to another. Now, the bigger question is, what comes next? The recent AI data infrastructure value chain doesn\u2019t differ greatly from the Modern Data Stack of 2022.\u00a0\nThe Next-Gen Data Stack (#Next-Gen Data Stack aka NDS) will learn from the mistakes of the Modern Data Stack but improve its efficiency.\u00a0\nKey properties of the Next-Gen Data Stack are\nWe started to see the pattern with open table formats. Major data infrastructure companies have started to embrace open table formats, and there are projects like Apache XTable and UniForm that have ensured these table formats are interoperable. Databricks and Snowflake announce the open-source versions of their catalogs on top of open table formats.\nThe Next-Gen Data Stack continues the modern data stack's philosophy of simplifying tool usage to enhance developer productivity. The goal is to create tools that are easy to use and accessible to a broad audience. If the tools become too complex, they risk failing to penetrate the mass market.\nThe Next-Gen Data Stack will emphasize cost optimization to address rising operational expenses strongly. Companies will increasingly seek cost-efficient solutions to set budgets and control expenses effectively. For instance, using S3 as a standard storage solution for event logging in infrastructures like Kafka illustrates this trend. Integrating the data stack, reducing infrastructure complexity, open interoperability, and enhanced visibility into spending versus value are key drivers of cost efficiency. By prioritizing these elements, the Next-Gen Data Stack aims to deliver a more economical and sustainable approach to data management.\nFinally, one of the reasons that resulted in the rapid fall of the Modern Data Stack is that it still requires an expert in the loop to run it efficiently. As Joe Reis mentioned in his Everything Ends - My Journey With the Modern Data Stack\u00a0\nThe system that fails the people, not the people, fails the system. I\u2019m a strong believer in this principle. It dictates many of my architectural decisions toward system design. \nThe Next-Gen Data Stack will embrace this principle and build a feedback loop with all batteries included to enforce best practices in building data engineering practices.\u00a0\nThe journey of the modern data stack is a fascinating tale of innovation, growth, and eventual re-evaluation. From its mysterious origins and meteoric rise, marked by the democratization of data processing and scalability, to its downfall, driven by high costs and integration challenges, the modern data stack has profoundly shaped the landscape of data engineering.\nAs we stand at the crossroads of this evolution, the lessons learned from the modern data stack are invaluable. The emergence of the Next-Gen Data Stack promises to build on these lessons, striving for open interoperability, enhanced developer productivity, and cost efficiency. By embracing these principles, the Next-Gen Data Stack aims to offer a more sustainable and user-friendly approach to data engineering.\nThe story of data infrastructure is far from over. As technology continues to advance and new challenges arise, the ability to adapt and innovate will remain crucial. The modern data stack has paved the way, and now the Next-Gen Data Stack is poised to lead us into the future, offering hope for a more integrated and efficient data ecosystem.\nIn this ever-evolving field, one thing is certain: the drive to harness the power of data will continue to inspire innovation, pushing the boundaries of what we can achieve. As we look ahead, the evolution of the data stack stands as a testament to the relentless pursuit of progress in data engineering."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-179", "title": "Data Engineering Weekly", "content": "Astro augments Airflow with enterprise-grade features to enhance productivity, meet scalability and availability demands across your data pipelines, and more.\nLearn More \u2192\nNotion writes about scaling the data lake by bringing critical data ingestion operations in-house. \nNotion migrated the insert heavy workload from Snowflake to Hudi. Hudi seems to be a de facto choice for CDC data lake features. \nNotion migrated the change data capture pipeline from FiveTran to Debezium.\nThe architecture resulted in over a million dollars in cost savings!!!\nhttps://www.notion.so/blog/building-and-scaling-notions-data-lake\nDoorDash writes about Graph Neural Network (GNN) models to create personalized restaurant recommendations. The blog highlights the advantages of GNN over traditional machine learning models, which struggle to discern relationships between various entities, such as users and restaurants, and edges, such as order.\nhttps://doordash.engineering/2024/06/25/doordash-customize-notifications-how-gnn-work/\nTable formats are definitely a hot topic in the industry now. Apache Paimon is a relatively low-key format spun out of Flink\u2019s table store. The author highlights Paimon\u2019s consistency model by examining the metadata model. The dataset organization as a set of LSM trees seems an interesting approach, which sounds similar to Google\u2019s Napa data warehouse system. \nhttps://jack-vanlightly.com/analyses/2024/7/3/understanding-apache-paimon-consistency-model-part-1\nAre you spending too much time trying to debug your Airflow DAGs? In less than 30 minutes, this free comprehensive course from Astronomer Academy will guide you through a straightforward approach to recognizing and resolving the most common debugging issues.\nStart Course \u2192\nExperimentation is a vital part of product design for any company of any scale. Canva writes about the evolution of experimentation system design and the current state of the function. \nhttps://www.canva.dev/blog/engineering/how-we-build-experiments-in-house/\nMigration of a large-scale data infrastructure brings its challenges. Slack writes about migrating its data infrastructure to EMR 6 and Spark 3. The blog highlights the pre-migration and post-migration steps, highlighting the importance of structured execution on such large-scale migration projects.\nhttps://slack.engineering/unlocking-efficiency-and-performance-navigating-the-spark-3-and-emr-6-upgrade-journey-at-slack/\nThe incident analysis is a vital part of the incident management lifecycle process to find the hidden flaws of the system design and prevent future issues. Meta writes about its usage of AI to understand the effectiveness of the incident response and root cause analytics.\nhttps://engineering.fb.com/2024/06/24/data-infrastructure/leveraging-ai-for-efficient-incident-response/\nDrift detection is a popular feature supported by data observability tools. The essence of drift monitoring is running statistical tests on time windows of data. The blog gives an overview of statistical techniques used in drift detection.\nhttps://medium.com/@linghuang_76674/drift-monitoring-architecture-aa57fc26a19c\nChunking is a crucial technique that involves breaking down large pieces of text into smaller, more manageable segments. This process is particularly important when working with large language models (LLMs) and semantic retrieval systems. The X thread is a good reference for measuring the relevancy of a vector search.\nThe blog highlights how the chunk size impacts the vector search and choosing the optimal size.\nhttps://ai.plainenglish.io/investigating-chunk-size-on-semantic-results-b465867d8ca1\nThe article discusses strategies for using production data in dbt development environments without high costs or permissions. It compares two dbt features - defer and clone - for referencing production tables, ultimately favoring a nightly cloning approach for specific models. The authors implement this using Airflow's dynamic task mapping to automatically clone tagged models to each contributor's development dataset, allowing seamless dbt builds with production-like data.\nhttps://medium.com/@dj.lemkes/how-to-choose-between-dbt-clone-and-dbt-defer-and-how-we-clone-for-all-contributors-4d7adeb21792\nThe databricks billing on Azure is a bit confusing. Most of the billing goes via the Azure system, where the deduction happens via negotiated billing with Daabricks. Software manufacturing and distribution are indeed different art. The blog explains how we can programmatically measure the real cost of Azure databricks SQL warehouse instances. \nhttps://gmusumeci.medium.com/how-to-calculate-the-cost-of-azure-databricks-sql-warehouse-instances-8baa73411057\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-178", "title": "Data Engineering Weekly", "content": "Astro augments Airflow with enterprise-grade features to enhance productivity, meet scalability and availability demands across your data pipelines, and more.\nLearn More \u2192\nThe economic impact of Gen AI is widely speculated, and we see few signs of impact. The paper highlights the substantial impact of generative AI on reducing demand for certain freelance jobs while increasing the complexity and pay of the remaining jobs, leading to greater competition and shifts in required skills. The key highlights of the paper,\n1. Decrease in Job Posts: The introduction of ChatGPT led to a 21% decrease in job posts for automation-prone jobs (such as writing and coding) within eight months compared to jobs requiring manual-intensive skills. Image-generating AI technologies resulted in a 17% decrease in job posts related to image creation.\n2. Increased Competition: Reducing job posts increased competition among freelancers. The remaining automation-prone jobs were more complex and offered higher pay.\n3. Job Complexity and Pay: Despite the decrease in job posts, the complexity and pay for the remaining automation-prone jobs increased.\n4. Specific Job Clusters Affected:\nWriting jobs saw the most significant decrease in demand (30.37%).\nSoftware, app, and web development jobs decreased by 20.62%.\nEngineering jobs saw a 10.42% decline.\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=4602944\nTracing the advancements from GPT-2 to GPT-4, the paper argues that AGI (Artificial General Intelligence) by 2027 is plausible. The paper highlights several challenges, including the need for massive industrial mobilization to support the growing demands for GPU, data centers, and power infrastructure.\nControlling AI systems that are much smarter than humans is an unsolved technical problem, and failure could lead to catastrophic outcomes. What do you all think? Do you think human society can handle human-level intelligent machines? \nhttps://situational-awareness.ai/\nYes, I added this article as a logical sequence of the previous two articles \ud83d\ude02 Though the promise of LLMs is amazing, enterprises struggle to integrate the system seamlessly without disturbing the workflow. Looming regulatory requirements, data quality, governance issues & model accuracy keep failing enterprises.\nhttps://gradientflow.substack.com/p/why-your-generative-ai-projects-are\nRun Airflow without the hassle and management complexity. Take Astro (the fully managed Airflow solution) for a test drive today and unlock a suite of features designed to simplify, optimize, and scale your data pipelines. For a limited time, new sign-ups will receive a complimentary Airflow Fundamentals Certification exam (normally $150).\nTry For Free \u2192\nThe article discusses the emergence of AI data infrastructure as a critical area for innovation. The authors emphasize the increasing need for high-quality data for training and inference, focusing on unstructured data pipelines, retrieval-augmented generation (RAG), data curation, and AI memory. It is a good reminder to the data industry that we need to solve the fundamentals of data engineering to utilize AI better.\nhttps://www.felicis.com/insight/ai-data-infrastructure\nDatabricks and Snowflake are talking a big game. So far, they've given us empty Github repositories and rewrites.\nI don\u2019t think anyone can better describe the catalog war than this.\nMarket pressure leads to marketing something that is not what it is and announcing that something is not ready yet. In all fairness, we can take it any day if it is a competition for open-source things. \nhttps://materializedview.io/p/data-lakehouse-catalog-reality-check\nData Engineers, however, kept writing ETL pipelines. Sure, you could pay Fivetran to sync your Salesforce data, and maybe Stripe had a native Snowflake connector, but there was no escaping the long tail of data needs.\nThe blog is a good summarization of the ever-changing and c\u2019 ever-changing and confusing role. The question essentially is, are we so back to building yaml frameworks? \nhttps://databased.pedramnavid.com/p/the-rise-of-the-data-platform-engineer\nCan we experiment on the experimentation process? By implementing \"meta-experiments,\" the team tested new features like low-power alerts, significantly boosting the quality of their A/B tests. This clever dogfooding enhanced their platform and gave the team a taste of their own medicine, fostering empathy for their users and uncovering pain points they hadn't experienced firsthand.\nhttps://booking.ai/meta-experiments-improving-experimentation-through-experimentation-6bdee314c512\nInstacart discusses its adaptive experimentation system for optimizing paid marketing budgets. The system uses a two-step process: \nIt models performance curves using inverse-propensity-weighted regression to ensure valid causal inference.\nIt employs Thompson Sampling to balance exploration and exploitation when choosing marketing actions. \nBy continuously updating its estimates and intelligently introducing random perturbations, this approach has significantly improved Instacart's marketing efficiency compared to traditional methods.\nhttps://tech.instacart.com/bandits-for-marketing-optimization-f5a63b9bfaa7\nIn this article, the author evaluates Small Language Models (SLMs) for use in Retrieval Augmented Generation (RAG) systems, comparing their performance to larger models using the Needle-In-A-Haystack benchmark. Some fine-tuned SLMs, particularly Gemma 2B and Llama2 7B, perform well in tasks similar to those in RAG applications, suggesting the potential for more resource-efficient and environmentally friendly alternatives to Large Language Models. However, the authors note that further research is needed to assess SLMs' capabilities fully in more complex scenarios typical of RAG systems.\nhttps://medium.com/data-science-at-microsoft/evaluating-rag-capabilities-of-small-language-models-e7531b3a5061\nField-level encryption is a data protection measure that encrypts individual sensitive fields within records, keeping data encrypted throughout its lifecycle and narrowing the data protection focus to key management. \nTo enable searching of encrypted data, GEICO uses k-anonymization, which involves storing truncated hash digests alongside encrypted values and allows for secure searches without knowing the encryption key. The approach balances security and performance, requiring careful tuning of the hash truncation length to manage the trade-off between protection against dictionary attacks and the number of false positives in search results.\nhttps://www.geico.com/techblog/searchable-field-level-encrypted-customer-pii/\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-177", "title": "Data Engineering Weekly", "content": "Astro augments Airflow with enterprise-grade features to enhance productivity, meet scalability and availability demands across your data pipelines, and more.\nLearn More \u2192\nThe impact of macroeconomic slowness results in increased focus on prioritizing reduced infrastructure spending. Redpoint\u2019s InfraRed report shows a promising sign of a rebound in spending, where much of the money goes to GenAI. A few highlights from the report\nUnstructured data goes mainstream.\nAI-driven code development is going mainstream now. \nhttps://www.redpoint.com/infrared/report/\nCompanies are increasing their investment in AI but struggle to integrate it with their product experience or find the right talent and infrastructure to empower it. Bessemer publishes the Data + AI infrastructure market map to help companies understand the landscape and the key players. \nhttps://www.bvp.com/atlas/roadmap-ai-infrastructure\nLLMs significantly reduce the entry barrier and are faster than ever for prototypes. The author highlights that while people ship initial deployments of LLM-based pipelines much faster than traditional ML pipelines, expectations become similarly tempered once they are in production. Should we term this as an AI product curve of disappointment? \nhttps://www.sh-reya.com/blog/ai-engineering-short/\nGain access to the latest trends and insights shaping the world of Apache Airflow\u2014the go-to platform for data pipeline development and orchestration.\nGet Your Copy \u2192\nA balanced take on the open source version of Unity Catalog, what is in the open source, and what is in the commercial version. While seeing the Unity Catalog code, I thought it might be a stripped-down version of the code used in production. But it seems a complete rewrite. It is still unclear if Databricks will use the open-source version in their prod.\nI guess it is okay to build something from scratch, but I was surprised it was announced on the stage as if Databricks open-source their production version of the Unity catalog. \nhttps://semyonsinchenko.github.io/ssinchenko/post/uniticatalog-first-look/\nNetflix publishes a recap of all the talks in the first Data Engineering open forum tech meetups. The blog contains a summary of each talk and a link to the YouTube channel with all the talks. \nhttps://netflixtechblog.com/a-recap-of-the-data-engineering-open-forum-at-netflix-6b4d4410b88f\nPinterest writes about the challenges of running Ray infrastructure in production and integration with the existing Pinterest ecosystem. The case study is a classic success of the platform engineering with a wrapers around the cluster to ease the usage and increases the adaptability. \nhttps://medium.com/pinterest-engineering/ray-infrastructure-at-pinterest-0248efe4fd52\nAnother platform engineering case study where Lyft writes about computing ETA to show the availability of the ride and its challenges in building real-time. The blog details the classification model, training approach and historical data analysis. \nhttps://eng.lyft.com/eta-estimated-time-of-arrival-reliability-at-lyft-d4ca2720bda8\nThe blog is an excellent case study of hyopoesis driven cost optimization with the detail analysis to verify the hypothesis. The author highlighted three hypothesis contributing cost in Google Cloud Dataflow pipeline.\nPhysical resources are underutilized.\nPhysical resources have not the best price-to-performance ratio.\nConfiguration of the Dataflow job is suboptimal and could be optimized.\nhttps://blog.allegro.tech/2024/06/cost-optimization-data-pipeline-gcp.html\nIt\u2019s common to find in many data trends prediction that this year will be the year of stream processing, however Michael Drogalis highlighted there are not many usecases for stream processing. \nRef: https://x.com/MichaelDrogalis/status/1782439448996970946\nThe author highlights the low latency, reactive usecases more ubiquitous than what we think? Question to the readers, what do you think of the current state of real-time data processing engines? Are there enough usecases?\nhttps://x.com/apurva1618/status/1803101801081975029?s=12&t=IJ8nTG5H26su2sWUh2bYSw\nIs parquet is still good enough for Machine Learning, Vector and Lake House workloads? \nInflux data conclude that while technical concerns about Parquet metadata are valid, the actual overhead is smaller than generally recognized. In fact, optimizing writer settings and simple implementation tweaks can reduce overhead by 30-40%. With significant additional implementation optimization, decode speeds could improve by up to 4x.\nhttps://www.influxdata.com/blog/how-good-parquet-wide-tables/\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-176", "title": "Data Engineering Weekly", "content": "Astro augments Airflow with enterprise-grade features to enhance productivity, meet scalability and availability demands across your data pipelines, and more.\nLearn More \u2192\nThis week brought many exciting developments, with Snowflake and Databricks announcing open-source catalogs. Unity Catalog's source code is available on GitHub, and people have already conducted exciting experiments with it. [Unity Catalog OSS with Hudi, Delta, Iceberg, and EMR + DuckDB]. \nOne of the big benefits of having a Hive Meastore catalog is that it enables many query engines to build executing engines on top of it, which creates a strong ecosystem. I\u2019m excited to see what Unity Catalog and Polaris Catalog bring.\nhttps://www.databricks.com/blog/open-sourcing-unity-catalog\nHigh-quality training data plays a critical role in the performance, accuracy, and quality of responses from a custom LLM. Regulative requirements and privacy concerns are often a big hurdle to training context-rich data. The paper Generative AI for Synthetic Data Generation: Methods, Challenges, and the Future highlights the current challenges and opportunities with synthetic data generation to train LLMs. Along the same line, NVIDIA open sources nmotran4 give developers a free, scalable way to generate synthetic data that can help build powerful LLMs.\nhttps://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/\nS3 Express One Zone, with low latency and write ops certainty, is promising. The author demonstrates a few emerging architecture patterns around the S3 Express One Zone and points out that it can offer low-latency writes suitable for high-throughput workloads; it becomes cost-effective mainly for high-throughput scenarios. Replication-based systems remain more economical at low to medium throughputs, especially with significant cross-AZ data transfer discounts. \nhttps://jack-vanlightly.com/blog/2024/6/10/a-cost-analysis-of-replication-vs-s3-express-one-zone-in-transactional-data-systems\nWith recent leaps in Generative AI, more and more teams are being asked to implement these use cases. This GenAI Cookbook is a starting place that will guide you through the technical and architectural requirements for building enterprise applications powered with AI and Airflow, including step-by-step instructions and real-world use cases.\nGet Your Copy \u2192\nConsumer rebalancing and Head-of-line (HOL) blocking are some of the most common challenges while operating Kafka at scale. The recent KIP-932 proposal suggests a message proxy service (MPS) for Kafka to decouple the Kafka message_reader thread (i.e., a group of 1 thread) and message_processing_writer threads. The blog explains KIP-932 and its potential benefits. I\u2019ve not read through the KIP-932 proposal, but I remember Uber doing a similar design in the past with Enabling Seamless Kafka Async Queuing with Consumer Proxy.\nhttps://medium.com/walmartglobaltech/reliably-processing-trillions-of-kafka-messages-per-day-23494f553ef9\nLiquid clustering liberates the hive-style static partitioning and organizes the data layout from the accessing pattern. The author explains the available pattern and provides an in-detail view of the Hilbert Curve Assignment. The official design document for liquid clustering is here.\nhttps://levelup.gitconnected.com/delta-lake-liquid-clustering-a-visual-explanation-b9d8782a9f33\nDoubleCloud has taken the robust capabilities of Apache Airflow and enhanced them with a fully managed service, providing you with a comprehensive toolkit for creating, scheduling, and monitoring workflows with unparalleled ease! With DoubleCloud's Managed Airflow, you get: -Hassle-free infrastructure management -Auto-scaling for effortless performance -Quick start with pre-packaged libraries.\nApply for early access and achieve effective data orchestration in under 10 minutes!\nhttps://double.cloud/services/managed-airflow/\nMeta uses bespoke training hardware with the newest chips possible and high-performance backend networks that are highly speed-optimized. It discusses the maintenance issues that Meta discusses in the OpsPlanner orchestrator for managing the GPU overlapping workloads. \nhttps://engineering.fb.com/2024/06/12/production-engineering/maintaining-large-scale-ai-capacity-meta/\nThe simplicity of DuckDB is an exciting part, and that combines with its advanced capabilities. The author highlights the power and flexibility of DuckDB for handling embeddings and performing vector searches. \nhttps://blog.brunk.io/posts/similarity-search-with-duckdb\nThe more metadata there is, the more readability of the model. It is often challenging as developers are not incentivized to produce quality metadata. Picnic writes about dbt-score, a non-opinionated, configurable linting tool to measure the completeness of dbt model metadata.  \nhttps://blog.picnic.nl/picnic-open-sources-dbt-score-linting-model-metadata-with-ease-428278f9f05b\nInteractive Analysis in Benchling allows scientists to perform real-time data transformation, visualization, and analysis without transferring it into other systems. The article highlights the architectural pattern and how it handles scalability and data sharing among apps.\nhttps://benchling.engineering/a-behind-the-scenes-look-at-building-interactive-analysis-capabilities-in-benchling-fa6ec1bab1e5\nAlibaba talks about Ant Group\u2019s real-time feature store, SkyLine. Skyline involves three key stages: computing inference, normalization, and deployment. The article goes in-depth on each layer and the optimization of SkyLine to build efficient feature serving. \nhttps://www.alibabacloud.com/blog/in-depth-application-of-flink-in-ant-group-real-time-feature-store_601288\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-175", "title": "Data Engineering Weekly", "content": "Astro augments Airflow with enterprise-grade features to enhance productivity, meet scalability and availability demands across your data pipelines, and more.\nLearn More \u2192\nWe should officially call the first week of June the data engineering week, as two major data companies are running their developer conference. There are many announcements, from Snowflake's open-sourcing Polaris Catalog to Databricks buying Tabular. I will write a separate blog on these announcements after the Databricks conference; in the meantime, I found the blog from Cube Research, a balanced article about Snowflake Summit. \nhttps://thecuberesearch.com/234-breaking-analysis-crystallizing-snowflake-summit-2024/\nDatabricks buying Tabluar certainly triggers interesting patterns in the data infrastructure. Databricks and Snowflake offer a data warehouse on top of cloud providers like AWS, Google Cloud, and Azure. Snowflake and Databricks acknowledge the support for Iceberg and move the battle to the data governance layer. However, all these cloud providers do offer competitive products. The architecture pattern establishes the baseline of how the cloud providers will eventually eat Snowflake & Databricks lunch. Will they co-exist or fight with each other? On the time will tell us. \nhttps://piethein.medium.com/integrating-azure-databricks-and-microsoft-fabric-0030d3cf5156\nLLM models are slowly emerging as the intelligent data storage layer. Similar to how data modeling techniques emerged during the burst of relation databases, we started to see similar strategies for fine-tuning and prompt templates. On a similar line, Open AI released the first draft of Model Speck as guidelines for researchers and data labelers to create data as part of a reinforcement learning technique from human feedback.\nhttps://cdn.openai.com/spec/model-spec-2024-05-08.html\nIf you're looking to build, maintain, and more efficiently manage your data pipelines, check out our comprehensive guide, Data Pipelines with Apache Airflow. This ebook covers practical use cases and provides an overview of key concepts and best practices, including how to set up Airflow in production environments, along with best practices for building, testing, and deploying Airflow DAGs.\nGet Ebook \u2192\nThe Databricks serverless offering intends to reduce the friction in maintaining the infrastructure and accelerate building value out of data. The author publishes the lessons learned from testing Databricks serverless SQL, where the author noted the runtime improvement saturated after the medium-scale warehouse.\nhttps://medium.com/@synccomputing/5-lessons-learned-from-testing-databricks-sql-serverless-dbt-cade8ded5646\nThe blog is an excellent recap for anyone starting their career as a data analyst/ data scientist. The data is meaningless unless we find a connection that builds a story. We often start with the other way around, constructing a story and trying to find the data to fit into our narration.  The author points out why a data scientist should be an objective truth seeker and open to accepting new insights.\nhttps://towardsdatascience.com/what-10-years-at-uber-meta-and-startups-taught-me-about-data-analytics-fd948b912556\nApache Kafka is the #1 open-source event streaming service, but the challenge of setting up and maintaining clusters can be a dealbreaker for many companies. And this is where DoubleCloud comes in: with our fully managed service for Apache Kafka, you can deploy production-ready clusters in just about 10 minutes. Combined with DoubleCloud's fully managed ClickHouse and Apache Airflow, you can get everything you need to build a real-time analytics infrastructure in one place.\nStart a free trial and see just how easy setting up real-time analytics can be!https://double.cloud/services/managed-kafka/\nNetflix writes about the talks from their internal  Causal Inference and Experimentation Summit 2024, highlighting the key talks.\nMetrics Projection for Growth A/B Tests\nA Systematic Framework for Evaluating Game Events\nDouble Machine Learning for Weighing Metrics Tradeoffs\nSurvey AB Tests with Heterogeneous Non-Response Bias\nDesign: The Intersection of Humans and Technology\nhttps://netflixtechblog.com/round-2-a-survey-of-causal-inference-applications-at-netflix-fd78328ee0bb\nPart 1: https://netflixtechblog.com/a-survey-of-causal-inference-applications-at-netflix-b62d25175e6f\nLinkedIn writes about integrating Graph Neural Network (GNN) technology into its talent solutions, exemplifying cutting-edge applications of AI in enhancing job matching accuracy and equity in hiring. LinkedIn tailors job recommendations more precisely by effectively mapping professional relationships and interactions through a dynamic graph structure and levels the playing field for underrepresented groups. This approach not only innovates within the realm of data engineering but also reinforces the potential of GNNs to transform industry practices by leveraging complex relationship data.\nhttps://www.linkedin.com/blog/engineering/talent/how-data-is-powering-skills-based-hiring-on-linkedin\nI\u2019m highly optimistic about AWS Express One Zone. As I have shared, its impact on data engineering is exciting. Warpstream writes about using Amazon S3 Express One Zone to power a cost-effective, low-latency streaming solution. \nI like all the details in the blog, but I can\u2019t stop laughing while seeing this comparison chat, a classic example of how to cheat people with charts. Dear author, please add the latency numbers in the y-axis.\n https://aws.amazon.com/blogs/storage/how-warpstream-enables-cost-effective-low-latency-streaming-with-amazon-s3-express-one-zone/\nAs the development of LLM-based applications increases, so does the need for efficient inference and serving of LLMs. TIL about the vLLM library. The author publishes an introduction to vLLM and a comparison study with other related libraries. \nhttps://medium.com/@wearegap/a-brief-introduction-to-optimized-batched-inference-with-vllm-deddf5423d0c\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-174", "title": "Data Engineering Weekly", "content": "Data Engineering Weekly is sponsored by Astronomer\u2014Enterprise-Grade Apache Airflow. Deliver data on time with the speed and scale your application demands.\nLearn More \u2192\nSeveral countries are working on building governance rules for Gen AI. Data sovereignty will play a vital role as countries formulate regulations. The AI Verify Foundation, a not-for-profit Foundation, a wholly-owned subsidiary of the Info-communications Media Development Authority of Singapore (IMDA), publishes its Model AI Governance Framework for Gen AI. The key highlights focus on\nAccountability\nData Quality & Governance\nTransparency in the development and deployment of AI models\nA structured approach to incident reporting and third-party testing\nAI for Public Good\nhttps://aiverifyfoundation.sg/wp-content/uploads/2024/05/Model-AI-Governance-Framework-for-Generative-AI-May-2024-1-1.pdf\nThe article overviews lessons learned from building an app with LLM and the best practices to follow. A few highlights \nPrompting Techniques: Effective prompting is crucial. Techniques like in-context learning, chain-of-thought, and providing relevant resources can significantly improve LLM performance.\nRetrieval-Augmented Generation (RAG): RAG effectively incorporates new information into LLMs, improving output quality by grounding responses in relevant documents. It is often preferable to fine-tuning due to the ease of updating and better handling of new knowledge. \nWorkflow Optimization: Decomposing complex tasks into smaller, manageable steps and prioritizing deterministic workflows can enhance the reliability and performance of LLM-based systems.\u00a0\nhttps://www.oreilly.com/radar/what-we-learned-from-a-year-of-building-with-llms-part-i/\nData is always an afterthought in many organizations. A founder starts with a vision and drives the product roadmap. Data plays a critical role in helping the founders \u2019s vision to iterate faster and grow the business. How should one think about a data strategy if you\u2019re a startup? The author highlights the structured approach to building data infrastructure, data management, and metrics. \nhttps://www.bvp.com/atlas/how-to-get-more-out-of-your-startups-data-strategy\nClickHouse is the fastest, most resource-efficient OLAP database, which queries billions of rows in milliseconds and is trusted by thousands of companies for real-time analytics. Run ClickHouse anywhere and enjoy up to 100x faster performance compared to other DBMSs while DoubleCloud takes care of the maintenance providing 24/7 monitoring along the way. Start a free trial and see just how easy it is to get ClickHouse\u2019s incredible speed for real-time analytics at scale!\nhttps://double.cloud/services/managed-clickhouse/\nUber is one of the largest Hadoop installations, with exabytes of data. Uber writes about its decision to move from on-prem batch data infrastructure to GCP. Uber detailed the migration strategy as a typical\nLift & Ship the same copy to minimize the disruption \nDeprecate in-house solutions slowly in favor of cloud solutions as a continuous refactoring of the stack. \nhttps://www.uber.com/blog/modernizing-ubers-data-infrastructure-with-gcp/\nThe article discusses how Meta employs a hybrid approach to improve the accuracy of its forecasts by integrating traditional statistical methods with advanced machine learning techniques. Meta combining different forecasting methods, leveraging ensemble learning and transfer learning principles. This approach allows Meta to achieve better adaptability and accuracy in predicting future trends by harnessing the strengths of various models and techniques\nhttps://medium.com/@AnalyticsAtMeta/forecasting-meta-balancing-art-and-science-92526e1ae36c\nManaging mass amounts of data efficiently is key in today\u2019s data-driven economy. Grindr knows this first hand, and set out to balance Snowflake usage with cost optimization. The resulting solution was SnowPatrol, an OSS app that alerts on anomalous Snowflake usage, powered by ML Airflow pipelines. Register for this webinar to get a behind the scenes look at how to implement this next-gen monitoring to proactively identify abnormal Snowflake usage.\nSave Your Spot \u2192\nState Management is a vital part of stream processing, and the author points out how painful it is to build and audit the state in the Flink application. TIL that the queryable state is deprecated, which surprises me too. The author highlights the new state reader in Spark Streaming and emphasizes the need to improve the observability and developer-friendliness of Flink state management. \nhttps://streamingdata.substack.com/p/my-biggest-issue-with-apache-flink\nSpotify writes the second part of building a data platform at Spotify and talks about scalability, the tooling we use and provide, and the value each building block brings to a data platform. By investing in tooling and documentation and fostering a strong community, Spotify ensures its data platform evolves to meet its ever-growing business demands.\nhttps://engineering.atspotify.com/2024/05/data-platform-explained-part-ii/\nDevoted Health writes about the performance issues with Looker\u2019s built-in LookML Validator, which checks the syntax, consistency, and enforcement of best practices. The performance issue impacts the users' productivity, and the blog explains how the data team built a custom LookML validator integrated with the IDE to improve its performance.  \nhttps://tech.devoted.com/building-a-custom-static-analysis-tool-for-looker-914f44b74c14\nData Product Thinking Shaping the data management to build a reliable, customer-centric data application. The tooling around supporting data products is on the rise. Adevinta writes about its data-building process using local scripts \u2192 spreadsheet emails, \u2192 data products. The blog highlights the critical factors for data products' success: standardization of producing data assets, uniform CI/ CD process, and standard testing methodologies. \nhttps://medium.com/adevinta-tech-blog/how-we-moved-from-local-scripts-and-spreadsheets-shared-by-email-to-data-products-edaec9228753\nIntegrating Airflow with dbt has its challenges. Should we adopt one DAG to execute all dbt models or one DAG per dbt model? Both are suboptimal. The author highlights adopting a \u201cmodel grouping\u201d approach to pack all the relevant dbt models into one DAG group to provide better isolation. \nhttps://medium.com/apache-airflow/how-we-orchestrate-2000-dbt-models-in-apache-airflow-90901504032d\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-173", "title": "Data Engineering Weekly", "content": "Is AI all about hype? What do humans spend their time on in a post-AGI world? There are many burning questions from our readers, too, and the author did an amazing compilation of some of the widely discussed questions around AI development. What is your burning question about AI?  \nhttps://byrnemluke.com/ideas/questions-about-AI\nBuilding a global scale distributed system with eleven 9s of durability and four 9s of availability is no easy feat. S3 is the state-of-the-art software system of our age. However, is that all enough? The author highlights some of the key missing features from S3.\nhttps://materializedview.io/p/s3-is-showing-its-age\nMeta writes about its transition to a composable data management system to improve interoperability, reusability, and engineering efficiency. By leveraging Velox, an open-source execution engine, Meta integrates common components across diverse data systems, reducing redundancy and enhancing innovation.\nhttps://engineering.fb.com/2024/05/22/data-infrastructure/composable-data-management-at-meta/\nLearn how LSports, a top provider of real-time sports data, improved its data analytics using DoubleCloud\u2019s Managed ClickHouse. Query times have been reduced from over 90 seconds to less than 0.5 seconds, enhancing real-time sports data analytics efficiency!\nRead the full success story and see how DoubleCloud takes analytics to the next level!\nhttps://double.cloud/resources/case-studies/lsports-fast-track-to-quick-queries-with-clickhouse/\nThere is a huge difference between data and easy-to-use data. The former can lead to bad decisions and sub-par experiences; the latter leads to a superb product and experience that\u00a0 enables the people who use our services to make quick, data-informed decisions.\nTrue that. Zillow writes about its approach to building easy-to-use data that adopts certified datasets, aka data products.\nhttps://www.zillow.com/tech/building-a-strong-foundation-to-accelerate-streeteasys-data-science-efforts/\nTweeq writes about its journey of building a data platform with cloud-agnostic open-source solutions and some integration challenges. It is refreshing to see an open stack after the Hadoop era.   \nhttps://engineering.tweeq.sa/tweeq-data-platform-journey-and-lessons-learned-clickhouse-dbt-dagster-and-superset-fa27a4a61904\nScale data pipelines to and from BigQuery for GenAI, Business Intelligence, and Operations. Feat. Jobin George (Data Analytics Solutions Architect at Google Cloud)\nGet all your BigQuery questions answered, and learn about:- Creating intelligent, reliable data flows to and from Google BigQuery in minutes- Best practices for operational data integration pipelines handling trillions of records- Optimizing ETL/ELT pipelines for enhanced business analytics and dashboards- Transformations to vector embeddings for GenAI and rapid experimentation with various LLMs- Real-world use cases for companies like DoorDash, Johnson & Johnson, and LinkedIn.\nhttps://nexla.com/resource/how-to-scale-data-integration-google-bigquery/\nI wrote about the need for a Notebook-like orchestration system: A Notebook is all I want or Don't. The author explores a similar thought process on reimagining the notebook cells as connected DAG and prefers simplicity. I\u2019m super excited to see the construction style of the Notebook and what comes next.\n https://marimo.io/blog/lessons-learned\nSQL or DataFrames, two programming models often used interchangeability in engineering data. It is a long standing question on people wondering In what situations should you use SQL instead of Pandas as a data scientist? and why one prefer dataframe over sql. SQLFrame is an interesting approach where it convert PySpark style DataFrame into SQL. The idea will infact open flexibility in programming data over the SQL-based data warehouse systems. \nhttps://towardsdev.com/sqlframe-turning-pyspark-into-a-universal-dataframe-api-e06a1c678f35\nMarketing is the key function to drive the business operation, and measuring the success of marketing campaign vital to strengthen the sales pipeline. Grab writes about its adoption of experimentation framework in measuring the marketing campaigns impact.\nhttps://engineering.grab.com/evaluate-business-impact-of-marketing-campaigns\nStar Schema vs OBT (One Big Table) data modeling is another ongoing debate in data engineering practitioners. The author making a case where the growing need for the real-time data analytics, the adoption of OBT model will continue to grow.\nhttps://bednarzadrian.medium.com/is-star-schema-a-thing-in-2024-a-closer-look-at-the-obts-8ac747d7fe50\nMany data pipeline process incremental data, which is not a lot in many business cases. Is massive scale data warehouses like Snowflake or data processing engines like Spark require for incremental processing? I guess not; at the same time the underlying architecture should support incremental data processing. I assume there will be a growing adoption of simpler data processing systems like DuckDB for incremental data processing. However the success depends on its flexibility, and ease of integration. The limitations like below is definitely a cognitive block for adoption. I\u2019m surprised that the DuckDB community has not addressed these interoperability concerns. \nUnfortunately, this is not immediately helpful for querying from DuckDB. There is no Snowflake catalog SDK available for DuckDB. Luckily, we can use the file system directly to read our data.\nhttps://medium.com/datamindedbe/quack-quack-ka-ching-cut-costs-by-querying-snowflake-from-duckdb-f19eff2fdf9d\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-172", "title": "Data Engineering Weekly", "content": "DEWCon is back this year on a grand scale on September 13th, 2024, in Bengaluru, India. We added some additional features to bring the data community together this year.\nBook a 1:1 session with experts on career, tech stack, team management, and more!!\n\"Ideas Jam Session,\u201d where you can talk about your idea/ prototypes in a 10-minute slot\nMore details on DEWCon will be in the coming weeks, and we will open the registration shortly.\nIf you want to speak at the conference, propose the talk here: https://forms.gle/21piE4B4e9VuShQ97.\nIf you want to sponsor DEWCon, express interest here: https://forms.gle/NSWPL6mjJEdR5ERd9.\nMany companies struggle to integrate LLM into their business applications. The author highlights the importance of minimizing surprises and providing a uniform user experience across business categories by referring to Jakob\u2019s law of UX.\nhttps://vickiboykis.com/2024/05/06/weve-been-put-in-the-vibe-space/\nHow do we provide the value of data and subsequent infrastructure around it? This is a long-standing question for both internal data teams and vendors. It is challenging enough for a research topic. The article about data asset pricing is one of the comprehensive thoughts I came across about pricing models, establishing two basic factors.\nData value depends on the users and the use cases\nData quality is multi-dimensional, and high-quality data costs more.\nhttps://pivotal.substack.com/p/how-to-price-a-data-asset\nOne of my burning questions is that there is not much going on in the columnar formats. An Empirical Evaluation of Columnar Storage Formats paper compares ORC and Parquet formats regarding efficiency, design choices, and what is lacking in that system. The blog compares the two modern-day alternatives for Parquet, Nimble & Lance. \nhttps://materializedview.io/p/nimble-and-lance-parquet-killers\nJoin us for a live webinar on May 22nd at 9 am PT | 12 pm ET, where we'll showcase how to accelerate the development of AI and embedded analytics solutions using Cube Cloud. Here's what you can expect to learn:\n- Simplify LLM Integration: Discover how Cube's native AI API streamlines the development of chatbots, copilots, and Slack apps with text-to-semantic layer queries, making it easier to integrate OpenAI and build intuitive natural language capabilities.- Rapidly Prototype Embedded Analytics: Learn about our new Chart Prototyping feature in Cube Cloud's Playground 2.0, which allows you to create, test, and deliver chart insights effectively. Generate TypeScript code, preview diverse chart types, and integrate using REST or GraphQL APIs.Attend to see product demos and learn how to build secure, accurate, and cost-effective AI and embedded analytics solutions.Register now and join us on May 22nd!\nhttps://cube.registration.goldcast.io/events/29bbd223-e862-44ff-821d-f3420033060c\nThe blog highlights some of the pressing issues in the industry.\nData Warehouses are increasingly building business-critical applications that lead to the rapid adoption of best practices from software engineering. We have already seen talks about data contracts, data products, etc.\nData teams and their stack are getting larger. The complexity of engineering data exponentially increases as the data size grows. \nAs testing becomes the core of reliable data system building, data professionals increasingly become domain experts. \nhttps://medium.com/@mikldd/data-about-data-from-1-000-conversations-with-data-teams-bf21496dd7ea\nLinkedIn's \"People You May Know\" feature is designed to help users expand their professional networks by suggesting new connections based on shared contacts, interests, and professional backgrounds. Recent updates have focused on improving recommendation quality and reducing bias, ensuring frequent and infrequent users benefit from relevant suggestions. \nNotably, empirical research has shown that connections with acquaintances, rather than close friends, are more effective in helping users find new job opportunities, validating the \"strength of weak ties\" theory.\nhttps://www.linkedin.com/blog/engineering/recommendations/building-a-large-scale-recommendation-system-people-you-may-know\nWhether you are struggling with query speed, infrastructure costs, or vendor-lock, DoubleCloud is here to help you out with the best-in class managed open-source tech! Read on to discover how Spectrio managed to reduce their query latency and infra costs while handling more than 200M+ rows by switching from Snowflake.\nhttps://double.cloud/resources/case-studies/spectrio-cut-costs-and-boosted-analytics-speed-with-doublecloud/\nPinterest writes about its multi-year journey of deprecating HBase with specialized databases such as Druid, Goku (an in-house time-series db), and TiDB. The blog highlights the key factors that lead to HBase deprecation.\nComplexity and Maintenance Overhead\nScalability and Performance Issues\nLack of Essential Features\nhttps://medium.com/pinterest-engineering/hbase-deprecation-at-pinterest-8a99e6c8e6b7\nThumbtack writes about unifying its machine learning model inference process to streamline deployment and improve efficiency. By consolidating various models into a single inference platform, Thumbtack better manages model versions, reduces latency, and scales its infrastructure seamlessly. This approach not only enhances performance but also simplifies the maintenance and deployment pipeline for their machine learning operations.\nhttps://medium.com/thumbtack-engineering/unifying-machine-learning-model-inference-at-thumbtack-eacb08d11680\nThe explainability of an analysis is one aspect I think LLM can help data practitioners. BuzzFeed writes about the same using ChatGPT to automate the creation of analytical summaries, blending data from various sources like SQL databases and Google Sheets. This approach improves efficiency by generating concise and accurate summaries, helping the team gain insights and make data-driven decisions.\nhttps://tech.buzzfeed.com/crafting-analytical-summaries-with-chat-gpt-1316ba5fbe7c\nPicnic uses large language models (LLMs) to enhance its search retrieval system, enabling more accurate and contextually relevant search results. By leveraging LLMs, Picnic improves the understanding of user queries and the matching of these queries to relevant products. I\u2019m doing some prototypes in blending keyword search and semantic search, and this is an existing domain to keep track of.\nhttps://blog.picnic.nl/enhancing-search-retrieval-with-large-language-models-llms-7c3748b26d72\nAgoda writes about how it solved the load-balancing challenges with Apache Kafka. The model essentially falls into two categories.\nLag-aware producers (if there is only a finite and known set of producers in the system)\nLag-aware consumers (if there is no finite set of producers in the system)\nThe blog discusses various algorithmic approaches one can take for both the approaches and their trade-offs.\nhttps://medium.com/agoda-engineering/how-we-solve-load-balancing-challenges-in-apache-kafka-8cd88fdad02b\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-171", "title": "Data Engineering Weekly", "content": "DEWCon is back this year on a grand scale on September 13th, 2024, in Bengaluru, India. This year, we added some additional features to bring the data community together.\nBook a 1:1 session with experts on career, tech stack, team management, and more!!\n\"Ideas Jam Session,\u201d where you can talk about your idea/ prototypes in a 10-minute slot\nMore details on DEWCon will be in the coming weeks, and we will open the registration shortly. \nIf you want to speak at the conference, propose the talk here: https://forms.gle/21piE4B4e9VuShQ97.\nIf you want to sponsor DEWCon, express interest here: https://forms.gle/NSWPL6mjJEdR5ERd9.\nGloss Genius describes its migration journey from dbt cloud to Airflow + custom Github actions. The post highlights how Gloss Genuis established feature parity with SlimCI and enhanced post-migration with open-source tooling. \nhttps://glossgenius.com/blog/how-we-migrated-from-dbt-cloud-and-scaled-our-data-development\nThe blogs compare the hype cycle of Big Data with Gen AI. The blog narrates the initial hype of Big Data, followed by talent shortages and lead time to build production applications. Since then, abstractions like Hive SQL have significantly simplified the barrier to entry, which has led to the commoditization of big data. The author predicts such abstraction and commoditization requirements for Gen AI.\nhttps://gradientflow.substack.com/p/learning-from-the-past-comparing\nOne of the biggest challenges in adopting Gen AI in enterprises is maintaining the client data as secure and private. Slack describes the challenges in maintaining AI secure & private that leads to the choice of RAG approaches with large contextual window LLM. \nhttps://slack.engineering/how-we-built-slack-ai-to-be-secure-and-private/\nDoubleCloud is here to provide you with best-in-class open-source solutions for building a data analytics infrastructure. From ingestion to visualization: all integrated, fully managed, and highly reliable, so you can save time and costs with zero-maintenance open-source tech.\nReady to give DoubleCloud a try? We have a special offer for Data Engineering Weekly readers: start a free trial and get $500 credits for free, so you can see just how easy it is to spin up your own analytics with us!\nhttps://double.cloud/\nRich contextual information is vital to bringing intelligence out of the data. Often, data categorization and contextualization are an afterthought in the data ecosystem. At scale, it becomes impossible to enrich the data assets manually. Uber writes about DataK9, an AI/ ML model that autocategorizes the data assets without manual labeling. It is one of the tools I wish Uber could open source. \nhttps://www.uber.com/blog/auto-categorizing-data-through-ai-ml/\nGoogle's latest blog post discusses using generative AI to enhance incident response processes. Google Security leveraged Large Language Models (LLMs) to significantly speed up the creation of security and privacy incident summaries, reducing the time needed by 51% and improving the quality of these communications. AI streamlines internal communications and ensures that incident responses are timely and effective, helping to maintain rigorous security standards.\nhttps://security.googleblog.com/2024/04/accelerating-incident-response-using.html\n The Ministry of Manpower (MOM) in Singapore writes about the usage of LLM & ML models to develop and deploy two AI products: the SSOC Autocoder, which efficiently categorizes job postings into relevant classifications, and Sensemaker, which extracts insights from large document volumes using AI. These tools have streamlined development processes, improved operational efficiency, and enabled data-driven decision-making across government agencies.\nhttps://medium.com/dsaid-govtech/productionising-llms-and-ml-models-with-analytics-gov-moms-journey-into-ai-solution-deployment-bad4ceb12df2\nLinkedIn's blog post discusses their approach to ensuring fair and responsible AI usage, emphasizing the importance of not amplifying biases within their AI systems. LinkedIn employs a privacy-preserving probabilistic race/ethnicity estimation system to measure and ensure 'Equal Treatment' across demographic groups without compromising member privacy. The system avoids individual race/ethnicity assignments and ensures data security & member control align with their Responsible AI Principles to provide equitable AI-driven experiences.\nhttps://www.linkedin.com/blog/engineering/responsible-ai/responsible-ai-update-testing-how-we-measure-bias-in-the-us\nThe earlier article on Slack & LinkedIn discussed the importance of protecting AI applications' privacy and security. However, not everyone operates responsibly to handle the power of AI. The blog points out that Gen AI is more capable than humans of creating disinformation, especially deep fake technologies. In a well-connected world with social media, the danger of Gen AI and disinformation is undeniable. \nhttps://hai.stanford.edu/news/disinformation-machine-how-susceptible-are-we-ai-propaganda\nThe Alibaba Cloud blog analyzes new features in Flink ML, focusing on online learning and inference capabilities and improved feature engineering algorithms. The blog discusses Flink ML's evolution into a real-time machine learning platform with enhanced infrastructure to support dynamic online learning processes, enabling continuous model updates and applications in various real-time scenarios. Flink ML\u2019s evolution aims to streamline operational workflows and enhance performance for diverse business applications.\nhttps://www.alibabacloud.com/blog/analysis-and-application-of-new-features-of-flink-ml_601119\nThe article from Atlassian discusses the importance of enhancing data platforms with robust deployment capabilities, focusing on the company's own experience with its internal data lake. Atlassian emphasizes how crucial it is for its data lake, utilized by over half of its employees, to support continuous growth and adaptability. The key takeaway is that integrating deployment capabilities can significantly improve the management and scalability of data platforms, ensuring they remain efficient and reliable as they evolve.\nhttps://www.atlassian.com/engineering/evolve-your-data-platform-with-a-deployment-capability\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-170", "title": "Data Engineering Weekly", "content": "One of the insightful articles is about the growing adoption of one large language model and the challenge it brings to machine unlearning. The motivation for Machine Unlearning is critical from the privacy perspective and for model correction, fixing outdated knowledge, and access revocation of the training dataset.  \nA key thought-provoking moment for me while reading the article is this quote.\nIn an ideal world, data should be thought of as \u201cborrowed\u201d (possibly unpermitted) and thus can be \u201creturned,\u201d and unlearning should enable such revocation.\nhttps://ai.stanford.edu/~kzliu/blog/unlearning\nConstantly adopting and implementing tech advancement with an existing system indicates efficient engineering. Uber wrote an in-depth article about the evolution of its centralized ML platform, Michelangelo.\nhttps://www.uber.com/blog/from-predictive-to-generative-ai/\nLinkedIn points out two critical flaws in a partitioned approach to data management.\nThe granularity of partition creation constrained data consumption. For instance, if partitions were created daily, consumers could only schedule daily jobs to consume new partitions.\nPartitions are created once, but their data can continually be updated.\nLinkedIn writes about LakeChime, a data trigger service that acts as a signal processor for downstream jobs to handle partition and snapshot table layouts.\nhttps://www.linkedin.com/blog/engineering/data-management/lakechime-a-data-trigger-service-for-modern-data-lakes\nHow Scoot Science Charted Its Journey with a Universal Semantic Layer\nLearn how Scoot Science optimized their embedded aquaculture analytics platform through a universal semantic layer in our upcoming webinar on May 15th at 9 AM PT / 12 PM ET.This session will provide insights into:- Scoot Science's SeaState platform and its data challenges- Overcoming data complexities with a semantic layer- Impact of a standardized data model on data processing and user experience- Streamlining data infrastructure for efficiency and innovation\nhttps://cube.registration.goldcast.io/events/11ebc02a-8e78-40fe-837e-2ab90fdc05d5\nBooking.com writes about the lessons learned from adopting Airflow on Google Cloud. The learning focuses on\nSetting up the local development environment\nPerformance tuning with Celery workers\nembedded documentation in Airflow DAG\nhttps://medium.com/booking-com-development/lessons-in-adopting-airflow-51821709cba4\nPayPal writes about its internal AI platform cosmos.ai, which provides MLOps capabilities that streamline processes like model training, deployment, and monitoring, significantly reducing complexity and costs. The platform also emphasizes extensibility and future-proofing against rapid technology changes, focusing on responsible AI usage, multi-tenancy, self-service capabilities, and seamless integration with existing systems.\nhttps://medium.com/paypal-tech/scaling-paypals-ai-capabilities-with-paypal-cosmos-ai-platform-e67a48e04691\nJoin this one-day virtual summit for strategies and best practices to elevate your data integration and drive innovation, with sessions from data leaders at T. Rowe Price, JPMorgan, T-Mobile, LiveRamp, and more:- Enhancing operational efficiency and app functionality with unified data - Feeding data into vector databases to deploy generative AI initiatives- Facilitating the preparation and management of data from multiple sources - Leveraging data integration for BI & Analytics, data science, and data exchange use cases\nhttps://www.dataintegrationsummit.com/\nAs much as Gen AI's potential is promising, mistrust and skepticism could encumber AI adoption. Hallucinations and the system's lack of explainability are the primary reasons for mistrust in Gen AI. The author highlights some key strategies to reduce the hallucinations and increase transparency.\nhttps://medium.com/data-science-at-microsoft/trustworthiness-of-generative-ai-reducing-hallucinations-and-increasing-transparency-a53dfe190ee1\nStream processing comes in different forms, with event stream processing, streaming databases, and stream-enabled analytical databases. The larger question is, should one consider a unified real-time platform? The author expands on the possibility of unified data platforms.\nhttps://sanjmo.medium.com/untangling-the-streaming-landscape-the-rise-of-unified-real-time-platforms-528f49318632\nEvery data team has this burning question: How do we pivot away from answering questions and building dashboards and toward being a strategic partner who has a real impact on the business? The author narrates a path toward achieving a predictable organizational outcome.\nIt\u2019s no surprise that everything starts with getting buy-in from the executive team. There are essentially two types of companies: those that believe in data and those that don\u2019t. It\u2019s an uphill battle for the data team if you end up in an organization where the executives don\u2019t believe in data for the decision-making process. \nhttps://sqlpatterns.com/p/transforming-a-data-culture\nWe practiced and discussed reusability in software engineering, but I never thought deeply about data analytics reuse. For me, It is always adding additional dimensions in a dashboard to bring reusability, but that won\u2019t be the case in ad-hoc analytics. The author discusses the common patterns of data analytics reuse, anti-patterns on each usage and best practices to mitigate technical debts. \nhttps://www.milesmcbain.com/posts/data-analysis-reuse/\nHaving a well-structured data model is always great, but we often handle semi-structured data. The fact that the nature of the event sourcing mostly deals with JSON structure adds more complexity. The LakeHouse format\u2019s in-build support for Map and Array gives the flexibility to handle semi-structured data. \nHowever, the Map and Array comes with its cost. There is no proper indexing support for these complex data types, causing query complications and unhappy customers. The LakeHouse formats should move beyond primitive types and inherent indexing support for complex data types for faster query performance.   \nhttps://dataengineeringcentral.substack.com/p/delta-lake-map-and-array-data-types\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/a-notebook-is-all-i-want-or-dont", "title": "Data Engineering Weekly", "content": "The tweet received strong reactions on LinkedIn and Twitter. To clarify, I quoted it as a Notebook-style development, but it is not exactly a Notebook. There is a lot of context missing in that tweet, so I decided to write a blog about it. \nPeople have reservations about using tools like Jupytor Notebook for the production pipeline for a good reason. Let\u2019s take a few common criticisms about running Notebook in production.\nVersion control is one of the common themes for the case against Notebook. However, modern Notebooks like Databricks seamlessly integrate with Git to build pull requests and code review processes. It is still not good enough, but it is a good start in the right direction. \nThe Notebook code is visible within its scope. Dependency management is also bound to the individual scope of the Notebook and prevents production-grade code quality. It is less of an issue with Jupyter Notebook, which allows the importation of other Notebook functions using the run command. Databricks notebooks have similar functionally termed shared notebooks to support this future. However, it forces us to rethink the shared lib model with the shared Notebook, which can be challenging for overall system design. \nThe code execution flow in typical Python programming differs from the driven execution model. There is a deterministic entry to the execution flow, whereas, in Notebook, one can execute any random cell. The cell execution model is most often state-driven and can produce a non-deterministic response when the execution of the cell becomes out-of-order.\nNotebook design tuned towards ad-hoc, non-standard exploration. There is no underlying semantics for unit testing the code and data testing build-in. It creates a lack of trustability in executing Notebooks in production. \nData orchestration engines like Airflow are built on the underlying Python programming language semantics. Enough tooling and ecosystems are available to build a CI/ CD process and an environment-specific build and deploy model. Notebooks don\u2019t have such semantics built-in, and it is always a challenge when someone can directly modify a production Notebook without any review process. \nNotebook has many advantages, but I want to highlight the top 3 from my perspective. \nThe data asset building is an interactive process. We iteratively understand the data's semantics, structure, and distribution as we build through the data asset. The Notebook style development is a perfect way to construct the data assets.\nAll Notebooks have built-in visualization support, which helps users understand the distribution of the data. We can generate plots and charts directly in the Notebook, making interpreting data easier and explaining it to non-technical stakeholders. \nNotebooks support intuitive code documentation, which allows developers to include explanations, thought processes, analysis conclusions, and project goals. Many orchestration engines try supporting the Markdown for documentation, yet it is less intuitive than a Notebook.\nDeveloper productivity is vital to a data team's function in building and growing a data-driven culture. It is hard to build a data-driven culture across the organization if it takes months instead of hours to build data assets. The current mode of data pipeline development requires expert data engineers, whereas all citizen developers use notebook-style development. It is common to see the Data Engineers rewrite Notebooks from the citizen developers to bring them to production-grade support.\nDo you think Notebook is bad for production? How are we trying to bridge the development and production gap? Let\u2019s take a couple of ways the data tools are trying to solve the problem.\nEvery data orchestration engine eventually shows love to YAML configuration-driven orchestration engines. Airflow successfully fought against Oozie\u2019s XML DSL, but the pattern never dies. Dagster seems on track to bring that dream into orchestration engines. \nIt is pretty common to see platform engineers think they can build a simple config-driven orchestration DSL that can abstract all the complexities. As the adoption happens, the complexity grows exceptionally, eventually becoming a hard programming language to learn and adopt. \nIs YAML the right approach to commoditize authoring data assets? I don\u2019t think so.\nUsing YAML and the general-purpose programming language requires some level of proficiency in coding. Here comes the no-code UI-driven data orchestration engines. All the legacy ETL tools offer UI-driven ETL solutions that lack version control, a review process, or software development methodologies. \nIs the UI-driven model  the right approach to commoditize authoring data assets? I don\u2019t think so.\nI believe both the config-driven and UI-driven data orchestration are suboptimal. We have seen a significant increase in developer productivity using copilot-style development. Mckinsy even called out Gen AI talent as Your next flight risk.\nThe common pattern in Notebook and copilot-driven development is that they are interactive development models equipped to build data assets faster. No-code and YAML are unsuitable for Copilot-driven development since the underlying implementation limits them, whereas general-purpose programming brings much more flexibility. \nAs of today, Notebooks are not an optimal solution for writing and operating production data pipelines. I completely agree with that statement. However, compared with the industry's approaches to solving this problem with YAML and no-code UI, I wonder why we are not innovating more on Notebook-style development.\nCombining the Copilot model with the Notebook-style interactive development model, which successfully addresses the existing flaws, will significantly impact the data engineering landscape. \nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-169", "title": "Data Engineering Weekly", "content": "The Data Product Builder platform is becoming increasingly important in enterprise data engineering. It offers more targeted and customized data asset building than the general-purpose data stack. \nIntuit writes about the growing data mesh strategy and the strategic focus area for building data products.\nSystematically organizing people, code, and data\nClearly defining ownership of each data product\nProviding tools for designing, authoring, deploying, and operating data products\nhttps://medium.com/intuit-engineering/the-data-mesh-strategy-behind-intuits-global-financial-technology-platform-db862fd45e0b\nContinuous learning is a vital part of a professional career. I consider writing a newsletter to be a learning process. Lyft writes about how it enabled continuous learning with technical training, Computer Science courses for Data Science, Seminar, and Brown Bag.\nhttps://eng.lyft.com/technical-learning-at-lyft-build-a-strong-data-science-team-a6628215513c\nNetflix's Data Gateway is a platform designed to enhance and safeguard the data tier by providing a unified interface for accessing data across various sources while ensuring security and compliance. It simplifies engineers' data access and management tasks by offering a single entry point with consistent API semantics and access controls, enabling efficient and secure data handling at scale. With features like caching, load balancing, and monitoring, the Data Gateway optimizes performance and reliability, supporting Netflix's complex data ecosystem.\nhttps://netflixtechblog.medium.com/data-gateway-a-platform-for-growing-and-protecting-the-data-tier-f1ed8db8f5c6\nQuantatec, a company based in Brazil focusing on vehicle and fleet management, needed to equip their team with an embedded analytics platform. The initial option was a static platform of hard-coded reports which offered only basic filtering options and was insufficient for the fleet managers who required more tailored features.\nDive into how Quantatec addressed this issue by integrating a semantic layer that:- Responds to the rising data demands- Enhances the speed of query responses- Provides self-service customization options- Facilitates conversational data interactions\nhttps://cube.dev/case-studies/customizable-embedded-dashboards-and-natural-language-ai-queries\nOne of the major challenges for many enterprises is to find LLM usage that can significantly differentiate their product experience and enable an efficient software development process to build these features. LinkedIn writes an exciting blog about what is and is not working. \nhttps://www.linkedin.com/blog/engineering/generative-ai/musings-on-building-a-generative-ai-product\nAirbnb writes about Brandometer, which utilizes AI to gauge brand perception on social media, enabling the company to measure its online reputation effectively. By analyzing vast amounts of social media data, the Brandometer identifies key brand attributes and sentiments, providing actionable insights for enhancing Airbnb's brand image and strategy. This AI-powered tool helps Airbnb stay attuned to customer feedback and market trends, contributing to its ongoing success in the competitive hospitality industry.\nhttps://medium.com/airbnb-engineering/airbnb-brandometer-powering-brand-perception-measurement-on-social-media-data-with-ai-c83019408051\nDoorDash is leveraging large language models to construct a comprehensive Product Knowledge Graph, facilitating efficient data organization and retrieval. By harnessing these models' capabilities, DoorDash aims to enhance user experience by enabling smarter search functionalities and personalized recommendations within its platform. This initiative demonstrates how advanced AI technologies can empower data engineering efforts to create more intelligent and user-centric systems in the food delivery industry.\nhttps://doordash.engineering/2024/04/23/building-doordashs-product-knowledge-graph-with-large-language-models/amp/\nGrab's data observability article discusses the implementation of the Iris platform, which is designed to enhance decision-making through comprehensive data monitoring and analysis. Iris tackles the challenge of extracting actionable insights from complex, cross-platform metrics by routing data in real-time to systems like InfluxDB and offline to AWS-based data lakes. The platform's structure allows for detailed tracking and analysis of CPU, memory, I/O, and job lifecycle metrics, facilitating proactive management and optimization of data processes.\nhttps://engineering.grab.com/data-observability\nThe article \"Data Architectures\" from Venkatesh discusses various patterns in data architecture that have evolved, such as Lambda, Kappa, and Delta architectures. These architectures are designed to handle massive amounts of data by defining how data moves through systems from ingestion to processing to storage. The article delves into the strengths and applications of each architecture type, highlighting their relevance in modern data-driven environments where real-time processing and data analytics are crucial.\nhttps://subrabytes.dev/dataarchitectures\nTeads team writes about the dbt unit-test framework in a two-part series. The blog highlights the ability to add support multiple unit testing in the same yaml file, and the usage of ephermal model for easier testing. \nPart 1: https://medium.com/teads-engineering/unit-testing-with-dbt-fb84f2ef7dd6\nPart 2: https://medium.com/teads-engineering/dbt-unit-test-framework-72d9ca60c69b\nAlert fatigue hinders developers productivity, and it is vital to understand signal from noise. Dave Flynn writes about tactics followed by Foodpanda to improve alert quality. The approach focused on\nTiered model\nWeigted alerts\nClear triage responsibility and expectations\nhttps://medium.com/inthepipeline/so-you-think-youve-got-dbt-test-bloat-37491fb330d5\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-168", "title": "Data Engineering Weekly", "content": "Meta is taking an interesting approach in the growing LLM market with the open source approach and distribution across all the leading cloud providers and data platforms. It is exciting to see Llama 3 with 70B parameters on par with GPT-3.5, which I believe has 175B parameters. \nhttps://ai.meta.com/blog/meta-llama-3/\nCan LLM make an impact in healthcare? Globally, the cost of healthcare is increasing significantly. I expect healthcare providers to experience soon what the taxi industry has gone through. The users might choose a good enough, low-cost option over the existing one. Medical tourism is one industry thriving on that front, and perhaps LLM is the answer. HuggingFace publishes the open Medical-LLM leaderboard.\nhttps://huggingface.co/blog/leaderboard-medicalllm\nAirbnb recently open-sourced Chronon, a declarative feature engineering framework. Stripe publishes details on how it adopted Chronon in partnership with Airbnb.  The blog narrates how Chronon fits into Stripe\u2019s online and offline requirements.\nhttps://stripe.com/blog/shepherd-how-stripe-adapted-chronon-to-scale-ml-feature-development\nOrganizations are exploring the potential of chatbots to answer questions presented in natural language. However, when these chatbots utilize generative AI, they risk producing incorrect replies. \nA semantic layer can solve this problem by creating the knowledge needed for 100% accuracy.Webinar Information:\ud83d\uddd3 Date: Wednesday, April 24th\ud83d\udd52 Time: 9 am PST | 12 pm ESTJoin us to discover:-The consequences of not using a semantic layer with LLMs in chatbots-How a semantic layer contributes to the effectiveness of an AI chatbot-The noticeable distinction in a live demonstration of an AI chatbot versus one lacking a semantic layer.\"\nhttps://cube.registration.goldcast.io/events/bd020457-cbec-4ebd-b6c0-7ea40430ec86\nA common design feature of modern data lakes and warehouses is that Inserts and deletes are fast, but the cost of scattered updates grows linearly with the table size. RevenueCat writes about solving such challenges with the ingestion table & consolidation table pattern. \nhttps://www.revenuecat.com/blog/engineering/data-ingestion-snowflake/\nIf you know how to count, you\u2019re an excellent data engineer. Counting is the hardest problem in data engineering. Canva writes about such challenges and narrates how it solves them. \nhttps://www.canva.dev/blog/engineering/scaling-to-count-billions/\nApache Hudi\u2019s Merge On Read (MoR) is a game changer in developing low-latency analytics on top of the data lake. Grab narrates how it integrated Debeizium, Kafka, and Apache Hudi to enable near real-time data analytics on the data lake. \nhttps://engineering.grab.com/enabling-near-realtime-data-analytics\nGoodData writes about using Apache Arrow to build an efficient service layer. The blog narrates using Apache Arrow Flight RPC to build data querying, post-processing, and caching layers.\n https://medium.com/gooddata-developers/building-a-modern-data-service-layer-with-apache-arrow-33ace768e3f1\nIt is always exciting to read the internals of an analytical platform, and Mixpanel published one such article. The blog narrates the common challenges with enterprise analytical platforms such as event-centric, schema on-read, and nested json structures. \nhttps://engineering.mixpanel.com/under-the-hood-of-mixpanels-infrastructure-0c7682125e9b\nAutodesk writes a classic data ingestion problem: What can go wrong with event collection? The blog highlights the lack of control in schema, schema drifting, and scalability as the key challenges. The result is to adopt data contract solutions with type standardization and auto-generate schemas. \nhttps://engineering.autotrader.co.uk/2024/04/05/auto-generating-snowplow-types.html\nSwiggy writes another classic data contract solution that automates mobile event verification. Event testing is one of the biggest challenges that increases complexity for mobile applications since the release cycle is not frequent. The blog narrates an automated contract validation workflow in Android applications.\nhttps://bytes.swiggy.com/automating-mobile-event-verification-1d840f39d300\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions.\n\n"}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-167", "title": "Data Engineering Weekly", "content": "Will AI agents soon become a common fixture in our homes and an integral part of our daily lives? Meta introduces the Open-Vocabulary Embodied Question Answering (OpenEQA) framework\u2014a new benchmark to measure an AI agent\u2019s understanding of its environment by probing it with open-vocabulary questions. Meta claims that by enhancing LLMs with the ability to \u201csee\u201d the world and situating them in a user\u2019s smart glasses or on a home robot, we can open up new applications and add value to people\u2019s lives.\nhttps://ai.meta.com/blog/openeqa-embodied-question-answering-robotics-ar-glasses/\nWith the 1-bit LLM model, the researchers are suggesting instead of FP16 (Full Precision floating-point number with 5-bits) or FP32 (Full Precision floating-point number with 6-bits), you can build an equally efficient model using ternary digit set \u2208 {-1, 0, 1}. It can drastically reduce the computational cost and energy requirement for training LLM, and it is an interesting development to watch.  \nhttps://aishwaryasrinivasan.substack.com/p/microsofts-1-bit-llm\nThe impact of LLM on software development is undeniable. Every survey demonstrates an increase in productivity in software development with copilot-type tools. Github shares some insights on how Github engineers use Github Copilot.\nhttps://github.blog/2024-04-09-4-ways-github-engineers-use-github-copilot/\nFrom cloud data warehouses and cloud-native enterprise applications to acquiring the right data and the joins, relationships, and calculations to correctly define a data model, there\u2019s still too much guesswork for data analysts and business users who are seeking insights. \nDiscover how a universal semantic layer is transforming modern business intelligence, making data more accessible and reliable for organizations striving for informed business decisions.\nhttps://cube.dev/blog/business-intelligence-with-universal-semantic-layer\nAirbnb open-sources Chronon, its ML Feature platform. The highlight of the design is the unique blending of batch and real-time events with the common feature definition.  The other key feature I like is the out-of-the-box support for backfilling with point-in-time accuracy, skew handling & computational efficiency.\nhttps://medium.com/airbnb-engineering/chronon-airbnbs-ml-feature-platform-is-now-open-source-d9c4dba859e8\nSoftware engineers and data engineers don\u2019t speak the same language\nIt reminds me of this famous quote\nThe Author highlights that the growth team should be aware of the skill differences. Understanding the difference between their interpretations of common terminology is a big step toward having better conversations with both teams.\nhttps://newsletters.databeats.community/p/collaborating-with-data-and-engineering-teams\nI\u2019m not a big fan of these comparison studies since it all depends on the nature of the data and the business use cases. However, TIL about Apache Paimon, a lake format that enables building a real-time lakehouse architecture with Flink and Spark for both streaming and batch operations. \nhttps://www.alibabacloud.com/blog/building-a-streaming-lakehouse-performance-comparison-between-paimon-and-hudi_601013\nStaying with the LakeHouse format, The 10-part series about Apache Hudi is an exciting read for me over the weekend. The blog post brings in-depth insights into how Apache Hudi works on the right side, read side & underlying storage formats. It's a good read that familiarizes you with the LakeHouse formats.  \nhttps://blog.datumagic.com/p/apache-hudi-from-zero-to-one-1010\nThe New York Times details its advancements in standardizing experimentation across its teams to refine the testing process and implement innovations efficiently. By centralizing its experimentation platform, the Times has streamlined test efficiency, accelerated decision-making, and strengthened data analysis capabilities. This systematic approach aids in aligning various team efforts and enhances overall product development.\nhttps://open.nytimes.com/milestones-on-our-journey-to-standardize-experimentation-at-the-new-york-times-2c6d32db0281\nContinuing the stories of standardizing experimentation, Grab writes about the GrabX Decision Engine. GrabX encompasses a pre-experiment advisor, a post-experiment analysis toolbox, and other advanced tools. It utilizes rule-based logic and machine learning models to evaluate millions of possibilities, ensuring the most efficient decisions are made instantaneously.\nhttps://engineering.grab.com/grabx-decision-engine\nIf someone asks me to define LLM, this is my one-line definition.\nLarge Language Models: Turning messy data into surprisingly coherent nonsense since 2023.\nHigh-quality data is the cornerstone of LLM. In this blog, Intel shares four data-cleaning techniques to improve LLM performance, with code examples and a step-by-step process. \nhttps://medium.com/intel-tech/four-data-cleaning-techniques-to-improve-large-language-model-llm-performance-77bee9003625\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-166", "title": "Data Engineering Weekly", "content": "The 2024 dbt\u2019s state of analytical engineering report is out. Poor data quality and unlcear data ownership remains the top challenges for the data teams. Data Mesh continuously gaining popularity among the enterprises. It is a stark difference from the Gartner report about data mesh. I guess only the time will tell who wins in the data mesh vs data fabric war. \nhttps://www.getdbt.com/resources/reports/state-of-analytics-engineering-2024\nConinue the week of insights into the world of data & AI landscape, the 2024 MAD landscape is out. The report pointed out the rise of LLM makes the unstructured data more important than ever, pressure to the Modern Data stack will continue to intensify as the cost of integration remains high, and the rise of \u201cModern AI Stack\u201d\nhttps://mattturck.com/mad2024/\nWill AI replace the coders? What will the future of software engineers be? EvalPlus builds a leadership board to demonstrate the efficiency of leading AI coder models. \nhttps://evalplus.github.io/leaderboard.html\nCloud Academy, a SaaS e-learning platform,\u00a0needed to deliver a seamless, highly available embedded analytics experience for their enterprise\u00a0customers.\u00a0They knew they needed a flexible caching layer and zero-downtime deployments.\n\u201cWith Cube, we\u2019ve been able to speed up time to release a new data model to production by 5x and decrease analytics downtime by 90%. \u201d\u00a0Dive deeper to learn how Cloud Academy sped up data model development and query performance with a semantic layer.\nhttps://cube.dev/case-studies/cloud-academy-and-cube\nThe author reflects Data Council 2024 conversations, the most popular data conference in the USA. The emerging of composable data stack, and open data stack is certainly an interesting trend to watch. A key highlight for me, \nI spoke to multiple data people stuck in legacy systems and still inching their way to the cloud. VCs have moved on from data catalogs, yet practitioners told me they look forward to solving data discovery.\nhttps://medium.com/vvus/data-council-2024-the-future-data-stack-is-composable-and-other-hot-takes-b6c5f2429e22\nLast week Intuit shared its key learning building Text 2 SQL, and Pinterest publishes the tech deep dive on how its internal Text2SQL work. The highlight for me is,\nThere is an ongoing table standardization effort at Pinterest to add tiering for the tables. We index only top-tier tables, promoting the use of these higher-quality datasets.\nI strongly believe the concept of Data Product will play a bigger role in data engineering. It is evident that it will become the foundation of trusted sources, which is essential to taking advantage of advancements from LLMs.\nhttps://medium.com/pinterest-engineering/how-we-built-text-to-sql-at-pinterest-30bad30dabff\nJoin us for this free data integration webinar featuring speakers Nikita (Director of Engineering at Doordash), Abhishek (Platform Architect at LiveRamp), and Darrel (Distinguished Engineer at Clearwater Analytics). RSVP now for practical advice on how to overcome complex data integration challenges such as: \u00a0\nAccelerating GenAI, analytics, & data product use cases/initiatives;\nAccessing data from various sources and a variety of formats;\nTransforming and preparing the data for downstream use cases;\nDelivering data to various destinations with high quality and scale.\nhttps://offers.nexla.com/data-integration-leader-series-webinar-04162024-1\nIn the ever-evolving landscape of data-driven decision-making, a well-structured data platform emerges as a critical asset. Spotify shares some of the critical triggers in an organization that leads to build data platform. \nhttps://engineering.atspotify.com/2024/04/data-platform-explained/\nReplit has developed a native AI model for code repair, leveraging the Language Server Protocol (LSP) diagnostics and operational transformations (OTs) to train a large language model (LLM) that fixes code errors directly within its IDE. This initiative aims to significantly reduce developers' time spent on debugging by improving the AI's understanding and interaction with the development environment. The model is trained using a dataset of code-diagnostic pairs and fine-tuned to predict line diffs that correct LSP-identified errors, showing promising results against larger models and existing benchmarks.\nhttps://blog.replit.com/code-repair\nThe rapid adoption of AI brings challenges for data engineering to design systems to handling sensitive data. The author writes a comprehensive article on strategies to handle sensitive data, maturity level of each organizations and how the solution differ for each maturity levels. \nhttps://blog.det.life/data-engineering-architectures-strategies-for-handling-sensitive-data-83292b997c17\nForget Modern Data Stack, Have you ever wonder what is Declarative Data Stack? The blog takes an example of SQL as an evidence of the success of a  declartive language. For the lack of better wording, we should further classify declartive languages as dynamic and static. SQL is a dynamic declarive language where one can express complex constraints, where YAML pretty much a static rule engine.\nhttps://blog.picnic.nl/yaml-developers-and-the-declarative-data-platforms-4719b7a1311c\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-165", "title": "Data Engineering Weekly", "content": "The productivity increase with GenAI is undeniable, and several startups are trying to solve the Text2SQL generation problem. Intuit wrote an exciting article about what it learned from rolling out the internal GenAI tool. My key highlight is that Excellent data documentation and \u201cclean data\u201d improve results. The blog further emphasizes its increased investment in Data Mesh and clean data.\nhttps://medium.com/intuit-engineering/how-intuit-data-analysts-write-sql-2x-faster-with-internal-genai-tool-c3b9d482208a\nCan we safely say PySpark killed Scala-based data pipelines? The blog is an excellent overview of all the improvements made to PySpark in 2023. I\u2019m looking forward to playing around with Testing API and Arrow-optimized UDF since UDF is the only reason I write Scala nowadays. \nhttps://www.databricks.com/blog/pyspark-2023-year-review\nThe advancement in AI/ML brings significant challenges for infrastructure to scale and support. The challenge multiplies when designing a singular AI/ML system amid rapid application and model advancements, from XGboost to deep learning recommendation and large language models. The blog narrates how Uber utilizes the existing infrastructure to support various AI/ML use cases. \nhttps://www.uber.com/blog/scaling-ai-ml-infrastructure-at-uber/\nLearn strategies to overcome slow queries in this guide on optimizing your data reporting for speed and responsiveness, aligning perfectly with your users' expectations.\nhttps://cube.dev/blog/sub-second-query-performance-for-data-apps\nOrganizations continuously generate data on a massive scale. I often noticed that the derived data is always > 10 times larger than the warehouse's raw data. The Netflix blog emphasizes the importance of finding the zombie data and the system design around deleting unused data. \nhttps://netflixtechblog.medium.com/navigating-the-netflix-data-deluge-the-imperative-of-effective-data-management-e39af70f81f7\nOne of the reasons why I advocate strongly and built solutions around data products, and contracts are that I firmly believe the data quality should be expressed from the data consumers' perspective. Data is only as good as the business value it provides, and the business value can only be seen from the consumer's perspective. \nI am delighted that LinkedIn\u2019s ValiData is focused on simplifying the data validation process through simple configuration files.\nhttps://www.linkedin.com/blog/engineering/data-management/scalable-automated-config-driven-data-validation\nDoorDash writes an insightful article about setting Multi-Tenancy in the Kafka cluster and the role of a consumer proxy in setting this up. The problem statement is certainly pretty interesting.\nTraditionally, an isolated environment such as staging is used to validate new features. But setting up a different data traffic pipeline in a staging environment to mimic billions of real-time events is difficult and inefficient, while requiring ongoing maintenance to keep data up-to-date. To address this challenge, the team at DoorDash embraced testing in production via a multi-tenant architecture, which leverages a single microservice stack for all kinds of traffic including test traffic.\nhttps://doordash.engineering/2024/03/27/setting-up-kafka-multi-tenancy/\nThis article explores how real-time streaming can significantly enhance generative AI applications by providing immediate, contextually relevant data for better adaptability and performance across various tasks. Is Real-time data streaming essential for maintaining the accuracy and relevance of AI-driven applications?  So far, I think Gen AI applications are a request/ response model. Please comment with your thoughts on the role of real-time streaming in the Gen AI application development lifecycle. \nhttps://aws.amazon.com/blogs/big-data/exploring-real-time-streaming-for-generative-ai-applications/\nPython took over the data world, hands down, as the conversation around Polars vs. Pandas increased. But what is the difference between Polars and Pandas? Polars written in Rust offer 5-10X more performance than Pandas!! The trend is a simple example of the rise of Rust in building data infrastructure. The PyO3 native Python crate for Rust makes building Python binding much simpler, which increases the adoption of an architecture pattern using the mix of Rust and Python. \nhttps://blog.jetbrains.com/dataspell/2023/08/polars-vs-pandas-what-s-the-difference/\nAn exciting case study from Yelp on designing privacy-preserving system design. Yelp writes about the phone number masking feature for its Services Marketplace, aiming to protect consumer privacy and build trust by allowing users and service professionals to communicate via phone calls and SMS without exchanging real phone numbers. The system design around phone number recycling and reuse is an exciting read.\nhttps://engineeringblog.yelp.com/2024/03/phone-number-masking-for-yelp-services-projects.html\nThe blog is a gentle reminder for our readers about all the nightmares around the timestamp in various processing engines. The blog discusses inaccuracies caused by int96 Parquet datatype, particularly in data created by Apache Spark. The author recommends using the following Spark config change as a fix.\nhttps://www.linkedin.com/pulse/julian-gregorian-timestamp-anomalies-trino-spark-harish-kumar-m-ws20c/\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-164", "title": "Data Engineering Weekly", "content": "This report has a lot of interesting insight into the enterprise adoption of Gen AI. Companies are more open to adopting Gen AI for their internal use cases but have reservations about rolling it out to their clients. The Gen AI budget is now rolling into regular software budgeting rather than an experimental budget. OpenAI has more production deployments, but Google is catching on to it. \n https://a16z.com/generative-ai-enterprise-2024/\nThis is a comprehensive overview of the state of the data streaming landscape in 2024. As we predicted in the key trends of 2023 about Apache Flink as a clear winner in the stream processing frameworks, we see Confluent offering Flink as a service. The author goes beyond comparing the tools to various offerings from streaming vendors in stream processing and Kafka protocol-supported systems. \nhttps://kai-waehner.medium.com/the-data-streaming-landscape-2024-6e078b1959b5\nUber writes about Model Excellence Scores (MES), a framework to improve the quality of machine learning (ML) systems at scale. MES framework evaluates model quality across various lifecycle stages\u2014prototyping, training, deployment, and prediction\u2014using Service Level Agreement (SLA) principles. MES measures and enforces quality through indicators, objectives, and agreements, enhancing visibility, fostering a quality-focused culture, and substantially improving prediction performance and operational efficiency.\nhttps://www.uber.com/en-SG/blog/enhancing-the-quality-of-machine-learning-systems-at-scale/\nAI Chatbot for Embedded Analytics powered by a Semantic Layer\nSpyne, a SaaS company, uses Cube's semantic layer to enhance customer-embedded analytics by incorporating an AI chatbot. Dive into Spyne's experience with:- Their search for query acceleration with pre-aggregations and caching- Developing new functionality with Open AI- Optimizing query cost with their data warehouse\nhttps://cube.dev/case-studies/embedded-analytics-and-ai-chatbot-on-one-semantic-layer\nCost is the major concern as the adoption of data lakes increases. The author writes excellent tips on optimizing Apache Hudi LakeHouse infrastructure by adopting indexes, partition pruning, handling incomplete versions, etc.\n https://blogs.halodoc.io/data-lake-cost-optimisation-strategies/amp/\nSalesforce AI Research introduced Moirai, a foundation model for universal time series forecasting comprising 27 billion observations spanning nine domains. It addresses challenges across multiple domains and frequencies with zero-shot forecasting capabilities. Moirai utilizes a large, diverse dataset and innovative techniques like any-variate attention and multiple patch-size projection layers to model complex, variable patterns. This approach allows Moirai to deliver superior forecasting performance without needing domain-specific models, demonstrating its effectiveness in familiar and novel scenarios.\nhttps://blog.salesforceairesearch.com/moirai/\nLogarithm indexes 100+GB/s of logs in real-time and thousands of queries a second!!! The logging engine to debug AI workflow logs is an excellent system design study if you\u2019re interested in it. At a high level, Logarithm work as follows,\nApplication processes emit logs using logging APIs. The APIs support emitting unstructured log lines and typed metadata key-value pairs (per line).\nA host-side agent discovers the format of lines and parses lines for common fields, such as timestamp, severity, process ID, and call site.\nThe resulting object is buffered and written to a distributed queue (for that log stream), providing durability guarantees with days of object lifetime.\nIngestion clusters read objects from queues and support additional parsing based on user-defined regex extraction rules. The extracted key-value pairs are written to the line\u2019s metadata.\nQuery clusters support interactive and bulk queries on one or more log streams with predicate filters on log text and metadata.\nhttps://engineering.fb.com/2024/03/18/data-infrastructure/logarithm-logging-engine-ai-training-workflows-services-meta/\nThe article discusses Freshworks' journey in modernizing its analytics data platform to handle increasing volumes of data efficiently. Initially using a legacy system that involved a mix of Python consumers, Ruby on Rails, and Apache Airflow for data ingestion and processing, Freshworks transitioned to a more scalable, distributed, and auto-scalable system using Apache Spark. This new system improved scalability, maintenance, cost-effectiveness, and performance, demonstrating significant savings and operational efficiencies in handling millions of messages per minute for analytics purposes.\nhttps://www.freshworks.com/saas/eng-blogs/modernizing-analytics-data-ingestion-pipeline-distributed-processing-engine/\nOk, Data Mesh is still alive!! Data Mesh is certainly a hotly debated topic in the data industry. Mercado Libre writes about its adoption of Data Mesh, which, necessitated by the company's rapid growth and diverse ecosystem, decentralizes data production across domains, leveraging cultural shifts and technological advancements for independent data management. Their implementation involved creating Data Mesh Environments (DMEs) for better infrastructure, ensuring homogeneous technology use, and establishing a singular user access point, significantly enhancing scalability and governance while supporting continuous innovation. \nhttps://medium.com/mercadolibre-tech/data-mesh-meli-building-highways-for-thousands-of-data-producers-0f41d8e08610\nLyft developed a reinforcement learning (RL) platform, particularly focusing on Contextual Bandits, to address decision-making challenges unsuited for supervised learning or optimization models. Lyft\u2019s RL approach involves testing actions, observing feedback through rewards, and updating models for improved decision-making, enabling continuous learning and adaptation to changing environments. The platform leverages its existing ML ecosystem, extends it for RL models, and incorporates lessons from deploying RL in real-world applications, emphasizing the power and challenges of RL in dynamic decision-making and optimization.\nhttps://eng.lyft.com/lyfts-reinforcement-learning-platform-670f77ff46ec\nInstacart introduced its Fraud Platform, Yoda, utilizing ClickHouse as its real-time datastore to quickly detect and respond to various fraud types, from fake accounts to payment fraud. Yoda enables analysts to create rules that differentiate legitimate from fraudulent activities, taking actions such as blocking transactions or disabling accounts. ClickHouse's data storage and processing efficiency, coupled with a self-serve, flexible system for fraud detection, significantly enhances Instacart's ability to maintain a trustworthy environment and safeguard its financial health.\nhttps://tech.instacart.com/real-time-fraud-detection-with-yoda-and-clickhouse-bd08e9dbe3f4\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-163", "title": "Data Engineering Weekly", "content": "Large language models have taken the world by storm, and every country is trying to evaluate its potential impact. India recently announced that all AI apps require government approval and dropped the plan later. \nOn similar trends, the article navigates to the complex EU AI Act, recently passed by the European Parliament, which introduces comprehensive regulations for machine learning models impacting EU citizens, focusing on mitigating risks to health, safety, and rights. It defines AI broadly, including any machine-based system influencing physical or virtual environments, and categorizes AI applications by risk, from banned \"Unacceptable Risk AI Systems\" to scrutinized \"High-Risk AI Systems.\" Compliance is mandatory, with strict penalties for violations, emphasizing the importance of data scientists familiarizing themselves with the law to avoid prohibited AI uses and ensure ethical, safe AI development. \nhttps://towardsdatascience.com/uncovering-the-eu-ai-act-22b10f946174\nData products are becoming a central part of the data pipeline strategy, bringing a new approach to building data as an asset. The author explores the evolving landscape of data product standards, highlighting the necessity for unified standards to ensure interoperability and manage the data economy effectively. It discusses the significance of data governance, sharing history, and generative AI's impact on data economy standards. It also introduces emerging standards like the Open Data Contract Standard and Data Product Descriptor Specification.\nAs you know, I\u2019m fascinated by data products and the potential to change the data engineering practice. Watch out, aldefi.io, for more updates. \nhttps://medium.com/exploring-the-frontier-of-data-products/exploring-the-frontier-of-data-products-seeking-insights-on-emerging-standards-9467ddd6f152\nMikkel is one of my favorite authors in the data engineering space, who always brings a unique perspective. Can we measure the cost of data incidents? Mikkel developed a cost function to measure and classify the incidents' severity.\nData quality cost = (avg. $ cost of incident x # of incidents) + (% of time spent x FTE cost)\nAre you measuring the cost of your data incidents? Please share your thoughts in the comments. \nhttps://medium.com/@mikldd/the-cost-of-data-incidents-53646b588601\nData governance is probably one of the vaguely defined terminologies that inherently brought its irrelevance in the modern data era. Ha, we can\u2019t use a modern data stack anymore \ud83d\ude2d\ud83d\ude2d\ud83d\ude2d\ud83d\ude2d. The author makes a valid argument about the problem with data governance.\nVague Definitions and Overreach\nMisplaced Focus on Policy and Compliance\nInadequate Understanding of Data Quality and Representation\nI agree with these comments; we need to better define data governance in alignment with the emerging AI standards. \nhttps://eric-sandosham.medium.com/the-problem-with-data-governance-2570f0573f3a\n\"Employing Tailored Embedded Business Insights Through Semantic Layers\nDonald Farmer, an expert in data analytics and the author of the O\u2019Reilly book \"Embedded Analytics,\" guides readers through a comprehensive 14-page report focused on using a semantic layer in embedded analytics. Within this report, he explores:- The challenge of low adoption rates of BI tools among users, even with the industry's overall success.- The potential of embedded analytics to enhance insights and efficiency across various contexts.- Examining the advantages and drawbacks associated with different approaches to embedding analytics.- The role of AI in shaping data experiences and the crucial role a semantic layer plays in ensuring accuracy in LLMs.\nhttps://cube.dev/embedded-analytics-guide\nNetflix writes about utilizing Metaflow, an open-source framework, to support a broad spectrum of ML and AI applications, streamlining the transition from prototype to production. Metaflow's integration with Netflix's data, compute, and orchestration platforms enables scalable and efficient deployment of diverse ML projects. This ecosystem supports domain-specific needs across Netflix, highlighting the company's commitment to leveraging advanced technology for data science, machine learning, and AI to enhance various business facets, from content recommendation to operational efficiency.\nhttps://netflixtechblog.com/supporting-diverse-ml-systems-at-netflix-2d2e6b6d205d\nThe article discusses building Meta's GenAI infrastructure, a system designed to train large AI models like Llama 3 efficiently. It covers hardware, software, storage, and network design choices for high throughput and reliability. Notably, Meta is committed to open source and open computing principles and aims to have 350,000 NVIDIA H100 GPUs by the end of 2024!!!\nhttps://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/\nVector databases are a new class designed to efficiently store and query high-dimensional vector representations of data, like embeddings from LLMs. As LLMs become more widely adopted, there is a growing need for scalable solutions to store and search through the vast amounts of data ingested and generated by these models. The article compares all the VectorDB available in the market. \nhttps://superlinked.com/vector-db-comparison/\nDifferential storage is a technique used in databases and file systems to reduce the storage space required by taking advantage of data redundancy. Here's how it works:\nWhen new data is written, only the differences (deltas) from the existing data are calculated and stored instead of storing the entire file or data object.\nThe original data and the stored deltas are combined to reconstruct the full data.\nMotherDuck writes an excellent article about integrating differential storage in DuckDB.\nhttps://motherduck.com/blog/differential-storage-building-block-for-data-warehouse/\nRay is gaining a lot of traction as the Python-based AI workloads increase. AnyScale writes why the leading companies are betting on Ray. The article covers the creation of Ray clusters within ACK for efficient AI and Python application scaling, integrating with various Alibaba Cloud services for improved log management and observability. Additionally, it emphasizes the use of Ray and ACK auto scalers for dynamic resource scaling, enhancing computing resource efficiency and utilization for machine learning and other intensive computational workloads\nhttps://www.alibabacloud.com/blog/best-practices-for-ray-clusters---ray-on-ack_600925\nKafka-tiered storage is a system design that utilizes S3 as supported storage for its tiered architecture. The article details the tiered storage approach's technical architecture and benefits, including better resource utilization and cost savings while maintaining Kafka's performance and scalability.\nhttps://developers.redhat.com/articles/2024/03/13/kafka-tiered-storage-deep-dive\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-162", "title": "Data Engineering Weekly", "content": "I am so grateful for the enthusiastic participants who made our Chennai Data Heroes- Community for Data Folks meetup vibrant! Big thanks to our insightful speakers,\nPradheep Arjunan - Shared insights on AZ's journey from on-prem to the cloud data warehouses.\nThanks to Ideas2IT Technologies for hosting us in their fantastic space.\nGoogle Research introduced Croissant, a new metadata format designed to make datasets ML-ready by standardizing the format, facilitating easier use in machine learning projects. Croissant builds upon existing standards like schema.org, enhancing them with layers specific to ML needs, including data organization and ML semantics. Major ML dataset repositories and frameworks support Croissant, simplifying the discovery, preparation, and utilization of datasets for machine learning practitioners by standardizing and organizing them.\nhttps://blog.research.google/2024/03/croissant-metadata-format-for-ml-ready.html\nThis article emphasizes the importance of data quality in data lakehouses, outlining the consequences of neglecting it and advocating for a structured approach through a governance framework. It distinguishes between intrusive and non-intrusive data quality management, advocating for clear governance to address quality issues. Furthermore, it presents a common lakehouse design, emphasizing data validation across bronze, silver, and gold layers, and suggests technologies and practices for maintaining high data quality, emphasizing continuous improvement and the critical role of standardization.\nhttps://piethein.medium.com/data-quality-within-lakehouses-0c9417ce0487\nNVIDIA's blog post on Trustworthy AI defines it as an AI development approach prioritizing safety and transparency, emphasizing the importance of developing AI systems that people can trust. The post outlines key principles such as privacy, safety, security, transparency, and non-discrimination, highlighting NVIDIA's initiatives like federated learning, guardrails for AI applications, and transparency tools. \nhttps://blogs.nvidia.com/blog/what-is-trustworthy-ai/\nCome join Cube\u2019s webinar on March 13 exploring how Ramsoft uses a universal semantic layer to improve the embedded analytics experience for external users.\nHear directly from Dhyan Shah, Ramsoft\u2019s Product Manager, as he determines factors for his technology choices that help him navigate working with LLMs, and how this experience will drive his future projects.Date: Wednesday, March 13, 2024 at 9 am PT | 12 pm ETFeatured Speakers:Jen Grant, COO of CubeDhyan Shah, Product Manager of Ramsoft\nhttps://bit.ly/cube-ramsoft-webinar\nIntuit is advancing its product development by creating custom in-house large language models (LLMs) to bridge the gap between off-the-shelf capabilities and specific customer needs. This culminated in the creation of GenOS, an operating system for developing GenAI-powered features. Intuit advocate starting with existing models and enhancing them through prompt engineering or training on domain-specific knowledge before considering custom LLM development. This approach, coupled with balancing model size against resource costs and continuously updating models to maintain accuracy, underlines a principled strategy for integrating GenAI into products and engineering workflows.\nhttps://stackoverflow.blog/2024/02/07/best-practices-for-building-llms/\nIntuit has developed a GenAI operating system, GenOS, to harness the power of generative AI and large language models (LLMs) for various financial services. This platform enables Intuit developers to access, compare, and customize various LLMs, including proprietary ones tailored for financial challenges, aiming to enhance productivity and create personalized experiences for their vast customer base. Addressing challenges such as maintaining a dynamic LLM catalog, matching models to specific use cases, fine-tuning for optimization, and promoting responsible development, Intuit's approach underscores the complexity and potential of deploying GenAI in financial technologies.\nhttps://medium.com/intuit-engineering/building-a-flexible-platform-for-optimal-use-of-llms-33a389cedf49\nNetflix has initiated a project to integrate Machine Learning (ML) with its existing rule-based classifier for operational automation in big data job management, focusing on auto-remediation. The system handles specific Spark job errors and significantly improves efficiency by automating the correction of memory configuration errors and other unclassified errors, demonstrating a notable reduction in error rates and operational costs. \nhttps://netflixtechblog.com/evolving-from-rule-based-classifier-machine-learning-powered-auto-remediation-in-netflix-data-039d5efd115b\nThe Microsoft article outlines strategies for evaluating Large Language Model (LLM) systems, highlighting the distinction between evaluating standalone LLMs and LLM-based systems. It emphasizes the iterative nature of evaluation, integrating continuous integration, evaluation, and deployment (CI/CE/CD) within LLMOps for managing LLM applications. The piece discusses the importance of tailoring evaluation metrics to specific use cases, the challenges of creating golden datasets for accurate evaluation, and the balance between offline and online evaluations for comprehensive assessment. It also explores the use of AI in generating and evaluating datasets, advocating for Responsible AI metrics to ensure ethical use.\nhttps://medium.com/data-science-at-microsoft/evaluating-llm-systems-metrics-challenges-and-best-practices-664ac25be7e5\nYelp has revamped its data pipeline to stream huge volumes of data in real time, focusing on building robust data abstractions for offline and streaming data consumers. Using a unified stream for all relevant business property data, Yelp ensures a consistent and user-friendly format for data consumers, eliminating the need to navigate between different business attributes and features. This approach enhances data discovery, consumption, and ease of use while addressing challenges such as weak encapsulation and maintenance difficulties in the previous architecture. \nhttps://engineeringblog.yelp.com/2024/03/building-data-abstractions-with-streaming-at-yelp.html\nWhile SOC 2 might seem outside a data engineer's wheelhouse, understanding its principles is key. Data engineers build the systems that store and process sensitive information. By following SOC 2 guidelines, they can implement strong authentication, encryption, and data handling practices. The author writes an excellent three-part series that introduces SOC2 compliance. \nhttps://blog.substrate.tools/soc2-part1/\nhttps://blog.substrate.tools/soc2-part2/\nhttps://blog.substrate.tools/soc-2-compliance-for-startups-and-first-timers-part-3/\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-161", "title": "Data Engineering Weekly", "content": "RudderStack is the Warehouse Native CDP, built to help data teams deliver value across the entire data activation lifecycle, from collection to unification and activation. Visit rudderstack.com to learn more.\nWe are thankful to Ideas2IT to host our first Data Hero\u2019s meetup. There will be food, networking, and real-world talks around data engineering. Here is the agenda,\n1) Data Application Lifecycle Management - Harish Kumar( Paypal)\nHear from the team in PayPal on how they build the data product lifecycle management (DPLM) systems.\n2) Why High-Quality Data Products Beats Complexity in Building LLM Apps - Ananth Packildurai\nI will walk through the evolution of model-centric to data-centric AI and how data products and DPLM (Data Product Lifecycle Management) systems are vital for an organization's system.\n3) DataOPS at AstraZeneca\nThe AstraZeneca team talks about data ops best practices internally established and what worked and what didn\u2019t work!!!\n4) Building Data Products and why should you? by Aswin James Christy( Qlik/Talend)\nAswin establishes a case for why you should start thinking about building data products.\nSignUp Link: https://nas.io/data-heroes/data-hereos-chennai-meetup\nPinterest detailed its journey of implementing AI-assisted development, highlighting its initial caution due to legal and security concerns and its eventual decision to adopt GitHub Copilot for its seamless IDE integration and compatibility with its tooling ecosystem. Through an extensive trial involving 200 developers, with an established feedback loop, ensuring Copilot's effectiveness and security. This approach led to a successful expansion of Copilot access across the engineering team, resulting in a significant increase in productivity and adoption, demonstrating a commitment to enhancing developer experience while maintaining safety and security standards.\nhttps://medium.com/pinterest-engineering/unlocking-ai-assisted-development-safely-from-idea-to-ga-4d68679161ef\nThe article introduces GraphRAG, a Microsoft Research innovation enhancing Large Language Models' (LLMs) capabilities in analyzing narrative private data through knowledge graphs. GraphRAG significantly improves question-and-answer performance over traditional vector similarity techniques using LLM-generated knowledge graphs for document analysis. This approach enables deeper insights into complex datasets that LLMs have not been trained on, demonstrating substantial improvements in data understanding and thematic discovery. \nhttps://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/\nTo open or not to open the OpenAI is a hotly debated topic in the industry\u2014the era where people started considering AI development a national asset. The NVIDIA blog on Sovereign AI emphasizes the importance of countries developing artificial intelligence capabilities using local infrastructure, data, and workforce. The article discusses the role of \"AI factories\" and national efforts worldwide to advance sovereign AI through partnerships, investments, and infrastructure development.\nhttps://blogs.nvidia.com/blog/what-is-sovereign-ai/\n\"Just like the data team, development teams are under pressure to work quickly and efficiently to accomplish their goals. It\u2019s not like development teams purposely make life difficult for their data team counterparts. They\u2019re just doing their jobs, and their incentives are, by nature, different from yours.\"\nHere, the team at RudderStack looks at the divide between data producers and consumers. They give a clear explanation for why it exists, and they detail four principles you can follow to bridge the gap. The article concludes with a look at data contracts as a concrete example of these principles in practice.\nhttps://www.rudderstack.com/blog/data-quality-best-practices-bridging-the-dev-data-divide/\nThe Databricks blog article delves into the latest enhancements for stateful pipelines in Apache Spark Structured Streaming, focusing on improving performance through optimized memory management, database write/flush performance, and changelog checkpointing. These improvements, part of Project Lightspeed, aim to reduce latency and increase throughput for stateful streaming queries, making them more efficient and cost-effective. The article also highlights sink-specific improvements and operator-specific enhancements that contribute to the overall performance boost. \nhttps://www.databricks.com/blog/deep-dive-latest-performance-improvements-stateful-pipelines-apache-spark-structured-streaming\nThe Zendesk team shares their journey of migrating legacy data pipelines to dbt, focusing on making them more reliable, efficient, and scalable. They tackled the challenge by starting with pipelines requiring frequent updates, implementing initial lift and shift to dbt with minimal changes, and then proceeding with basic and advanced refactoring for optimization. The migration enhanced data quality, lineage visibility, performance improvements, cost reductions, and better reliability and scalability, setting a robust foundation for future expansions and onboarding.\ndbt at Zendesk \u2014 Part I: Setting Foundations for Scalability\ndbt at Zendesk \u2014 Part 2: supercharging dbt with Dynamic Stage\ndbt at Zendesk \u2014 Part III: lift-and-shift playbook\nGrab discusses its near real-time data analytics implementation in their data lake, highlighting the transition from traditional data storage to the Hudi format for efficient processing. The article explains the challenges with frequent updates in the vanilla data lake setup and how Hudi allows for faster writes and guarantees atomicity, consistency, isolation, and durability (ACID).\u00a0\nhttps://engineering.grab.com/enabling-near-realtime-data-analytics\nThe two-part series from Rippling details their journey of building a custom Security Information and Event Management (SIEM) system, emphasizing an engineering-first approach for enhanced security and operational efficiency. Part 1 covers the need for a bespoke SIEM, focusing on scalability, efficient log management, and seamless AWS integration, among other requirements. \nPart 2 focuses on its security data lakehouse and its modular design and highlights the implementation of a security data lakehouse using Snowflake, emphasizing efficient log ingestion, modularization with Terraform, and custom log pullers.\n Part 1: Why did we need to build our own SIEM?\nPart 2: Rippling's security data lakehouse and modular design\nDoorDash writes about its in-house search engine to address scalability and efficiency issues identified with Elasticsearch's previous search architecture. The new system, built on Apache Lucene, features a segment-replication model, separates indexing from searching traffic, and significantly reduces latency and hardware costs. The system design focuses on a horizontally scalable, general-purpose engine with enhanced query understanding and planning, aiming to support DoorDash's rapid growth and complex document relationships more effectively. \nhttps://doordash.engineering/2024/02/27/introducing-doordashs-in-house-search-engine/\nThe AWS Big Data Blog discusses the importance of data governance in the age of generative AI, emphasizing the need for robust data management strategies to ensure data quality, privacy, and security across structured and unstructured data sources. The article highlights how enterprises can leverage AWS services to manage data governance effectively, ensuring responsible AI use by incorporating comprehensive data governance steps in data pipelines and user interaction workflows. This approach helps maintain accuracy, relevance, and compliance in generative AI applications. \nhttps://aws.amazon.com/blogs/big-data/data-governance-in-the-age-of-generative-ai/\nThe article outlines seven tips for building a modern data team: emphasizing the importance of the team's composition and adaptability, advocating for a blend of centralized and decentralized approaches, promoting a full-stack data team concept rather than seeking full-stack engineers, valuing mindset and principles over mere certifications, ensuring the team enjoys their work, and aligning data architecture with organizational structure as per Conway's Law. It stresses the need to understand the technological and human aspects of data team dynamics, including fostering skill liquidity and focusing on first principles for problem-solving over tool-specific knowledge. \nhttps://medium.com/everestengineering/how-to-build-a-modern-data-team-seven-tips-for-success-a4d97e427d45\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-160", "title": "Data Engineering Weekly", "content": "RudderStack is the Warehouse Native customer data platform, built to help data teams deliver value across the entire data activation lifecycle, from collection to unification and activation. Visit rudderstack.com to learn more.\nLast week, we asked our readers if we should bring DEWCon to Europe. I\u2019m super grateful to see so many people participate in the poll. I\u2019m super delighted to see the result.\n88% of people voted, Yes for DEWCon!!! Regarding which city to host the DEWCon event, Berlin and Amsterdam got an almost equal vote!! Are you ready to take Europe's data engineering scene to the next level? DEWCon is coming, and we need YOUR help! Our India event was a huge success because we kept the focus on practitioners, not vendors. If you want to shape the future of data in Europe, join our core working group. Let's build this together! Fill out the form below.\nhttps://forms.gle/f8sbAgzQe5AVT9bC9\nThough the online events are amazing, nothing can match the in-person networking and learning from each other. Just after the DEWCon conference, we heard the data practitioners want similar events in their cities to increase knowledge sharing. We can\u2019t run a full-fledged data conference in each city but can do so with informal, low-key events. Please reach out if you would like to start a Data Hero chapter in your city. I would love to help set it up. As a first step, we are bringing our first low-key Data Hero\u2019s meetup in Chennai, India, on March 8th. If you\u2019re around Chennai on March 8th, please register here. \nhttps://nas.io/data-heroes/data-hereos-chennai-meetup\nThe Berkeley AI Research (BAIR) blog discusses the evolution from focusing on single large language models (LLMs) to developing compound AI systems that integrate multiple components, including LLMs, for superior performance. This shift is driven by the need for more sophisticated approaches to tackle complex tasks, as compound systems can leverage the strengths of various components to achieve better results than any single model could on its own. The article emphasizes that this trend towards compound systems opens new avenues for optimizing AI application design, promising significant improvements in AI's effectiveness and efficiency.\nhttps://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems\nThe Mozilla AI blog post discusses the NeurIPS Large Language Model Efficiency Challenge, which evaluates the efficiency of fine-tuning large language models (LLMs) on specialized data within 24 hours using a single GPU. This challenge highlighted the complexities of LLM evaluation, emphasizing the importance of community consensus on evaluation metrics and sharing evaluation artifacts as open-source. The competition fostered practical insights into model and infrastructure optimization, underscoring the evolving landscape of LLM efficiency and evaluation practices.\nhttps://blog.mozilla.ai/exploring-llm-evaluation-at-scale-with-the-neurips-large-language-model-efficiency-challenge/\nGigaOm delves into detailed evaluations of various semantic layers and metric stores in a comprehensive Sonar report. Explore this report to grasp the concept of a semantic layer, comprehend its functionality and applications, and gain insightful comparisons among diverse vendors.\nhttps://cube.dev/gigaom-rates-cube-as-a-leader-for-semantic-layers\nThis article offers a fascinating historical perspective on stream processing through the evolution of Apache Samza. The article explores various design choices and how they impacted the Apache Samza adoption in the industry. \nMy challenge with Samza during my time at Slack is the decision to co-locate Samza's state in Kafka. At that time, operating Kafka comes with its challenges. Samza\u2019s stream-stream join relies on Kafka\u2019s key partition to shift the streaming operation burden to Kafka. I saw a similar design echo in Kafka streams, and my reaction back then was this. \nhttps://materializedview.io/p/from-samza-to-flink-a-decade-of-stream\nPinterest Engineering shared its journey towards implementing AI-assisted development, focusing on balancing innovation, safety, security, and cost. They opted for GitHub Copilot, emphasizing a thorough evaluation process, cross-functional collaboration, and extensive trials to address potential risks and gather feedback. The positive outcome led to a significant increase in developer productivity and Copilot adoption, paving the way for further improvements and safe, efficient AI integration in their development processes. \nhttps://medium.com/pinterest-engineering/unlocking-ai-assisted-development-safely-from-idea-to-ga-4d68679161ef\nThe GitHub Blog explains how AI code generation, like GitHub Copilot, transforms software development by automating code creation and assisting developers across various programming languages. It outlines the process from simple autocompletion to complex code suggestions based on natural language comments and direct interaction with AI, enhancing productivity, reducing mental load, and minimizing context switching. The post emphasizes the importance of human oversight in ensuring code quality and alignment with project goals.\nhttps://github.blog/2024-02-22-how-ai-code-generation-works/\n\"Just like the data team, development teams are under pressure to work quickly and efficiently to accomplish their goals. It\u2019s not like development teams purposely make life difficult for their data team counterparts. They\u2019re just doing their jobs, and their incentives are, by nature, different from yours.\"\nHere, the team at RudderStack looks at the divide between data producers and consumers. They give a clear explanation for why it exists, and they detail four principles you can follow to bridge the gap. The article concludes with a look at data contracts as a concrete example of these principles in practice.\nhttps://www.rudderstack.com/blog/data-quality-best-practices-bridging-the-dev-data-divide/\nSwiggy's blog post reflects on a year of implementing generative AI, highlighting their journey from exploration to institutionalization. They focused on enhancing customer experiences and operational efficiency through customized AI models, addressing latency and data integrity challenges. Key application areas included catalog enrichment, review summarization, and neural search, with a strategic approach to managing risks and responsibly leveraging AI's potential.\nhttps://bytes.swiggy.com/reflecting-on-a-year-of-generative-ai-at-swiggy-a-brief-review-of-achievements-learnings-and-13a9671dc624\nPayPal's integration of Apache Spark 3 and NVIDIA GPUs has led to up to 70% cost savings in cloud expenses for big data and AI applications, processing petabytes of data across hundreds of thousands of jobs. By leveraging Spark RAPIDS, PayPal optimized data processing using GPU acceleration, overcoming latency optimization and data hallucination challenges. This initiative reduced costs and enhanced computational efficiency, demonstrating the significant benefits of GPUs in big data environments.\nhttps://medium.com/paypal-tech/leveraging-spark-3-and-nvidias-gpus-to-reduce-cloud-cost-by-up-to-70-for-big-data-pipelines-e0bc02ec4f88\nThe New York Times Games Data Team revamped its data architecture to enhance analytics and reporting capabilities following the acquisition of Wordle and a surge in user activity. They developed a new data structure with explicit aggregation layers, leading to a threefold increase in dashboard usage and more efficient data analysis processes. This overhaul improved the team's ability to generate insights, democratize data, and support the launch of new games, demonstrating the value of thoughtful data management in product development.\nhttps://open.nytimes.com/how-the-new-york-times-games-data-team-revamped-its-reporting-8af7e7c7bc97\nThe author outlines a shift towards intelligent data governance by design, contrasting with traditional post-implementation governance efforts. It presents a practical example of embedding data quality, integrity, and security controls into the data processes from the beginning, emphasizing this approach's efficiency and future-proofing benefits. Organizations can achieve scalable and cost-effective data governance by focusing on preventive measures at data capture, maintaining data quality at the source, ensuring protected data access, and making data flows discoverable by design.\nhttps://medium.com/@willemkoenders/intelligent-data-governance-by-design-a-practical-example-30f2bbf1bf91\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-159", "title": "Data Engineering Weekly", "content": "RudderStack is the Warehouse Native CDP, built to help data teams deliver value across the entire data activation lifecycle, from collection to unification and activation. Visit rudderstack.com to learn more.\nAswin and I started DEWCon as a fun experiment. Midway through organizing the conference, fear washed over us. Had we bitten off more than we could chew? Was this simply too ambitious? We have no sponsors. Our hope is only with the amazing community of data practitioners who constantly support us. \nOne thing I learned while writing Data Engineering Weekly is that persistence and consistency are the keys to success. Eventually, we got some amazing sponsors who supported us to cover the venue cost!!! As soon as we opened the registration, we got oversubscribed. We had to close the conference registration five days before because our venue partner told us they couldn't support more people. \nThe community gave us overwhelming support, and all the questions we got were: when are you going to do next year? With all the encouragement, we thought, how about taking one step further? How about bringing DEWCon to Europe, too? \nLike before, We are scared. We have no idea about running a conference in Europe. So, we want your support in understanding the landscape by sharing your insights in the poll. Please add your thoughts if you live in Europe.\nIf you have any other choice of the city or would like to share your thoughts, please add them to the comments.\nThe week is full of retrospect for the Modern Data Stack :-) The author shares a unique point of view on how the lack of collaboration among the MDS companies leads to its loss of popularity.\nModern data stack vendors chose speed, and never attempted to truly build something together. But most partnerships were temporary and transactional agreements of mutual convenience that went co-sponsored afterparty deep.\nIt is true. At the same time, in a growth-driven economy, it is simply not possible. If you don\u2019t grow, you don\u2019t exist. So, it is an eventual reality that one steps into another product. \nhttps://benn.substack.com/p/the-problem-was-the-product\nJoe writes another excellent retrospect for Modern Data Stack, walking down memory lane of the early and golden days of the Modern Data Stack. One can\u2019t deny the role of Redshift in bringing the cloud data warehouse to the masses, starting the end of the Big Data era with Hadoop.\nAll the retrospect keeps me wondering, So What is Next? We are so over the Big Data Era to Modern Data Stack. What are we stepping into? \nhttps://joereis.substack.com/p/everything-ends-my-journey-with-the\nLearn about Cube, the universal semantic layer, in an upcoming technical webinar. See how recent product updates make it easier for developers and data engineers to debug data models, work in a BI-like UI, and fine-tune deployments for better query performance.\nRegister for our webinar to explore Cube Cloud and learn about the convenient UI for easier data modeling. Elevate your data skills!\nhttps://bit.ly/playground-2-webinar\nData Ownership is the fundamental construct to build reliable data engineering practices. A recent survey from dbt shows that 43% of the respondents answered that ambiguous data ownership was their biggest challenge. The author writes about the basic construct of data ownership and how to utilize dbt to reduce friction. \nI believe the data ownership problem is much deeper than simple metadata management. Data Ownership is the fundamental construct for Data Products. In a recent poll, 75% of data practitioners say either they are implementing or are thinking of implementing Data Products. \nYet, there is no tool for the data producers to Define, Design, Collaborate,  Test, & Continuously Monitor the  Data Products. The current tools are fragmented, which further complicates the Data Producers, leads them to failure in Data Ownership. \nhttps://medium.com/@mikldd/data-ownership-a-practical-guide-ae306d49866f\nWe started to see more and more tools, especially around Python rewritten in Rust, with much better performance. uv, the Python package tool alternative to pip and poetry, is the recent addition to the trend. The author summarizes why Rust is gaining momentum in data engineering by pointing out some of the upcoming tools written in Rust. \nhttps://abhijrathod.medium.com/rust-the-rising-star-in-data-engineering-embracing-the-power-of-performance-safety-and-cb63a492cc5f\n\"Just like the data team, development teams are under pressure to work quickly and efficiently to accomplish their goals. It\u2019s not like development teams purposely make life difficult for their data team counterparts. They\u2019re just doing their jobs, and their incentives are, by nature, different from yours.\"\nHere, the team at RudderStack looks at the divide between data producers and consumers. They give a clear explanation for why it exists, and they detail four principles you can follow to bridge the gap. The article concludes with a look at data contracts as a concrete example of these principles in practice.\nhttps://www.rudderstack.com/blog/data-quality-best-practices-bridging-the-dev-data-divide/\nOne of the repeating patterns I heard in many of the data engineers is how to set up incremental data processing across the system efficiently. The author writes about how the dbt incremental model helps them reduce the BigQuery cost 100-200X!! \nhttps://blog.stackademic.com/reducing-bigquery-costs-by-100-200x-with-dbt-incremental-models-c4375b945b69\nHow do we integrate Gen-AI into the existing workflow? What is the real business impactful use case? It is a burning question for every product manager in the world now. The authors share the top 5 lessons learned in turning ideas into AI use cases. \n \nhttps://medium.com/ft-product-technology/turning-ideas-into-ai-use-cases-the-product-manager-point-of-view-f5e4aa7fe0af\nThe People You May Know feature is probably one of LinkedIn's best growth hacking techniques to build the network effect. The blog narrates how LinkedIn built and scaled a large-scale recommendation system to handle over a billion items while ensuring high relevance and low serving latency. \nhttps://www.linkedin.com/blog/engineering/recommendations/building-a-large-scale-recommendation-system-people-you-may-know\nDoorDash writes about fractional factorial designs to test multiple hypotheses simultaneously, reducing the number of necessary experiments. Fractional factorial design selects a subset of the possible combinations of factors to run as experiments. The research paper Business Policy Experiments using Fractional Factorial Designs: Consumer Retention on DoorDash is an excellent read to dive deep into.  \nhttps://doordash.engineering/2024/02/13/experiment-faster-and-with-less-effort/\nExpedia writes about its ML Platform Orchestrator, its design, and some early successes with applying this approach to accelerate ML model experimentation. The design focuses on reusability, ease, and a low-code approach. Every orchestration engine becomes efficient only if it seamlessly integrates with the existing infrastructure. Expedia shares how integrating with its experimentation platform helps productivity gains. \nhttps://medium.com/expedia-group-tech/powering-ml-platform-orchestration-and-experimentation-a0574b97af30\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-158", "title": "Data Engineering Weekly", "content": "RudderStack is the Warehouse Native CDP, built to help data teams deliver value across the entire data activation lifecycle, from collection to unification and activation. Visit rudderstack.com to learn more.\nData products represent a way of packaging and delivering data that provides specific, actionable value to users. The structured approach the data products bring to create, access, manage, and delete data assets is certainly appealing. What do you think of Data Products? How far is your organization adopting it? Please share your thoughts on the survey. \nSurvey Link: https://www.linkedin.com/posts/ananthdurai_the-fresh-view-of-data-products-is-certainly-activity-7162593157195452416-AexB\nIs Modern Data Stack still a useful idea? I agree the cloud-native is no longer a differentiator as it becomes a default. \nHowever, I disagree that MDS is no longer valid because of market shifts or AI investments. \nWith the modern data stack, we built hyper-specialized tools that resulted in high integration and cloud cost. It leads to hiring more developers and more lead time to produce value. The cloud-native data stack is still valid but with an integrated workflow to build and manage the data assets. \nhttps://roundup.getdbt.com/p/is-the-modern-data-stack-still-a\n\u201cA number alone is never the answer for a great data team. It's what we\u2019re going to do as a result of it,\u201d says Solmaz Shahalizadeh, former vice president and head of data at Shopify.\nEvery data leader is going through this dilemma: how to influence action and better integrate the data team functions to multiply the organization's efficiency. The blog is an excellent guide on when and how to build a data team, the important qualities of a data team, and the organizational structure. \nhttps://www.bvp.com/atlas/what-founders-need-to-know-to-build-a-high-performing-data-team\nThe article is a good reminder to rethink what changed from pre and post-COVID in building data platforms. One common thing is that every company produces high-volume data, historically reserved only for big Internet companies. Apart from that, do you think anything significant changed? Let me know in the comments. \nhttps://towardsdatascience.com/building-a-data-platform-in-2024-d63c736cccef\nPopular open-source tools like Airbyte and Snowplow have recently changed their licenses. Is there really a future for building open-source data tools anymore? However, a big kudos to Alireza for building the open-source data landscape.\n \nhttps://alirezasadeghi1.medium.com/open-source-data-engineering-landscape-2024-8a56d23b7fdb\nLeading data teams leverage their customer data to deliver high-impact machine learning projects like churn prediction and personalized recommendation systems to create significant competitive advantages for their companies. If you have a data quality problem, success like this can seem out of reach.\nPoor data quality leads to lackluster results and frustrated stakeholders, but fixing bad data can become an endless task that keeps you from key initiatives. To help you drive data quality at the source, RudderStack just launched a Data Quality Toolkit. It includes features for collaborative event definitions, violation management, real-time schema fixes, and monitoring and alerting. With the toolkit, you can spend less time wrangling and more time helping your business drive revenue.\nhttps://www.rudderstack.com/blog/announcing-the-data-quality-toolkit-guarantee-quality-data-from-the-source/\nWhat is an efficient abstraction for data programming? It is a long-debated topic, and SQL is one of the hard-criticized languages. [See: 10 Things I hate about SQL]. For a change, this time, it is the data frame. The author points out three key areas where the data frame is lacking.\nLeaky Abstraction: Dataframes don't inherently express the structure, constraints, and business logic embedded in the data. This knowledge is implicit and known only to the developer or data expert.\nLack of Type Safety: Since dataframes are dynamically typed, most errors surface only at runtime. The lack of type safety contrasts with strongly typed languages, where you get more safeguards at compile time.\nDifficulty in Software Maintenance: Each step in a data pipeline operates on a black box (the dataframe). Understanding modifications requires knowing the entire history of transformations and implicit assumptions.\nhttps://medium.com/@cautaerts/a-dataframe-is-a-bad-abstraction-8b2d84fa373f\nShould we promote events as a building block for data modeling? The author thinks so, claiming the traditional data models designed for order and marketing data don't easily scale to handle behavioral analysis needs. The layered approach from raw event collection, refined to qualifying events to make it accessible for insights, is an excellent read. \nhttps://substack.timodechau.com/p/eventify-everything-data-modeling\nIt is one of my best reads this week, and I am super excited about the idea of a foundation model for time-series forecasting.  Most Deep Learning architectures require long and involved training and validation cycles before a customer can test the model on a new time series. In contrast, a foundation model for time-series forecasting can provide decent out-of-the-box forecasts on unseen data without additional training, enabling users to focus on business use cases.\nhttps://blog.research.google/2024/02/a-decoder-only-foundation-model-for.html\nThe author discusses building a data framework called the Common Computation Framework (CCF). The blog narrates the challenges of managing data in silos and the need for a centralized solution. The CCF uses DBT, a data modeling tool, to create standardized metrics. The standardization allows for better data discoverability, ownership, and quality. \nhttps://medium.com/uc-engineering/from-silos-to-standardization-leveraging-dbt-for-a-democratized-data-framework-f444dcd07cd9\nAn excellent read from the Back Market data team about various approaches to accelerating data access's democratization. The blog narrates the traditional approach from SQL Training to Data Catalog to simplify the tooling with low code data visualization tooling & data champions program. \n \nhttps://engineering.backmarket.com/back-markets-journey-towards-data-self-service-89b278d6617a\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u201d opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-157", "title": "Data Engineering Weekly", "content": "RudderStack is the Warehouse Native CDP, built to help data teams deliver value across the entire data activation lifecycle, from collection to unification and activation. Visit rudderstack.com to learn more.\nJoe raised a very fundamental question in data engineering. What is Data Modeling, and what is not? He rightly points out that many data engineers will name any of the modeling techniques as data modeling. They are indeed the core part of data modeling, yet it is still a technique rather than a definition. \nJoe went on to define the data modeling as follows: \nA data model is a structured representation that organizes and standardizes data to enable and guide human and machine behavior, inform decision-making, and facilitate actions. \nThe definition indeed elevates the purpose of the data modeling techniques. If I were to define data modeling at a very high level, my definition of data modeling is;\nEvery business, in a way, is a state machine. The user journey, sales process, marketing campaign, everything falls under a state machine. Data modeling is a collaborative process across business units to capture state changes in business activity.\nThe state-machine analogy is vital; your design approach and usage will significantly improve once you get that perspective. \nhttps://practicaldatamodeling.substack.com/p/what-data-modeling-is-not\nhttps://practicaldatamodeling.substack.com/p/my-definition-of-data-modeling-for\nGrab writes an excellent blog about data exploration on stream processing. The solution centered around Notebook opens a Flink Session for the Kafka stream and continues the exploration. \nIt opens some old memory; try to solve this problem first with Presto-Kafka connector and then using OLAP engines like Druid & Apache Pinot. The challenges of connecting Kafka, obviously with more ad-hoc consumers, saturate the network bandwidth (though Kafka improved in this way to allow consumption from the follower\u2019s node, the problem persists). The challenge with the OLAP engine is that it is an additional system to maintain, and indexing everything is too costly. \nI do believe there is an open-ended deep tech problem to support exploratory analytics in stream processing. \nhttps://engineering.grab.com/rethinking-streaming-processing-data-exploration\nSQL standards are like toothbrushes: everyone agrees they're essential, but no one wants to use someone else's.\nThe blog is an excellent reminder no matter how standardized your tools are, the migration from one DB to another is always a nightmare. \nhttps://medium.com/datamindedbe/7-lessons-learned-migrating-dbt-code-from-snowflake-to-trino-42fc907f0202\nCond\u00e9 Nast writes its transformation journey adopting Databricks & LakeHouse architecture, moving away from Presto and Data Lake. It is noteworthy to see the reasoning of Databricks over Snowflake with the data science workload.  \nhttps://medium.com/@bxh_io/our-transformation-journey-toward-an-open-data-platform-b6f869b6a173\nLeading data teams leverage their customer data to deliver high-impact machine learning projects like churn prediction and personalized recommendation systems to create significant competitive advantages for their companies. If you have a data quality problem, success like this can seem out of reach.\nPoor data quality leads to lackluster results and frustrated stakeholders, but fixing bad data can become an endless task that keeps you from key initiatives. To help you drive data quality at the source, RudderStack just launched a Data Quality Toolkit. It includes features for collaborative event definitions, violation management, real-time schema fixes, and monitoring and alerting. With the toolkit, you can spend less time wrangling and more time helping your business drive revenue.\nhttps://www.rudderstack.com/blog/announcing-the-data-quality-toolkit-guarantee-quality-data-from-the-source/\nRapido, on the other hand, takes a journey to improving the efficiency of the Trino query engine by adopting the Trino query adoption process. It also reminds me there is no modern alternative to Secor. \nPlease comment in the thread if you\u2019re using any alternative for Secor in your pipeline. \nhttps://medium.com/rapido-labs/data-platform-rapido-part-i-cheap-efficient-and-scalable-analytics-52662111b2d2\nHow you measure a business activity will differ depending on whom you\u2019re asking and which business unit you are asking. The worst part is that the same question may result in a completely different answer to the same person at a different time. Klaviyo writes about one of the experiences and the process they took to standardize the reporting and metrics definitions.  \nhttps://klaviyo.tech/data-dictionary-how-i-learned-to-stop-worrying-and-love-reporting-standardization-2c756a226549\nSnowpipe Streaming: A feature created by Snowflake to support streaming writes to underlying Snowflake tables. The author takes the Snowpipe Java streaming SDK and deep-dives into various stages of how it works internally. It's an excellent read if you're a snowflake user.\nhttps://blog.yuvalitzchakov.com/snowpipe-streaming-deep-dive/\nA slow build is a bottleneck in disguise, turning the CI/CD pipeline from a speedway into a scenic route. The goal is to deliver, not detour.\nMeta writes about its technique to speed the machine learning iteration with faster application build and package. The build process uses the Buck2 build engine with remote execution API to prevent the non-deterministic nature of tooling and build rules.  \nhttps://engineering.fb.com/2024/01/29/ml-applications/improving-machine-learning-iteration-speed-with-faster-application-build-and-packaging/\nSimple tooling can solve a complicated process; I found the query to analyze cost is simple but useful for many cost optimization techniques.\nHow are you analyzing the cost of your infrastructure? \nhttps://medium.com/@shwetastha1/decoding-bigquery-expenses-the-ultimate-query-for-analyzing-your-analysis-costs-2e163bb28538\nGoing back to the basics, I found the blog is a simplified explanation of PostgreSQL indexes. The blog talks about the index properties and various indexes available in Postgres.\nhttps://medium.com/@aroragarima/summarizing-postgresql-indexes-53ae5ca3e6f8\nSpeaking of Postgres, one can\u2019t deny its impact in the industry and with the storage engines. We started to see many \u201cServerless Postgres as Service\u201d companies. The blog is an excellent summarization of the infamous Looking Back at Postgres paper. \nhttps://muratbuffalo.blogspot.com/2024/01/looking-back-at-postgres.html\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employer\u2019\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-156", "title": "Data Engineering Weekly", "content": "RudderStack is the Warehouse Native CDP, built to help data teams deliver value across the entire data activation lifecycle, from collection to unification and activation. Visit rudderstack.com to learn more.\nRunning the DEWCon conference with Aswin last October in Bengaluru, India, was a thrilling experience. I had a chance to meet some of the amazing humans of data engineering. I did very little to run the conference; all our volunteers and the participants made the event a huge success. Thanks to all our sponsors, OneHouse, Informatica, E6Data, and Skyflow. \nThe unanimous question we got at the evening party is: when are you going to do next year!!! The year is already here, so stay tuned for the announcements.\nhttps://www.youtube.com/playlist?list=PLPR7LiWyYxc-hZHk-KA0VNejJ_3JbrOTA\nLeboncoin discusses the evolution in data engineering, from a simple shell script to adopting data mesh principles, due to the company's significant growth. The article narrates various stages, including business intelligence, data platform, event-bus backbone, and machine learning, leading to decentralizing data engineering expertise and code. The company's journey reflects the evolving data engineering practices to meet business growth instead of a top-down approach. \nhttps://medium.com/leboncoin-tech-blog/from-a-hack-to-a-data-mesh-approach-the-18-year-evolution-of-data-engineering-at-leboncoin-b234fc05f091\nSnap writes about its Airflow infrastructure evolution by combining multiple isolated instances into a multi-tenant system with RBAC enablement. AIP-46 significantly improved runtime isolation for the task and DAG parsing, which is a key factor in enabling consolidation. \nhttps://medium.com/apache-airflow/airflow-evolution-at-snap-c988cdd95abd\nThe change data capture (CDC) is an integral pattern of event-driven architecture that brings its own set of challenges to implementation. The Macquarie team writes about implementing CDC by comparing Solace vs. Kafka as a possible connector along with Debezium. TIL about Solace and its support for hierarchical queue support!!!\nhttps://medium.com/macquarie-engineering-blog/real-time-data-processing-using-change-data-capture-and-event-driven-architecture-006cf30cc449\nS3 has another important characteristic. For the standard storage class, downloading data from S3 is free - it only incurs standard data transfer charges if you\u2019re downloading it between regions or the public Internet. Moreover, uploading to S3 - in any storage class - is also free!\nThe author narrates how one can think of using this property to slash data transfer costs in AWS. While this won't work for latency-sensitive cases, it's a great option for cost-conscious data transfers. \nhttps://www.bitsand.cloud/posts/slashing-data-transfer-costs/\nLeading data teams leverage their customer data to deliver high-impact machine learning projects like churn prediction and personalized recommendation systems to create significant competitive advantages for their companies. If you have a data quality problem, success like this can seem out of reach.\nPoor data quality leads to lackluster results and frustrated stakeholders, but fixing bad data can become an endless task that keeps you from key initiatives. To help you drive data quality at the source, RudderStack just launched a Data Quality Toolkit. It includes features for collaborative event definitions, violation management, real-time schema fixes, and monitoring and alerting. With the toolkit, you can spend less time wrangling and more time helping your business drive revenue.\nhttps://www.rudderstack.com/blog/announcing-the-data-quality-toolkit-guarantee-quality-data-from-the-source/\nResponsible AI is of utmost importance as it requires the consideration of societal values, moral and ethical aspects, fairness, privacy, transparency, and accountability in the development and use of AI systems. Microsoft writes about RAI (Responsible AI) tools in both open source and as part of the Azure platform. \nhttps://medium.com/data-science-at-microsoft/responsible-ai-in-action-part-3-tools-to-help-969e45cac11b\nReading about how data processing is mapped with the business use cases is always delightful. The combination of near real-time and bulk data processing is common in many business process requirements. I hope we will see more frameworks combine these two patterns, aka the expansion of workflow manager for both real-time and batch jobs. \nhttps://www.linkedin.com/blog/engineering/data-streaming-processing/improving-recruiting-efficiency-with-hybrid-bulk-data-processing-framework\nIt is strictly not product marketing for Atlassian features :-) LLM greatly impacts how we build and use the software. However, incorporating LLM into an existing user experience and the product features is a bigger challenge for many organizations. It is exciting to see Atlassian sharing such product features utilizing LLM.\nhttps://www.atlassian.com/engineering/the-future-of-automation-at-atlassian-generating-confluence-automation-rules-with-large-language-models\nThe salary benchmarks always bring interesting insights, and the benchmark in France for the data engineering salary is particularly interesting as prime Large Language Models have emerged in France recently.  Let me know in the comments what you think of this benchmark. \nhttps://moderndatanetwork.medium.com/how-much-data-professionals-make-in-france-the-mdn-annual-benchmark-0f77f706b79c\nI recently stumbled upon the DataFlint tool and found it very exciting.  One of my constant complaints about Apache Spark is needing someone to examine the Spark history server to fine-tune the performance. It is exciting to see open-source APM tools like DataFlint. \nhttps://medium.com/@menishmueli/fixing-small-files-performance-issues-in-apache-spark-using-dataflint-49ffe3eb755f\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-155", "title": "Data Engineering Weekly", "content": "RudderStack is the Warehouse Native CDP, built to help data teams deliver value across the entire data activation lifecycle, from collection to unification and activation. Visit rudderstack.com to learn more.\nIt is an eventful 2023 in machine learning and AI research. The author summarizes the top 10 AI-specific research papers in 2023. \nhttps://magazine.sebastianraschka.com/p/10-ai-research-papers-2023\nInteresting article on the impact of search engine optimization (SEO) on the quality of search engine results. The article claims that modern search engines are significantly affected by SEO strategies, with search results being biased towards those who can profit the most from specific terms. \nhttps://danluu.com/seo-spam/\nOrganizing the data team to maximize data-driven organization is an evergreen industry issue. TBH, centralized or decentralized depends on who has more influence in an organization. The author summarized the pros and cons of various data team structures in an organization.\nhttps://medium.com/coriers/centralized-vs-decentralized-vs-federated-data-teams-05dc14e8338d\nDatadog shared insights at the Crunch Conference on scaling self-serve analytics for Datadog's 5,000 employees. The approach revolves around empowering all team members to make data-informed decisions independently, leveraging a suite of tools built on open-source technologies. This strategy involves data intake, transformation, quality, discovery, and reporting, supporting various user needs and promoting a data-driven culture within the organization.\nhttps://www.datadoghq.com/blog/engineering/crunchconf-talk-self-serve-analytics/\nOur warehouse-native approach ensures Predictions are fully auditable, so you can thoroughly audit models and fit metrics, runs, and outputs.\nRudderStack just announced a new ML product called Predictions that can use any data in your warehouse to automatically produce churn and conversion scores without MLOps. It\u2019s built for data practitioners, so models are transparent, and it gives you control to tweak and tune. For advanced use cases, you can even migrate to a version-controlled, code-based workflow to create custom predictive features. A thorough quickstart guide, created in partnership with Snowflake, is available, complete with a sample dataset so you can test-drive the tool.\nhttps://www.rudderstack.com/blog/announcing-rudderstack-predictions-automate-churn-and-conversion-scores-in-your-warehouse/\nGrab Engineering writes about enhancing its Kafka on the Kubernetes platform for improved fault tolerance. The team redesign addresses worker node terminations without manual intervention, ensuring seamless data streaming. The solution employs AWS services and dynamic configurations to maintain uninterrupted operations, resulting in robust and efficient data handling technologies.\nhttps://engineering.grab.com/kafka-on-kubernetes\nMyntra, the Indian fashion e-commerce giant, writes about its data-serving layer, Bifrost. The data serving layer focuses on deep personalization through a rich data platform. Bifrost empowers Myntra to integrate, manage, and utilize data, enhancing the customer experience through targeted personalization and streamlined analytics processes.\nhttps://medium.com/myntra-engineering/bifrost-data-serving-layer-at-myntra-b75e35e1ff7c\nTeads' blog post discusses unit testing with dbt, highlighting its advantages and limitations, especially regarding macros. The author engages in an interesting discussion on these shortfalls and acknowledges recent developments within the dbt community. This perspective offers valuable insights into the evolving landscape of dbt tests.\nhttps://medium.com/teads-engineering/unit-testing-with-dbt-fb84f2ef7dd6\nPinterest addresses the challenge of discrepancies between online and offline metrics in its ads ranking system. The article explores scenarios where offline model improvements don't always align with online performance gains. The blog discusses hypotheses for this mismatch, including misalignment between offline evaluation metrics and online business metrics and complexities in their large-scale machine learning systems. The approach involves rigorous analysis and hypothesis testing to understand and mitigate these discrepancies, providing valuable insights into the intricate workings of Pinterest's ad ranking algorithms.\nhttps://medium.com/pinterest-engineering/handling-online-offline-discrepancy-in-pinterest-ads-ranking-system-8fd662da4c2d\nWalmart writes about enhancing its Experimentation Platform, Expo, by introducing self-serve custom metrics (UCM) to analyze the impact of site changes, like its Homepage redesign. This new feature in Expo allows users to create and link custom metrics to experiments, providing deeper insights into customer interactions and the effectiveness of changes. \nhttps://medium.com/walmartglobaltech/empowering-deeper-a-b-test-insights-at-walmart-through-self-serve-custom-metrics-for-709cd947e7a8\nLanceDB writes about its integration with Polars, a high-performance DataFrame library. This collaboration enhances data analysis and manipulation capabilities, leveraging Polars' efficiency and speed within the LanceDB environment. \nhttps://blog.lancedb.com/lancedb-polars-2d5eb32a8aa3\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-trends-with-aswin", "title": "Data Engineering Weekly", "content": "Welcome to another insightful edition of Data Engineering Weekly. As we approach the end of 2023, it's an opportune time to reflect on the key trends and developments that have shaped the field of data engineering this year. In this article, we'll summarize the crucial points from a recent podcast featuring Ananth and Ashwin, two prominent voices in the data engineering community.\nA significant part of our discussion revolved around the maturity model in data engineering. Organizations must recognize their current position in the data maturity spectrum to make informed decisions about adopting new technologies. This approach ensures that adopting new tools and practices aligns with the organization's readiness and specific needs.\n2023 witnessed a substantial impact of AI and large language models in data engineering. These technologies are increasingly automating processes like ETL, improving data quality management, and evolving the landscape of data tools. Integrating AI into data workflows is not just a trend but a paradigm shift, making data processes more efficient and intelligent.\nLakehouse architectures have been at the forefront of data engineering discussions this year. The key focus has been interoperability among different data lake formats and the seamless integration of structured and unstructured data. This evolution marks a significant step towards more flexible and powerful data management systems.\nThe modern data stack (MDS) has been a hot topic, with debates around its sustainability and effectiveness. While MDS has driven hyper-specialization in product categories, challenges in integration and overlapping tool categories have raised questions about its long-term viability. The future of MDS remains a subject of keen interest as we move into 2024.\nCost optimization has emerged as a priority in data engineering projects. With the shift to cloud services, managing costs effectively while maintaining performance has become a critical concern. This trend underscores the need for efficient architectures that balance performance with cost-effectiveness.\nStreaming architectures have gained significant traction, with Apache Flink leading the way. Its growing adoption highlights the industry's shift towards real-time data processing and analytics. The support and innovation around Apache Flink suggest a continued focus on streaming architectures in the coming year.\nAs we look towards 2024, there's a sense of excitement about the potential changes in fundamental layers like S3 Express and the broader impact of large language models. The anticipation is for more intelligent data platforms that effectively combine AI capabilities with human expertise, driving innovation and efficiency in data engineering.\nIn conclusion, 2023 has been a year of significant developments and shifts in data engineering. As we move into 2024, we will likely focus on refining these trends and exploring new frontiers in AI, lake house architectures, and streaming technologies. Stay tuned for more updates and insights in the next editions of Data Engineering Weekly. Happy holidays, and here's to a groundbreaking 2024 in data engineering!"}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-154", "title": "Data Engineering Weekly", "content": "RudderStack is the Warehouse Native CDP, built to help data teams deliver value across the entire data activation lifecycle, from collection to unification and activation. Visit rudderstack.com to learn more.\nSanjeev & Rajesh, as usual, share their excellent observations about data & AI industry trends. I love the rising, stable, and declining format for categorizing data engineering trends. My take on the rising trends\n\ud83d\udfe1 Intelligent Data Platform: Yes, I fully agree. We barely starting to scratch the surface of its possibility. However, I\u2019m less optimistic about the \u201cmulti-engine\u201d orchestrator part. \n\ud83d\udd34AI Agents: This is not possible until the data tools figure out a way to integrate into their workflow to use AI Agent instead of relying on users to enter the prompts. No 1 rule of the product experience is \u201cDon\u2019t make the user think\u201d; for me, \u201cprompting\u201d makes me think a lot.\n\ud83d\udfe2 Personalized AI Stack: Yes, I fully agree \n\ud83d\udfe2 AI Governance: Yes, I fully agree \nOn the Declining side, my thoughts are shared in a simple LinkedIn post here.\nhttps://sanjmo.medium.com/unveiling-the-crystal-ball-2024-data-and-ai-trends-74164da31cf8\nDoorDash writes about its architecture and policy to protect user privacy at scale. The technique for geomasking is an excellent read. \nhttps://doordash.engineering/2023/11/14/privacy-engineering-at-doordash-drive/\nrollout is much more than turning on/off a feature flag. Your rollout can make/break the core experience of your product without showing much visual change. The author explores why data PMs have to be more cautious in planning and executing a rollout.\nhttps://thedataproductmanager.substack.com/p/rollout-roulette-why-should-data\nIs it centralized or Distributed? Which data team org structure works very best for a company? I believe there is no one answer to it; the author explores the various organization model and their adoption of Mercado Libre.\n \nhttps://medium.com/mercadolibre-tech/how-do-we-structure-a-data-team-here-at-mercado-libre-e7533f78cfb8\nThe integration supports warehouse-based diffing, making it the most performant Reverse ETL solution for Trino.\nRudderStack just launched Trino as a Reverse ETL source. It's the only Trino Reverse ETL solution that supports warehouse-based CDC. With Rudderstack and Trino, you can also create custom SQL queries for building data models \u2014 execute these using Trino via RudderStack and seamlessly deliver your data to your downstream business tools. Read the announcement for more details.\nhttps://www.rudderstack.com/blog/feature-launch-trino-reverse-etl-source/\nGoogle writes about exciting advancements in ML for ML. The blog explores how Google uses ML to improve the efficiency of ML workloads! \nhttps://blog.research.google/2023/12/advancements-in-machine-learning-for.html\nMeta writes about HawkEye, a toolkit for monitoring and debugging machine learning workflows, enhancing the efficiency of resolving production issues. It features advanced algorithms for isolating model-related problems, diagnosing prediction anomalies, and identifying training data issues. The tool aims to streamline debugging processes and is evolving to support broader community applications.\nhttps://engineering.fb.com/2023/12/19/data-infrastructure/hawkeye-ai-debugging-meta/\nInstacart Ads writes about its Unified Browse pCTR model using Deep Learning to improve ad relevance and performance across browsing surfaces. The model replaced multiple legacy XGBoost models, addressing limitations like disparate training datasets and maintenance complexity. The unified model, leveraging high-cardinality features and deep learning frameworks, significantly improved metrics like AUC-PR and AUC-ROC, enhancing user profiling and ad performance. \nhttps://tech.instacart.com/one-model-to-serve-them-all-0eb6bf60b00d\nLinkedIn operates one of the largest Apache Hadoop clusters, facing challenges in deploying code changes due to its scale. LinkedIn developed a Rolling Upgrade (RU) framework, enabling smooth deployments across over 55,000 hosts and 20 clusters with minimal downtime. This framework automates and monitors deployments, significantly reducing manual intervention and achieving over 99% success rates in upgrades, enhancing the reliability and efficiency of their big data infrastructure.\nhttps://engineering.linkedin.com/blog/2023/deployment-of-exabyte-backed-big-data-components\nExpedia writes about its journey to build streaming high-volume data over WebSockets. Using a single WebSocket handler, Expedia struggled with latency and scalability when handling large Kafka topics. To address this, Expedia developed a more efficient system separating WebSocket session handling from data filtering. Expedia writes about how it utilized Kafka for data distribution, Postgres for event-driven cache updates, and Kubernetes for scalable deployment. \nhttps://medium.com/expedia-group-tech/explore-near-real-time-streaming-data-using-a-web-ui-da116079a74a\nShould we structure the pull requests? Hell yes. How can we build the same for dbt projects?\nThe author explores what would be a perfect PR template for a dbt project.\nhttps://medium.com/inthepipeline/the-anatomy-of-a-perfect-pull-request-comment-for-dbt-data-projects-433b23e1cb31\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-153", "title": "Data Engineering Weekly", "content": "RudderStack is the Warehouse Native CDP, built to help data teams deliver value across the entire data activation lifecycle, from collection to unification and activation. Visit rudderstack.com to learn more.\nThe post is not directly related to Data Engineering but system operations in general. I included this post because I often see high-pitched LinkedIn posts stating it is the human fault, especially around data quality issues. \nHow about we play this game: whenever any LinkedIn post blames humans for data quality issues, replace \u201chuman error\u201d and just read it as \u201cthey don\u2019t understand how the system worked.\u201d \nhttps://surfingcomplexity.blog/2023/12/10/human-error-means-they-dont-understand-how-the-system-worked/\nNetflix publishes the tech talk videos of their internal data summit. It is great to see an internal tech talk with a series focus on data engineering. My highlight is the talk about the data processing pattern around incremental data pipelines. Databrick\u2019s autoloader works in a similar way using RocksDB. Are there any mainstream orchestration engines supporting these patterns out of the box? \nhttps://netflixtechblog.com/our-first-netflix-data-engineering-summit-f326b0589102\nInstacart writes an article exploring prompt techniques used for internal productivity tooling. It is one of the structured articles I came across about prompt techniques that discusses Chain of Thoughts, ReAct, and advanced prompting techniques to boost productivity.  \nhttps://tech.instacart.com/monte-carlo-puppetry-and-laughter-the-unexpected-joys-of-prompt-engineering-4b9272e0c4eb\nLinkedIn writes about its efforts to balance the utility of post analytics with the privacy of its members. The article discusses the design of PEDAL (Privacy Enhanced Data Analytics Layer), a mid-tier service between applications and backend services like Pinot, to implement differential privacy, including differentially private algorithms, a metadata store, and a privacy loss tracker.\nhttps://engineering.linkedin.com/blog/2023/privacy-preserving-single-post-analytics\nThe integration supports warehouse-based diffing, making it the most performant Reverse ETL solution for Trino.\nRudderStack just launched Trino as a Reverse ETL source. It's the only Trino Reverse ETL solution that supports warehouse-based CDC. With Rudderstack and Trino, you can also create custom SQL queries for building data models \u2014 execute these using Trino via RudderStack and seamlessly deliver your data to your downstream business tools. Read the announcement for more details.\nhttps://www.rudderstack.com/blog/feature-launch-trino-reverse-etl-source/\nSquareUp writes a practical guide to training a GenAI model using GPT2, emphasizing its advantages over larger models like GPT3.5 for certain applications. GPT2 is highlighted for its open-source nature, smaller size, and ability to run on mobile devices, including offline operation. The article covers the essentials of Seq2Seq models, training processes, and considerations for efficient training, such as GPU VRAM, GPU Compute, Max Length, Batch Size, and Number of Records. \nhttps://developer.squareup.com/blog/how-to-train-your-own-genai-model/\nThe ultimate goal of any data platform is to allow data scientists to declare their features rather than explicitly specify how to construct them on top of different execution platforms. PayPal talks about declarative feature engineering and how it reduces hidden tech debt and decreases the total cost of ownership. \n\nhttps://medium.com/paypal-tech/declarative-feature-engineering-at-paypal-eddcae81c06d\nPicnic writes about how it leverages advanced machine learning models, including Temporal Fusion Transformers and Tide, for demand forecasting to minimize food waste and meet customer demand efficiently. It is great to see the article emphasizes the importance of maintaining data quality and regularly retraining models to address data drift.\nhttps://blog.picnic.nl/running-demand-forecasting-machine-learning-models-at-scale-bd058c9d4aa7\nVimeo writes about its migration journey from HBase+ Phoenix infrastructure to ClickHouse. The article discusses the growing video analytical issues, scalability challenges with HBase, and ClickHouse integration with Apache Spark. \nhttps://medium.com/vimeo-engineering-blog/clickhouse-is-in-the-house-413862c8ac28\nAdyan writes about its journey and challenges to achieve reliability at scale for operating Apache Airflow. The article focuses on four key areas.\n Setting up a scalable multi-tenant Airflow setup\n Handling machine failures\n Handling task priority\n Maximizing user productivity\n\nhttps://medium.com/apache-airflow/apache-airflow-at-adyen-our-journey-and-challenges-to-achieve-reliability-at-scale-c5535a7061bf\nMixpanel writes about the challenges in marketing architecture with multi-level sync from different apps, the delays with reverse ETL tooling, and the need for real-time architecture for marketing ops. The article discusses the available options to bring near-real-time infrastructure to marketing ops, such as\nBuild classic real-time infrastructure with stream processing frameworks\nUse low-code/ no-code integration tools\nConnect all internal SaaS tools with their native integrations.\nhttps://engineering.mixpanel.com/how-mixpanel-built-a-fast-lane-for-our-modern-data-stack-680701736f8c\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-152", "title": "Data Engineering Weekly", "content": "RudderStack, one of the leading alternatives to Segment, is the Warehouse Native CDP, built to help data teams deliver value across the entire data activation lifecycle, from collection to unification and activation. Visit\u00a0rudderstack.com\u00a0to learn more.\nOne of the hotly debated and many companies struggling with is to build an agile data strategy to drive business value. There is no shortage of data, but data management is still hard, which requires skillful execution and alignment across the org. Capital One shares its view as part of a case study from Harvard Business Review. \nhttps://www.capitalone.com/tech/cloud/data-transformation-emerging-technologies/\nDuring my stay in India, I had multiple conversations with fintech data practitioners in India, and many pointed out the challenges with data sharing while preserving privacy & security. Although a range of privacy techniques exist, no \u2018one size fits all\u2019 solution exists. Thoughtworks discusses Anonymesh architecture with case studies on how it is implemented.\nhttps://www.thoughtworks.com/insights/articles/anonymesh-data-sharing-meets-privacy-and-security\nAn amazing compilation of ML system design articles from various companies. I can\u2019t wait for someone to build a bot around it!!!\nhttps://www.evidentlyai.com/ml-system-design\nThe paper \u201cChatGPT\u2019s First Anniversary: Are Open-Source Large Language Models Catching up?\u201d provides an extensive overview of the advancements in open-source Large Language Models (LLMs) compared to ChatGPT. It discusses the rapid progress in open-source LLMs, noting that they are beginning to match or surpass ChatGPT in various domains. The study includes a comprehensive analysis of different LLMs across multiple benchmarks and tasks, highlighting areas where open-source models excel or lag. Additionally, it covers the development trends, best practices in training open-source LLMs, potential issues, and the implications for research and business sectors.\nhttps://arxiv.org/pdf/2311.16989.pdf\nWhen data science teams get bogged down with data quality issues, pressure to show some kind of value increases, so they begin to prioritize projects based on data availability instead of impact and business need.\nLackluster AI/ML results often stem from poor data quality. Here, the team at RudderStack unpacks the problem, shares best practices for solving it at the source, and details how superior data quality enables data science teams to do their best work.\nhttps://www.rudderstack.com/blog/your-aiml-success-starts-with-data-quality/\nDevelopers are increasingly adopting AI to increase productivity, and GitHub Copilor plays a crucial role. GitHub writes about its experimentation strategy with LLM to evolve GitHub Copilot. The key principles are,\nPredictable\nTolerable\nSteerable\nVerifiable\nhttps://github.blog/2023-12-06-how-were-experimenting-with-llms-to-evolve-github-copilot/\nWhen I asked why Spark had no viable alternative, folks pointed me to Ray and Dask. \nThe blog is an excellent comparison study of Ray vs. Dask\u2019s performance. Note that the comparison study is on a specific set of features. \nhttps://emergentmethods.medium.com/ray-vs-dask-lessons-learned-serving-240k-models-per-day-in-real-time-7863c8968a1f\nThe author discusses Low-rank adaptation (LoRA), an efficient method for finetuning large language models (LLMs) with limited computing. Experiments on a 7B parameter LLM showed LoRA enables fast finetuning on 1 GPU with surprising consistency. Tuning hyperparameters like rank and dataset diversity is key. Quantized LoRA trades some performance for 33% memory savings.\nhttps://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms\nOne of the 2024 predictions is that \u201cThe Great Battle of LakeHouses\u201d will amplify greatly in 2024. As a data practitioner, I feel OneTable is one of the critical projects in the LakeHouse era. The OneTable, as promised in this article, enables seamless interoperability among Hudi, Iceberg, and Delta Lake. From a data architecture point of view, this enables a lot of flexibility in integrating multiple systems. The author discusses the OneTable sync mechanism among all three major LakeHouse formats in this blog.\nhttps://dipankar-tnt.medium.com/onetable-interoperability-for-apache-hudi-iceberg-delta-lake-bb8b27dd288d\nNetflix wrote a deep-dive article about Psyberg\u2019s incremental data processing pipeline framework. The blog discusses Psyberg\u2019s two operational models, stateless & stateful data processing.\nKey aspects of Psyberg:\nThe initialization phase computes a range of data that needs reprocessing for a pipeline based on parameters. Stores metadata to utilize later.\nWrite Audit Publish (WAP) process validates writes before publishing. Handles appending or overwriting with late data automatically based on stateless or stateful mode.\nAudits using Psyberg metadata ensure data integrity and completeness.\nA high watermark update marks the latest data timestamp, so the next run processes only new changes. \nhttps://netflixtechblog.com/2-diving-deeper-into-psyberg-stateless-vs-stateful-data-processing-1d273b3aaefb\nAgility and Experimentation are the key design principles while designing ML infrastructure. Agoda shares the same value and discusses how it builds Featureflow, a robust machine-learning pipeline designed to achieve the new S-curve of experimentation velocity\nhttps://medium.com/agoda-engineering/featureflow-democratizing-ml-for-agoda-aec7a6c45b30\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-151", "title": "Data Engineering Weekly", "content": "RudderStack, one of the leading alternatives to Segment, is the Warehouse Native CDP, built to help data teams deliver value across the entire data activation lifecycle, from collection to unification and activation. Visit\u00a0rudderstack.com\u00a0to learn more.\nLLM is slowly changing the application architecture landscape as it becomes integral to app development. Github writes an excellent blog to capture the current state of the LLM integration architecture. \nhttps://github.blog/2023-10-30-the-architecture-of-todays-llm-applications/\nUnderstanding Gen-AI becomes a mandatory skill for application developers and data engineers. I found this GitHub tutorial from Microsoft to be an excellent resource to get started with Gen-AI if you\u2019re beginning your journey to understand the landscape.\nhttps://github.com/microsoft/generative-ai-for-beginners\n If it can be measured, it can be improved\nAnother excellent post from Airbnb discusses measuring the data quality and suggestions to improve it. In a typical Carrot & stick approach, a thoughtful system design with an incentive to improve goes a long way over the stick approach, as noted by the author.\nWe made the decision that we could no longer rely on enforcement to scale data quality at Airbnb, and we instead needed to rely on the incentivization of both the data producer and consumer.\nhttps://medium.com/airbnb-engineering/data-quality-score-the-next-chapter-of-data-quality-at-airbnb-851dccda19c3\nNetflix writes about its incremental processing design with its orchestration engine Maestro on top of Iceberg. The blog is an excellent read to understand late-arriving data, backfilling, and incremental processing complications.  \nhttps://netflixtechblog.com/incremental-processing-using-netflix-maestro-and-apache-iceberg-b8ba072ddeeb\nWhen data science teams get bogged down with data quality issues, pressure to show some kind of value increases, so they begin to prioritize projects based on data availability instead of impact and business need.\nLackluster AI/ML results often stem from poor data quality. Here, the team at RudderStack unpacks the problem, shares best practices for solving it at the source, and details how superior data quality enables data science teams to do their best work.\nhttps://www.rudderstack.com/blog/your-aiml-success-starts-with-data-quality/\nEverything breaks at scale, and that is a fine position to be in. \nWhatnot shares such a case study on how the growth of dbt projects leads to longer build time, unstable builds, and bloating of unused models. I like the 3G model with Guardrails, Guidelines & Gadget, which I\u2019m sure I will use more often :-). The solution is as simple but highly effective as adopting incremental data processing and applying ownership and lining style conventions. \nhttps://medium.com/whatnot-engineering/managing-a-dynamic-dbt-project-929db0a134fb\nSome legacy OLAP engines, like Druid, have high operational costs and low ROI. I experienced similar drawbacks to what Lyft is talking about in Druid. Lyft writes about its refined architecture with ClickHouse and some of the challenges they found with ClickHouse during the migration. \nOne of the highlights for me in the blog is the next step of ClickHouse adoption; as a believer in bringing \u201cquery to the data\u201d rather than \u201cdata to query,\u201d I love it. \nMove Flink SQL to ClickHouse\u2014certain Flink transformations can be directly done in the destination in ClickHouse. We plan to leverage multiple new use cases in ClickHouse.\nhttps://eng.lyft.com/druid-deprecation-and-clickhouse-adoption-at-lyft-120af37651fd\nA seamless lookback, aka reconciliation pipeline support, is a must-have for your data infrastructure to support data pipelines. Netflix writes about its membership data pipeline and how it supports the lookback approach.\nhttps://netflixtechblog.com/1-streamlining-membership-data-engineering-at-netflix-with-psyberg-f68830617dd1\nDoorDash writes about ML Workbench, a centralized hub providing space for accomplishing tasks throughout the machine learning lifecycle, such as building, training, tuning, and deploying machine learning models in a production-ready environment. The workspace evolution is an exciting read for incremental architectural improvements. \nhttps://doordash.engineering/2023/11/28/transforming-mlops-at-doordash-with-machine-learning-workbench/\nConsumers come and go. \nPartitions, ever-present. \nRebalancing, the awkward middle child.\nKafka rebalancing has come a long way since then, and the author walks back to us the memory lane of Kafka rebalancing and the advancements made ever since. \nhttps://www.responsive.dev/blog/kafka-streams-history-of-rebalancing\nCloud blob storage like S3 has become the standard for storing large volumes of data, yet we have not talked about how optimal its interfaces are. The author explores the current complication of accessing blob storage and what can be improved to optimize the access. \n \nhttps://substack.com/@bytearray/posts\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/thoughts-on-amazon-express-one-and", "title": "Data Engineering Weekly", "content": "AWS S3 Express One Zone sparks some delight in the data infrastructure. In case you missed it, please read the AWS announcement here.\nhttps://aws.amazon.com/s3/storage-classes/express-one-zone/\nAmazon S3 Express One Zone is a high-performance,\u00a0single-availability Zone storage class purpose-built to deliver consistent single-digit millisecond data access for your most frequently accessed\u00a0data and latency-sensitive applications.\u00a0S3 Express One Zone can improve data access speeds by 10x and reduce request costs by 50% compared to S3 Standard and scales to process millions of requests per minute. While you have always been able to choose a specific AWS Region to store your S3 data, with S3 Express One Zone, you can select a specific AWS Availability Zone within an AWS Region to store your data. You need to co-locate your storage with your compute resources in the same Availability Zone to optimize further performance, which helps lower compute costs and run workloads faster.\nLet\u2019s revisit the current state of the data infrastructure before discussing the S3 Express. There are two critical properties of data warehouse access patterns.\nData freshness matters a lot\u2014the more recent the data, the more frequently it is accessed. \nHuman in-the-loop, application integration, or machine-driven intelligence can be essential for many near-real-time applications, but subsecond latency is not always necessary. Minute-level latency is often sufficient.\nS3 intelligent tiered storage provides a fine balance between the cost and the duration of the data retention. However, the real-time insight on accessing the recent data remains a big challenge. Several tools are trying to solve this problem, as highlighted in the current state of the Data Architecture.\nThe combination of stream processing + OLAP storage like Pinot.\nThe caching layer from the BI tools with in-memory databases is on the BI tools side. DuckDB is the recent attempt to build in-process OLAP engines. \nThe LakeHouse tools, such as Apache Hudi, support incremental querying to reduce the latency.\nThere are many tooling and architectural patterns to make the recent data more accessible to make business decisions and application integrations. \nThere is always a Google Paper to discuss the industry's new paradigm shifts. Google published Napa: Powering Scalable Data Warehousing with Robust Query Performance at Google. The paper discusses trade-offs among data freshness, resource cost, and query performance. \nIn the current state of the data infrastructure, we use a combination of multiple specialized data storage and processing engines to achieve this balance. The Total Cost of Ownership [TCO] and the operational burden are pretty high with the current state of the architecture. Many companies restrict themselves to batch processing with high latency to access the data to keep the design simple enough to manage. Presto tried with RaptorX. Previously, we even tried to query Kafka directly using Presto-Kafka Connector. It doesn\u2019t fly. We tried ingesting all the events in Pinot/ Druid systems, but that comes with its operational cost. \nThere is hope and disappointment with the S3 Express. Here are a few interesting reads.\n S3 Express One Zone, Not Quite What I Hoped For\nS3 Express is all you need. \nI don\u2019t think S3 Express can be a write-through cache system. AWS File Cache tried to play that role. Here is what I think S3 Express can potentially change the current state of the architecture. \nS3 Express will open up the serverless data architecture, separating storage and computing from the mainstream data processing industry at all levels. We will see emerging patterns like\nStream ingestion directly into S3 Express [WarpStream already does it]\nReplicate S3 Express to S3 Standard for fault tolerance\nStream Processing on top of S3 Express.\nStream processing is an important aspect I believe S3 Express will greatly disrupt. Flink-like systems bring Data to the Query, whereas OLAP engines like Pinot bring Query to Data. I\u2019m in favor of Pinot-like systems that bring Query to Data. \nAs you can see in the diagram, S3 Express significantly reduces TCO, though it is 8X more expensive than the S3 Standard Storage. \nI believe 8X the cost of S3 Express than S3 Standard is more tolerable than operating multiple distributed & stateful systems to achieve the same data processing capabilities. \nSoftware is an abstract layer on top of the storage, computation, and networking devices. Any disruption in the underlying infrastructure will significantly change the value proposition of the software layer. I believe S3 Express is one such change, and I don\u2019t doubt the other major cloud services will follow. \nIn short, We will see the LakeHouse systems implementing Napa-like architecture to let the users choose the trade-off among data Freshness, Resource Cost, and Query Performance. \nThe question all boils down to efficient data storage formats with high compression that support faster data access. The next level of data infrastructure software will focus on these two aspects rather than building stateful systems like Kafka. It will amplify the innovations on LakeHouse formats like Apache Hudi, Iceberg & Delta. I like Pinot Data Indexing techniques, which can play a major role in introducing S3 express, especially in the serving & BI layer. \n "}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-in-retrospect-key", "title": "Data Engineering Weekly", "content": "Wow, can you believe it's almost time to wave goodbye \ud83d\udc4b to 2023? This year still feels like a fresh newbie, especially since we just bounced back from the pandemic rollercoaster \ud83c\udfa2 of last year. The past few months have been a bit of a health hiccup for me \ud83e\udd12, but hey, I'm all smiles \ud83d\ude0a about how things are turning out. \nIt\u2019s the end of the year, and there will be a lot of buzz about what the next five years in data engineering might bring. But here's a thought - why not look back to uncover the patterns? \ud83e\udd14 Reflecting on the past is crucial; it gives us a solid foundation to understand how trends have evolved and what's worked (or not). It's like piecing together a puzzle from history to get a clearer picture of the future. So, instead of guessing what's ahead, let's explore the past and those patterns. Who knows what insights we might uncover? \ud83d\udd75\ufe0f\u200d\u2642\ufe0f\ud83d\udcc8\nBefore we delve into the patterns, it's important to remember that data infrastructure maturity model. \nCompanies are at various stages of this model, which means certain patterns and predictions may align with your organization's maturity level while others might not. It's essential to consider your specific context and not follow trends and buzzwords unthinkingly. Remember this as we explore these patterns to make the most informed decisions for your situation. \nI\u2019m not certainly talking about OpenAI boardroom drama. LLM is indeed starting to make an impact on the way we work. We saw a fleet of announcements from Data Catalogs tools on how LLM can help to auto-generate documentation [See: How Generative AI Is Making Data Catalogs Smarter]. We\u2019ve seen a fleet of tools like TextToSQL; Slack bots to ask questions to your data warehouse, Chat interface for spreadsheets, and even the English SDK for Spark!\nI believe the impact of LLM will go further down in the stack with data storage formats in the coming years. Let me know your thoughts in the comments. \nOne of the hot topics in the data industry is which LakeHouse format to choose. The data industry clearly understands the power of blob storage, and using S3 as a database is not a new concept either. We saw back-to-back articles comparing the features and the performance of Delta Lake, Apache Hudi, and Iceberg. Companies started to invest in either format. The companies backing Apache Hudi and Iceberg write articles about comparative ACID support in both the platforms here and here. There are attempts to bring interoperability among the LakeHouse format, such as OneTable.  \nI believe capturing the mindshare of the file format is vital for any of these companies. Once captured, there are a lot of market opportunities to move up the stack with Data Catalog, Data Governance, and Data Quality. Databricks is already doing a lot of these move-up stack products. However, I do believe there is tons of innovation left in the deep engineering of LakeHouse, such as\nIn a way, you can think of DuckDB-like systems as a feature of LakeHouse rather than a separate product. \nBundling vs. UnBundling is one of the hotly debated topics in 2023. There were many comparisons of the Modern Data Stack (MDS) as the follower of Unix Philosophy. But I had my doubts.\nAs Benn Stancil noted in his blog, \u201cThe data industry is going to consolidate\u201d is a pretty boring prediction to make these days. \nThe categories are merging. Data Catalogs, for instance, step into Data Quality & Data Observability. Data Catalogs is no longer a standalone category. Databricks announced a fleet of features for catalogs, governance, observability, and orchestration. Microsoft is entering into the integrated infrastructure with Fabric. Unless the Fed reduces the interest rate to zero, the unified platform is the trend for foreseeing.\nIn 2023, we have seen companies shifting their focus to cost optimization; data infrastructure is the center of cost optimization. The Instacart S1 filling set the data industry a hot debate on Snowflake billing, known as Snowflake Tax. \nI also noticed a common pattern where many companies write about their migration journey from Redshift. It is Databricks vs. Snowflake, where both companies ran their data conference on the same day. \nThe Modern Data Stack categories certainly fall in with the cost sensitivity of the companies. You have a $1M budget; $700K goes to data warehouses such as Databricks or Snowflake. All the modern data stack companies are fighting for the remaining budget, which puts them a commodity product to sell. The cost of integrating the Modern Data Stack is also pretty high. The cost sensitivity in the market and the rise of integrated solutions like Databricks and Microsoft are where the companies will go in 2024. The sequence of Redshift migration is one part of it. \nWe can safely say that in 2023, Flink will be the undisputed leader in streaming frameworks. It is one of the less tracked patterns, but if you noticed deeply, a fleet of companies are offering \u201cFlink as a Service.\u201d Confluent, the company behind Kafka, Kafka Stream, and KSQL, launched Flink as a Service on its cloud product. \nHowever, I believe there is tons of innovation left on the streaming side of it. It is still operationally expensive, and the feature parity with batch analytics is not there yet. LinkedIn is trying to bridge the gap with the Apache Beam adapter for streaming and the batch layer. \nWe will continue seeing innovation in streaming in the coming days. One of the engines I\u2019m excited to watch is an alternative to Apache Spark. Snowflake recently acquired Ponder to bring some alternatives to Apache Spark. 2024 will be an exciting year, and I look forward to watching how it shapes together. \nThese are some of the patterns I thought of. Please leave comments on the trends you\u2019ve noticed in 2023. Let\u2019s chat.\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-150", "title": "Data Engineering Weekly", "content": "RudderStack is the Warehouse Native CDP, built to help data teams deliver value across the entire data activation lifecycle, from collection to unification and activation. Visit rudderstack.com to learn more.\nLearnings from our journey\nIn hindsight, we wish we had invested in enabling Flink SQL on the DataMesh platform much earlier. If we had the Data Mesh SQL Processor earlier, we would\u2019ve been able to avoid spending engineering resources to build smaller building blocks such as the Union Processor, Column Rename Processor, Projection, and Filtering Processor.\nThat is a very wise quote from the Netflix engineering team. In the past, I tried to caution people trying to build another SQL-like abstraction but can\u2019t help but burn the finger before realizing how hard it is to implement simpler abstractions like SQL. The blog is a classic case study for data engineers who like to build SQL-like abstractions. \nhttps://netflixtechblog.com/streaming-sql-in-data-mesh-0d83f5a00d08\nThe impact of machine learning on productivity and communication platforms like Dropbox is undeniable. Dropbox writes one such case study about implementing an ML-powered file organization named \u201cSmart Move.\u201d\n\nhttps://engineering.hometogo.com/a-b-testing-at-hometogo-when-and-why-we-do-it-52ef063eae08\nDeleting is the hardest part of data management, and Meta writes about its internal system design to automate the data removal process. The step involves continuous monitoring, flagging & alerting, and auto-deletion. \nhttps://engineering.fb.com/2023/10/31/data-infrastructure/automating-data-removal/\nExperimentation is a cultural thing of an organization. Either you believe in experimentation, or you don\u2019t believe in it. HomeToGo writes about the classification of user-related A/B testing and when it makes sense to test. \nhttps://engineering.hometogo.com/a-b-testing-at-hometogo-when-and-why-we-do-it-52ef063eae08\nInterested in learning how some of the best teams achieve data & AI reliability at scale? Learn from today's top data leaders and architects at The Data Observability Summit on how to build more trustworthy and reliable data & AI products with the latest technologies, processes, and strategies shaping our industry (yes, LLMs will be on the table).\nRSVP NOW\nS3 has become the backbone of internet storage, offering durability, availability, and scalability that few can match. Increasingly, S3 is used as the core persistence layer for infrastructure services. The author explores S3 as a persistent layer architecture, its challenges, and infrastructure startup opportunities built on top of S3. \nhttps://medium.com/innovationendeavors/s3-as-the-universal-infrastructure-backend-a104a8cc6991\nCloud data warehouses bring flexibility and user-friendliness with additional infrastructure costs. The lack of visibility is often the frequent culprit in all surprise cloud costs. BlaBlaCar writes about system design and patterns for controlling the data platform cost on top of BigQuery.\nhttps://medium.com/blablacar/controlling-our-data-platform-costs-at-blablacar-b05a47926414\n\"This reality puts data teams in the middle of two conflicting pressures. On one end, the mandate to be compliant is clear and direct. On the other end, the demand for velocity and business growth is loud and clear.\"\nRudderStack just introduced a set of features that simplify compliance across the entire data lifecycle. The compliance toolkits enable you to manage consent, collection, storage, and deletion all from one central platform. Read the announcement for a detailed breakdown of the toolkit components.\nhttps://www.rudderstack.com/blog/announcing-the-compliance-toolkit-painless-customer-data-compliance-in-one-platform/\nGrammar checking is essential for writing, and I use it extensively. Google writes an excellent blog on an efficient grammar correction model based on the state-of-the-art EdiT5 model architecture. The blog is an excellent source for designing large-scale systems.\nhttps://blog.research.google/2023/10/grammar-checking-at-google-search-scale.html\nLLM is certainly becoming an integral part of the modern application architecture. Github writes about the modern architectural pattern for LLM applications and the five steps to building an LLM application. \nThough LLM is becoming part of the architecture, the challenges in building LLM apps remain the same. In this two-part series, Microsft explores the challenges of building LLM apps.\nPart 1: https://medium.com/data-science-at-microsoft/why-is-it-so-hard-to-ship-a-simple-llm-feature-ba7de31ffae0\nPart 2: https://medium.com/data-science-at-microsoft/challenges-of-building-llm-apps-part-2-building-copilots-e71fcf8ec12f\nI came across this Github and found it an excellent resource if you are a beginner in the Gen-AI space. The Github repo contains 12 lessons, each covering a key aspect of Generative AI principles and application development.\nhttps://github.com/microsoft/generative-ai-for-beginners\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-149", "title": "Data Engineering Weekly", "content": "RudderStack is the Warehouse Native CDP, built to help data teams deliver value across the entire data activation lifecycle, from collection to unification and activation. Visit rudderstack.com to learn more.\nThere was a recent discussion about why Apache Beam is not widely adopted or why there are few case studies around it. The promise of a unified programming model with a pluggable runtime engine (samza, Spark & Flink) is certainly appealing. LinkedIn shares its success story of using Apache Beam across business use cases. \nhttps://engineering.linkedin.com/blog/2023/revolutionizing-real-time-streaming-processing--4-trillion-event\nData lineage is essential in operating a data pipeline, yet it is hard to build. The explosion of modern data stacks made it much harder to build. Criteo shares its experience building data lineage from data stewards manually mapping the relationship to automation with pattern recognition. \nhttps://medium.com/criteo-engineering/how-we-compute-data-lineage-at-criteo-b3f09fc5c577\nGusto writes about the infrastructure components of building internal people analytics by combining SaaS products like Workday, GreenHouse, and other learning management systems. Though the blog describes a standard integration pattern, there are a couple of common patterns you can see across many SaaS integrations. \nThe multi-hop system integrations to handle access control of the sensitive data [WorkDay \u2192 Okta \u2192 Active Directory]\nThe complexity of handling standard objects vs custom objects.\nhttps://engineering.gusto.com/data-engineering-on-people-data/\nIt is exciting to see some real-world case studies about adopting the Data Mesh pattern. BlaBlaCar shares its experience bringing Data Mesh into its organization, the human factors around it, and the organizational structure for it. I would love to see how the technical architecture pattern differs from the standard data infrastructure and what additional components are required to build for data mesh soon. \nhttps://medium.com/blablacar/11-lessons-learned-managing-a-platform-team-within-a-data-mesh-0b191b7652ce\nInterested in learning how some of the best teams achieve data & AI reliability at scale? Learn from today's top data leaders and architects at The Data Observability Summit on how to build more trustworthy and reliable data & AI products with the latest technologies, processes, and strategies shaping our industry (yes, LLMs will be on the table).\nRSVP NOW\nLLMs are challenging to deploy for real-world applications due to their sheer size. For instance, serving a single 175 billion LLM requires at least 350GB of GPU memory using specialized infrastructure. Such computational requirements are inaccessible for many research teams, especially for applications that require low latency performance. Google Research introduce distilling step-by-step, a new simple mechanism that allows us to train smaller task-specific models with much less training data than required by standard fine-tuning or distillation approaches that outperform few-shot prompted LLMs\u2019 performance.\nhttps://blog.research.google/2023/09/distilling-step-by-step-outperforming.html\nThe impact of LLM in the data management space slowly unfolding. The sheer volume of data making it impossible to manage cataloging at scale. Grab shares its story of how it uses LLM-powered data classification for data entities at scale.\nhttps://engineering.grab.com/llm-powered-data-classification\nWe started to see LLM-powered application started to make impact, and slowly entering into the production-scale applications. But how does the Large Language Model works? The author explains the basics of LLM. \nhttps://medium.com/data-science-at-microsoft/how-large-language-models-work-91c362f5b78f\n\"This reality puts data teams in the middle of two conflicting pressures. On one end, the mandate to be compliant is clear and direct. On the other end, the demand for velocity and business growth is loud and clear.\"\nRudderStack just introduced a set of features that simplify compliance across the entire data lifecycle. The compliance toolkits enable you to manage consent, collection, storage, and deletion all from one central platform. Read the announcement for a detailed breakdown of the toolkit components.\nhttps://www.rudderstack.com/blog/announcing-the-compliance-toolkit-painless-customer-data-compliance-in-one-platform/\nThe nearest-neighbor algorithm, is a fundamental method in machine learning and pattern recognition. nearest neighbor algorithm comes handy in classification & regression tasks, recommender system, and anomaly detection. Spotify writes about its new battle tested nearest-neighbor search library Voyager, and guess what its open-source.\nhttps://engineering.atspotify.com/2023/10/introducing-voyager-spotifys-new-nearest-neighbor-search-library/\nMaster Data Management, to my surprise rarely discussed in the modern data stack era, where customer data platform takes the center stage. Picnic writes about its MDM journey by adopting Salesforce cloud data platform.\nhttps://blog.picnic.nl/the-art-of-master-data-management-at-picnic-48b5cf978221\nIndeed, there is no real big competition for Apache Spark for a long time. The recent Ponder acquisition by Snowflake triggers interesting conversation around the alternate computing platform for Apache Spark. The author compares Ray vs Spark as the future of distributed computing. \nhttps://medium.com/@nasdag/ray-vs-spark-the-future-of-distributed-computing-b10b9caa5b82\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-148", "title": "Data Engineering Weekly", "content": "RudderStack Profiles takes the SaaS guesswork and SQL grunt work out of building complete customer profiles so you can quickly ship actionable, enriched data to every downstream team. See how it works today.\nNaming things is always one of the hardest problems in computer science. We all know how messy the confluence page or Google Doc directory can be, where it is hard to search and find a piece of information. Dropbox writes an exciting approach to applying machine learning to identify the date formats in file names to standardize naming conventions. \nhttps://dropbox.tech/machine-learning/using-ml-to-identify-date-formats-in-file-names\nSearch functionality undeniably plays a pivotal role in the user experience of e-commerce apps. Whatnot writes about using LLM for query expansion, auto-correct spellings, and the possibility of expanding to a semantic query expansion. \nhttps://medium.com/whatnot-engineering/enhancing-search-using-large-language-models-f9dcb988bdb9\nI liked this article because it covers the basic 101 of data pipelines on maintaining a healthy infrastructure. The author talks about the founding layer to focus on healthy pipelines such as,\nData: where is the data stored? What is the data behavior?\nResource Used: how many resources should be allocated for our data pipeline?\nPartitioning: how should we partition our table (in Hadoop)?\nJob Scheduling: how frequently should our data pipeline run?\nData Dependency: does the pipeline depend on the data of other tables?\nhttps://medium.com/agoda-engineering/how-to-design-maintain-a-high-performing-data-pipeline-63b1603b8e4a\nCriteo makes a strong case for recommender system problems as user preference rather than pattern recognition. The blog talks about the current state of the recsys, ongoing work, and the basic user model recommendation. \nInstead of approaching the Recommender System problem as a pattern recognition task, we should view it as figuring out the user\u2019s preferences and then giving them the best browsing experience.\nhttps://medium.com/criteo-engineering/recommender-systems-need-a-user-model-c3b3790311bf\nAnnouncing keynotes from three data & AI pioneers, Moneyball expert Billy Beane, poker legend Annie Duke, and SVP of Product Management at Salesforce AI, Nga Phan! Register to learn how to build more trustworthy and reliable data & AI products with the latest technologies, processes, and strategies shaping our industry (yes, LLMs will be on the table).\nRSVP NOW\nLove it or Hate it, Hive is one of the formidable SQL query engines that changed the big data landscape. In theory, migrating from HiveQL to Spark SQL is just switching the execution engine, yet it is not that simple. Line shared an excellent case study of complications around moving HiveQL and Spark SQL and how the team addresses each issue systematically. \nhttps://engineering.linecorp.com/en/blog/from-hiveql-to-sparksql-troubleshooting\nThe blog is a classic example of debugging non-deterministic application errors in production. The blog narrates the approach, first focusing on bandaging the problem to buy more time and then simulating the problem by each layer to narrow down the issue. The systematic debugging approach explained in the blog can be applied to any engineering part, so I highly recommend reading this blog. \nhttps://medium.com/pinterest-engineering/lessons-from-debugging-a-tricky-direct-memory-leak-f638c722d9f2\nThe search index pipeline is my favorite kind of data pipeline problem to solve. The Vinted Engineering blog talks about the current state of the search index pipeline with a 7-minute wait time for live indexing and the weeks of waiting time for backfilling and how the CDC with Flink pipeline reduces the indexing time and improves the efficiency of the search pipeline. \nhttps://vinted.engineering/2023/09/25/search-indexing-pipeline/\n\"After your Profiles projects complete running, your customer 360 data is automatically synced to a Redis instance through a RudderStack reverse ETL job. The Activation API sits on top of that Redis instance.\"\nRudderStack just released its Activation API. The API gives you access to the Customer 360 data from your RudderStack Profiles projects, so you can bring real-time personalization into any user experience. Learn more and reach out for early access on the blog.\nhttps://www.rudderstack.com/blog/announcing-rudderstacks-activation-api/\nEntity resolution is an integral part of a data pipeline every data engineer encounters daily. Joining the sales data from Salesforce, marketing data from Marketo, and the product data from Clickstream is a good enough case where entity resolution will play a critical role. The blog is another \u201cGo Back to Basics\u201d for me, which sketches all the available techniques for entity resolution. \nhttps://towardsdatascience.com/entity-resolution-identifying-real-world-entities-in-noisy-data-3e8c59f4f41c\nOn Call reveals the true nature of the system's stability, and data engineering is no different from any other software infrastructure. What I liked about the articles, the author was true to himself and shared the lessons learned and the thought process to fix the oncall issues in this six-part blog series. I wish more data engineers would start sharing their on-call experience soon. \nData Engineering On call 1\nData Engineering On call 2\nData Engineering On call 3\nData Engineering On call 4\nData Engineering On call 5\nData Engineering On call 6\nThere are a lot of similarities between observability and data engineering if you watch closely. Think of tracing as a special case of an event, with a strongly typed & fixed schema span, then the world of data engineering practices fully applicable to observability engineering. Jaeger shares such a case with the choice of ClickHouse, a columnar storage engine for a data warehouse as a backend for storing tracing. \nhttps://medium.com/jaegertracing/making-design-decisions-for-clickhouse-as-a-core-storage-backend-in-jaeger-62bf90a979d\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-147", "title": "Data Engineering Weekly", "content": "RudderStack Profiles takes the SaaS guesswork and SQL grunt work out of building complete customer profiles so you can quickly ship actionable, enriched data to every downstream team. See how it works today.\nThe cost & effort value proportion for a Data Catalog implementation is always questionable in a large-scale data infrastructure. Thoughtworks, in combination with Adevinta, published a three-phase approach to measure the value of a data catalog.\nSearch and Discrimination!\nRequest Access to Data\nValidate with Exploration\nhttps://www.thoughtworks.com/insights/blog/data-strategy/measuring-the-value-of-a-data-catalog\nRamp writes about its machine learning infrastructure and the choice of Metaflow for running the ML workload. The exciting part of the article is about the comparison of Metaflow with Airflow and the decision not to use Airflow for ML workload. \nAirflow isn\u2019t meant to process the data. It instead, say, triggers a Spark job that processes the data. It\u2019s the same with machine learning.\nhttps://engineering.ramp.com/metaflow-production-ml\nWhatnot writes about its LLM usage to enrich multimodal content moderation, fulfillment, bidding irregularities, and general fraud protection. The blog talks about the limitations of rule engines and how LLM can enrich additional context to make the rule engine more effective. \nhttps://medium.com/whatnot-engineering/how-whatnot-utilizes-generative-ai-to-enhance-trust-and-safety-c7968eb6315e\nInterested in learning how some of the best teams achieve data & AI reliability at scale? Learn from today's top data leaders and architects at The Data Observability Summit on how to build more trustworthy and reliable data & AI products with the latest technologies, processes, and strategies shaping our industry (yes, LLMs will be on the table).\nRSVP NOW\nThough the possibility of LLM is very attractive, it is still hard to ship a simple LLM feature. The blog specifically focuses on the following areas of LLM implementations.\nwhich model to pick \nwhen LLM responses don\u2019t match the desired output\nwhen edge cases are pointy\nthe importance of implementing responsible AI\nhttps://medium.com/data-science-at-microsoft/why-is-it-so-hard-to-ship-a-simple-llm-feature-ba7de31ffae0\nLLM commoditizes the usage of AI features in the applications. It also brings a lot of attention to the Responsible AI. Atlassian writes about the importance of responsible AI and how an enterprise can think about it.\nYour app and its documentation should be very clear about:\nThe models or third-party AI providers you're using.\nThe customer data that is being processed by AI and for what purpose?\nThe situations where users will interact with AI in your app.\nhttps://blog.developer.atlassian.com/responsible-ai-building-trustworthy-ai-apps/\nData governance has often been misunderstood as a controlling bureaucratic process. But at its core, data governance isn\u2019t about control. It\u2019s about helping data teams work better together.Atlan is hosting a virtual conference where modern data teams, like Nasdaq, HelloFresh, and SMG Swiss Marketplace Group, uncover how they are rethinking governance \u2014 and ushering in the data governance 3.0 era with automation, collaboration, and AI.\nCheck out the agenda and save your spot to join Re: Govern on October 05\nPinterest writes about its assessment of the ML developer velocity bottlenecks and delves deeper into the adoption of Ray, the open-source framework to scale AI and machine learning workloads. The blog is an excellent read about the \u201cScale First, Learn Last\u201d problem in the data processing world. \nhttps://medium.com/pinterest-engineering/last-mile-data-processing-with-ray-629affbf34ff\nIn the last couple of weeks, I heard a lot of buzz around the elementary package for dbt, though I\u2019ve not tried it myself yet. The author lists some of the features of the elementary package, and the top 3 are,\nSeveral different anomaly tests\nAdvanced schema tests\nEnd-to-end pipeline view (Airflow, GitHub Actions, Looker, and Tableau)\nhttps://leo-godin.medium.com/are-you-using-elementary-for-dbt-f9a56ecbef42\n\"The new integration makes it easy for data teams to get the most value out of their Kafka implementation by automatically forwarding streams to key business tools and standardizing schemas for data that is used in identity resolution and customer 360 projects.\"\nMany teams struggle to effectively use their customer data from Kafka to drive value downstream because of the custom integration and pipeline work required. Those challenges are a thing of the past with RudderStack\u2019s Kafka source integration.\nhttps://www.rudderstack.com/blog/announcing-the-rudderstack-kafka-source-integration/\nDailymotion writes an exciting article about reinventing the recommender engine with vector databases. The blog narrates how it uses Open AI for candidate generation and Re-Ranking to build the opinion-based recommender system. \nhttps://medium.com/dailymotion/reinvent-your-recommender-system-using-vector-database-and-opinion-mining-a4fadf97d020\nThe DataFrames are not only for Python!!! The Polars DataFrame support for JavaScript is a game changer. Jupyter Labs writes about the support for Deno Kernel, the first javascript language runtime with a built-in Jupyter kernel.\nhttps://blog.jupyter.org/bringing-modern-javascript-to-the-jupyter-notebook-fc998095081e\nAgoda writes about its Kafka infrastructure by adopting a control plane-style architecture model. The message forwarder style architecture becomes increasingly common as tightly coupling the Kafka topic with the producer increases the operational burden. \nhttps://medium.com/agoda-engineering/how-agoda-manages-1-8-trillion-events-per-day-on-kafka-1d6c3f4a7ad1\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-146", "title": "Data Engineering Weekly", "content": "RudderStack Profiles takes the SaaS guesswork and SQL grunt work out of building complete customer profiles so you can quickly ship actionable, enriched data to every downstream team. See how it works today.\nThe blog post compares benchmarks for the popular dbt SQL engines Trino, DuckDB, and Spark. However, one could argue this is not a head-to-head comparison since all three engines are designed for different hardware constraints. You can see that DuckDB throws OOM in a few benchmarks. But the key point here is if we can shrink the incremental data processing that can fit into a single machine, the greater the cost efficiency of data infrastructure.  \nhttps://medium.com/datamindedbe/head-to-head-comparison-of-dbt-sql-engines-497d71535881\nAn interesting article about Kimball\u2019s dimensional modeling. The blog narrates the key concepts of the Kimball model and a modern outlook on the concepts. \nMy take on the Kimball model is that All the techniques defined in the Kimball model, from bus matrix and confirmed dimensions to slowly changing dimensions, conceptually remain the same. All these concepts fundamentally try to achieve data consistency across the board. However, the logical and physical data modeling is designed when there is storage scarcity. We are no longer bound by the same storage limitation when these techniques become mainstream. We need to take these concepts but should rethink to fit the data model to take advantage of both the software and hardware advancements. \nhttps://faithfacts.substack.com/p/an-appropriately-unhinged-deep-dive\nPayPal is running an impressive Kafka fleet, consisting of over 1,500 brokers that host over 20,000 topics and close to 2,000 Mirror Maker nodes that mirror the data among the clusters, offering 99.99% availability for the Kafka clusters. The blog narrates the cluster management practices focusing on ACL, config services, Kafka client SDKs, and QA environment. \nhttps://medium.com/paypal-tech/scaling-kafka-to-support-paypals-data-growth-a0b4da420fab\nData governance has often been misunderstood as a controlling bureaucratic process. But at its core, data governance isn\u2019t about control. It\u2019s about helping data teams work better together.Atlan is hosting a virtual conference where modern data teams, like Nasdaq, HelloFresh, and SMG Swiss Marketplace Group, uncover how they are rethinking governance \u2014 and ushering in the data governance 3.0 era with automation, collaboration, and AI.\nCheck out the agenda and save your spot to join Re: Govern on October 05\nThe AI workload is a network and computationally intensive. Meta writes about Arcadia, a unified system that simulates AI training clusters' compute, memory, and network performance. The simulation of AI workload helps to accurately model the performance of compute, memory, and network components within large-scale AI training clusters.\nhttps://engineering.fb.com/2023/09/07/data-infrastructure/arcadia-end-to-end-ai-system-performance-simulator/\nGihub shared how to build an enterprise LLM application from a product management perspective. It is an excellent read for anyone thinking of building LLM-powered product features. \nhttps://github.blog/2023-09-06-how-to-build-an-enterprise-llm-application-lessons-from-github-copilot/\nInterested in learning how some of the best teams achieve data & AI reliability at scale? Learn from today's top data leaders and architects at The Data Observability Summit how to build more trustworthy and reliable data & AI products with the latest technologies, processes, and strategies shaping our industry (yes, LLMs will be on the table).\nRSVP NOW\nInstacart writes about Ava, its internal productivity tool built on GPT 4. I love the prompt exchange feature where users can browse popular prompts, search for something specific, or create their own and share them with the rest of the company. \nhttps://tech.instacart.com/scaling-productivity-with-ava-instacarts-internal-ai-assistant-ed7f02558d84\nNextdoor writes about its embedding model journey from using pre-trained to fine-tuning the embedding applications. The blog discusses its usage with the pre-trained model and how it uses historical user interactions to fine-tune the embedding from unlabeled data and the labeled user feedback. \nhttps://engblog.nextdoor.com/from-pre-trained-to-fine-tuned-nextdoors-path-to-effective-embedding-applications-3a13b56d91aa\n\u201cWithout stitching each unique identifier to the user, systems and teams will wrongly assume that these three transactions are from three distinct users. Worse yet, you are unable to calculate \u2014 or inaccurately calculate \u2014 important computed traits like user_total_revenue because of the fragmented user identities.\u201d\nCustomer 360 is, at the end of the day, a data problem. Here, the team at RudderStack details how to solve identity resolution in the warehouse and build user features to create activation-ready customer profiles.\nhttps://www.rudderstack.com/blog/how-to-create-a-customer-360/\nEtsy writes about its attempt to build a stateful real-time ML model training over a hackathon. The author acknowledges it is far from in the production but made enough case on the benefit of having an incremental model building. The initial estimation of saving $212k annual cloud cost and latency reduction from 40 hours to near real-time is an impactful case study for a hackathon project. \nhttps://www.etsy.com/codeascraft/the-so-fine-real-time-ml-paradigm\nWalmart writes about its Machine Learning platform architecture following the best-of-the-breed model. The hybrid cloud platform builds on Kubernetes, Airflow, and a set of microservices. The platform focuses on data ingestion & preparation, feature engineering & model training, model experimentation, model evaluation & deployment with monitoring and governance. \nhttps://medium.com/walmartglobaltech/machine-learning-platform-at-walmart-b06819825ef7\nIn 2021, ML was siloed at Pinterest with 10+ different ML frameworks relying on different deep learning frameworks, framework versions, and boilerplate logic to connect with our ML platform. \nPinterest discusses the challenges of running disjointed ML infrastructure in an organization and how it stale the innovation speed. The author narrates about MLEnv, a full-stack ML developer framework that aims to make ML engineers more productive by abstracting technical complexities irrelevant to ML modeling. \nhttps://medium.com/pinterest-engineering/mlenv-standardizing-ml-at-pinterest-under-one-ml-engine-to-accelerate-innovation-e2b30b2f6768\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/dewcon-2023-the-talks-that-have-me", "title": "Data Engineering Weekly", "content": "Last night, I was walking through the DEWCon registration and was delighted to see all the major unicorns of India and the prominent MNCs register to attend the conference.\nSo, here's a little backstory about how DEWCon came into being, and it's somewhat amusing. I was talking to Aswin one day and mentioned that I'd be in Bengaluru soon. \"We should catch up,\" I said. Aswin floated the idea of making it a meetup\u2014inviting some data engineers and having an in-depth conversation about our field. \nIn a half-joking manner, I threw out there, \"Why not turn it into a full-blown data engineering conference?\" We both chuckled and moved on. But the next day, it dawned on us that India didn't have a good data engineering conference\u2014nothing like the Data Council in the U.S. or Crunch Conf in Europe. \nAnd that's when our casual chat started to feel like a lightbulb moment. The joke suddenly seemed like a really good idea, and the rest, as they say, is history. We are genuinely trying our best and incredibly thankful to Nandini Raja, Shreeshail Degnial, Abdul Khader Shaik & Sabarish volunteers for the conference to take it to the next level.\nWe are still waiting for the title from a few speakers, so I\u2019m adding the one we know and vetted upfront. \nFull Agenda: https://dewcon.ai/agenda/\nVinoth will be talking about evolution of the Data Engineering stack from Data Warehouse \u2192 Data Lake \u2192 Lake House architecture and how the Lake House architecture will shape the data engineering landscape in the future. I\u2019m excited to hear from Vinoth, who shaped the Lake House architecture in many ways in the industry. \nJoe Reis will talk about the importance of data modeling and how data modeling plays a critical role in the modern data stack. Data modeling is my favorite topic. The industry is swinging a pendulum from rigid centralized data modeling to throwing everything in a data lake to again data modeling. I\u2019m excited to hear Joe\u2019s take on this.\nThe discussion around modern data stack is something I\u2019m excited to have with Vinoth Chandar, Joe Reis, Divyansh Saini of Houseware & Shuveb Hussain of ZipStack. It is a perfect panel to discuss about the best-of-the-bread solutions vs one integrated solutions. \nThe adoption of LakeHouse is inevitably increasing. What architecture should you adopt to implement LakeHouse in your company? What kind of challenges you will be facing?  I\u2019m hosting a fireside chat with Ramesh from Branch Metrics and Ankur from Walmart about their experience operating scalable LakeHouse architecture. \nI told Lakshmi the other day that only a few people realize how hard it is to have a reconciliation pipeline for bookkeeping. I\u2019m excited to hear more about Flipkart, one of the pioneers in Indian e-commerce venture, about its reconciliation product. \nMany streaming frameworks are at our disposal, But before using any streaming. Engines, one must master fundamental concepts like Watermark, late-arriving data, etc. I\u2019m looking forward to Kamran's talk covering some basics of stream processing engines. \nLLM dominates the world, and products like ChatGPT live up to that expectation. There is a rush from every enterprise company to figure out how to incorporate GenAI into their product features and business operations. I\u2019m excited to see the talk from Karin on data enrichment with LLM. \nData is a product, and data as a Product is gaining tremendous attention across the board. Vidhya\u2019s talk focuses on, from theory to adoption, how an organization should think about positioning data as a product to build great products with data.\nAI/ML has a profound effect on human life and the community. It\u2019s essential to adopt Responsible AI practices early on in the journey. Data Quality & handling sensitive data is critical in building the foundation for Responsible AI. In this talk, Venkatesh walks us through such a journey of data to responsible AI/ML.\nMany focus on LLM with fancy product features, but what I liked about this talk is the focus on improving productivity and stakeholder experience. \nThe goal of DEWCon to create a healthy environment for the data practitoers to discuss the current trends in the industry, share their reference architecture and have fun. I learned a lot in many hallway conversation during the conference, so don\u2019t miss out attending the conference. I heard the after party at the Taj is awesome!!!\nRegistration Here: https://dewcon.ai/\nUse the discount code: DATAHERO for 50% discount\n "}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-145", "title": "Data Engineering Weekly", "content": "RudderStack Profiles takes the SaaS guesswork and SQL grunt work out of building complete customer profiles so you can quickly ship actionable, enriched data to every downstream team. See how it works today.\nHey folks! \ud83d\udce3 Exciting news! We've finalized the agenda for the conference, and we will be launching middle of this week. \ud83c\udfa4 And guess what? We've given our conference website a fresh look. \ud83c\udf10\nTickets are selling fast; use the code DATAHERO for a special discount. \ud83c\udf9f\ufe0f Oh, and if your company's thinking of bulk booking, drop an email to ananth@dataengineeringweekly.com to get some awesome discounts. \ud83d\udce9\nLooking forward to seeing you all! \ud83d\udc4b\ud83d\ude42\nRegister Now \u2192\nOn LinkedIn, you may hear this frequently, \"We did this 30 years ago.\" or \"Don't reinvent the wheel.\". The reality is that software is an abstract layer that interacts with the hardware components like the CPU, Network IO & memory. Any advancement in this hardware layer will significantly influence the software architecture, so we often see \"what goes around, comes around.\" \nThe author summarizes the last 15 years of data computation infrastructure and the emerging trend of composable data systems. \nhttps://wesmckinney.com/blog/looking-back-15-years/\nAs a data engineer, I love to work with the Notebook, but the challenge always comes while running through the standard pipelining practices. The orchestration engines like Airflow & Dagsters provide sophisticated pipeline authoring tooling, which is often hard to integrate with the notebooks. I often want to click the \u201cSchedule this Notebook\u201d button and automatically generate the Airflow code to schedule and commit in Github. On a similar experience, Meta writes about its automation process to simplify Notebook scheduling.\nhttps://engineering.fb.com/2023/08/29/security/scheduling-jupyter-notebooks-meta/\nThe LakeHouse architecture brings the best of the database and the data lake into the data infrastructure. Which one to choose? Which one runs faster than the other? Following a benchmark result between Delta & Iceberg, Apache Hudi added its benchmark in the same repo. It\u2019s exciting to see this benchmark happen in a public Github, which brings more openness into the ecosystem. \nI do not believe in the benchmark as the answer is always \u201cIt depends on your workload,\u201d and the author rightly mentioned it in the article. \nOne key thing to remember when running TPC-DS benchmarks comparing Hudi, Delta, and Iceberg is that by default, Delta + Iceberg is optimized for append-only workloads, while Hudi is, by default, optimized for mutable workloads.\nhttps://medium.com/@kywe665/delta-hudi-iceberg-a-benchmark-compilation-a5630c69cffc\nInterested in learning how some of the best teams achieve data & AI reliability at scale? Learn from today's top data leaders and architects at The Data Observability Summit on how to build more trustworthy and reliable data & AI products with the latest technologies, processes, and strategies shaping our industry (yes, LLMs will be on the table).\nRSVP NOW\nLast week, I came across some interesting file formats on top of the existing columnar formats like Parquet. GraphAr is a standardized file format for graph data, computation/storage system independent, and provides a set of interfaces for generating, accessing, and transforming these formatted files. It piqued my interest in my outstanding question: Can we model DataWarehouse as a graph rather than a relational model? \nhttps://blog.graphscope.io/graphar-a-standard-data-file-format-for-graph-data-storage-and-retrieval-765a2efba519\nGeoParquet is a second data format I came across this week. GeoParquet is an incubating Open Geospatial Consortium (OGC) standard that adds interoperable geospatial types (Point, Line, Polygon) to Parquet. The author explores the performance of GeoParquet with DuckDB. Combining WebAssembly, DuckDB, and GeoParquet formats brings exciting interactive web applications like visualizing 1 million building footprints with 7 million total coordinates for the U.S. state of Utah and calculating their area on the fly.\nhttps://cloudnativegeo.org/blog/2023/08/performance-explorations-of-geoparquet-and-duckdb/\n\u201cWithout stitching each unique identifier to the user, systems, and teams will wrongly assume that these three transactions are from three distinct users. Worse yet, you are unable to calculate \u2014 or inaccurately calculate \u2014 important computed traits like user_total_revenue because of the fragmented user identities.\u201d\nCustomer 360 is, at the end of the day, a data problem. Here, the team at RudderStack details how to solve identity resolution in the warehouse and build user features to create activation-ready customer profiles.\nhttps://www.rudderstack.com/blog/how-to-create-a-customer-360/\nEvery system will fail in production with unknown errors. The author writes about the set of common dbt error codes and how to debug them. \nRelated Note: If you're like me and get a kick out of system failures in production environments, you should definitely read the article \"Operating Effectively in High Surprise Mode.\" It's a great resource for anyone who wants to learn how to deal with unexpected events calmly and collectedly.\nhttps://www.arecadata.com/the-definitive-guide-for-debugging-dbt/\nExperimentation is the core of many online business operations, and we\u2019ve seen many companies trying to democratize the experimentation platform. On a similar line, Adevinta writes about Fisher, a Python package that enables Data Scientists to run straightforward hypothesis testing and to produce comprehensive reports with very few lines of code.\nhttps://medium.com/adevinta-tech-blog/how-we-matured-fisher-our-a-b-testing-package-110c99b993fc\nSince 2019, the data mesh has woven itself into every blog post, event presentation, and webinar. But 4 years later, in 2023 \u2014 where has the data mesh gotten us? Does its promise of a decentralized dreamland hold true?Atlan is bringing together data leaders like Abhinav Sivasailam (CEO, Levers Labs), Barr Moses (Co-founder & CEO, Monte Carlo), Scott Hirleman (Founder & CEO, Data Mesh Understanding), Teresa Tung (Cloud First Chief Technologist, Accenture), Tristan Handy (Founder & CEO, dbt Labs), Prukalpa Sankar (Co-founder, Atlan), and more at the next edition of the Great Data Debate to discuss the state of data mesh \u2013 tech toolkit and cultural shift required to implement data mesh.\nWatch the Recording of the Great Data Debate \u2192\nThe adoption of LLM in enterprises is slowly increasing, and so are the techniques and infrastructure around Fine-Tuning prompts. Databricks writes about LoRA, an improved fine-tuning method where instead of fine-tuning all the weights of the pre-trained large language model, two smaller matrices that approximate this larger matrix are fine-tuned. \nhttps://www.databricks.com/blog/efficient-fine-tuning-lora-guide-llms\nIn another exciting read on Fine-Tunning LLM, the author shared a lesson learned from the recent SF deep learning meetup. The author also highlights how Lora (Low-Rank Adaptation of Large Language Models) can get the same accuracy as end-to-end fine tuning but is much cheaper in training since you update a small set of parameters.\nThe Instacart S1 filling triggered some interesting conversations across many forums.\nMy take on this is that I admire what Instacart engineering can build on top of Snowflake. Anyone who has worked in a high-growth startup like Instacart can resonate with the \"growth~cost-cutting cycles,\" known as the Pendulum Effect. Organizations swing the operating principles as growth at any cost in one year and then swing the pendulum towards cost optimization, hoping the chaos results in optimal resource usage efficiency. \nHowever, the pendulum effect is not an excuse for inefficient system design. Engineers must be aware of the cost-fit function of their solution, and I learned the system design lesson during an incident mitigation process. \nAWS writes an excellent article on the same, discussing how to develop a cost-aware engineering culture. \nhttps://aws.amazon.com/blogs/apn/develop-a-cost-aware-culture-for-optimized-and-efficient-saas-growth/\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-144", "title": "Data Engineering Weekly", "content": "RudderStack Profiles takes the SaaS guesswork, and SQL grunt work out of building complete customer profiles so you can quickly ship actionable, enriched data to every downstream team. See how it works today.\nHey folks! \ud83d\udce3 Exciting news! We've finalized the agenda for the conference and we will be launching middle of this week. \ud83c\udfa4 And guess what? We've given our conference website a fresh look. \ud83c\udf10\nTickets are selling fast; use the code DATAHERO for a special discount. \ud83c\udf9f\ufe0f Oh, and if your company's thinking of bulk booking, drop an email to ananth@dataengineeringweekly.com to get some awesome discounts. \ud83d\udce9\nLooking forward to seeing you all! \ud83d\udc4b\ud83d\ude42\nRegister Now \u2192\nAI will automate 25-50% of white collar work including data analysis. Does that will data teams shrink in size? On the contrary, while AI can automate some work, it will also demand much more from data teams.\nThough it is a short essay, the author remind is AI & data team relationship is a typical Braess's paradox. The ease of data access and democratization increase the need for better quality with contextual data. So all in data folks, Let\u2019s do it.\nhttps://www.linkedin.com/pulse/paradox-ai-data-teams-how-automation-increase-demand-tomasz-tunguz/\nThe rise of Open Source LLM model giving tough fight with the proprietary LLM models like OpenAI. AnyScale published LLM accuracy comparison among Llama 2 vs GPT 4 and Human, it is amazing to see all models almost equal at the human level.\nhttps://www.anyscale.com/blog/llama-2-is-about-as-factually-accurate-as-gpt-4-for-summaries-and-is-30x-cheaper\nNetflix writes about consolidating multiple machine learning models used for different recommendation tasks into one multi-task model. This approach simplified the system's architecture, improved model performance, and enhanced maintainability. Netflix narrates how it addressed the challenges in both offline (training) and online (deployment) phases, introducing a unified request context and a generic API. \nhttps://netflixtechblog.medium.com/lessons-learnt-from-consolidating-ml-models-in-a-large-scale-recommendation-system-870c5ea5eb4a\nData trust is at an all-time low, and teams are feeling the pain. Our latest report highlights the impact of bad data on your bottom line (did you know that poor data quality impacts 31% of revenue?!) and how the best teams are reducing incident resolution times?\nAccess the Report\nSoftware engineering practice have tons of best practices, all the way from code formating to code coverage to API design patterns. Machine Learning Systems pose their own specific challenges, and Booking.com narrates its quality model for Machine Learning.\nhttps://booking.ai/how-good-are-your-ml-best-practices-fd7722262437\nThe competition in the data transformation layer is heating up. While dbt is certainly the most popular transformation layer, we\u2019ve seen emerging alternatives like SQLMesh and SDF [Semantic Data Fabric]. You can\u2019t say that one is better than the others; the author gives an excellent walkthrough of all three transformation engines.\nhttps://datajargon.substack.com/p/dbt-vs-sdf-vs-sqlmesh\nAugust 30, Join Wyze\u2019s Director of Data Engineering, Wei Zhou, and Senior Data Scientist, Pei Guo, to learn how they\u2019re using RudderStack and Snowflake to collect clean, comprehensive data, quickly model it into an identity graph and customer 360 tables, then make that data available to their AI team for modeling directly inside of Snowflake\u2019s Data Cloud.\nRegister now\nData Testing is an integral part of data transformation lifecycle. DataCoves writes an excellent article comparing various data testing options available to use with dbt. I like the approach of categorizing the testing strategy as generic test & singular tests.\nhttps://datacoves.com/post/dbt-test-options\nData engineering has missed the boat on the \u201cdevops movement\u201d and rarely benefit from the sanity and peace-of-mind it provides to modern engineers. They didn\u2019t miss the boat because they didn\u2019t show up, they missed the boat because the ticket was too expensive for their cargo.\nMaxime Beauchemin - The Downfall of the Data Engineer\nI still see many data teams trying to mimic the dev/ stage/ prod environment in the data pipeline, which brings more confusion than solving the problem. The author argues why dev-stage-prod is bad and encourages data sandbox/ branching strategy to test the pipeline.\nhttps://enigma.com/blog/post/dev-stage-prod-is-the-wrong-pattern-for-data-pipelines\nSince 2019, the data mesh has woven itself into every blog post, event presentation, and webinar. But 4 years later, in 2023 \u2014 where has the data mesh gotten us? Does its promise of a decentralized dreamland hold true?Atlan is bringing together data leaders like Abhinav Sivasailam (CEO, Levers Labs), Barr Moses (Co-founder & CEO, Monte Carlo), Scott Hirleman (Founder & CEO, Data Mesh Understanding), Teresa Tung (Cloud First Chief Technologist, Accenture), Tristan Handy (Founder & CEO, dbt Labs), Prukalpa Sankar (Co-founder, Atlan), and more at the next edition of the Great Data Debate to discuss the state of data mesh \u2013 tech toolkit and cultural shift required to implement data mesh.\nWatch the Recording of the Great Data Debate \u2192\nThe major selling point of dbt cloud is its robust CI/CD pipeline support, but can you achieve the same without a commercial license from dbt labs, using only the open-source dbt core? dbt cloud natively designs the 'Slim CI' job pattern to test only the modified dbt models when someone creates a pull request in your dbt Git repository. The author explains how to implement the `Slim CI` pattern using dbt core.\nhttps://paulfry999.medium.com/v0-4-pre-chatgpt-how-to-create-ci-cd-pipelines-for-dbt-core-88e68ab506dd\nCost fit function is vital for the architecture decision, and often plays a significant role in the choice of technology and the success of an organization. \nAs we noticed in Instacart\u2019s S1 filling, the Snowflake billing trends,\nIt would be an amazing case study to see how Instacart lower the Snowflake billing, on a similar line HelloFresh writes about its approach to drive the cost optimization in Snowflake. \nhttps://engineering.hellofresh.com/data-driven-snowflake-optimisation-at-hellofresh-55a5b56aa9af\nPython making its way to real-time stream processing, but I\u2019ve seen less articles about the usage of PyFlink. Alibaba team writes a comprehensive article about PyFlink from the basics to managing the python dependencies.\nhttps://www.alibabacloud.com/blog/all-you-need-to-know-about-pyflink_600306\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-143", "title": "Data Engineering Weekly", "content": "RudderStack Profiles takes the SaaS guesswork, and SQL grunt work out of building complete customer profiles, so you can quickly ship actionable, enriched data to every downstream team. See how it works today.\nHey folks! \ud83d\udce3 Exciting news! We've received 30+ talk submissions and have confirmed all our speakers for the conference. \ud83c\udfa4 And guess what? We've given our conference website a fresh look. \ud83c\udf10\nTickets are selling fast; use the code DATAHERO for a special discount. \ud83c\udf9f\ufe0f Oh, and if your company's thinking of bulk booking, drop an email to ananth@dataengineeringweekly.com to get some awesome discounts. \ud83d\udce9\nLooking forward to seeing you all! \ud83d\udc4b\ud83d\ude42\nRegister Now \u2192\n2023 is where we see the best minds and the money spent on improving LLM. Though the promise of LLM is great, many operational challenges remain open. The author gives an excellent overview of open challenges in LLM as of now.\nReduce and measure hallucinations\nOptimize context length and context construction\nIncorporate other data modalities\nMake LLMs faster and cheaper\nDesign a new model architecture\nDevelop GPU alternatives\nMake agents usable\nImprove learning from human preference\nImprove the efficiency of the chat interface\nBuild LLMs for non-English languages\nhttps://huyenchip.com/2023/08/16/llm-research-open-challenges.html\nIt is the most asked and debated question in the data engineering space how GEN-AI can revolutionize data engineering. We\u2019ve seen text-to-SQL generators, Gen-AI SDK, auto-generate documentation, etc. The blog narrates the potential possibilities of LLM\u2019s impact in each stage of the data warehouse.  \nhttps://www.alibabacloud.com/blog/how-generative-ai-can-revolutionize-data-engineering_600290\nThough LLM is hotly debated, many companies are still trying to figure out how LLM can help to improve their product experience. We see a floating of how-to\u2019s on LLM, LangChain, and other related technologies. I found the article from Microsoft to be a good summarization of building a Q&A system for any document. \nhttps://medium.com/data-science-at-microsoft/fundamentals-of-building-with-llms-question-answer-on-any-document-with-chatgpt-in-30-lines-of-9f0d436baff1\nSince 2019, the data mesh has woven itself into every blog post, event presentation, and webinar. But 4 years later, in 2023 \u2014 where has the data mesh gotten us? Does its promise of a decentralized dreamland hold true? Atlan is bringing together data leaders like Abhinav Sivasailam (CEO, Levers Labs), Barr Moses (Co-founder & CEO, Monte Carlo), Scott Hirleman (Founder & CEO, Data Mesh Understanding), Teresa Tung (Cloud First Chief Technologist, Accenture), Tristan Handy (Founder & CEO, dbt Labs), Prukalpa Sankar (Co-founder, Atlan), and more at the next edition of the Great Data Debate to discuss the state of data mesh \u2013 tech toolkit and cultural shift required to implement data mesh.\nWatch the Recording of the Great Data Debate \u2192\nInstacart wrote two back-to-back blogs this week about the vision behind ML/ AI and the usage of dbt. \nMost of these ML models were trained either on laptops or custom infrastructure developed within each team with no common patterns, and sometimes it took more than a month to put a model into production. In early 2021, we built an in-house ML platform to enable our teams to construct, deploy, serve, and manage ML models and features at scale, ensuring their efficacy, dependability, and security throughout the ML lifecycle.\nThe blog is an excellent reminder to start somewhere small, then standardize as a platform and improve. \nhttps://tech.instacart.com/supercharging-ml-ai-foundations-at-instacart-d48214a2b511\nInstacart writes about its adoption of dbt as the data transformation tool. The integration story is a repeating design pattern where an adopted code parses the manifest.json file and constructs dynamic DAGs in Airflow. \nI\u2019m curious to know if the DAGs are generated dynamically or run in a compile-time and checked in as a code. I\u2019ve seen a pretty unstable system with dynamic DAG generation and switched to DAG generation at compile time. \nhttps://tech.instacart.com/adopting-dbt-as-the-data-transformation-tool-at-instacart-36c74bc407df.\nInterested in learning how some of the best teams achieve data & AI reliability at scale? Learn from today's top data leaders and architects at The Data Observability Summit on how to build more trustworthy and reliable data & AI products with the latest technologies, processes, and strategies shaping our industry (yes, LLMs will be on the table).\nRSVP NOW\nDuckDB is truly becoming the defacto federated query engine for analytics. The recent addition of the Postgres Scanner looks very promising. I\u2019m looking forward to DuckDB integration with the LakeHouse systems like Hudi, Iceberg, and DeltaLake. The blog narrates how to use Postgres\u2019s pgvector to run a vector similarity search using DuckDB. \nhttps://blog.lancedb.com/vector-similarity-search-with-duckdb-44dec043532a.\nIn the digital realm, experimentation isn't just a strategy; it's the lifeblood of innovation and the compass that guides success.\nSpotify writes three basic guiding principles to use the experimentation method better and achieve a more tangible impact for the business in a mature context.\nStart with the decision that needs to be made.\nUtilize localization to innovate for homogeneous populations.\nBreak the feature apart into its most critical pieces.\nhttps://engineering.atspotify.com/2023/08/experimentation-at-spotify-three-lessons-for-maximizing-impact-in-innovation/\nAugust 30, Join Wyze\u2019s Director of Data Engineering, Wei Zhou, and Senior Data Scientist, Pei Guo, to learn how they\u2019re using RudderStack and Snowflake to collect clean, comprehensive data, quickly model it into an identity graph and customer 360 tables, then make that data available to their AI team for modeling directly inside of Snowflake\u2019s Data Cloud.\nRegister now\nUser segmentation divides a company's audience into groups based on shared characteristics, behaviors, or needs. These groups, or segments, enable businesses to deliver more tailored marketing and product experiences. Companies can achieve greater engagement and conversion rates by understanding and targeting specific segments.\nGrab writes about its user segmentation platform architecture, focusing on segment creation & segment serving and challenges in running at scale. \nhttps://engineering.grab.com/streamlining-grabs-segmentation-platform\nSpeaking of User Segmentation challenges, one of the key challenges in Segment creation is Identity Resolution, especially with anonymous user visits. \nThe author narrates the current challenges with identity resolution and potential steps to build robust systems to help the user segmentation process. \nhttps://sarahsnewsletter.substack.com/p/guide-to-anonymous-identity-resolution\nEverything breaks at scale, which is a unique place to learn to operate a system. Lyft shares such cases of scaling challenges and uncertainty with Flink Streaming\u2019s Kinesis connector. The chapter-by-chapter explanation of the system's behavior is an exciting read. \nhttps://eng.lyft.com/wheres-my-data-a-unique-encounter-with-flink-streaming-s-kinesis-connector-6da3b11b164a\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-142", "title": "Data Engineering Weekly", "content": "RudderStack Profiles takes the SaaS guesswork, and SQL grunt work out of building complete customer profiles, so you can quickly ship actionable, enriched data to every downstream team. See how it works today.\nGreat news! We've overcome some unexpected hiccups, and guess what? Conference registration is officially OPEN! \ud83c\udf89\nMark your calendars: DEWCon is happening on October 12th at the luxurious Taj Hotel on MG Road. \ud83c\udfe8\nWe got some exciting keynotes lined up. Joe Reis, author of \"The Fundamentals of Data Engineering,\" and Vinoth Chandar, creator of  Apache Hudi and founder of OneHouse.ai. \ud83d\udcda\ud83d\udca1 will be diving into the latest industry buzz. \ud83d\ude80 Stay tuned for all the details! \ud83d\udd0d\nhttps://www.dewcon.ai/\nAI plays an important role in what people see on Meta\u2019s platforms. Meta writes about mult\u2014stage ranking approach with several well-defined stages, each focusing on different objectives and algorithms.\nRetrieval\nFirst-stage ranking\nSecond-stage ranking\nFinal reranking\nhttps://engineering.fb.com/2023/08/09/ml-applications/scaling-instagram-explore-recommendations-system/\nSparkSQL is a stop-gap to bride Spark RDD accessible to a mass audience, and ever since, there has been a constant effort to improve its SQL performance. Databricks attempt to solve this by Photon. Gluten from Intel & Kyligence is taking an open-source attempt to accelerate Spark SQL queries. \nhttps://medium.com/intel-analytics-software/accelerate-spark-sql-queries-with-gluten-9000b65d1b4e\nGithub: https://github.com/oap-project/gluten\nThe application of GenAI has huge potential to disrupt the customer support industry. Vimeo writes about indexing Zendesk\u2019s articles in a vector database to improve search performance and use ConversationalRetrievalQAChain to connect to the Vector store and LLM. \nhttps://medium.com/vimeo-engineering-blog/from-idea-to-reality-elevating-our-customer-support-through-generative-ai-101a2c5ea680\nSince 2019, the data mesh has woven itself into every blog post, event presentation, and webinar. But 4 years later, in 2023 \u2014 where has the data mesh gotten us? Does its promise of a decentralized dreamland hold true?Atlan is bringing together data leaders like Abhinav Sivasailam (CEO, Levers Labs), Barr Moses (Co-founder & CEO, Monte Carlo), Scott Hirleman (Founder & CEO, Data Mesh Understanding), Teresa Tung (Cloud First Chief Technologist, Accenture), Tristan Handy (Founder & CEO, dbt Labs), Prukalpa Sankar (Co-founder, Atlan), and more at the next edition of the Great Data Debate to discuss the state of data mesh \u2013 tech toolkit and cultural shift required to implement data mesh.\nLearn more and sign up to join the Great Data Debate on August 16 \u2192\nAn accurate delivery time estimation has become crucial in ensuring customer satisfaction and building trust in e-commerce platforms. OLX writes an exciting article discussing every step of a data science project, from defining the problem and success criteria to taking the solution to production and identifying the next steps.\nhttps://tech.olx.com/machine-learning-for-delivery-time-estimation-1-591c8df849a0\nContinue discussing the delivery time estimation problem, Swiggy writes about challenges in building a food delivery estimation service. The food delivery service is a multi-stage business process where each step has its feature to predict the delivery estimation. \nFood Preparation Features\nAssignment and First Mile features\nLast Mile Features\nhttps://bytes.swiggy.com/predicting-food-delivery-time-at-cart-cda23a84ba63\nInterested in learning how some of the best teams achieve data & AI reliability at scale? Join us for the third annual IMPACT - The Data Observability Summit. Hear from today's top data leaders and architects on how to build more trustworthy and reliable data & AI products with the latest technologies, processes, and strategies shaping our industry (yes, LLMs will be on the table).\nSave-the-date\nBreaking from the delivery estimation, Just Eat, a food delivery service writes about a step-by-step guide to building a listwise ranking TF recommender. Listwise ranking, often referred to within the context of Learning to Rank (L2R) in the machine learning domain, is an approach to ranking where entire lists of items are used as training examples rather than individual items or pairs of items. \nhttps://medium.com/justeattakeaway-tech/building-a-listwise-ranking-tf-recommender-a-step-by-step-guide-727e572860b8\nSports Analytics is an exciting domain, and I\u2019m sure many of our readers are interested. DraftKings writes an in-depth article about its Sports Intelligence team mission, a high-level architectural overview, and the data science software development lifecycle for MLOps.\nhttps://medium.com/draftkings-engineering/intro-sports-intelligence-draftkings-51c285bb3737\nMonday, August 7, join the RudderStack engineering team to learn the foundational principles of constructing a high-performance, scalable architecture for identity resolution using SQL. The team will give an overview of customer 360 architecture, detail the mechanics of using SQL for identity resolution in the warehouse, and demonstrate using declarative YAML to simplify the process.\nhttps://www.rudderstack.com/events/data-engineering-for-customer-360/\nOnboarding is a vital aspect that defines the engineering culture of a company. A best-in-class onboarding process is a reflection of inclusive and empathetic engineering culture. \nFaire writes about their onboarding process and course structure to better educate engineers and stakeholders to be familiar with data engineering.\nhttps://craft.faire.com/the-building-blocks-of-faires-data-team-onboarding-628229b043b6\nIncremental data processing & Time Travel are the core access patterns of Data Pipelines, and that is the foundation of transactional LakeHouse formats like Apache Hudi. However, I do think SQL is not fully reflecting the underlying pattern. The author narrates different types of queries with Apache Hudi is a good foundational understanding of thinking about LakeHouse access patterns. \nhttps://medium.com/@simpsons/different-query-types-with-apache-hudi-e14c2064cfd6\nDask is a flexible parallel computing library for analytics. As Python takes center stage in AI computing, reading about parallel processing engines like Dask is fascinating. The blog narrates the integration of Dask with Dagster for analytical pipeline design. \nhttps://medium.com/coiled-hq/dask-dagster-and-coiled-for-production-analysis-at-onlineapp-f22eb2573967\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-141", "title": "Data Engineering Weekly", "content": "RudderStack Profiles takes the SaaS guesswork and SQL grunt work out of building complete customer profiles, so you can quickly ship actionable, enriched data to every downstream team. See how it works today.\nGreat news! We've overcome some unexpected hiccups, and guess what? Conference registration is officially OPEN! \ud83c\udf89\nAlthough we're still working on linking the payment processing with our dewcon.ai domain, we don't want to keep you waiting any longer. So, let's not wait for that and get you registered right away!\nMark your calendars: DEWCon is happening on October 12th at the luxurious Taj Hotel on MG Road. \ud83c\udfe8\nAnd hold your breath for this one: Joe Reis, the author of \"The Fundamentals of Data Engineering,\" will be giving a captivating talk at our conference! \ud83c\udfa4 You definitely don't want to miss this!\nI can't contain my excitement; I know you can't either. So, let's meet in person at DEWCon and make this conference unforgettable! \ud83e\udd1d\nDon't waste any more time; hit that registration button now to secure your spot. See you there! \ud83d\ude03\ud83c\udf8a\nClick here to Register for DEWCon - 2023 \u2192\nLast week a sequence of blog posts revealed the underlying ambition of the data orchestration companies. \nThe first one that caught my eye is Astronomer published a  blog post Introducing Cosmos 1.0: the best way to run dbt Core in Airflow. The blog narrates how to schedule dbt jobs in Airflow by parsing dbt's manifest.json file, and auto construct Airflow tasks. \nThe second one is from Dagster published Orchestrating dbt\u2122 with Dagster, where Dagster narrates why Dagster provides a greater depth of orchestration features when compared to dbt-specific tools like dbt Cloud.\nAt Data Engineering Weekly, I wrote an article 15 months back narrating how dbt is not a separate model but part of the larger orchestration system. [See: Bundling Vs. Unbundling: The Tale of Airflow Operator and dbt Ref]. Since then,  I have wondered why dbt labs' product strategy is not focused on the orchestration engine in public. From a user point-of-view, dbt labs seem comfortable using other scheduling engines, well, not anymore. I noticed an article from dbt labs that was \"Seemingly easy, but in reality hard,\" highlighting the roadmap of the orchestration engine, yet I still need to get will it supports workloads other than dbt. \nMy mental model for the dbt workload is more similar to Spark's RDD [It is one of the reasons why I am excited about dbt-duckdb integration]. \nThe goal of the Spark RDD framework is to minimize the data shuffle, as the dbt can potentially minimize the number of database calls. \nThe orchestration layer is the key to making this reality, and I'm thrilled that more players in this space will provide a better orchestration engine than what we have now in the industry.\nHootsuite exactly captures the current state of orchestration dbt models with Airflow!!\nThe solution obviously a custom parser of dbt manifest.json \ud83d\ude0a\nhttps://medium.com/hootsuite-engineering/automating-dbt-airflow-9efac348059d\nSince we are at dbt, Zendesk writes about its dynamic staging layer and the auto-generation of the dbt model. One of the standard challenges in a data pipeline is de-duplication. The author narrates how Zendesk builds a generic configurable framework to de-duplicate.\nhttps://zendesk.engineering/dbt-at-zendesk-part-2-supercharging-dbt-with-dynamic-stage-4703a49d1c30\nSince 2019, the data mesh has woven itself into every blog post, event presentation, and webinar. But 4 years later, in 2023 \u2014 where has the data mesh gotten us? Does its promise of a decentralized dreamland hold true?Atlan is bringing together data leaders like Abhinav Sivasailam (CEO, Levers Labs), Barr Moses (Co-founder & CEO, Monte Carlo), Scott Hirleman (Founder & CEO, Data Mesh Understanding), Teresa Tung (Cloud First Chief Technologist, Accenture), Tristan Handy (Founder & CEO, dbt Labs), Prukalpa Sankar (Co-founder, Atlan), and more at the next edition of the Great Data Debate to discuss the state of data mesh \u2013 tech toolkit and cultural shift required to implement data mesh.\nLearn more and sign up to join the Great Data Debate on August 16 \u2192\nDuchDB writes about the support for Arrow Database Connectivity (ADBC) protocol support highlighting the performance benefit from ODBC. The main difference between ADBC vs. ODBC/JDBC is that the result does not need to be transformed into a row-wise format. The performance result is impressive for ADBC integration for DuckDB integration, with 38x% faster than ODBC. \nhttps://duckdb.org/2023/08/04/adbc.html\nThough the blog is not directly related to Data engineering, One of the key questions we always encounter in an organization. \nHow do you measure the impact of the Data Team in an organization?\nThe internal stakeholders are the largest user base for the data team, and the blog narrates how to measure the value of internal tools. \nhttps://developer.squareup.com/blog/how-to-measure-the-value-of-internal-tools/\nIn 2023, data observability is a must-have for companies seeking to reduce time, resources, and budget spent firefighting unreliable or anomalous data while unlocking new opportunities to cut costs and drive growth. Not sure where to start or what to look for in a data observability tool? Look no further than the Gartner latest report.\nAccess the Report\nIs Flink a better choice for streaming or Spark streaming? AWS writes an excellent blog comparing the features of Apache Flink and Apache Spark for streaming use cases. \nhttps://aws.amazon.com/blogs/big-data/a-side-by-side-comparison-of-apache-spark-and-apache-flink-for-common-streaming-use-cases/\nOperating a stateful application like Apache Kafka is no easy feat, and you need to automate as much as possible. Cruise Control from LinkedIn is one of my favorite tools for managing the Apache Kafka cluster. Red Hat narrates some useful automation to do with Cruise Control for Kafka cluster optimization. If you\u2019re managing Kafka in-house, I highly recommend using Cruise Control if you\u2019ve not already!!!\nhttps://developers.redhat.com/articles/2023/07/05/how-use-kafka-cruise-control-cluster-optimization\nIf your operations involve managing large-scale databases and you can dedicate resources for Kafka's administration and maintenance, then Kafka is your tool. RudderStack, on the other hand, is an excellent choice for businesses that are looking for a streamlined solution to collect, unify, and activate customer data from various sources to various destinations without Kafka's complexities.\nHere\u2019s an interesting breakdown of event streaming approaches that compare RudderStack to Apache Kafka. While it may be comparing apples and oranges, the two platforms can be used to achieve the same ends, and this piece provides a helpful framework to determine when it's appropriate to use each tool.\nhttps://www.rudderstack.com/blog/real-time-event-streaming-rudderstack-vs-apache-kafka/\nThe challenges in fraud detection arise from fraudsters continuously innovating their fraudulent behaviors, making it difficult for traditional supervised machine learning models to detect new patterns. Grab leans on an unsupervised learning model and writes about GraphBEAN, which can detect anomalous behaviors without the need for label supervision.\nhttps://engineering.grab.com/graph-anomaly-model\nIs a Graph a better data structure to model than a tabular model? I think so, so does the author. The customer journey is complex. Despite countless efforts to do so, very few have been able to successfully derive insights or predictions out of a sequence of behaviors. The blog talks about modeling customer journeys with graph node embeddings. \nhttps://medium.com/data-science-at-microsoft/decoding-the-customer-journey-with-graph-node-embeddings-74eb983e9847\nFuzzy matching is a complex process in a data pipeline. Intuit writes about its open-source Fuzzy-Matcher to find similarities in records.\nI think the impact of Vector Databases in similarity matching, especially in Master Data Management, is something I\u2019m excited about. Is anyone using Vector Database for your Data Similarities work? Curious to hear your architecture on this.\nhttps://medium.com/intuit-engineering/open-source-fuzzy-matcher-finding-data-similarities-in-records-33e4879ef4fd\nGithub: https://github.com/intuit/fuzzy-matcher\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-140", "title": "Data Engineering Weekly", "content": "RudderStack Profiles takes the SaaS guesswork and SQL grunt work out of building complete customer profiles, so you can quickly ship actionable, enriched data to every downstream team. See how it works today.\nHeads up, folks! \ud83d\udce3 The paper submissions phase for DewCon is now wrapped up! \ud83c\udf89 Thanks to each of you who has contributed your awesome proposals - you rock! \ud83d\ude4c\ud83d\udcaf Right now, we're sorting out a few snags with the registration payment system, but no stress! \ud83d\udeab\ud83d\udcbc We're burning the midnight oil to get this sorted ASAP \ud83c\udf19\ud83d\udcbb. Stay tuned for updates, and keep the excitement rolling! \ud83d\ude04\ud83c\udfa2\ud83d\ude80\nPresto, potentially ranking as one of the most influential open-source initiatives of the past ten years, stands shoulder to shoulder with the likes of Apache Kafka. With its pivotal role in the Big Data revolution, handling large datasets is so seamless that we've nearly forgotten the 'Big' in Big Data. Meta writes the story of the challenges of operating Presto at scale and its architectural choices. \nhttp://highscalability.com/blog/2023/7/16/lessons-learned-running-presto-at-meta-scale.html\nWe handle Petabytes of data!!! All our data is in S3!!!\nOh, just throw this data in S3 and possibly apply intelligent tiering. \nIf you\u2019re a. data engineer, you won\u2019t be surprised to hear this. S3 is often quoted as the backbone of the internet. The blog from All Things Distributed goes behind the scene on the challenges of operating S3 with the data placement strategy and managing machine heat and human factors. \nhttps://www.allthingsdistributed.com/2023/07/building-and-operating-a-pretty-big-storage-system.html\nBy Data & Data Engineering, we often associate with business operations and product analytics. But the power of the data pipeline to systematically measure things goes beyond business analytics. Slack writes about Service Delivery Index, a simple reliability data pipeline, and its impact on operating systems at scale. \nhttps://slack.engineering/service-delivery-index-a-driver-for-reliability/\nAt Data Engineering Weekly, We already discussed the importance of SPQR [Security, Performance, Quality, Reliability] metrics for operating systems at scale. In case you missed it, this is a must-watch talk\nSince 2019, the data mesh has woven itself into every blog post, event presentation, and webinar. But 4 years later, in 2023 \u2014 where has the data mesh gotten us? Does its promise of a decentralized dreamland hold true? Atlan is bringing together data leaders like Abhinav Sivasailam (CEO, Levers Labs), Barr Moses (Co-founder & CEO, Monte Carlo), Scott Hirleman (Founder & CEO, Data Mesh Understanding), Teresa Tung (Cloud First Chief Technologist, Accenture), Tristan Handy (Founder & CEO, dbt Labs), Prukalpa Sankar (Co-founder, Atlan), and more at the next edition of the Great Data Debate to discuss the state of data mesh \u2013 tech toolkit and cultural shift required to implement data mesh.\nLearn more and sign up to join the Great Data Debate on August 16 \u2192\nLambda and Kappa are two real-time data processing architectures. Lambda combines batch and real-time processing to efficiently handle large data volumes, while Kappa focuses solely on streaming processing. Kappa\u2019s simplicity offers better maintainability, but it poses challenges for implementing backfill mechanisms and ensuring data consistency, especially with out-of-order events.\nI\u2019m sure every data engineer ran into this problem at some point while looking to bridge the gap between real-time and batch processing. Airbnb writes about Riverbed - a Lambda-like data framework that abstracts the complexities of maintaining materialized views, enabling faster product iterations.\nhttps://medium.com/airbnb-engineering/riverbed-optimizing-data-access-at-airbnbs-scale-c37ecf6456d9\nAn efficient data access control significantly simplifies the data infrastructure and brings a seamless user experience. Pinterest writes about its design choices of designing an access control system by migrating from restricting access to data in S3 using dedicated service instances where different clusters of instances were granted access to specific datasets. The tiered dataset access to individual users is a much simpler and scalable solution from my recent experience designing access control systems. \nhttps://medium.com/pinterest-engineering/securely-scaling-big-data-access-controls-at-pinterest-bbc3406a1695\nIn 2023, data observability is a must-have for companies seeking to reduce time, resources, and budget spent firefighting unreliable or anomalous data while unlocking new opportunities to cut costs and drive growth. Not sure where to start or what to look for in a data observability tool? Look no further than the Gartner latest report.\nAccess the Report\nA couple of months back, AutoGPT - an AI agent that utilizes OpenAI's GPT-4 or GPT-3.5 APIs to execute tasks based on natural language goals autonomously, showed the power of LLM-powered autonomous agents. What are the steps and considerations to consider while building such autonomous AI agents? The author writes a complete guide to building LLM-powered autonomous agents. \nhttps://lilianweng.github.io/posts/2023-06-23-agent/\nIn essence, we can use pre-trained large language models for new tasks in two main ways: in-context learning and finetuning.\nThe blog is an excellent guide to thinking about finetuning methods, with three categorizations of approaches.\nFeature-Based Approach\nFinetuning I \u2013 Updating The Output Layers\nFinetuning II \u2013 Updating All Layers\nhttps://magazine.sebastianraschka.com/p/finetuning-large-language-models\nIf your operations involve managing large-scale databases and you can dedicate resources for Kafka's administration and maintenance, then Kafka is your tool. RudderStack, on the other hand, is an excellent choice for businesses that are looking for a streamlined solution to collect, unify, and activate customer data from various sources to various destinations without Kafka's complexities.\nHere\u2019s an interesting breakdown of event streaming approaches that compare RudderStack to Apache Kafka. While it may be comparing apples and oranges, the two platforms can be used to achieve the same ends, and this piece provides a helpful framework to determine when it's appropriate to use each tool.\nhttps://www.rudderstack.com/blog/real-time-event-streaming-rudderstack-vs-apache-kafka/\nThe promise of LLM in enterprise applications is immense, but many companies don\u2019t have a clear product and compliance strategy to adopt LLM in their architecture. The blog is an excellent checklist for companies to think about LLM and how to integrate it into the product and collaborative software development process. \nhttps://gradientflow.com/enterprise-generative-ai-unfolded/\nDo we need a separate data store for Vector storage? Can Vector Datastore simplify the entity resolutions? Can it democratize the recommendation engine? There are many potential possibilities and questions on vector databases. AWS writes a blog to extend these questions on demonstrating the role of vector data stores in Gen-AI applications.\nhttps://aws.amazon.com/blogs/database/the-role-of-vector-datastores-in-generative-ai-applications/\nThe world would be in a better place if all the computing problems could be solved in a single computing instance.\nThe author demonstrates the same, comparing DuckDB with other industry-leading data processing frameworks. However, comparing DuckDB with Spark or Elasticsearch is not exactly an Apple-to-Apple comparison since DuckDB is an in-memory data processing engine. In contrast, the like of Spark is designed to run in a massively parallel distributed data processing. \nDuckDB brings an exciting data architecture challenge to the industry. Do you really need massively parallel data processing engines all the time? Can you design your data architecture in such a way as to utilize in-memory data processing efficiently? \nhttps://medium.com/walmartglobaltech/duckdb-vs-the-titans-spark-elasticsearch-mongodb-a-comparative-study-in-performance-and-cost-5366b27d5aaa\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-139", "title": "Data Engineering Weekly", "content": "RudderStack Profiles takes the SaaS guesswork and SQL grunt work out of building complete customer profiles, so you can quickly ship actionable, enriched data to every downstream team. See how it works today.\nData Engineering Weekly is back in action after a week of summer break. I tried my best to write last week, but the travel took its toll. While on travel Aswin & I were busy preparing for DewCon. \nI'm happy to see the wide variety of talk proposals from people management, Gen AI, Machine Learning, modern data stack, and more!!!\nWe are extending the call for speakers for three more days!!! The last day to submit the proposal is now July 26th, at midnight EST!!! \nSubmit Your Conference Talk Proposal Here\nDeep Learning and Gen AI are speculated to impact software architecture greatly. Investing in AI also means reserving GPU computation capacity. \nSource: https://twitter.com/theinformation/status/1681639525125181442\nBut what features are important if you want to buy a new GPU? GPU RAM, cores, tensor cores, caches? How to make a cost-efficient choice? This blog post will delve into these questions, tackle common misconceptions, and give you an intuitive understanding of how to think about GPUs.\nhttps://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/\nData Mesh is a widely discussed and debated topic on the possibility of decentralized data management at scale. The author unpacks the federated design for data management in a phase-by-phase manner by adding more components as expanding.  \nhttps://piethein.medium.com/data-management-at-scale-91118a1a7d83\nData Contract is gaining traction in the industry as we witness implemented strategies published by the companies. The reference architecture published by Whatnot is probably the cleanest one I\u2019ve seen for Data Contracts.\nI heard a similar adoption success story for Schemata, which I was unaware that they have been using Schemata for a long time!!!  Overall an exciting time for a data contract in 2024. \nhttps://medium.com/whatnot-engineering/data-contracts-in-the-modern-data-stack-d42cb2442dbd\nHow do you create documentation for 1000s of data assets in minutes? Write SQL queries without learning SQL? Find the right, trusted data by simply asking a question. If you\u2019re searching for an answer, it\u2019s Atlan AI.Atlan AI leverages metadata that Atlan captures across the data stack to make AI part of your data stack. Now, you can get hours back by letting Atlan AI draft your documentation and write your queries. And you won\u2019t have to ask 3 different people in your team just to find the right, trusted data \u2014 you just need your AI copilot.\nWatch the Atlan AI demo and join the Atlan AI waitlist \u2192\nConverting between the user domain and document domain is the realm of prompt engineering\nGithub writes an excellent blog on a developer\u2019s guide to prompt engineering & LLM by walking through the prompt engineering pipeline for Github CoPilot. The blog classifies the pattern of prompt engineering from its experience building Github CoPilot. \nhttps://github.blog/2023-07-17-prompt-engineering-guide-generative-ai-llms/.\nThere is a growing list of companies banning ChatGPT, and Apple is the latest company to join the list. There is a growing concern about data governance, with information leakage considered a significant security leakage. The blog overviews potential information leakage and how to minimize it. \nhttps://www.thoughtworks.com/insights/blog/genai-practical-governance--how-to-reduce-information-security-r\nData trust is on every data team\u2019s mind, but how do you create and maintain it? To help answer those questions, we surveyed over 200 data teams to benchmark data product adoption rates and how data leaders can improve them. Access the guide today to see how your peers are boosting data adoption and maximizing the return on their data products.\nAccess the Report\nThere is a lot of conversation about engineering management for data, how undervalued data analysts are, and democratizing data access. However, we focused very little on implementing data science projects from a program management perspective. Kudos to the author; the blog is an excellent narration of technical program management for data science and its challenges. \nhttps://medium.com/expedia-group-tech/demystifying-technical-program-management-in-data-science-at-expedia-group-b068d1fd01b0\nWe always strive to simplify the ETL process with more abstraction to increase the velocity of data creation and the management process. Aibnb writes about one such declarative framework to simplify the feature engineering process.\nCombining data sources from batch, event stream, and service endpoints as an api is an excellent abstraction for acquiring the data sources.  \nhttps://medium.com/airbnb-engineering/chronon-a-declarative-feature-engineering-framework-b7b8ce796e04\nWith Profiles, you can create features using pre-defined projects in the user interface or a version-controlled config file, and all of the queries and computations are taken care of automatically.\nNow every team can build a customer360 in Snowflake with RudderStack Profiles. The new product is a data unification tool that handles the heavy lifting of identity resolution for you, streamlines user feature development, and automatically builds a customer 360 table so that you can ship high-impact projects faster.\nhttps://www.rudderstack.com/blog/introducing-rudderstack-profiles/\nIs one DAG to rule all, or multiple DAGs working together is a good approach? It is inevitable in Airflow to have multiple DAGs coordinate with each other. The blog talks about the pros and cons of cross-DAG dependency management options in Airflow, such as,\nTriggers\nSensors\nDatasets\nAPI\nhttps://medium.com/datamindedbe/cross-dag-dependencies-in-apache-airflow-a-comprehensive-guide-88cbc0bc68d0\nOneHouse recently announced OneTable, an interoperable format for all three major LakeHouse formats, such as Hudi, DeltaLake, and Iceberg. Databricks also announced UniForm, similar to OneTable. It is exciting to see LinkedIn arrive at a similar solution with the data vendors and write about OpenHouse API to bring interoperability among the LakeHouse formats. \nAre we reinventing Hive Metastore?!! \ud83d\ude0a\nhttps://engineering.linkedin.com/blog/2023/taking-charge-of-tables--introducing-openhouse-for-big-data-mana\nPinterest runs 130+ Flink jobs across eight multi-tenant production YARN clusters, and the largest Flink job is 2,000+ cores!!! The blog is an excellent overview of problems running multi-tenant data processing systems without CPU isolation.  The solution overview with container placement in YARN is an exciting read. \nhttps://medium.com/pinterest-engineering/tuning-flink-clusters-for-stability-and-efficiency-50d3d50384ed\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-138", "title": "Data Engineering Weekly", "content": "RudderStack Profiles takes the SaaS guesswork, and SQL grunt work out of building complete customer profiles, so you can quickly ship actionable, enriched data to every downstream team. See how it works today.\nWe announced the call for speakers only last week, and we are overwhelmed by the response. The call for speakers will be open until the 21st of July. We will start communicating with the selected speakers early next week, So stay tuned!!!\nSubmit Your Conference Talk Proposal Here\nA few of our friends asked me and Aswin what motivated us to launch the DewCon. In the last year, many talented data founders contacted me and discussed their startups and challenges. I even angel-invested in a couple of companies. At the same time, I noticed a forum missing that connects data practitioners, data founders, the decision makers of leading multinational companies, and venture capitalists. DewCon is an attempt to create a forum to facilitate a healthy collaboration from all parts of the data practitioners. \nWe are seeking sponsors for DewCon - 2023 edition to help lower the ticketing price for the data practitioners so that we can open as many developers to attend and participate in the forum. If you\u2019re interested in sponsoring DewCon, please get in touch with us by filling out the form. Every Premium conference speaker will get exclusive coverage in Data Engineering Weekly, so your reachability is not limited to one geographical area. \nHello DewCon Sponsors, Please Click Here and Enter Your Details. We will reach you ASAP.\nCongrats Andrew Jones for the brand new book on Data Contracts - The topic is very close to my heart. I briefly reviewed the book, and it looks like a solid one. I liked the chapter. It talks about how to get adoption in your organization, a sample implementation, and the contract-driven architecture. Thank you for the reference mention of Schemata in the book. \nhttps://andrew-jones.medium.com/data-contracts-the-book-out-now-f456f113dfa4\nIt is an exciting blog post + video interview from Capital One focusing on the people and technology aspect of democratizing the machine learning practice across the org. The platform approach to enable the citizen machine learning engineers is a great perspective while building both the Data & ML platform. \nhttps://medium.com/capital-one-tech/democratizing-machine-learning-5041f5605b67\nTime interval data processing is the foundation of data engineering; regardless it\u2019s batch or real-time.  Architectural patterns like Lambda Architecture and Kappa Architecture emerged to bridge the gap between real-time and batch data processing. Each architectural pattern has its limitation. The author discusses through limitations of these architectural patterns and walks us through to find if we can establish a  Quasi-Real-Time data warehouse. \nhttps://www.alibabacloud.com/blog/the-thinking-and-design-of-a-quasi-real-time-data-warehouse-with-stream-and-batch-integration_600147\nSeeing how many companies/open-source databases branch out of ClickHouse is amazing. ByteDance open sources ByConity, a scalable cloud-native data warehouse built on top of ClickHouse using FoundationDB for its metadata storage. The blog narrates the optimization and cluster architectural changes by ByteDance on top of ClickHouse. \nhttps://byconity.github.io/blog/2023-05-24-byconity-announcement-opensources-its-cloudnative-data-warehouse\nHow do you create documentation for 1000s of data assets in minutes? Write SQL queries without learning SQL? Find the right, trusted data by simply asking a question. If you\u2019re searching for an answer, it\u2019s Atlan AI. Atlan AI leverages metadata that Atlan captures across the data stack to make AI part of your data stack. Now, you can get hours back by letting Atlan AI draft your documentation and write your queries. And you won\u2019t have to ask 3 different people in your team just to find the right, trusted data \u2014 you just need your AI copilot.\nWatch the Atlan AI demo and join the Atlan AI waitlist \u2192\nCost is on everyone\u2019s top of mind, but achieving a $2M yearly savings is no easy engineering feat. RazerPay writes about its high-level data platform architecture and optimization strategies to reduce platform expenses. I like the simple principle behind the cost-saving mechanism.\nReduce, Remove, Replace and Reuse\nhttps://engineering.razorpay.com/reducing-data-platform-cost-by-2m-d8f82285c4ae\nI\u2019ve been thinking deeply about the impact of the Large Language Model in the data warehouse. Data Warehouses are fundamentally an information retrieval system where humans seek knowledge retrieval. So all the data modeling techniques are tuned toward the human understanding of the data and its relationship. LLM turns out pretty good at information retrieval than humans (in theory, as the promise holds bright). It made me wonder why can\u2019t we start data modeling so that machines can understand better, so we can build a better information retrieval system.\nThe author narrates a similar thought, examining the role of columnar lineage and data residency in the era of LLM. \nhttps://medium.com/@moving-the-needle/llms-columnar-lineage-data-residency-ae06f0418170\nData trust is on every data team\u2019s mind, but how do you create and maintain it? To help answer those questions, we surveyed over 200 data teams to benchmark data product adoption rates and how data leaders can improve them. Access the guide today to see how your peers are boosting data adoption and maximizing the return on their data products.\nAccess the Report\nContinue our conversation on LLM & Data analytics, Microsoft DataScience group starts with a perfect question.\nWhat if we could teach ChatGPT to leverage such tools and the thought process behind them to analyze problems within specific domains, particularly business analytics?\nThe author writes an agent as a data engineer and the data scientist with a prompt template to demonstrate automating the data analytics with ChatGPT\nhttps://medium.com/data-science-at-microsoft/automating-data-analytics-with-chatgpt-827a51eaa2c\nThe Ecosystem Discovery team at Square has shared their insights into effective website testing, focusing on their use of data-driven experimentation for optimization. Key learnings include creating a metric hierarchy and trade-off matrix for clearer decision-making, ensuring accurate segmentation of visitor data, and phasing A/B test traffic to manage risk. The team also emphasized the importance of internal collaboration, investment in automated testing, documentation of best practices, and constructively handling negative test results. \nhttps://developer.squareup.com/blog/lessons-learned-from-running-web-experiments/\nWith Profiles, you can create features using pre-defined projects in the user interface or a version-controlled config file, and all of the queries and computations are taken care of automatically.\nNow every team can build a customer360 in Snowflake with RudderStack Profiles. The new product is a data unification tool that handles the heavy lifting of identity resolution for you, streamlines user feature development, and automatically builds a customer 360 table so that you can ship high-impact projects faster.\nThe author explains a similar problem in Tabular/ Iceberg and the solution to work around it. TBH, I can\u2019t grasp the solution yet, since there are links to previous articles with more context. \nIf anyone understands how the \u201cMerge Into.\u201d works on Iceberg, Hudi & DeltaLake, please ping me; we love to get on our podcast and discuss it to spread the understanding. \nhttps://tabular.medium.com/the-cdc-merge-pattern-b6f8b564177a\nThe cross-AZ network traffic is often the most expensive operation in Kafka infrastructure. In the past, we reduced the producer's cost to ensure the producer writes to the brokers in the same AZ, but consumer reads introduce the cross-AZ network traffic. Kafka 2.3 introduced the ability for consumers to fetch from partition replicas. This opens the door to a more cost-efficient design. Grab writes about its Kafka Consumer design to overcome the cross-AZ network traffic and the impact of it. \nhttps://engineering.grab.com/zero-traffic-cost\nRegardless of the advancements in the LakeHouse systems, the expert in the loop in the form of a data engineer is inevitable. Adevinta writes about tuning databricks SQL workload by optimizing the storage layout, adopting DeltaLake format, and the effectiveness of the serverless SQL, small file compaction strategy, etc.,\nhttps://medium.com/adevinta-tech-blog/six-tried-and-tested-ways-to-turbocharge-databricks-sql-3a7f000586cb.\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions.\n"}, {"url": "https://www.dataengineeringweekly.com/p/dew-133-how-to-implement-write-audit", "title": "Data Engineering Weekly", "content": "Welcome to another episode of Data Engineering Weekly. Aswin and I select 3 to 4 articles from each edition of Data Engineering Weekly and discuss them from the author\u2019s and our perspectives.\nOn DEW #133, we selected the following article\nI wrote extensively about the WAP pattern in my latest article, An Engineering Guide to Data Quality - A Data Contract Perspective. Super excited to see a complete guide on implementing the WAP pattern in Iceberg, Hudi, and of course, with LakeFs.\nhttps://lakefs.io/blog/how-to-implement-write-audit-publish/\nStaying with the vector search, a new class of Vector Databases is emerging in the market to improve the semantic search experiences. The author writes an excellent introduction to vector databases and their applications.\nhttps://blog.devgenius.io/vector-database-concepts-and-examples-f73d7e683d3e\nData Testing and Data Observability are widely discussed topics in Data Engineering Weekly. However, both techniques test once the transformation task is completed. Can we test SQL business logic during the development phase itself? Perhaps unit test the pipeline?\nThe author writes an exciting article about adopting unit testing in the data pipeline by producing sample tables during the development. We will see more tools around the unit test framework for the data pipeline soon. I don\u2019t think testing data quality on all the PRs against the production database is not a cost-effective solution. We can do better than that, tbh.\nhttps://medium.com/policygenius-stories/data-warehouse-testing-strategies-for-better-data-quality-d5514f6a0dc9"}, {"url": "https://www.dataengineeringweekly.com/p/dew-132-the-new-generative-ai-infra", "title": "Data Engineering Weekly", "content": "Welcome to another episode of Data Engineering Weekly. Aswin and I select 3 to 4 articles from each edition of Data Engineering Weekly and discuss them from the author\u2019s and our perspectives.\nOn DEW #132, we selected the following article\nGenerative AI has taken the tech industry by storm. In Q1 2023, a whopping $1.7B was invested into gen AI startups. Cowboy ventures unbundle the various categories of Generative AI infra stack here.\nhttps://medium.com/cowboy-ventures/the-new-infra-stack-for-generative-ai-9db8f294dc3f\nEffective cost management in data engineering is crucial as it maximizes the value gained from data insights while minimizing expenses. It ensures sustainable and scalable data operations, fostering a balanced business growth path in the data-driven era. Coinbase writes one case about cost management for Databricks and how they use the open-source overwatch tool to manage Databrick\u2019s cost.\nhttps://www.coinbase.com/blog/databricks-cost-management-at-coinbase\nEntity resolution, a crucial process that identifies and links records representing the same entity across various data sources, is indispensable for generating powerful insights about relationships and identities. This process, often leveraging fuzzy matching techniques, not only enhances data quality but also facilitates nuanced decision-making by effectively managing relationships and tracking potential matches among data records. Walmart writes about the pros and cons of approaching fuzzy matching with rule-based and ML-based matching.\nhttps://medium.com/walmartglobaltech/exploring-an-entity-resolution-framework-across-various-use-cases-cb172632e4ae\nSo DuckDB, Is it hype? or does it have the real potential to bring architectural changes to the data warehouse? The author explains how DuckDB works and the potential impact of DuckDB in Data Engineering.\nhttps://mattpalmer.io/posts/whats-the-hype-duckdb/"}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-137", "title": "Data Engineering Weekly", "content": "RudderStack Profiles takes the SaaS guesswork, and SQL grunt work out of building complete customer profiles, so you can quickly ship actionable, enriched data to every downstream team. See how it works today. \nData Engineering is advancing rapidly, and it's not just about keeping up - it's about shaping the future! As part of this dynamic community, DEW thrilled to announce the Data Eng & AI conference to strengthen the community even stronger.\nThis conference isn't just about talks - it's about collaboration, learning, and leading the way. So, let's shape the future of Data Engineering together. Please send your talk proposal today, and let's collaborate to make this the most exciting, insightful, and interactive Data Eng & AI conference yet! Please watch out for this space for the conference registration page soon!!!\ud83c\udf1f\nSubmit Your Conference Talk Proposal Here\nData pipeline often requires multiple hop through the system to serve the end customers. LinkedIn write about Hoptimator for auto generated Flink pipeline with multiple stages of systems. Though the system not supporting any LinkedIn production ecosystem, it is an exciting open source project to watch. \nhttps://engineering.linkedin.com/blog/2023/declarative-data-pipelines-with-hoptimator\nThe blog narates the incremental update and the overall update strategy of a typical legacy datawarehouses. \nhttps://blog.devgenius.io/building-a-data-warehouse-for-traditional-industry-722513505c0c\nBlaBlaCar shares its lessons learned from 45 people team divided into six teams: 5 multidisciplinary squads and one platform-oriented team. I noticed a Twitter conversation debating whether the data team reports to the CFO and BlaBlaCar aligns with the CTO org. \nhttps://medium.com/blablacar/scaling-data-teams-5-learnings-from-blablacar-9e00949957f3\nHow do you create documentation for 1000s of data assets in minutes? Write SQL queries without learning SQL? Find the right, trusted data by simply asking a question. If you\u2019re searching for an answer, it\u2019s Atlan AI. \nAtlan AI leverages metadata that Atlan captures across the data stack to make AI part of your data stack. You can get hours back by letting Atlan AI draft your documentation and write your queries. And you won\u2019t have to ask 3 different people in your team just to find the right, trusted data \u2014 you just need your AI copilot.\nWatch the Atlan AI demo and join the Atlan AI waitlist \u2192\nThe article gave me a very interesting perspective, and I thought about how the oncall structure should be extra cautious during these business milestones. Aligning Data Pipeline operations with the business milestone dates is an interesting experiment to try at work for me.\nhttps://medium.com/@ranjayd/the-first-and-last-day-of-the-month-in-data-engineering-dcf39995d6bc\nOf all the LLM hype, I'm very excited about the emergence of vector databases. The author made an excellent attempt to describe what is vector databases to a 5-year-old. \nhttps://medium.com/geekculture/explain-like-im-5-vector-database-hype-bd936fd319ff\nVector databases are great, but it takes years to mature. Can't we use the vector feature in the existing databases? MongoDB Atlas recently announced the vector search capabilities for MongoDB. Similarly, the blog narrates how to use vector searching in Postgres using PGVector.\nhttps://www.timescale.com/blog/postgresql-as-a-vector-database-create-store-and-query-openai-embeddings-with-pgvector/\nMore data teams are putting data product managers at the helm of their most important data projects and assets. Should you? Our latest guide defines the roles and reponsibilities of data product managers, key differences between managers and engineers, how to measure success, and more.\nGet the Guide\nLyft writes about LyftLearn, its internal real-time machine learning platform. The blog narrates various components & challenges of building a real-time machine learning infrastructure along with the business case to support the real-time system.\nhttps://eng.lyft.com/building-real-time-machine-learning-foundations-at-lyft-6dd99b385a4e\nEnsemble learning is a machine learning concept where multiple models (often called \"base models\" or \"weak learners\") are trained to solve the same problem. The primary idea behind ensemble methods is that a group of weak learners can come together to form a strong learner.\nDoorDash writes about ELITE, an inhouse ensemble model for realtime forecasting to balance the cost, speed & correctness. \nhttps://doordash.engineering/2023/06/20/how-doordash-built-an-ensemble-learning-model-for-time-series-forecasting/\nWith Profiles, you can create features using pre-defined projects in the user interface or a version-controlled config file, and all of the queries and computations are taken care of automatically.\nNow, every team can build a customer360 in Snowflake (see our\u00a0Snowflake Integrations) with RudderStack Profiles. The new product is a data unification tool that handles the heavy lifting of identity resolution for you, streamlines user feature development, and automatically builds a customer 360 table so you can ship high-impact projects faster.\nThe ARIMA model is widely used in industry due to its versatility and interpretability. It adeptly handles diverse time series data, capturing trends and seasonality for reliable forecasting. Capital One writes about an introduction to ARIMA models for time series forecasting in Python with tips, best practices, and examples. \nhttps://www.capitalone.com/tech/machine-learning/arima-model-time-series-forecasting/\nThe usage of Machine Learning spread across all aspect of the application lifecycle. Spotify writes about its usage of machine learning to target in-app messages. The article remind me to re-read about the contextual bandit which is an excellent refresher. \nhttps://engineering.atspotify.com/2023/06/experimenting-with-machine-learning-to-target-in-app-messaging/\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions.\n"}, {"url": "https://www.dataengineeringweekly.com/p/the-week-of-data-conference-extravaganza", "title": "Data Engineering Weekly", "content": "I\u2019m at the Databricks summit as I\u2019m writing my thoughts on the Snowflake and Databricks announcements. It is my first visit to San Francisco after spending almost a decade here. My flight got 6 hours delayed (yep), and I had to sleep in Denver Airport at night to catch the connecting flight. But hey, I met my friends after a long time and got my copy of \u201cFundamentals of Data Engineering\u201d signed by Joe Reis & Matt Housely. If you\u2019re starting data engineering, I highly recommend reading it.\n There are many exciting announcements from both the data conferences, and I want to highlight a few themes here.\nDatabricks announced the support of Natural Language Query as part of LakeHouseIQ. In an episode of MLOps Weekly, Josh Wills talks about \u201cTaping the Shoulder problem,\u201d explaining how the analyst gets disrupted frequently with ad-hoc requests. The quest to simplify data access is there forever, but with the advancement in LLM, I think it will become a reality. Databricks and Snowflake are better places to index the data and its metadata to enable natural language query capabilities. \nThe obvious question follows, how do we know the foundation model gives the correct answer? Well, how do we know a human writes the SQL for an ad-hoc request, which often goes through a zero review process, wrote the correct SQL query? The Adhoc request often looks for guidelines or verifying a hypothesis. It aims to achieve a directionally correct answer in a short amount of time. It is still the early days of natural language query, but I\u2019m excited about the progress in this space. \nSnowflake is moving beyond a SQL data warehouse. Snowflake\u2019s Snowpark already supports running Java & Python code on its platform. It expanded its capabilities to include fully managed container service, MLOps, Feature Store, and Model serving. Snowflake adopted Iceberg as a LakeHouse format and announced tons of performance improvement in querying the Iceberg external table. I believe that a year from now, both Databricks & Snowflake features will look alike, if not already. \nDatabricks announcement around Unity Catalog is truly exciting. The Unity Catalog now supports the search and exploration of catalogs, as we know of the current modern data catalog tools. On top of it, it does support access control for queries and maintains the permission model. Unity Catalog is expanding its territory with access control but LLM-driven natural language query for your data and, most importantly, the data observability for your data on the Databricks platform.\nThe direction Unity Catalog is moving is certainly raising the bar for the Data Calaogs. A few data catalog companies have already announced their support for LLM-driven search, auto-generate documentation, etc. The question remains how far the data catalog tools can go with just the metadata. \nThough the product features for Snowflake and Databricks converge architecturally, Snowflake and Databricks are making different execution strategies. Snowflake is going after enabling app ecosystems where modern data stack can deploy their application, whereas Databricks is going after a fully integrated secure experience. \n The modern data stack has some truly novel ideas: Snowflake as a Unix-like platform and all the modern data stack piped together to bring the best of the world experience. However, the problem is every modern data stack company has started to claim that they are the control center aks the UNIX terminal of the modern data stack. It significantly increases the friction in integrating this system, which soon becomes a nightmare. It leads to the infamous Snowflake Tax, which drives users away from the platform or adds a limitation to Snowflake usage. Snowflake announced Snowflake Performance Index to optimize the performance and cost this week to reduce the impact. It will be interesting to see how far Snowflake will go with the App ecosystem model vs. the fully integrated data management platform.\nAll these announcements from Snowflake\u2019s container support and Databricks LakeHouseIQ require enormous computing capabilities, which is possible only with those cloud providers. I exclude Google Cloud since I rarely see Google Cloud users using either Snowflake or Databricks. \nSnowflake and Databricks are racing to build the data intelligence layer on top of the cloud infrastructure layer. The last time a similar competition happened was between Hortonworks vs. Cloudera. AWS EMR replicated the exact Hadoop layer and burned these two companies (combined). Today I was walking around the block, and I saw \ud83d\udc47\ud83c\udffc\nI wonder what Snowflake and Databricks learn from Cloudera and Hortonworks. \nMicrosoft recently announced a literal Databricks clone as Microsoft Fabric. AWS has some form of these features as AWS LakeFormation. Google Cloud is always a pioneer in the data intelligence layer. \nIn the race to LLM, the quest to simplify AI & Machine Learning, and competing products with access to money and human power, I think we are in a golden age of data innovation. I\u2019m excited to see how the data engineering domain changes in the next 24 months!!!! \nIf you follow Databricks & Snowflake conference and announcement, please comment on your favorite feature announcement from either of the companies. "}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-136", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nLLM is slowly hitting the enterprise architecture, and every company is looking to see how they can adopt LLM with their private data. A16Z published a reference architecture for the emerging LLM stack. The design focuses on a three-layer system design.\nData preprocessing & Embedding\nPrompt construction & retrieval\nPrompt execution & inference \nhttps://a16z.com/2023/06/20/emerging-architectures-for-llm-applications/\nEvery attention in the industry on ChatGPT, while the interactive experience is amazing, is that the future of human-to-computer interface? The author thinks not and lists thought-provoking points about how these interfaces work. \nThe highlight for me from my thinking from the article; ChatGPT can write code and make you a document, but who is responsible for the outcome? What should the human-machine interface look like if the human is responsible for an outcome? How much control human should I have? is chatbots are the right interface for this workflow?\nhttps://wattenberger.com/thoughts/boo-chatbots\nWe\u2019ve seen the rise of vector databases in the market. The cost, vendor locking, and the properties of the data storage engine are frequent points of discussion in the vector database space. The author argues why the data lakes should support vector format and how the LanceDB data format supports it. \nhttps://blog.lancedb.com/why-dataframe-libraries-need-to-understand-vector-embeddings-291343efd5c8\nRecently there were several announcements about new Large Language Models (LLMs) that can consume an extremely large context window, such as 65K tokens (MPT-7B-StoryWriter-65k+ by MosaicML) or even 100K tokens (Introducing 100K Context Windows by Antropic). What is the significance of these higher context windows? The author explains about context window in LLM, its limitations, computational complexities, and optimization techniques. \nhttps://blog.gopenai.com/how-to-speed-up-llms-and-use-100k-context-window-all-tricks-in-one-place-ffd40577b4c\nBuilding a data strategy from scratch? You're not alone! Join Monte Carlo, dbt, and Shiftkey's VP of Data & Analytics, John Steinmetz. on June 20th as he shares his experiences building the technologies, team structure, and KPIs necessary to align data and analytics success with larger business objectives.\nRegister Today\nVectorization has a wide variety of applications, but which method to use? How to measure the accuracy? Financial Times writes about evaluating the vectorization model for the following use cases. \nArticle Clustering\nBreadth of Readership\nArticle Recommendation\nTrending Topics\nhttps://medium.com/ft-product-technology/article-vectorisation-reloaded-391084f82549\nSwitching the gear from the world of LLMs to the data world, Eppo writes an excellent article on building and managing incremental pipelines at scale inspired by Dagster\u2019s asset model. The blog narrates the essence of the asset model below.\nSuppose that data asset C depends on B, and B depends on A. A planning phase may look like this:\nC is out of date and needs the most recent two days of data from B\nB is also out of date and needs the most recent two days of data from A\nA is out of date and will need the most recent two days of data from the customer-provided data source\nhttps://www.geteppo.com/blog/incremental-pipelines-managing-state-at-scale\n\"Data teams that successfully facilitate data activation do drive more business impact. They\u2019re also able to break out of order-taking purgatory and position themselves as strategic partners within the business.\"\nAn end-to-end look at\u00a0data activation\u00a0and all of the data engineering work that's required to make it happen. The RudderStack team highlights important details that can help you more efficiently and effectively facilitate data activation, including why you should think of data activation as a continuous cycle, not a linear process. https://www.rudderstack.com/blog/the-data-activation-lifecycle/\nI\u2019ve seen organizations adopting centralized analytics teams, switching to decentralization, and then back to centralization. Deciding between centralization or decentralization is usually not a permanent choice. The author explains how each state will look in an organization.\nhttps://xgumara.medium.com/navigating-the-spectrum-of-centralization-vs-decentralization-in-analytics-teams-eb6eb240917b\n\ud83d\udea8 Folks, if you\u2019re using Avro or Protobuf Timestamp data types for collecting events, especially the event timestamp, please don\u2019t. Use the UNIX timestamp with millisecond precision. \nIf this statement does not convince you, please read the hard lesson learned from Etsy.\nhttps://www.etsy.com/codeascraft/the-problem-with-timeseries-data-in-machine-learning-feature-systems\nKafka\u2019s exactly-once-delivery semantics triggers a few interesting conversations in the past. Debezium writes a blog about its exactly-once semantics support with Kafka and narrates the case of what will happen if a database connection breaks.\nhttps://debezium.io/blog/2023/06/22/towards-exactly-once-delivery/\nData Lakehouses are growing leaps and bounds from transaction supports and subsecond latency query capabilities to even supporting the vector data structure. The cost is an obvious concern as we add more data, and the author narrates how they are adopting Lake House cost-saving methods to save their storage cost by up to 78%. \nhttps://blog.xendit.engineer/optimizing-costs-of-a-data-lakehouse-3cb6777b2f94\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-135", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nDespite all the attention AI garners, some high-profile missteps have reminded the world once again of \u201cgarbage in, garbage out.\u201d If we ignore the underlying data management principles, then the output can\u2019t be trusted.\nI can\u2019t agree more on this. Data management is critical for any organization to succeed in this AI world. Sanjeev highlights three options for an organization to adopt with their private model. \nThe blog narrates LLM training options, Storage & retrieval, and the value chain to use LLM in your private data. \nhttps://sanjmo.medium.com/how-to-use-large-language-models-llms-on-private-data-a-data-strategy-guide-812cfd7c5c79\nAn exciting article from LinkedIn is about optimizing the Avro format reader & writer for efficient TensorFlow data processing. \nThe optimization around prefetching data with a separate thread, the decision not to support complex data types, and the complexity around Avro\u2019s sequential block read are informative to know more about Avro. \nhttps://engineering.linkedin.com/blog/2023/open-sourcing-avrotensordataset--a-performant-tensorflow-dataset\nThe quality and velocity in the experimentation truly will differentiate a product from its competitors. Spotify shares one such story of how it increases the velocity & quality of the experimentation on its home page. TIL, Spotify runs 250+ experimentation annually on its home page!!!\nhttps://engineering.atspotify.com/2023/06/experimenting-at-scale-the-spotify-home-way/\nMost often, hypothesis testing is done by calculating how much data is needed to reach a decision. This calculation must be done upfront and requires waiting until sufficient data has been collected. A major benefit of sequential testing is that it does allow for interim analyses while maintaining the correct alpha error rate.\nStaying on experimentation and AB testing, Booking.com narrates the types of sequential testing and its variations, especially focusing on Group Sequential Design. \nhttps://booking.ai/sequential-testing-at-booking-com-650954a569c7\nBuilding a data strategy from scratch? You're not alone! Join Monte Carlo, dbt, and Shiftkey's VP of Data & Analytics, John Steinmetz. on June 20th as he shares his experiences building the technologies, team structure, and KPIs necessary to align data and analytics success with larger business objectives.\nRegister Today\nOne of my favorite way to learn new systems is to tail its logs and starring at it for some time. Then start chasing where these logs are coming from the code; you know the system much better now. \nI\u2019m super thrilled to see the blog. I found fewer blogs focus on a technical deep-dive into the internals of dbt, and this one follows my favorite way of learning!!! Kudos to the author. \nhttps://leo-godin.medium.com/understanding-dbt-runtime-environment-1fd28592bbd\nPII detection & prevention plays a critical role in data governance. The blog narrates the journey of PII engineering from the discovery phase, creating policies around PII data, Snowflake roles hierarchy to prevent unauthorized access, and dbt macros to enable data masking.\nhttps://medium.com/voi-engineering/pii-data-privacy-in-snowflake-b523d38b02ff\nJoin RudderStack, Snowflake, and BlastX on Wednesday, 6/21, to learn how to set up a hybrid implementation to get the most out of Google Analytics 4. Plus, learn how to build web analytics on your data cloud to go beyond Google Analytics and answer harder questions.\nhttps://www.rudderstack.com/events/2-keys-to-overcoming-the-limitations-of-ga4/\nThe analytical databases, especially the one focusing on customer-facing applications, started to support multiple index types. It significantly increases the efficiency of the systems and provides more flexibility for faster delivery of analytical products. Apache Pinot is one such system that provides such capabilities, and thrilled to read about Apache Doris too. \nhttps://blog.devgenius.io/replacing-apache-hive-elasticsearch-and-postgresql-with-apache-doris-de3840cdc792\nOkay, I\u2019ve to admit. Eventually, I came to terms with JSON. It is inevitable in data processing whether we like it or not. Of late, I started to explore more JSON indexing, and I found an informative article from Cockroach DB on supporting forward indexes on JSON data and its challenges. \nhttps://www.cockroachlabs.com/blog/forward-indexes-on-json-columns/\nOkay, folks, I\u2019ve never seen any positive article about Redshift in the past. All I noticed was, \u201cOur migration journey from Redshift to <insert your db>. \ud83d\ude0a Is it a sympathetic selection? It could be. Also, the author explains Redshift architecture and various types of nodes and the key features. \nhttps://m.mage.ai/aws-redshift-robust-and-scalable-data-warehousing-4d0083904ca7\nThe first time hearing of migration to Greenplume, even though it is from MySQL. The modern data stack is dead indeed :-) \nHowever, what is interesting to me is that the migration challenges listed by the author are true for any one database to another. SQL, in theory, should be a standard, but unfortunately, that is not the case in the real world. \nhttps://medium.com/@skysum5/from-mysql-to-greenplum-lessons-and-insights-from-an-etl-pipeline-migration-journey-2a7a19ced4a5\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-134", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nAirbnb writes about its next-generation data management platform, \u201cMetis,\u201d with a modernized data catalog approach.\nThe architecture resembles any typical modern data catalog with search and integration features. The data quality score and per-column-based popularity measure are exciting usage metrics for streamlining data management. \nhttps://medium.com/airbnb-engineering/metis-building-airbnbs-next-generation-data-management-platform-d2c5219edf19\nThe adoption of the Lakehouse architecture increasing, but what format to choose? AWS writes a comprehensive analysis of all the Lakehouse formats and their support within the AWS ecosystem. \nhttps://aws.amazon.com/blogs/big-data/choosing-an-open-table-format-for-your-transactional-data-lake-on-aws/\nMostly thanks to AWS, which had mastered the craft of taking open-source infrastructure projects and building commercial services around them, many open-source projects exchanged their permissible licenses for \u201cCopyleft\u201d or \u201cShareAlike\u201d (SA) alternatives.\nThe recent frenzy around large language models (LLM) increases the dilemma around the open source vs. non-commercial use licensing model.  The author highlights the recent trend of increasing non-commercial & restrictive licenses. \nhttps://towardsdatascience.com/the-golden-age-of-open-source-in-ai-is-coming-to-an-end-7fd35a52b786\nExciting article on lessons learned from the pharma industry on building data infrastructure. Though some of the learning is specific to the pharma industry, we can also draw parallels to other industries. Satisfying regulatory requirements like Sox2, Hippa, and many enterprise orgs dream of Fedramp compliance requires much effort.  The author advocates avoiding the time-consuming regulatory process during the initial stages of the team by restricting data sourcing to its velocity. As an industry, we must build better tools to bring this compliance out of the box. \nhttps://achimplueckebaum.medium.com/unlocking-the-potential-of-data-lessons-learned-from-leading-data42-9726c035d05a\nMore data teams are putting data product managers at the helm of their most important data projects and assets. Should you? Our latest guide defines the roles and reponsibilities of data product managers, key differences between managers and engineers, how to measure success, and more. \nGet the Guide\nI\u2019m experimenting with a few vector databases and found this article provides an interesting perspective. \nThe current landscape demonstrates a dedicated vector database vs. exciting databases that support vector search. \nOne thing to note here, search infrastructure is always separate data storage from the operational data store in any org. The newer vector databases have a great chance of success if they can simplify the operational burden and maximize the out-of-the-box feature. \nhttps://medium.com/data-engineer-things/why-you-shouldnt-invest-in-vector-databases-c0cd3f59d23c\nData Ownership is a hard problem to solve. As the organization grows, the complexity increases, and so does the ownership. A typical application can have 6 to 8 different code paths managed by different teams that can lead to user sign-up. All can emit the same event schema [assuming we successfully standardize schema, that is another hard problem]; now, who owns the sign-up event?\nThe recent poll mentioned in the article demonstrates the same. The author demonstrates why data contract is a hard problem and how we can use dbt to bring data contract to the data engineering teams.\nhttps://medium.com/@mikldd/activating-ownership-with-data-contracts-in-dbt-4f2de41c4657\nLike the composable approach, The Warehouse Native CDP solves the data silo problem by building around the warehouse, but it deploys the integration, real-time transformation, and activation layers as a connected, governable, and observable end-to-end system.\nHere, the RudderStack team examines prevailing approaches to the Customer Data Platform, including the recent composable CDP movement, and introduces a new approach that they believe best delivers the end goal \u2014 easy activation of complete customer profiles.\nhttps://www.rudderstack.com/blog/evolution-customer-data-platform/\nA must-read article if you are into the operational efficiency of streaming data pipelines. The author narrates the optimization process starting with the right questions, and drills down further on each profiling and performance optimization aspect. \nIs there a bottleneck?\nIs the pipeline performing optimally?\nWill it continue to scale with increased load?\nThe questions are the founding block for any system optimization.\nhttps://eng.lyft.com/gotchas-of-streaming-pipelines-profiling-performance-improvements-301439f46412\nLLM is certainly the town's talk, but it comes with its limitation.\nKnowledge cutoff [ChatGPT has a \"knowledge cutoff\" in 2021, which means it knows nothing about the events of 2022 and 2023]\nHallucinations [LLMs train to deliver results in an authentic tone, even if the result can be wrong]\nThe lack of user customization\nThe blog narrates Fine-Tuning and Retrieval-Augmented Generation, two techniques to bridge these gaps.\nhttps://medium.com/neo4j/knowledge-graphs-llms-fine-tuning-vs-retrieval-augmented-generation-30e875d63a35\nThe direct cross-site tracking of user activity brings more harm to the consumer than good. We have seen the browsers and mobile operating systems preventing third-party tracking. Criteo writes about the attribution & reporting proposal by various browsers that leverages Privacy Enhancing Technology [PET].\nhttps://medium.com/criteo-engineering/pets-for-attribution-and-reporting-key-insights-to-take-away-af17d985cdf2\nDatabricks recently announced the extension of its UnityCatalog to Hive Metastore API, which brings an interesting discussion in the open vs. portable article. The author narrates the difference between open and portable systems. \nThe larger question is, at what point does a commercial company build a portable system? The answer is obviously to capture an exciting, successful open-source framework. It is true for Kafka protocol, Hive metastore, and many others. Open is the first step to bringing portable systems into the industry.\nhttps://medium.com/@mbhide/open-vs-portable-c1beda131ab6\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/dew-131-dbt-model-contract-instacart", "title": "Data Engineering Weekly", "content": "Welcome to another episode of Data Engineering Weekly. Aswin and I select 3 to 4 articles from each edition of Data Engineering Weekly and discuss them from the author\u2019s and our perspectives.\nOn DEW #131, we selected the following article\ndbt introduces model contract with 1.5 release. There were a few critics of the dbt model implementation, such as The False Promise of dbt Contracts. I found the argument made in the false promise of the dbt contract surprising, especially the below comments.\nAs a model owner, if I change the columns or types in the SQL, it's usually intentional. - My immediate no reaction was, Hmm, Not really.\nHowever, as with any initial system iteration, the dbt model contract implementation has pros and cons. I\u2019m sure it will evolve as the adoption increases. The author did an amazing job writing a balanced view of dbt model contract.\nhttps://medium.com/geekculture/dbt-model-contracts-importance-and-pitfalls-20b113358ad7\nInstacart writes about its journey of building its ads measurement platform. A couple of thing stands out for me in the blog.\nThe Event store is moving from S3/ parquet storage to DeltaLake storage\u2014a sign of LakeHouse format adoption across the board.\nInstacart adoption of Databricks ecosystem along with Snowflake.\nThe move to rewrite SQL into a composable Spark SQL pipeline for better readability and testing.\nhttps://tech.instacart.com/how-instacart-ads-modularized-data-pipelines-with-lakehouse-architecture-and-spark-e9863e28488d\nThe blog is an excellent overview of server-side event tracking. The author highlights how the event tracking is always close to the UI flow than the business flow and all the possible things wrong with frontend event tracking. A must-read article if you\u2019re passionate about event tracking like me.\nThis Schema change could\u2019ve been a JIRA ticket!!!\nI found the article excellent workflow automation on top of the familiar ticketing system, JIRA. The blog narrates the challenges with Glue Crawler and how selectively applying the db changes management using JIRA help to overcome its technical debt of running 6+ hours custom crawler.\nhttps://medium.com/credit-saison-india/using-jira-to-automate-updations-and-additions-of-glue-tables-58d39adf9940"}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-133", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\n\u201cThe metrics you choose as a measure of success inherently shape the essence of the company you aspire to build\u201d - A Data Practitioner. \nSometimes, the metrics you don\u2019t also have the same impact. Do you need data/ metrics at all? Should the metrics define how you operate your business? An interesting take on operating without metrics. \nhttps://world.hey.com/dhh/the-luxury-of-working-without-metrics-02e5dbac\nI wrote extensively about the WAP pattern in my latest article, An Engineering Guide to Data Quality - A Data Contract Perspective. Super excited to see a complete guide on implementing the WAP pattern in Iceberg, Hudi, and of course, with LakeFs. \nhttps://lakefs.io/blog/how-to-implement-write-audit-publish/\nOne of the challenges in commoditizing data processing engines like Spark is that it requires an expert user to understand and operate this system. Uber writes about automating the detection of Spark anti-patterns to educate the users better and help reduce their processing costs. \nhttps://www.uber.com/en-US/blog/spark-analysers-catching-anti-patterns-in-spark-apps/\nData Testing and Data Observability are widely discussed topics in Data Engineering Weekly. However, both techniques test once the transformation task is completed. Can we test SQL business logic during the development phase itself? Perhaps unit test the pipeline? \nThe author writes an exciting article about adopting unit testing in the data pipeline by producing sample tables during the development. We will see more tools around the unit test framework for the data pipeline soon. I don\u2019t think testing data quality on all the PRs against the production database is not a cost-effective solution. We can do better than that, tbh. \nhttps://medium.com/policygenius-stories/data-warehouse-testing-strategies-for-better-data-quality-d5514f6a0dc9\nData trust is at an all-time low, and teams are feeling the pain. Our latest report highlights the impact of bad data on your bottom line (did you know that poor data quality impacts 31% of revenue?!) and how the best teams are reducing incident resolution times.\nAccess the Report\nLast month or so, I experimented with vector search with embedding. I found the article that compares search efficiency among ElasticSearch, Azure Search, and custom vector search tables in DeltaLake and Apache Hudi interesting.\nhttps://www.linkedin.com/pulse/text-based-search-from-elastic-vector-kaushik-muniandi/\nStaying with the vector search, a new class of Vector Databases is emerging in the market to improve the semantic search experiences. The author writes an excellent introduction to vector databases and their applications.\nhttps://blog.devgenius.io/vector-database-concepts-and-examples-f73d7e683d3e\nSchema management at the data ingestion service and the DLQ (Dead Letter Queue) pattern is emerging as the standard architecture pattern in event processing. Koho writes about its architecture to handle DLQ and schema management.\nhttps://koho.dev/handling-schema-evolution-in-the-data-pipelines-at-koho-314472111477\nLike the composable approach, The Warehouse Native CDP solves the data silo problem by building around the warehouse, but it deploys the integration, real-time transformation, and activation layers as a connected, governable, and observable end-to-end system.\nHere, the RudderStack team examines prevailing approaches to the Customer Data Platform, including the recent composable CDP movement, and introduces a new approach that they believe best delivers the end goal \u2014 easy activation of complete customer profiles.\nhttps://www.rudderstack.com/blog/evolution-customer-data-platform/\nA query engine can leverage the expressivity of SQL over structured data and join it with unstructured context from a vector database - The future of data processing.\nMany of the real-world data, all the way from medical images to astro monitoring, are unstructured data. The future we wish to live in is the promise of vector search combined with SQL to access structured information. \nhttps://medium.com/llamaindex-blog/combining-text-to-sql-with-semantic-search-for-retrieval-augmented-generation-c60af30ec3b\nGrab writes about the shift-left approach, where the data producer tags the PII information carried over the downstream processing system to take action for it. \nI\u2019m thrilled to see this approach reflecting the vision of Schemata with its Protobuf definition. The Schemata Open Contract definition has a comprehensive coverage of PII & Data Classification types here\nhttps://github.com/ananthdurai/schemata/blob/main/src/opencontract/v1/org/schemata/protobuf/schemata.proto#L87\nhttps://engineering.grab.com/pii-masking\nThe PII masking is part of the privacy-enhancing technique. As a data practitioner, it is essential to understand privacy-enhancing technologies. The author writes a comprehensive guide about PET.  \nhttps://martinfowler.com/articles/intro-pet.html\nI\u2019ve not tried PandasAI, but the promise of it looks exciting. \ndata = pd.read_csv('dataset.csv')\ndata_cleaned = pdai.impute_missing_values(data)\nIf the data cleaning is as simple as calling a function, working with data will be a delight. \nhttps://levelup.gitconnected.com/introducing-pandasai-the-generative-ai-python-library-568a971af014\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-132", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make collecting data from every application, website, and SaaS platform easy, then activating it in your warehouse and business tools. Sign up free to test out the tool today.\nDEW has been recognized as the number one individually run data newsletter in the industry, according to the latest AirByte poll! I am truly humbled and overwhelmed by your continuous support and encouragement. \nOver the last six months, many readers contacted me and asked about growing in their career as a data engineer or how to switch to data engineering. More qualified people than me are here to write and guide our audience. If you want to write a career guidance series for Data Engineering Weekly, Please DM me on LinkedIn. I'm more than happy to collaborate and help the community. \nOver the weekend, I found this an excellent thread on how Slack uses Kafka; I want to highlight this one piece.\nI\u2019m thrilled to see this and the design choices we made at that time still echoing. The fundamental design principle that made possible the cut-over operational model is\nThe producer [Murron] builds with dynamic routing capabilities to reroute traffic to multiple dest.\nTreat Kafka as immutable infra\nAdopt multi-instance over multi-tenant\nWe discussed Murron's design in detail here if anyone wants to know more about it. \nGenerative AI has taken the tech industry by storm. In Q1 2023, a whopping $1.7B was invested into gen AI startups. Cowboy ventures unbundle the various categories of Generative AI infra stack here. \nhttps://medium.com/cowboy-ventures/the-new-infra-stack-for-generative-ai-9db8f294dc3f\nContinue to focus on LLM; every company in the world is trying to find how LLM fit into their product offering and user experience. HoneyComb writes an excellent article from a developer perspective showing the hard part of integrating LLM into a product experience. \nhttps://www.honeycomb.io/blog/hard-stuff-nobody-talks-about-llm\nEffective cost management in data engineering is crucial as it maximizes the value gained from data insights while minimizing expenses. It ensures sustainable and scalable data operations, fostering a balanced business growth path in the data-driven era. Coinbase writes one case about cost management for Databricks and how they use the open-source overwatch tool to manage Databrick\u2019s cost. \nhttps://www.coinbase.com/blog/databricks-cost-management-at-coinbase\nEntity resolution, a crucial process that identifies and links records representing the same entity across various data sources, is indispensable for generating powerful insights about relationships and identities. This process, often leveraging fuzzy matching techniques, not only enhances data quality but also facilitates nuanced decision-making by effectively managing relationships and tracking potential matches among data records. Walmart writes about the pros and cons of approaching fuzzy matching with rule-based and ML-based matching.\nhttps://medium.com/walmartglobaltech/exploring-an-entity-resolution-framework-across-various-use-cases-cb172632e4ae\nAs data leaders, one of our top priorities is to measure ROI. From tracking the efficacy of marketing campaigns to understanding the root cause of new spikes in user engagement, we\u2019re tasked with keeping tabs on the business's health at all levels. But what about the ROI of our own teams? Watch a panel of data leaders as they discuss how to build strategies for measuring data team ROI.Watch On-demand\nEvery new technology starts somewhere small in adoption and grows over time, especially complex systems like Zendesk. It is exciting to see the case study of dbt coming out from Zendesk, focusing on foundations and scalability. \nhttps://zendesk.engineering/dbt-at-zendesk-part-i-setting-foundations-for-scalability-34b55e6a6aa1\nEvent Collection at scale brings its challenges. Instawork writes about its in-house solution for event logging systems. The blog narrates the working on the event collector, writing to Kafka, and the S3 storage with Lambda triggers. \nhttps://engineering.instawork.com/unlocking-the-power-of-data-how-we-scaled-our-analytics-with-an-in-house-event-logging-platform-520d5b58f651\nGet this email recently? Love it or hate it, GA4 is a fact of life for many of us. Getting the most out of the tool requires a hybrid implementation to capture data server-side and client-side, but the tools Google provides make this setup complicated and unfulfilling. That\u2019s why RudderStack built a hybrid deployment option for their GA4 integration. It\u2019s a single-step deployment that makes capturing all the data you need for attribution easy while ensuring optimal site performance and ad blocker resiliency.\nLearn how to implement GA4 for ad blocker resilience and accurate attribution.\nAs a community, I was always concerned that we seldom discussed designing data engineering for regulatory requirements. I'm glad to see the article where the author explains what is Right to Be Forgotten (RTBF) and discusses the architectural pattern in Google BigQuery. \nhttps://medium.com/@florian.trehaut/ensuring-gdpr-compliance-on-gcp-bigquery-efficiently-managing-the-right-to-be-forgotten-a76137944633\nSo DuckDB, Is it hype? or does it have the real potential to bring architectural changes to the data warehouse? The author explains how DuckDB works and the potential impact of DuckDB in Data Engineering. \nhttps://mattpalmer.io/posts/whats-the-hype-duckdb/\nIf I put on a purist Data Engineer hat, The Data Orchestration, Data Lineage, Data Testing, and Data Catalogs all of them should be one system. They are not a separate category. \nI\u2019m glad to read the take on the orchestration engine expressing similar thoughts and questioning why there is little innovation in the orchestration space.\nhttps://medium.com/@hugolu87/why-orchestration-is-the-next-hot-thing-in-data-69bc32402446\nAdopting technology is not only about the individual tool; you need an ecosystem of supporting tools. The author writes about such an ecosystem of tools for dbt, narrating the usage of\ndbt-project-evaluator\nsql-fluff\npre-commit-dbt\ndbt-coverage\nPR Template\nhttps://medium.com/cts-technologies/crafting-your-dbt-development-workflow-35577d3b573d\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/dew-129-doordashs-generative-ai-europe", "title": "Data Engineering Weekly", "content": "Welcome to another episode of Data Engineering Weekly. Aswin and I select 3 to 4 articles from each edition of Data Engineering Weekly and discuss them from the author\u2019s and our perspectives.\nOn DEW #129, we selected the following article\nGenerative AI took the industry by storm, and every company is trying to figure out what it means to them. DoorDash writes about its discovery of Generative AI and its application to boost its business.\nThe assistance of customers in completing tasks\nBetter tailored and interactive discovery [Recommendation]\nGeneration of personalized content and merchandising\nExtraction of structured information\nEnhancement of employee productivity\nhttps://doordash.engineering/2023/04/26/doordash-identifies-five-big-areas-for-using-generative-ai/\nFascinating findings on Europe\u2019s data salary among various countries. The key findings are\nGerman-based roles pay lower.\nLondon and Dublin-based roles have the highest compensations. The Dublin sample is skewed to more senior roles, with 55% of reported salaries being senior, which is more indicative of the sample than jobs in Dublin paying higher than in London.\nThe top 75% percentile jobs in Amsterdam, London, and Dublin pay nearly 50% more than those in Berlin\nhttps://medium.com/@mikldd/europe-data-salary-benchmark-2023-b68cea57923d\nThe article by Trivago discusses the integration of data validation with Great Expectations. It presents a well-balanced case study that emphasizes the significance of data validation and the necessity for sophisticated statistical validation methods.\nhttps://tech.trivago.com/post/2023-04-25-implementing-data-validation-with-great-expectations-in-hybrid-environments.html\n\u201cEvents as a source of truth\u201d is a simple but powerful idea to persist the state of the business entity as a sequence of state-changing events. How to build such a system? Expedia writes about the review stream system to demonstrate how it adopted the event-first approach.\nhttps://medium.com/expedia-group-tech/how-expedia-reviews-engineering-is-using-event-streams-as-a-source-of-truth-d3df616cccd8"}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-131", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make collecting data from every application, website, and SaaS platform easy, then activating it in your warehouse and business tools. Sign up free to test out the tool today.\ndbt introduces model contract with 1.5 release. There were a few critics of the dbt model implementation, such as The False Promise of dbt Contracts. I found the argument made in  the false promise of the dbt contract surprising, especially the below comments.\nAs a model owner, if I change the columns or types in the SQL, it's usually intentional. - My immediate no reaction was, Hmm, Not really. \nHowever, as with any initial system iteration, the dbt model contract implementation has pros and cons. I\u2019m sure it will evolve as the adoption increases. The author did an amazing job writing a balanced view of dbt model contract.\nhttps://medium.com/geekculture/dbt-model-contracts-importance-and-pitfalls-20b113358ad7\nInstacart writes about its journey of building its ads measurement platform. A couple of thing stands out for me in the blog.\nThe Event store is moving from S3/ parquet storage to DeltaLake storage\u2014a sign of LakeHouse format adoption across the board.\nInstacart adoption of Databricks ecosystem along with Snowflake.\nThe move to rewrite SQL into a composable Spark SQL pipeline for better readability and testing. \nhttps://tech.instacart.com/how-instacart-ads-modularized-data-pipelines-with-lakehouse-architecture-and-spark-e9863e28488d\nThe blog is an excellent overview of server-side event tracking. The author highlights how the event tracking is always close to the UI flow than the business flow and all the possible things wrong with frontend event tracking. A must-read article if you\u2019re passionate about event tracking like me. \nhttps://hipsterdatastack.substack.com/p/the-extensive-guide-for-server-side\nWe started to see an increasing pattern of minimizing the data infrastructure and standardization of the data platform. I can also see a wave of migration from data warehouses to LakeHouse formats. Compass writes one such case to move from disparate data infrastructure to Databricks.\nhttps://medium.com/compass-true-north/enterprise-data-platform-compass-4f96eeec1894\nAs data leaders, one of our top priorities is to measure ROI. From tracking the efficacy of marketing campaigns to understanding the root cause of new spikes in user engagement, we\u2019re tasked with keeping tabs on the business's health at all levels. But what about the ROI of our own teams? Watch a panel of data leaders as they discuss how to build strategies for measuring data team ROI.Watch On-demand\nBuilding and serving customer-facing analytics brings a full-stack data engineering complexity. Whatnot writes about its end-to-end seller analytical dashboard pipeline flow with dbt, Rockset, and Snowflake.\nhttps://medium.com/whatnot-engineering/building-the-seller-analytics-dashboard-ccffd2a0151a\nChurn prediction and simulation is a vital part of business operation, and Pinterest writes about an ML approach to predict advertiser churn and prevention. The blog narrates the choice of Gradient Boosting Decision Tree (GBDT) architecture and the use of the SHAP library to estimate the feature contribution to model probability output.\nhttps://medium.com/pinterest-engineering/an-ml-based-approach-to-proactive-advertiser-churn-prevention-3a7c0c335016\nGet this email recently? Love it or hate it, GA4 is a fact of life for many of us. Getting the most out of the tool requires a hybrid implementation to capture data server-side and client-side, but the tools Google provides make this setup complicated and unfulfilling. That\u2019s why RudderStack built a hybrid deployment option for their GA4 integration. It\u2019s a single-step deployment that makes it easy to capture all the data you need for attribution while ensuring optimal site performance and ad blocker resiliency.\nLearn how to implement GA4 for ad blocker resilience and accurate attribution.\nGraph analytics have wide and exciting applications in data analytics. Representing analytical data in graph vs. relational data structure is a quest for many. Microsoft writes about customer journey analytics using Neo4j.\nhttps://medium.com/data-science-at-microsoft/using-graphs-to-model-and-analyze-the-customer-journey-4b1f1e9f3696\nMachine learning model development without a structured process is like trying to assemble Ikea furniture in the dark \u2013 prepare for chaos, confusion, and possibly a few extra screws! Walmart writes about how not to get into those chaos and best practices using open-source tools.\nhttps://medium.com/walmartglobaltech/rapid-reliable-ml-experiments-using-mlops-best-practices-7f01e563cb3e\nThere were a few interesting comments on BigQuery about how best tech it is and its worst marketing. Reddit writes about monitoring the slot utilization-based query scheduler in real-time to address the consumers pressing question; why my query is slow!!!\nhttps://www.reddit.com/r/RedditEng/comments/13iat74/wrangling_bigquery_at_reddit/\nStaying on monitoring and observability of the data pipeline, LMWN writes about monitoring the Apache Spark pipeline at scale. The blog narrates the overview of its batch pipeline, challenges, and monitoring techniques for observing spark data pipelines.\nhttps://medium.com/@artthananstr/how-we-monitor-thousands-of-spark-data-pipelines-a918c7c7916a\nThis Schema change could\u2019ve been a JIRA ticket!!!\nI found the article excellent workflow automation on top of the familiar ticketing system, JIRA.  The blog narrates the challenges with Glue Crawler and how selectively applying the db changes management using JIRA help to overcome its technical debt of running 6+ hours custom crawler.\nhttps://medium.com/credit-saison-india/using-jira-to-automate-updations-and-additions-of-glue-tables-58d39adf9940\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/an-engineering-guide-to-data-quality", "title": "Data Engineering Weekly", "content": "In the first part of this series, we talked about design patterns for data creation and the pros & cons of each system from the data contract perspective. In the second part, we will focus on architectural patterns to implement data quality from a data contract perspective.\nI posted this LinkedIn post that sparked some exciting conversation. I won\u2019t bore you with the importance of data quality in the blog. Instead, Let\u2019s examine the current data pipeline architecture and ask why data quality is expensive. \nInstead of looking at the implementation of the data quality frameworks, Let's examine the architectural patterns of the data pipeline. Those patterns will slowly reveal the answer to our burning question. But before doing that, let's revisit some of the basic theories of the data pipeline. \nJust like the CAP theorem, there's a balance to be struck between speed, correctness, and Time in a data pipeline. You can prioritize either speed or correctness, but not both simultaneously.\nWhy I\u2019m making this claim? Ensuring correctness can slow down the pipeline. It involves thorough checks and balances, including data validation, error detection, and possibly manual review. The bias toward correctness will increase the processing time, which may not be feasible when speed is a priority.\nData testing and data observability are two important aspects of data quality. Data testing ensures that data meets specific requirements. Data observability monitors data to identify and troubleshoot issues.\nData testing and data observability are complementary approaches to data quality. Data testing helps to prevent problems from occurring, while data observability helps to identify and troubleshoot problems that do occur.\nNow we understand the trade-off between speed & correctness and the difference between data testing and observability. Let\u2019s talk about the data processing types. The industry follows two categories of data processing techniques to handle the trade-off between speed and correctness.\nReal-Time Data processing\nBatch Data Processing\nThough there are two categories of data processing techniques, they follow the same pattern for the data quality architecture. We call this pattern as WAP [Write-Audit-Publish] Pattern.\nThe WAP pattern follows a three-step process\nThe write phase results from a data ingestion or data transformation step. In the 'Write' stage, we capture the computed data in a log or a staging area. \nThe 'Audit' phase follows, where we audit the data changes against specific rules or conditions. The audit process can include a variety of checks, such as schema compatibility, data constraints, and other forms of data contract.\nPublish is the final stage, assuming the Audit phase successfully evaluates the data contract. The system can fail or apply a circuit breaker in the pipeline if the data contract fails. \nThere are two types of WAP pattern implementation in the industry. Let\u2019s dive into both architectural patterns and see how to adopt them in real-time and batch data processing. \nThe Two-Phase WAP, as the name suggests, follows two copy processes. \n\nThe Write phase writes the data into a staging environment. \nThe Audit step then kicks in, which runs through all the data contract checks.\n If the Audit step succeeds, the data in the staging environment gets copied to the production. The staging environment then gets cleaned up as part of the pipeline or as a scheduled clean-up job.\nThe Two-Phase WAP pattern follows the \u201cFronting Queue\u201d pattern. Since Kafka is almost synonymous with real-time data processing, we often call this a \u201cFronting Kafka\u201d pattern.\nThe Fronting Kafka pattern follows a two-cluster approach.\nLike the staging environment, Fronting Kafka receives all the events without validation.\nA streaming consumer, often implemented in stream processing frameworks like Flink or Spark, consumes the events from the fronting Kafka and runs through data contract validation.\nNetflix\u2019s Data Mesh is a classic example Two-Phase Fronting Kafka pattern implementation.\nThe One-Phase WAP adopts a Zero-Copy data contract validation approach.\nThe One-Phase WAP is structurally similar to the Two-Phase WAP, except in the One-Phase WAP, there is no deep copy from staging to the production environment. \nThe modern LakeHouse format, such as Apache Iceberg & Apache Hudi, support a Zero-Copy WAP pattern implementation. In the Iceberg case, it is a simple two-step config change.\nSet write.wap.enabled=true in the table\nSet spark.wap.id=<UUID> in the Spark job\nOne-Phase WAP removes the need for an intermediate queue by adopting the event router pattern derived from the message router pattern. \nThe event routers typically follow a few characteristics\n Event Routers can broadcast the same events from one-to-many destinations.\nEvent Routers typically don\u2019t alter the payload.\nEvent Routers can add additional metadata to the envelope of the event.\nData Quality is expensive because almost all the available data tools don\u2019t support data quality as a first-class semantics, which leads to bespoke two-phase WAP pattern implementation.\nIf you look at the popular data transformation dbt, the model building lacks in-build one-phase WAP support. The same goes for the popular orchestration engine Airflow, where the Airflow has separate operators (SQLCheckOperators) for the data quality check. These design patterns lead to disjointed data quality tools that add more cost to the pipeline operation than solving the problem. \nThe steps dbt is taking to introduce data contract semantics are welcoming and positive progress. This is where the data processing engines and the data warehouse require more integration to minimize the data creation cost, enable fast backfilling, and promote the data quality as the first-class semantics in the data pipeline.\nThe batch data processing, we do have many mature tools. I think it is a question of improving the functionality of it. \nHowever, on the real-time event routing side, I\u2019m not convinced that we are there yet. There is not enough tool available to make the data creation process efficient. \nSo We decided to write one for event routing that can validate data contracts [schema & data quality], route the events to multiple destinations, and enable faster debugging. We code name it \u201cNobu,\u201d and I can\u2019t write more about Nobu in the last part of this series. Stay Tuned. \nhttps://medium.com/everything-full-stack/action-position-data-quality-assessment-framework-d833f6b77b7\nhttps://calogica.com/assets/wap_dbt_bigquery.pdf\nhttps://modern-cdo.medium.com/taming-data-quality-with-circuit-breakers-dbe550d3ca78\nhttps://netflixtechblog.com/data-mesh-a-data-movement-and-processing-platform-netflix-1288bcab2873\nhttps://netflixtechblog.com/kafka-inside-keystone-pipeline-dd5aeabaf6bb\nhttps://netflixtechblog.com/evolution-of-the-netflix-data-pipeline-da246ca36905\nhttps://www.enterpriseintegrationpatterns.com/patterns/messaging/MessageRouter.html"}, {"url": "https://www.dataengineeringweekly.com/p/unlocking-data-stream-processing-84f", "title": "Data Engineering Weekly", "content": "Put yourself in the shoes of an IT department employee working for a large company. The finance department has requested your assistance with their annual balance sheet, explicitly matching all entries from the company's bank account with receipts submitted by employees for their professional expenses. Your colleague, Helen from finance, optimistically informs you that this should be easy since all the data has been entered into the company's databases. \nHowever, upon examining the bank statements and receipts tables, you quickly realize that matching these entries will be far from straightforward:\nBanking table: a table with schema automatically generated by the bank (later referred to as Table_banking_index). Great!\nReceipt table (later referred to as table_receipts_index):\nIt turns out that all the receipts were manually entered into the system, which creates unstructured data that is error-prone. This data collection method was chosen because it was simple to deploy, with each employee responsible for their own receipts. Unfortunately, this approach results in unstructured data that is difficult to work with.\nFor example, here are some rows corresponding to your colleague Elizabeth:\nTry to perform a regular join in this situation: it will be impossible due to the unstructured nature of the data. This is not uncommon in the banking industry, as an estimated 80% of banking data is unstructured. Additionally, different banking systems may use other naming conventions, further complicating data-matching efforts. In this context, making a regular join in this context is impossible. You must manually join these tables or ask employees to enter their receipts again following a specific schema. In both cases, it will be tedious and very frustrating.\nLuckily, there is a solution: fuzzy joins.\nIn this article, we'll delve into the world of fuzzy joins and explore how they can help you make the most out of your data. Fuzzy joins provide an efficient solution for dealing with dirty and incomplete data. Enriching and cleaning the data through fuzzy joins can save time and reduce frustration while gaining valuable insights and discovering hidden patterns that can help drive business decisions. You\u2019ll also see how Pathway, a Python framework for streaming data, can do real-time fuzzy joins. So, let's dive in!\nFuzzy join is a data enrichment technique that allows you to match data based on approximate rather than exact matches. Fuzzy join enables you to perform data joins even if your data contains misspellings, typos, or other variations. With fuzzy join, you can uncover hidden patterns, identify new opportunities, and make better decisions.\nData enrichment is crucial because it is what turns raw data into pure gold. By adding new information or filling in missing data points, data analysts and engineers can enhance their datasets and gain new insights that would be impossible with raw data alone. However, as we have seen previously, data enrichment is often tricky and time-consuming, especially when dealing with messy or incomplete datasets. Joining tables with typos or simply different formats can be challenging. Fortunately, fuzzy joins allow you to match data based on approximate rather than exact matches and uncover hidden patterns.\n\nIn our previous example, we would like to have the following matchings:\nSuch matching is impossible with a traditional join, as the entry \u201cJames P.: Cloud services $5048\u201d does not match any of the values in the different columns of the associated row in the banking table. Worse, the name is incorrect, \u201cJames P.\u201d instead of \u201cJames Paletta.\u201d The only correct value is the amount, which may not be unique in the table: there is no easy way to match without approximation.\nThat\u2019s where fuzzy join comes in: the power of fuzzy join lies in its ability to match data based on similarity rather than exactness. Fuzzy join considers variations in the spelling, word order, and other factors to find possible matches between datasets. This means that fuzzy joins can still find meaningful relationships between data points even when the data is messy or incomplete.\nFuzzy join relies on different approximations, determining its success:\nYou can learn more about how fuzzy join works in this article which describes different types of fuzzy joins.\nUsing those approximations, a system using fuzzy join can overcome the most common hardships:\nThose approximation techniques allow fuzzy joins to be applied to various data types, including text, numeric, and geographic data. This allows data analysts and engineers to choose the best matching method for their specific use case and achieve the most accurate results.\nFuzzy joins have applications beyond accounting. This technique is widely used across industries to enhance data quality and usability by uncovering hidden patterns.\nOne common use case for fuzzy joins is tracking a single entity across multiple tables, which is particularly useful for fraud detection and loan prediction in the banking sector. Fuzzy joins can identify suspicious transactions and accounts by matching data from various sources, such as credit card transactions and social media profiles. In addition, fuzzy join is used in marketing to improve customer segmentation and achieve a unified view of customers. For example, all articles related to a particular company can be automatically grouped using fuzzy join:\nWhile fuzzy joins are a powerful and versatile tool for data enrichment, there are some potential drawbacks.\nOne of the main challenges with fuzzy joins is determining an appropriate level of fuzziness or approximation. If the threshold is too high, the resulting matches may be too broad and include false positives. Setting the threshold too low may exclude relevant data points. Finding the right balance can require some trial and error and may be time-consuming.\nAn example of how fuzzy joins can lead to false positives is matching Jon Thomson in York (UK) with John Thompson in New York (US). However, it's essential to remember that homonyms are not unique to fuzzy join and pose a challenge for standard joins.\nAnother potential issue is the computational cost of fuzzy joins. Depending on the size and complexity of the matched datasets, the process can be resource-intensive and time-consuming. This can be especially problematic for organizations with limited computing resources or time-sensitive projects.\nIn addition, fuzzy joins might not be appropriate for certain types of data or use cases. For example, a more precise matching method might be necessary if data accuracy is critical (e.g., in medical or financial contexts). Fuzzy joins might also struggle with highly complex or unstructured data.\nAfter you have accomplished the fuzzy join for Helen, the head of the finance department is impressed with the results and decides to integrate the fuzzy join process into the organization's daily operations. The finance team handles a substantial amount of data from the entire group daily. The finance head emphasizes that this data needs to be up-to-date so the team can make informed decisions based on accurate information.\nThis scenario is very different from previously regarding data volume and time: you have to perform a real-time fuzzy join on streaming data.\nSince fuzzy joins are resource-intensive, they bring their own challenges in this streaming context. Every new data entry alters the fuzzy join results, necessitating frequent recomputations. Such batch processing on a streaming database is resource-intensive.\nHowever, there is a solution: Pathway, a Python framework for real-time data stream processing that automates the updates. With Pathway, you can set up your processing pipeline and request a fuzzy join. The platform will ingest new streaming data points for you, keeping the fuzzy join updated without recomputing everything from the beginning.\u00a0\nUsing Pathway's connectors, you can create two tables with two input streams, one from the bank and one from the receipts, and then easily perform a fuzzy join between them:\nIt's straightforward and is done in a single operation, just like a regular join. Furthermore, the three tables are updated accordingly as new data points are received. This means that the matching table is adjusted to reflect any new data that comes in.\nCheck out Pathway's fuzzy join example to see how this works in practice.\nFuzzy join is a \u201csmart replacement\u201d for traditional joins that can handle discrepancies in data. Smart replacements are techniques used in computer science to improve the performance and accuracy of an operation. They're designed to make tasks more efficient and effective by providing innovative solutions to common problems.\nPathway offers several smart replacements for typical table operations:\nThese smart replacements allow you to enrich your data easily. Pathway can update its results in real-time, providing better performance than traditional methods.\nData enrichment is a powerful tool that turns raw data into a treasure trove of valuable insights. By filling in missing data points and adding new information, data analysts and engineers can uncover hidden patterns and gain new perspectives that would be impossible with raw data alone.\nHowever, data enrichment can be tricky and time-consuming, especially when dealing with messy or incomplete datasets. That's where fuzzy joins come in. These powerful tools allow you to match data based on approximate rather than exact matches, making it easier to uncover patterns and gain insights from your data.\nWhether you're working in finance, marketing, or any other field that deals with data, fuzzy joins can help you overcome the challenges of messy or incomplete data and make the most of your data. And with tools like Pathway, you can even perform fuzzy joins in real-time, keeping your data up-to-date and accurate without wasting precious resources.\nSo if you want to get the most out of your data and gain new insights that will help you make better decisions and drive business growth, give fuzzy joins a try. You'll be amazed at what you can discover with the right tools and techniques!\nDon\u2019t hesitate to contact us on our discord server if you have questions about fuzzy join or smart replacements."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-130", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make collecting data from every application, website, and SaaS platform easy, then activating it in your warehouse and business tools. Sign up free to test out the tool today.\nPayPal this week released its data contract template. It is exciting to see reference architectures now coming out from different companies. There are eight main sections, including a catch-all section. The pricing part is a surprising addition.\nDemographics\nDataset & schema\nData quality\nPricing\nStakeholders\nRoles\nService-level agreement\nOther properties\nI noticed a few interesting LinkedIn comments about whether YAML is the right format. I prefer IDL solutions like ProtoBuf, Avro, or Smithy. However, Data Contract is a productivity software. It is a tooling problem to convert one data format to another, so any format that can make your team productive, go for it.\n(e.g.) Schemata's internal data format is protocol agnostic. You can do document.sh to convert any data format to json format. Even better, you can turn the ProtoBuf format into a data modeling tool\u2014an example schema definition with Schemata. \nSchemata Github: https://github.com/ananthdurai/schemata/\nPayPal Data Contract Template: https://github.com/paypal/data-contract-template/tree/main/docs\nWe started seeing increased reference articles from companies about adopting the Data Mesh concept. I found the author well articulated the skepticism which is worth a debate. The author highlights the following six points, which have a lot of merit.\nNot all data is valuable. \nData productization is one more thing to do.\nThere\u2019s not enough data competence around.\nUnfettered federated governance won\u2019t work.\nThen there\u2019s this central self-service platform.\nMost people don\u2019t find data sexy.\nLet me know what you all think in the comments.\nhttps://medium.com/@hannes.rollin/six-reasons-why-data-mesh-will-fail-195886c89bdd\nAre you ready to adopt Headless Lakehouse? What is Headless Lakehouse?\nA headless lakehouse (aka configurable compute) can be defined as a unified data architecture that provides a seamless way to access and manage data across different computing systems, storage locations, and formats. It enables different systems and users to access, analyze and use the data easily, promoting agility and scalability in data management and analysis.\nIt is a valid problem statement with the existing LakeHouse format. The LakeHouse format is often tightly coupled with the vendors, leaning towards a vertically integrated data platform. \nhttps://medium.com/microsoftazure/headless-lakehouse-63b0a5d27068\nStaying on the LakeHouse format, Walmart writes about its choice of the Lakehouse format by comparing all three major formats. The winner for them is Apache Hudi.\nhttps://medium.com/walmartglobaltech/lakehouse-at-fortune-1-scale-480bcb10391b\nAs data leaders, one of our top priorities is to measure ROI. From tracking the efficacy of marketing campaigns to understanding the root cause of new spikes in user engagement, we\u2019re tasked with keeping tabs on the business's health at all levels. But what about the ROI of our own teams? Watch a panel of data leaders as they discuss how to build strategies for measuring data team ROI.Watch On-demand\neBay writes about its architecture to build similarity product search engine using vector similarity. The blog discusses the batch and the near-real-time data pipeline, the adoption growth, and how it generates millions of dollars in annual revenue.\nhttps://tech.ebayinc.com/engineering/ebays-blazingly-fast-billion-scale-vector-similarity-engine/\nIntuit shares its journey towards democratizing AI and accelerating ML model development from months to weeks. The article highlights their use of AutoML to automate ML model building which leads to the creation of a centralized ML platform. This approach enables rapid development and improved collaboration and drives significant business impact across various Intuit product lines.\nhttps://medium.com/intuit-engineering/democratizing-ai-to-accelerate-ml-model-development-in-weeks-vs-months-9e895e3239a9\nWe realized we didn\u2019t need an edge-cluster mapping table with the left and right nodes as columns. We could instead use a node-cluster mapping table and have each edge add a mini-cluster to it upon initialization.\nPrinciple AI/ML Engineer, Justin Driemeyer, details the steps the team at RudderStack took to optimize their identity resolution algorithm for performance after an update to meet a requirement for point-in-time correct materials that resulted in unacceptably long runtimes.\nhttps://www.rudderstack.com/blog/how-we-optimized-rudderstacks-identity-resolution-algorithm-for-performance/\nIs Data a support organization in your company? I\u2019ve seen this happen many times. How can a data team move beyond a support team to a growth-oriented team? The author shares an elegant 4-step mantra for all the data teams.\nhttps://xgumara.medium.com/from-support-oriented-to-growth-oriented-data-teams-1d6b7c692b7e\nThe article is an excellent summarization of the current SQL unit testing landscape. The author left with a few thought-provoking comments in the end.\nAre we there to standardize the data testing?\nDo we have a data testing culture?\nData mocking is still an unsolved problem.\nhttps://towardsdatascience.com/the-sql-unit-testing-landscape-2023-7a8c5f986dd3\nWhen people approach me for suggestions for implementing Data Catalog & Data Documentation, I always suggest following a few things.\nAdopt Documentation as a Code principle.\nBuild a static site using any static site generator, Voila; your data catalog is ready.\nI\u2019m delighted to see Funding Circle writes the same principle in building and maintaining data documentation. \nhttps://medium.com/funding-circle/how-we-manage-documentation-at-funding-circle-for-our-data-platform-960a422b9b2e\nThough the blog does not directly discuss the data warehouse, the article is an excellent reference implementation to save S3 cost in your data lake. It is vital to know the S3 storage classes, the distribution of your data, and when to apply tiered storage. \nhttps://www.canva.dev/blog/engineering/optimising-s3-savings/\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-129", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make collecting data from every application, website, and SaaS platform easy, then activating it in your warehouse and business tools. Sign up free to test out the tool today.\nGenerative AI took the industry by storm, and every company is trying to figure out what it means to them. DoorDash writes about its discovery of Generative AI and its application to boost its business.\nThe assistance of customers in completing tasks\nBetter tailored and interactive discovery [Recommendation]\nGeneration of personalized content and merchandising \nExtraction of structured information \nEnhancement of employee productivity\nhttps://doordash.engineering/2023/04/26/doordash-identifies-five-big-areas-for-using-generative-ai/\nCheckout.com writes about its CI/CD pipeline process on deploying dbt with over 27 dbt projects focusing on different aspects across the business, 100+ dbt developers contributing to these projects every week, and 1000+ models.\nhttps://medium.com/checkout-com-techblog/building-dbt-ci-cd-at-scale-365358f64b6f\nFascinating findings on Europe\u2019s data salary among various countries. The key findings are\nGerman-based roles pay lower. \nLondon and Dublin-based roles have the highest compensations. The Dublin sample is skewed to more senior roles, with 55% of reported salaries being senior, so this is more indicative of the sample than jobs in Dublin paying higher than London\nThe top 75% percentile jobs in Amsterdam, London, and Dublin pay nearly 50% more than those in Berlin\nhttps://medium.com/@mikldd/europe-data-salary-benchmark-2023-b68cea57923d\nIt is crucial to guarantee the reliability and accuracy of ML models before deploying them to production. Intuit provides insightful information about performing a set of tests called machine learning model sanity checks in a pre-production environment. These tests aim to identify any systematic errors and biases in the models, thus helping to ensure that they function as intended when deployed to production. It follows a simple four-step process.\nEnsure online model scores sink to the output store. \nRescore with the offline model. \nCompare the online model score to the offline model score.\nCheck input data.\nhttps://medium.com/intuit-engineering/how-to-streamline-ml-model-deployment-automated-sanity-checks-64a23166fdc5\nAs data leaders, one of our top priorities is to measure ROI. From tracking the efficacy of marketing campaigns to understanding the root cause of new spikes in user engagement, we\u2019re tasked with keeping tabs on the business's health at all levels. But what about the ROI of our own teams? Watch a panel of data leaders as they discuss how to build strategies for measuring data team ROI.Watch On-demand\nThe article by Trivago discusses the integration of data validation with Great Expectations. It presents a well-balanced case study that emphasizes the significance of data validation and the necessity for sophisticated statistical validation methods.\nhttps://tech.trivago.com/post/2023-04-25-implementing-data-validation-with-great-expectations-in-hybrid-environments.html\nThe article discusses the difficulties of processing continuous data streams for calculating unbounded events and global states. However, I don\u2019t fully understand the versioning solution and how the systems coordinate it. Also, is the versioning approach similar to the LakeHouse system design? \ud83e\udd14\nhttps://medium.com/data-science-at-microsoft/reasoning-about-change-lessons-learned-from-building-a-near-real-time-system-for-azure-pricing-34049816ffbd\nWe realized we didn\u2019t need an edge-cluster mapping table with the left and right nodes as columns. We could instead use a node-cluster mapping table and have each edge add a mini-cluster to it upon initialization.\nPrinciple AI/ML Engineer, Justin Driemeyer, details the steps the team at RudderStack took to optimize their identity resolution algorithm for performance after an update to meet a requirement for point-in-time correct materials that resulted in unacceptably long runtimes.\nhttps://www.rudderstack.com/blog/how-we-optimized-rudderstacks-identity-resolution-algorithm-for-performance/\n\u201cEvents as a source of truth\u201d is a simple but powerful idea to persist the state of the business entity as a sequence of state-changing events. How to build such a system? Expedia writes about the review stream system to demonstrate how it adopted the event-first approach. \nhttps://medium.com/expedia-group-tech/how-expedia-reviews-engineering-is-using-event-streams-as-a-source-of-truth-d3df616cccd8\nChurn prediction is vital for a company. A timely prediction can save tons of customer acquisition costs and increase customer retention. The article is an excellent reference to avoid common mistakes while making churn predictions. \nhttps://medium.com/@swansburg.justin/top-ten-mistakes-data-scientists-make-while-building-churn-models-d773bb7deaa5\nWTTJ Tech has an interesting story to share about data migration. In this case, they moved from PostgreSQL to Snowflake instead of Redshift. The article's highlight is that it covers the technical aspects of the transition and details the full migration plan, including moving dashboards and other important elements.\nhttps://medium.com/wttj-tech/from-postgresql-to-snowflake-a-data-migration-story-5fd17f778019\nThe current economic situation brings a refreshed look at cost control in various companies. Every 1% optimization on cost requirements should lead to celebration and acknowledgment. The author provides a few hacks while running dbt on BigQuery to save \ud83d\udcb5.\nhttps://medium.com/@alexroperez4/saving-with-bigquery-dbt-35937b1cf628\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/dew-124-state-of-analytics-engineering", "title": "Data Engineering Weekly", "content": "Welcome to another episode of Data Engineering Weekly. Aswin and I select 3 to 4 articles from each edition of Data Engineering Weekly and discuss them from the author\u2019s and our perspectives. \nOn DEW #124, we selected the following article\ndbt publishes the state of analytical [data???\ud83e\udd14] engineering. If you follow Data Engineering Weekly, We actively talk about data contracts & how data is a collaboration problem, not just an ETL problem. The state of analytical engineering survey validates it as two of the top 5 concerns are data ownership & collaboration between the data producer & consumer. Here are the top 5 key learnings from the report.\n46% of respondents plan to invest more in data quality and observability this year\u2014 the most popular area for future investment.\nLack of coordination between data producers and data consumers is perceived by all respondents to be this year\u2019s top threat to the ecosystem.\nData and analytics engineers are most likely to believe they have clear goals and are most likely to agree their work is valued.\n71% of respondents rated data team productivity and agility positively, while data ownership ranked as a top concern for most.\nAnalytics leaders are most concerned with stakeholder needs. 42% say their top concern is \u201cData isn\u2019t where business users need it.\u201d\nhttps://www.getdbt.com/state-of-analytics-engineering-2023/\nVery fascinating to read about the potential impact of LLM in the future of dbt and analytical consulting. The author predicts we are at the beginning of the industrial revolution of computing.\nFuture iterations of generative AI, public services such as ChatGPT, and domain-specific versions of these underlying models will make IT and computing to date look like the spinning jenny that was the start of the industrial revolution.\n\ud83e\udd3a\ud83e\udd3a\ud83e\udd3a\ud83e\udd3a\ud83e\udd3a\ud83e\udd3a\ud83e\udd3a\ud83e\udd3a\ud83e\udd3aMay the best LLM wins!! \ud83e\udd3a\ud83e\udd3a\ud83e\udd3a\ud83e\udd3a\ud83e\udd3a\ud83e\udd3a\nhttps://www.rittmananalytics.com/blog/2023/3/26/chatgpt-large-language-models-and-the-future-of-dbt-and-analytics-consulting\nOne of the curses of adopting Lambda Architecture is the need for rewriting business logic in both streaming and batch pipelines. Spark attempt to solve this by creating a unified RDD model for streaming and batch; Flink introduces the table format to bridge the gap in batch processing. LinkedIn writes about its experience adopting Apache Beam\u2019s approach, where Apache Beam follows unified pipeline abstraction that can run in any target data processing runtime such as Samza, Spark & Flink.\nhttps://engineering.linkedin.com/blog/2023/unified-streaming-and-batch-pipelines-at-linkedin--reducing-proc\nWix writes about managing schema for 2000 (\ud83d\ude2c) microservices by standardizing schema structure with protobuf and Kafka schema registry. Some exciting reads include patterns like an internal Wix Docs approach & integration of the documentation publishing as part of the CI/ CD pipelines.\nhttps://medium.com/wix-engineering/how-wix-manages-schemas-for-kafka-and-grpc-used-by-2000-microservices-2117416ea17b"}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-128", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make collecting data from every application, website, and SaaS platform easy, then activating it in your warehouse and business tools. Sign up free to test out the tool today.\nData Engineering Weekly is joining forces with Rudderstack and The Data Stack Show to bring you The State of Data Engineering Survey 2023! \ud83d\udcca\u23f1\ufe0fGot 5 minutes? \u23f1\ufe0f Lend us a hand and share your valuable insights on:\ud83c\udfaf Data team priorities for 2023\ud83d\udc65 Team dynamics\ud83d\udee0\ufe0f Data stacks\ud83d\udd0d Identity resolution strategies\ud83d\udcda Data team rolesPlease help us create a comprehensive report that will be featured in Data Engineering Weekly \ud83d\uddde\ufe0f. Plus, I'll hop on a special episode of The Data Stack Show to discuss the results \ud83c\udf99\ufe0f.And the cherry on top? \ud83c\udf52 We'll send exclusive The Data Stack Show swag just for participating! \ud83c\udf81Take the survey now! \u27a1\ufe0f RudderStack.com/survey \u2b05\ufe0f\nA lot of exciting announcements from dbt this week on the next big step forwards for analytical engineering. Yes, dbt brings the native experience of data contract and domain ownership into the data transformation layer. I applauded dbt team for this step since this is the community's first opinionated take on data transformation that I\u2019ve seen in a long time. Considering the dbt\u2019s large community and widespread adoption, they are entitled and obligated to show the path toward a productive data transformation process. \nI believe we have reached a point in the industry where we no longer need to explain what is data contract is and why it is beneficial. However, the data contract should start with the developers and the data practitioners during the data creation process. \nI open-sourced Schemata last year, the industry's first Data Contract as a Code (DCC). We are working on some exciting data contract implementation on top of Schemata. I love to show you what we are working on and get initial feedback before opensource the solution. Please add your contact details and love to chat with you all. Together, let's build a state-of-the-art data contract system.\u00a0\ud83d\udcaa\ud83c\udffd\nClick the link to connect \ud83d\udd87\ufe0f https://forms.gle/WT4AQsUPFLpMNCyk7 \n\ud83d\udd17 https://www.getdbt.com/blog/analytics-engineering-next-step-forwards/ \nReplit writes about building your own Large Language Model using Databricks, Hugging Face, and MosaicML. The highlight of the blog for me is\nLLMs require an immense amount of data to train. Training them requires building robust data pipelines that are highly optimized yet flexible enough to easily include new public and proprietary data sources.\nAs the adoption of LLM increases, the need for robust data pipeline increases. It is an excellent time to be in data engineering. \n\ud83d\udd17 https://blog.replit.com/llm-training\nOpenAI recently announced the support of plugins for ChatGPT. Content platforms like Medium can expose their content to ChatGPT; based on the user prompt and installed plugins, ChatGPT can trigger the correct API of your plugins to retrieve a piece of content and do some manipulation on it. Medium writes about how to write a ChatGPT plugin and debug and deploy the application. \n\ud83d\udd17 https://medium.engineering/building-a-chatgpt-plugin-for-medium-6813b59e4b24\nAs I continued my quest to learn more about LLM and its infrastructure, the author explained LLM, Agents, and Utils via LangChain pretty well. The LangChain documentation around the types of chains is an excellent read to get to know the LLM chain pattern more in-depth.\n\ud83d\udd17 https://towardsdatascience.com/a-gentle-intro-to-chaining-llms-agents-and-utils-via-langchain-16cd385fca81\nAs data leaders, one of our top priorities is to measure ROI. From tracking the efficacy of marketing campaigns to understanding the root cause of new spikes in user engagement, we\u2019re tasked with keeping tabs on the business's health at all levels. But what about the ROI of our own teams? Watch a panel of data leaders as they discuss how to build strategies for measuring data team ROI.Watch On-demand\nOne of the significant challenges of running an EMR cluster is it is often an isolated environment from the rest of the organization's computing infrastructure. Running data engineering workload on Kubernetes, even as one pod per node model, helps share the organization's tools and infrastructure. The author writes about how Instacart builds Flink as a Service on the top of Kubernetes instead of EMR.\n\ud83d\udd17https://tech.instacart.com/building-a-flink-self-serve-platform-on-kubernetes-at-scale-c11ef19aef10\nData drift is a process of detecting the anomaly of changes in the characteristics of each metric over time, such as their average, how spread out they are, and how often they occur. Capital One open-sourced\u00a0Data Profiler, a standalone library to detect data drifting. In this blog, the author expands on Data Profiler and narrates how it integrated with KubeFlow to build Data Drift Detection as a Service.\u00a0\n\ud83d\udd17https://medium.com/@CapitalOneTech/data-profiler-data-drift-model-monitoring-tool-capital-one-e69631f5a058\nFind out how Phantom transitioned from siloed analytics to a warehouse-first stack that enables A/B experimentation directly on top of the data warehouse. You'll learn from Eppo founder Chetan Sharma, RudderStack DevRel leader Sara Mashfej, and Phantom Senior Data Engineer Ricardo Pinho.\nRegister today\nFor the first time, Adyen reveals essential aspects of data engineering, providing insight into how the role fits seamlessly within the larger data landscape. The blog delves into the diverse components of the job, emphasizing the importance of teamwork and collaboration in attaining success in this domain.\n\ud83d\udd17https://medium.com/adyen/data-engineering-at-adyen-ccded12a6eb\nWix publishes video\u2019s from its Wix Data Engineering meetups (or should we call it as low-key events?) that focuses on optimizing Spark with Iceberg, ensuring data quality with great expectations, and elevating code review practices using game theory.\n\ud83d\udd17https://medium.com/wix-engineering/a-comprehensive-approach-to-efficient-data-engineering-f9e5ff4b967f\nOne challenge of sourcing a change stream from the operational store is that we have to denormalize it at some point. The best place to do the denormalization is at the source itself; otherwise, we will spend expensive real-time streaming join to denormalize the structure. RazerPay writes about the challenges in denormalization in real-time and the lessons learned along the line. \n\ud83d\udd17Part 1: https://engineering.razorpay.com/real-time-denormalized-data-streaming-platform-part-1-9f3c730dd9c6\n\ud83d\udd17Part 2: https://engineering.razorpay.com/real-time-denormalized-data-streaming-platform-part-2-97dfff40fd8d\nExperimentation platform brings its own challenges where usually the experimentation pipeline is the last pipeline runs and aggregate metrics :-) The author shares the top 10 lessons from building an experimentation platform.\nEnsure data is fit for purpose\nKeep complex data transformations outside the experimentation platform\nStart with simple statistical methodology\nRecognise that methods for understanding anomalies in the data are more important than advanced statistical techniques\nConsider the impacts of outliers / extreme observations\nFocus on early stopping techniques before variance reduction\nConsider scalability right from the start\nCache EVERYTHING\nUse task parallelisation wherever possible\nConsider both scheduled and ad hoc analysis use cases \u2014 but don\u2019t build one system for both\n\ud83d\udd17https://medium.com/seek-blog/10-lessons-from-building-an-experimentation-platform-ded851715683\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions.\n\n\n\n"}, {"url": "https://www.dataengineeringweekly.com/p/lessons-learned-building-products", "title": "Data Engineering Weekly", "content": "Welcome to another episode of Data Engineering Weekly Radio. Ananth and Aswin discussed a blog from BuzzFeed that shares lessons learned from building products powered by generative AI. The blog highlights how generative AI can be integrated into a company's work culture and workflow to enhance creativity rather than replace jobs. BuzzFeed provided their employees with intuitive access to APIs and integrated the technology into Slack for better collaboration.\nSome of the lessons learned from BuzzFeed's experience include:\nGetting the technology into the hands of creative employees to amplify their creativity.\nEffective prompts are a result of close collaboration between writers and engineers.\nModeration is essential and requires building guardrails into the prompts.\nDemystifying the technical concepts behind the technology can lead to better applications and tools.\nEducating users about the limitations and benefits of generative AI.\nThe economics of using generative AI can be challenging, especially for hands-on business models.\nThe conversation also touched upon the non-deterministic nature of generative AI systems, the importance of prompt engineering, and the potential challenges in integrating generative AI into data engineering workflows. As technology progresses, it is expected that the economics of generative AI will become more favorable for businesses.\nhttps://tech.buzzfeed.com/lessons-learned-building-products-powered-by-generative-ai-7f6c23bff376\nMoving on, We discuss the importance of on-call culture in data engineering teams. We emphasize the significance of data pipelines and their impact on businesses. With a focus on communication, ownership, and documentation, we highlight how data engineers should prioritize and address issues in data systems.\nWe also discuss the importance of on-call rotation, runbooks, and tools like PagerDuty and Airflow to streamline alerts and responses. Additionally, we mention the value of having an on-call handoff process, where one engineer summarizes their experiences and alerts during their on-call period, allowing for improvements and a better understanding of common issues.\nOverall, this conversation stresses the need for a learning culture within data engineering teams, focusing on building robust systems, improving team culture, and increasing productivity.\nhttps://towardsdatascience.com/how-to-build-an-on-call-culture-in-a-data-engineering-team-7856fac0c99\nFinally, Ananth and Aswin discuss an article about adopting dimensional data modeling in hyper-growth companies. We appreciate the learning culture and emphasize balancing speed, maturity, scale, and stability.\nWe highlight how dimensional modeling was initially essential due to limited computing and expensive storage. However, as storage became cheaper and computing more accessible, dimensional modeling was often overlooked, leading to data junkyards. In the current landscape, it's important to maintain business-aware domain-driven data marts and acknowledge that dimensional modeling still has a role.\nThe conversation also touches upon the challenges of tracking slowly changing dimensions and the responsibility of data architects, engineers, and analytical engineers in identifying and implementing such dimensions. We discuss the need for a fine balance between design thinking and experimentation and stress the importance of finding the right mix of correctness and agility for each company.\nhttps://medium.com/whatnot-engineering/same-data-sturdier-frame-layering-in-dimensional-data-modeling-at-whatnot-5e6a548ee713\n"}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-127", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make collecting data from every application, website, and SaaS platform easy, then activating it in your warehouse and business tools. Sign up free to test out the tool today.\nData Engineering Weekly is joining forces with Rudderstack and The Data Stack Show to bring you The State of Data Engineering Survey 2023! \ud83d\udcca\u23f1\ufe0fGot 5 minutes? \u23f1\ufe0f Lend us a hand and share your valuable insights on:\ud83c\udfaf Data team priorities for 2023\ud83d\udc65 Team dynamics\ud83d\udee0\ufe0f Data stacks\ud83d\udd0d Identity resolution strategies\ud83d\udcda Data team rolesPlease help us create a comprehensive report that will be featured in Data Engineering Weekly \ud83d\uddde\ufe0f. Plus, I'll hop on a special episode of The Data Stack Show to discuss the results \ud83c\udf99\ufe0f.And the cherry on top? \ud83c\udf52 We'll send exclusive The Data Stack Show swag just for participating! \ud83c\udf81Take the survey now! \u27a1\ufe0f RudderStack.com/survey \u2b05\ufe0f\nThe article is one of the best reads of 2023 for me. I print this out and read it a couple of times. The blog narrates the difference between deterministic and non-deterministic system design and the challenges of building LLM applications in production. The author explains the difference between prompting and fine-tuning and when to choose what. \nhttps://huyenchip.com/2023/04/11/llm-engineering.html\nThe flow control in the LLM application is an exciting read, and a generalized programming model will emerge soon. The speed of innovation in this space is mind-blowing when I came across Microsoft Semantic Kernal.\nMicrosoft Semantic Kernal, a lightweight SDK that lets you easily mix conventional programming languages with the latest in Large Language Model (LLM) AI \"prompts\" with templating, chaining, and planning capabilities out-of-the-box.\nhttps://learn.microsoft.com/en-us/semantic-kernel/whatissk\nWow \ud83d\ude4c\ud83c\udffc I applaud \ud83d\udc4f\ud83c\udffd author\u2019s thought process on writing the hot take and kept it hot indeed \ud83d\ude0a The hot takes are\ndbt lacks some basic functionality expected of a best-in-class tool\nDeploying a \u201cproduction\u201d data warehouse is unnecessarily hard and gated by tribal knowledge.\nRedshift is no longer a true competitor in the warehouse space.\nAirflow is obsolete.\nAirbyte is not production-grade software.\n#1 On dbt; Yes, I completely agree with the author\u2019s take that it lacks the basic functionality expected of a best-in-class tool. For me personally, the fact that dbt doesn\u2019t have a backfill mechanism, no easy option to build a date-time partition table, and always requires relying on Airflow as a scheduler engine is \ud83e\udd2f Benn Stancil recently wrote about the dbt dilemma as peacetime dbt vs. war time dbt in the article, How dbt succeeds.  \n#2 I agree; permission is a mess. However, as the data stack consolidation accelerates, we will see a much better permission model from Snowflake and Databricks. \n#3 Yes, AWS, please #SaveRedshift\n#4 On Airflow, hmmm still trying to figure it out. Do I like Airflow? No, but the current state of mind is that a known devil is better than an unknown devil.\n#5. I\u2019ve not tried Airbyte yet but I heard a similar sentiment from other folks. \nhttps://mattpalmer.io/posts/hot-takes/\nIt is exciting to see wide adoption and implementation study of the Metric Layer from various companies. DoorDash writes about its journey to build a Metric Layer for experimentation. One of the things that stands out for me in the article is the debate around pre-aggregation vs. lazy aggregation. While reading Airbnb\u2019s Minerva blog about XRF[ eXecutive Reporting Framework] that aggregate, I can\u2019t stop wondering how expensive this system will be. \nAs someone who contributed early to Apache Pinot, I believe, as the author highlighted in the blog, that Apache Pinot or a Pinot-like system would play a significant role in the metrics layer. \nhttps://doordash.engineering/2023/04/12/using-metrics-layer-to-standardize-and-scale-experimentation-at-doordash/\nMeta introduced a new term, \u201cAccessible Analytics,\u201d - self-describing to the extent that it doesn\u2019t require specialized skills to draw meaningful insights from it.\nMeta shares its ever-changing landscape of data engineering.  The author narrates how data engineering focused more on data integration at the initial stage and how it shifted towards building Accessible Analytics. Analytical Engineering is the role often quoted for data engineers who do Accessible Analytics. Apart from marketing terms, it is fascinating to see how the role changes along with the maturity of an organization. \nhttps://medium.com/@AnalyticsAtMeta/the-future-of-the-data-engineer-part-i-32bd125465be\nHelloFresh writes about a self-serving low-code ETL tool that enables the analytical team to self-import the data sources. The abstraction builds on a known data model with slowly changing dimensions. \nhttps://engineering.hellofresh.com/enabling-teams-access-to-their-data-with-a-low-code-etl-tool-a7bc5fc2457a\nAs data leaders, one of our top priorities is to measure ROI. From tracking the efficacy of marketing campaigns to understanding the root cause of new spikes in user engagement, we\u2019re tasked with keeping tabs on the business's health at all levels. But what about the ROI of our own teams? Watch a panel of data leaders as they discuss how to build strategies for measuring data team ROI.Watch On-demand\nAWS u-24tb1.metal instances offering a 24TiB memory \ud83d\udcbe\ud83d\udcbe\ud83d\udcbe and the advancement of RDMA bringing memory-centric data engineering. DuckDB, a leading in-memory columnar db, is an exciting system to watch out for. The author writes one such case using dbt & DuckDb for the analytical workload instead of Apache Spark. \nhttps://medium.com/datamindedbe/use-dbt-and-duckdb-instead-of-spark-in-data-pipelines-9063a31ea2b5\nEntity Resolution is the fundamental challenge in the Data Integration system. Traditionally we tried or still solving using MDM (Master Data Management) systems. The CDP (Customer Data Platform) pushed away folks from Entity resolution since the use cases like marketing campaign don\u2019t requires strict survivorship rules.  The author takes the Entity Matching problem and describes how GPT-3 with Modern Data Stack (MDS) works. \nI am profoundly interested in Master Data Management, and you can expect more guest articles and interviews in Data Engineering Weekly shortly!!!\nhttps://towardsdatascience.com/is-this-you-entity-matching-in-the-modern-data-stack-with-large-language-models-19a730373b26\nFind out how Phantom transitioned from siloed analytics to a warehouse-first stack that enables A/B experimentation directly on top of the data warehouse. You'll learn from Eppo founder Chetan Sharma, RudderStack DevRel leader Sara Mashfej, and Phantom Senior Data Engineer Ricardo Pinho.\nRegister today\nData debt is the cost of avoiding or delaying investment in maintaining, updating, or managing data assets, leading to decreased efficiency, increased costs, and potential risks.\nExcellent narration of data debt (Surprisingly, this is my first time hearing this term). The author narrates some of the root causes of data debt, and one of the primary causes of data debt is? You guessed it correctly. \"No proper data contracts/collaboration between software and data engineers\u201d\nhttps://medium.com/@diogo22santos/why-data-debt-is-the-next-technical-debt-you-need-to-worry-about-bc1731f732ff\nOne of the core challenges for a data team is to prove the ROI and measure the team's effectiveness. The author narrates an exciting approach based on The Value Driver Tree (VDT) method. By breaking down a value metric into components and linking data and analytics to these drivers, teams can demonstrate their direct impact on company growth and ensure a focus on high-value initiatives.\nhttps://medium.com/zs-associates/identifying-data-driven-use-cases-with-a-value-driver-tree-bd5795e26e21\nSpotify writes about leveraging Google Dataflow to generate large-scale machine learning-based podcast previews, improving user experience and content discoverability. The system utilizes parallel processing and data partitioning techniques to handle millions of podcast episodes efficiently, enabling scalable and cost-effective podcast preview generation.\nhttps://engineering.atspotify.com/2023/04/large-scale-generation-of-ml-podcast-previews-at-spotify-with-google-dataflow/\nEtsy introduces Barista, a system designed to streamline machine learning model deployment, offering improved flexibility, reliability, and efficiency. Barista provides a unified interface for deploying and managing models across various platforms and languages, removing complexities tied to infrastructure and deployment specifics. Barista empowers data scientists and engineers to focus on model development and optimization by automating infrastructure provisioning and deployment workflows. The system also supports versioning, rollback, and monitoring capabilities, ensuring smooth model updates and transitions. With Barista, Etsy aims to reduce time-to-production and enhance the overall Machine Learning lifecycle, ultimately driving business value and innovation.\nhttps://www.etsy.com/codeascraft/barista-enabling-greater-flexibility-in-machine-learning-model-deployment\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/podcast-dbt-reimagined-change-data", "title": "Data Engineering Weekly", "content": "The challenge with this, having the Jinja templating, I found out two things. One is like; it is on runtime. So you have to build it and then run some simulations to understand whether you did it correctly or not.\nJinja Templates also add cognitive load. The developers have to know how the Jinja template will work; how SQL will work, and it becomes a bit difficult to read and understand.\nIn this conversation with Aswin, we discuss the article \"DBT Reimagined\" by Pedram Navid. We talked about the strengths and weaknesses of DBT and what we would like to see in a future version of the tool.\nAswin agrees with Pedram Navid that a DSL would be better than a templated language for DBT. He also points out that the Jinja templating system can be difficult to read and understand.\nI agree with both Aswin and Pedram Navid. A DSL would be a great way to improve DBT. It would make the tool more powerful and easier to use.\nI'm also interested in a native programming language for DBT. It would allow developers to write their own custom functions and operators, giving them even more flexibility in using the tool.\nThe conversation shifts to the advantages of DSL over templated code, and they discuss other tools like SQL Mesh, Malloy, and an internal tool by Criteo. I believe that more experimentation with SQL is needed.\nOverall, the article \"DBT Reimagined\" is a valuable contribution to discussing the future of data transformation tools. It raises some important questions about the strengths and weaknesses of DBT and offers some interesting ideas for how to improve.\nhttps://medium.com/brexeng/change-data-capture-at-brex-c71263616dd7\nAswin provided a great definition of CDC, explaining it as a mechanism to listen to database replication logs and capture, stream, and reproduce data in real time\ud83d\udd52. He shared his first encounter with CDC back in 2013, working on a Proof of Concept (POC) for a bank\ud83c\udfe6.\nAswin explains that CDC is a way to capture changes made to data in a database. This can be useful for a variety of reasons, such as:\nAuditing: CDC can be used to track changes made to data, which can be useful for auditing purposes.\nCompliance: CDC can be used to ensure that data complies with regulations.\nData replication: CDC can replicate data from one database to another.\nData integration: CDC can be used to integrate data from multiple sources.\nAswin also discusses some of the challenges of using the CDC, such as:\nComplexity: CDC can be a complex process to implement.\nCost: CDC can be a costly process to implement.\nPerformance: CDC can impact the performance of the database.\nSo, in a summary of the conversation about change data capture (CDC):\nCDC is a way to capture changes made to data in a database.\nCDC can be used for various purposes, such as auditing, compliance, data replication, and integration.\nCDC can be implemented using a variety of tools, such as Debezium.\nSome of the challenges of the CDC include latency, cost, and performance.\nCDC can\u2019t carry business context, which can be expensive to recreate. \nOverall, CDC is a valuable tool for data engineers.\nhttps://medium.com/@maxillis/on-data-products-and-how-to-describe-them-76ae1b7abda4\nThe library example is close to heart for Aswin since his father started his career as a librarian! \ud83d\udcd6\n\ud83d\udc68\u200d\ud83d\udcbb Aswin highlights Max's broad definition of data products, including data sets, tables, views, APIs, and machine learning models. Anand agrees that BI dashboards can also be data products. \ud83d\udcca\n\ud83d\udd0dWe emphasize the importance of exposing tribal knowledge and democratizing the data product world. Max's journey from skeptic to believer in data products is very admirable. \ud83c\udf1f\n\ud83d\udcddWe dive into data products' structural and behavioral properties and Max's detailed description of build-time and runtime properties. They also appreciate the idea of reference queries to facilitate data consumption. \ud83e\udde9\n\ud83d\ude80In conclusion, Max's blog post on data products is one of the best written up on data products around! Big thanks to Max for sharing his thoughts! \ud83d\ude4c\n\n\n\n\n"}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-126", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make collecting data from every application, website, and SaaS platform easy, then activating it in your warehouse and business tools. Sign up free to test out the tool today.\nIf you notice this week\u2019s edition, most of the article talks about AI, specifically Generative AI. It\u2019s purely accidental, and I\u2019m amazed the pattern emerged while curating the article. \ud83d\ude00\nWhile writing An Engineering Guide to Data Creation - A Data Contract Perspective, I thought a bit more about the application code changes and their impact on click stream events. \nApplication code changes too often, but click stream tracking event structure remains the same. Application developers can accidentally remove the click stream tracking event at any point in application code changes.\nI\u2019m so curious to know your take on this problem. Please vote and share your experience.\nStanford HAI publishes the 2023 AI index report. Key highlights outlined by the report\nIndustry races ahead of academia. \nPerformance saturation on traditional benchmarks. \nAI is both helping and harming the environment.\nThe world\u2019s best new scientist \u2026 AI?\nThe number of incidents concerning the misuse of AI is rapidly rising. \nThe demand for AI-related professional skills is increasing across virtually every American industrial sector.\nFor the first time in the last decade, year-over-year private investment in AI decreased. \nWhile the proportion of companies adopting AI has plateaued, the companies that have adopted AI continue to pull ahead. \nPolicymaker interest in AI is on the rise.\nChinese citizens are among those who feel the most positively about AI products and services. Americans...not so much.\nAI trustability and its misuse will continue to be debated in the coming years. The Google research paper \"Because AI is 100% right and safe\": User Attitudes and Sources of AI Authority in India describes the challenges in AI & trustability. \nhttps://aiindex.stanford.edu/report/\nDownload the full report: https://aiindex.stanford.edu/wp-content/uploads/2023/04/HAI_AI-Index-Report_2023.pdf.\nThe article explains how GPT works in a simplified way taking from a naive probabilistic model of words, words to meaning and meaning to the relationship. The blog also discusses two burning questions everyone is debating.\nCan the Model Think?\nWill GPT destroy society? \nI will leave the readers to think about these two burning questions. Personally, AI spreading misinformation amplifies human tribalism is much more troubling than the Terminator scenario. \nhttps://confusedbit.dev/posts/how_does_gpt_work/\nThe Generative AI\u2019s Gold Rush is here, and it\u2019s just the beginning. ChatGPT and other recently released generative AI models promise to automate tasks previously thought to be solely in human creativity and reasoning, from writing to creating graphics to summarizing and analyzing data. The article discusses the positive and the negative side of AI\u2019s Gold Rush. \nhttps://www.technologyreview.com/2023/03/25/1070275/chatgpt-revolutionize-economy-decide-what-looks-like/\nLinkedIn writes about its learning from adopting GenerativeAI into the product features. I\u2019m sure many companies trying to explore what is Generative AI means to their product. LinkedIn rightly pointed out how tooling and experimental engineering culture are key to democratizing new technologies in an engineering organization. \nhttps://engineering.linkedin.com/blog/2023/our-learnings-from-the-early-days-of-generative-ai\nAs data leaders, one of our top priorities is to measure ROI. From tracking the efficacy of marketing campaigns to understanding the root cause of new spikes in user engagement, we\u2019re tasked with keeping tabs on the health of the business at all levels. But what about the ROI of our own teams? Watch a panel of data leaders as they discuss how to build strategies for measuring data team ROI.Watch On-demand\nAI development ecosystems are increasingly complex and challenging to maintain, and technology companies need to develop highly efficient systems to build, serve, and improve their AI models for production applications. Meta writes about the measurement process it developed internally for specific metrics about AI systems to make managing the models effective and efficient. \nhttps://ai.facebook.com/blog/meta-ai-ecosystem-management-metrics/\nThe author brings an interesting perspective on the growth of AI in comparison with AWS's growth. The author highlights \nAI -- in particular, the advancements in large language models (LLMs) -- is starting to feel like the beginning of another platform shift. This isn't a shift from the cloud; it is a platform shift within a different category, but it has the same potential to change how we build and deliver software fundamentally.\nhttps://mitchellh.com/writing/ai-through-a-cloud-lens\nFind out how Phantom transitioned from siloed analytics to a warehouse-first stack that enables A/B experimentation directly on top of the data warehouse. You'll learn from Eppo founder Chetan Sharma, RudderStack DevRel leader Sara Mashfej, and Phantom Senior Data Engineer Ricardo Pinho.\nRegister today\nThe author takes an in-depth analysis of why MLOps is mostly data engineering. The author broadly split the MLOps into four categories and explains the relevance of data engineering.  \nDeployment & Serving of models, i.e., OctoML\nModel Quality and Monitoring, i.e., Weights & Biases\nModel training, i.e. AWS Sagemaker\nFeature Stores, i.e., Tecton\nhttps://www.cpard.xyz/posts/mlops_is_mostly_data_engineering/\nLyft writes about its use cases for the recommendation system by leveraging a set of machine learning models to predict a rider\u2019s propensity to convert into each mode and customize the rankings based on it.\nhttps://eng.lyft.com/the-recommendation-system-at-lyft-67bc9dcc1793\nThe Real-Time Analytic Summit is on April 25-26 in downtown San Francisco, CA. Come and hear talks from companies like StarTree, Confluent, LinkedIn, DoorDash, Imply, and Uber on how they are advancing the state-of-the-art in user-facing analytics delivered instantly.\nGo to rtasummit.com and register with DEW30 for 30% off.\nThere are  many debates about the data contract can bring complexity to the developer workflow. The author writes about why GoCardless engineers love the data contract. The author narrates the developer experience in three categories of delight\nFreebies [ The pub-sub system and DLQ]\nAutonomy [ decision on the structure of the data internal to the development team]\nGolden Path [Best practices enforced out of the box]\nhttps://medium.com/gocardless-tech/3-things-our-software-engineers-love-about-data-contracts-3106e1f1602d\nOkay, folks, enough talking about Generative AI and its impact. The author asks a real question. Can we get rid of SFTP? Possible?\nThe author writes an excellent summary of the state of intra-company data exchanges and the complexity associated with them. \nhttps://djpardis.medium.com/the-state-of-data-exchange-31049fa229f0\nLooker recently announced the Looker Modeler, a standalone metric layer. It is exciting to see the innovation around the metrics store is accelerating, and I can see many application-driven startups coming on top of it. The author demonstrates how to build a metric store using MetricFlow, Python, DuckDB, dbt, and Streamlit.\nhttps://towardsdatascience.com/metrics-store-in-action-76b16a928b97\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/what-happened-at-data-council-2023", "title": "Data Engineering Weekly", "content": "Hey folks, have you heard about the Data Council conference in Austin? The three-day event was jam-packed with exciting discussions and innovative ideas on data engineering and infrastructure, data science and algorithms, MLOps, generative AI, streaming infrastructure, analytics, and data culture and community.\u00a0\n\"People are so nice in the data community. Meeting them and brainstorming with many ideas and various thought processes is amazing. It was an amazing experience; The conference is mostly like a jam of different thought processes, ideas, and entrepreneurship.\nThe keynote by Shrishanka from AcrylData talked about how data catalogs are becoming the control center for pipelines, a game-changer for the industry.\nI also had a chance to attend a session on Malloy, a new way of thinking about SQL queries. It was experimental but had some cool ideas on abstracting complicated SQL queries. ChatGPT will change the game in terms of data engineering jobs and productivity. Charge GPT, for example, has improved my productivity by 60%. And generative AI is becoming so advanced that it can produce dynamic SQL code in just a few lines.\nBut of course, with all this innovation and change, there are still questions about the future. Will Snowflake and Databricks outsource data governance experience to other companies? Will the modern data stack become more mature and consolidated? These are the big questions we need to ask as we move forward in the world of data.\u00a0\nThe talk by Uber on their Ubermetric system migrating from ElasticSearch to Apache Pinot - which, by the way, is an incredibly flexible and powerful system. We also chatted about Pinot's semi-structured storage support, which is important in modern data engineering.\u00a0\nNow, let's talk about something (non)controversial: the idea that big data is dead. DuckDB brought up three intriguing points to back up this claim.\u00a0\nNot every company has Big Data.\nThe availability of instances with higher memory is becoming a commodity\nEven with the companies have big data; they do only incremental processing, which can be small enough\u00a0\nAbhi Sivasailam presented a thought-provoking approach to metric standardization. He introduced the concept of \"metric trees\" - connecting high-level metrics to other metrics and building semantics around them. The best part? You can create a whole tree structure that shows the impact of one metric on another. Imagine the possibilities! You could simulate your business performance by tweaking the metric tree, which is mind-blowing!\nAnother amazing talk was about cross-company data exchange, where Pardis discussed various ways companies share data, like APIs, file uploads, or even Snowflake sharing. But the real question is: How do we deal with revenue sharing, data governance, and preventing sensitive data leaks? Pardis's startup General Folders, is tackling this issue, becoming the \"Dropbox\" of data exchange. How cool is that?\nTo wrap it up, three key learnings from the conference were:\nThe intriguing idea is that \"big data is dead\" and how it impacts data infrastructure architecture.\nData Catalog as a control plane for modern data stack? Is it a dream or reality?\nThe growing importance of data contracts and the fascinating idea of metric trees.\nOverall, the Data Council conference was an incredible experience, and I can't wait to see what they have in store for us next year."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-125", "title": "Data Engineering Weekly", "content": "RudderStack Transformations lets you customize event data in real-time with your own JavaScript or Python code. Now you can win $1,000 cash by contributing a Transformation to our open-source library.\nhttps://www.rudderstack.com/blog/join-the-transformations-challenge-for-a-chance-to-win/\nThe Real-Time Analytic Summit is on April 25-26 in downtown San Francisco, CA. Come and hear talks from companies like StarTree, Confluent, LinkedIn, DoorDash, Imply, and Uber on how they are advancing the state-of-the-art in user-facing analytics delivered instantly. \nGo to rtasummit.com and register with DEW30 for 30% off.\nPresto and Kafka are the two systems that greatly impacted data infrastructure in the last decade. As with any good system, Presto went through many optimizations. Meta writes an exciting paper detailing how the Presto infrastructure is evolving, focusing on three areas.\n Latency & Efficiency\nScalability & Reliability\nGoing Beyond Data Analytics use cases\nhttps://research.facebook.com/publications/presto-a-decade-of-sql-analytics-at-meta/\nClick here to read the paper.\nTwitter open-source its recommendation engine code. There are some interesting threads on Twitter, but the highlight for me is the design of the Tweet search system. The cluster split approach to store real-time, protected, and archive tweets is an excellent reference model for designing enterprise search engines.\nhttps://blog.twitter.com/engineering/en_us/topics/open-source/2023/twitter-recommendation-algorithm\nTweet Search System (EarlyBird) Design\nhttps://github.com/twitter/the-algorithm/blob/main/src/java/com/twitter/search/README.md\nData is the new code: it is the training data that determines the maximum possible quality of an ML solution. The model only determines the degree to which that maximum quality is realized; in a sense, the model is a lossy compiler for the data. \nGoogle announces DataPerf, the first community, and platform to build leaderboards for data benchmarks. \nI echoed a similar statement here. The Data creation part of a big untapped market in data engineering. \nhttps://ai.googleblog.com/2023/03/data-centric-ml-benchmarking-announcing.html\nMicrosoft writes about its open-source Data Science collaboration package. The approach focuses on standardizing the folder and the file names to simplify the collaboration.\nhttps://medium.com/data-science-at-microsoft/building-a-collaboration-platform-for-a-data-science-team-b37d1d4e3a31\nAs data leaders, one of our top priorities is to measure ROI. From tracking the efficacy of marketing campaigns to understanding the root cause of new spikes in user engagement, we\u2019re tasked with keeping tabs on the health of the business at all levels. But what about the ROI of our own teams? Watch a panel of data leaders as they discuss how to build strategies for measuring data team ROI.Watch On-demand\nPicnic writes about its Change Data Capture pipeline and the lessons learned while integrating Debezium with Postgres. The highlight of the blog for me is,\nHowever, the JSON data the connector produces is a self-describing JSON, meaning that each event has its schema definition attached to it. This blows up the messages massively in size, which increases storage costs.\nAfter the initial phase of running Debezium with JSON, we migrated the data to Avro as serialization format, because \u2014 being a binary format \u2014 it is much more compact, efficient, and supports the use of a schema registry.\nThere is a criticism of how Json loses the context as it travels through the pipelines and the need for self-describing json schema along with the payload. As the author points out, it is simply not a scalable approach. A schema control plan and event-sourcing data plane are more scalable architecture patterns.\nhttps://blog.picnic.nl/using-change-data-capture-for-warehouse-analytics-a1b23c074781\nData Engineering Weekly recently published An Engineering Guide to Data Creation; If you want to chat about Event Creation, please reach out via LinkedIn.\nWell, another week and another moving from the Redshift blog featuring in Data Engineering Weekly. Seriously, come on, Redshift team!!\nKaltura writes about its challenges in maintaining Redshift and the migration strategy to move to Databricks. The blog highlights some of the immediate wins but also highlights the new set of challenges with Databricks. \nhttps://medium.com/kaltura-tech/moving-from-redshift-based-architecture-to-databricks-delta-lake-7a17be6449d7\nWith Device Mode Transformations, you can transform data sent to downstream integrations running in device mode. When destination integrations are set up in device mode, RudderStack loads that tool's native SDK asynchronously and sends event data directly to the destination from the device itself (i.e., from the browser or mobile app).\nRudderStack Product manager, Badri Veeraragavan, details a few big updates to RudderStack's beloved data transformation feature. New features include Python Transformations (including Libraries and Transformations API), Transformation Templates, and Device Mode Transformations. 75% of RudderStack users already leverage Transformations, and now they're even more powerful.\nhttps://www.rudderstack.com/blog/transformations-move-faster-and-build-data-trust/\nNordnet writes about its first journey adopting the Data Mesh concept with the streaming-first, event-driven approach. The blog rightly calls out the challenges of adopting an events-only approach and mitigating it with a \u201cstate dump,\u201d aka bootstrapping approach. \nhttps://medium.com/nordnet-tech/our-first-steps-towards-data-mesh-on-google-cloud-platform-e4aa8eb70236\nTil about Apache Doris, it seems an exciting system to explore. The author highlights the following cases of why Tencent migrated from ClickHouse to Apache Doris.\nPartial Update\nHigh storage cost\nHigh maintenance cost \nI recently had a chance to evaluate ClickHouse and came to a similar conclusion.  The \u201cupsert\u201d operation is not well supported in ClickHouse, which increases the cost and decreases the production cluster stability, directly impacting the system's reliability. \nI found in these design patterns Apache Pinot does much better optimization than many of the OLAP engines. \nhttps://medium.com/geekculture/tencent-data-engineer-why-we-go-from-clickhouse-to-apache-doris-db120f324290\nThe possibility of semantic search in product discovery experience is something I\u2019m looking forward to it, and that is why I found this article very interesting. The author writes about how BlackSquare is rethinking whisky discovery using ChatGPT. \nhttps://data.blacksquare.io/rethinking-product-search-in-the-world-of-chatgpt-57b435b5ce3c\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/podcast-analysis-on-mad-machine-learning", "title": "Data Engineering Weekly", "content": "In this episode of Data Engineering Weekly Radio, we delve into modern data stacks under pressure and the potential consolidation of the data industry. We refer to a four-part article series that explores the data infrastructure landscape and the Software as a Service (SaaS) products available in data engineering, machine learning, and artificial intelligence.\nWe discussed that the siloed nature of many data products has led to industry consolidation, ultimately benefiting customers. Throughout our discussion, we touch on how the Modern Data Stack (MDS) movement has resulted in various specialized tools in areas such as ingestion, cataloging, governance, and quality. However, we also acknowledge that as budgets tighten and CFOs become more cautious, the market is now experiencing a push toward bundling and consolidation.\nIn this consolidation, we explore the roles of large players like Snowflake, Databricks, and Microsoft and cloud companies like AWS and Google. We debate who will be the \"control center\" of the data workload, as many companies claim to be the central component in the data ecosystem. As hosts, we agree it's difficult to predict the industry's future, but we anticipate the market will mature and settle soon.\nWe discussed the potential consolidation of various tools and categories in the modern data stack, including ETL, reverse ETL, data quality, observability, and data catalogs. Consolidation is likely, as many of these tools share common ground and can benefit from unified experiences for users. We also explored how tools like DBT, Airflow, and Databricks could emit information about data lineage, potentially leading to a \"catalog of catalogs\" that centralizes the visualization and governance of data.\nWe suggested that the convergence of data quality, observability, and catalogs would revolve around ensuring clean, trusted data that is easily discoverable. We also touched on the role of data lineage and pondered whether the control of data lineage would translate to control over the entire data stack. We considered the possibility that orchestration engines might step into data quality, observability, and catalogs, leading to further consolidation in the industry.\nWe also acknowledged the shift in conversation within the data community from focusing on technology comparisons to examining organizational landscapes and the production and consumption of data. We agreed that there is still much room for innovation in this space and that consolidating features is more beneficial than competing with one another.\nWe contemplated how tools like DBT might extend their capabilities by tackling other aspects of the data stack, such as ingestion. Additionally, we discussed the potential consolidation in the MLOps space, with various tools stepping on each other's territory as they address customer needs.\nOverall, we emphasized the importance of unifying user experiences and blurring the lines between individual categories in the data infrastructure landscape. We also noted the parallels between feature stores and data products, suggesting that there may be further convergence between MLOps and data engineering practices in the future. Ultimately, customer delight and experience are the driving forces behind these developments.\nWe also discussed ETL's potential future, the rise of zero ETL, and its challenges. Additionally, we touched on the growing importance of data products and contracts, emphasizing the need for a contract-first approach in building successful data products.\nWe also shared our thoughts on the potential convergence of various categories, like data cataloging and data contracts, which could give rise to more comprehensive and powerful data solutions. Furthermore, we discussed the significance of interfaces and their potential to shape the future of the data stack.\nIn conclusion, Matt Turck's blog provided us with an excellent opportunity to discuss and analyze the current trends in the data industry. We look forward to seeing how these trends continue to evolve and shape the future of data management and analytics. Until the next edition, take care, and see you all!\n\nhttps://mattturck.com/mad2023/\nhttps://mattturck.com/mad2023-part-iii/\n"}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-124", "title": "Data Engineering Weekly", "content": "RudderStack Transformations lets you customize event data in real time with your own JavaScript or Python code. Now you can win $1,000 cash by contributing a Transformation to our open-source library.\nhttps://www.rudderstack.com/blog/join-the-transformations-challenge-for-a-chance-to-win/\nI\u2019m excited to attend this year\u2019s Data Council, Austin conference. Last year around this time, Bundling vs. Unbundling was the talk of the town. With the current economic climate, I suppose there is a consensus that bundling is inevitable. I\u2019m excited to listen to all the excellent speakers at the conference, and looking forward to the week of learning. \nData Engineering Weekly readers get a 20% discount by applying \nPromo Code: DataWeekly20\nData Council website: https://www.datacouncil.ai/austin\nThe Real-Time Analytic Summit is on April 25-26 in downtown San Francisco, CA. Come and hear talks from companies like StarTree, Confluent, LinkedIn, DoorDash, Imply, and Uber on how they are advancing the state-of-the-art in user-facing analytics delivered instantly. \nGo to rtasummit.com and register with DEW30 for 30% off.\ndbt publishes the state of analytical [data???\ud83e\udd14] engineering. If you follow Data Engineering Weekly, We actively talk about data contracts & how data is a collaboration problem, not just an ETL problem. The state of analytical engineering survey validates it as two of the top 5 concerns are data ownership & collaboration between the data producer & consumer. Here are the top 5 key learnings from the report.\n46% of respondents plan to invest more in data quality and observability this year\u2014 the most popular area for future investment.\nLack of coordination between data producers and data consumers are perceived by all respondents to be this year\u2019s top threat to the ecosystem.\nData engineers and analytics engineers are most likely to believe they have clear goals, and most likely to agree their work is valued.\n71% of respondents rated data team productivity and agility positively, while data ownership ranked as a top concern for most.\nAnalytics leaders are most concerned with stakeholder needs. 42% say their top concern is \u201cData isn\u2019t where business users need it.\u201d\nhttps://www.getdbt.com/state-of-analytics-engineering-2023/\nVery fascinating to read about the potential impact of LLM in the future of dbt and analytical consulting. The author predicts we are at the beginning of the industrial revolution of computing. \nFuture iterations of generative AI, public services such as ChatGPT, and domain-specific versions of these underlying models will make IT, and computing to date look like the spinning jenny that was the start of the industrial revolution.\n\ud83e\udd3a\ud83e\udd3a\ud83e\udd3a\ud83e\udd3a\ud83e\udd3a\ud83e\udd3a\ud83e\udd3a\ud83e\udd3a\ud83e\udd3aMay the best LLM wins!! \ud83e\udd3a\ud83e\udd3a\ud83e\udd3a\ud83e\udd3a\ud83e\udd3a\ud83e\udd3a\nhttps://www.rittmananalytics.com/blog/2023/3/26/chatgpt-large-language-models-and-the-future-of-dbt-and-analytics-consulting\nOpen source has revolutionized enterprise applications by enabling rapid innovation, collaboration, and cost reduction while increasing transparency and interoperability. Redpoint ventures write about the top 25 open-source companies ranked by adoption, momentum, usage, and health.  \nhttps://cloudinfrastructure.substack.com/p/introducing-the-redpoint-open-source\nNYT publishes an article on data in the life of a senior analyst. The blog highlights that the job is not just writing SQL but providing a strategic business solution for an organization. \nhttps://open.nytimes.com/day-in-the-life-of-a-senior-analyst-in-the-data-and-insights-group-626c5e1e94f1\nStitch Fix writes about the efficient use of generative AI for their marketing and product content generation. I predict that generative AI will greatly impact last-mile analytics delivery. The BI dashboard tools could be simple prompts. It is an exciting breakthrough to watch and adapt.  \nhttps://multithreaded.stitchfix.com/blog/2023/03/06/expert-in-the-loop-generative-ai-at-stitch-fix/\nLearn how Blend, a cloud infrastructure platform powering digital experiences for some of the world\u2019s largest financial institutions, combined cloud-based data transformations and data observability to deliver trustworthy insights faster.\nJoin Live Session\nOne of the curses of adopting Lambda Architecture is the need for rewriting business logic in both streaming and batch pipelines. Spark attempt to solve this by creating a unified RDD model for streaming and batch; Flink introduces the table format to bridge the gap in batch processing. LinkedIn writes about its experience adopting Apache Beam\u2019s approach, where Apache Beam follows unified pipeline abstraction that can run in any target data processing runtime such as Samza, Spark & Flink. \nhttps://engineering.linkedin.com/blog/2023/unified-streaming-and-batch-pipelines-at-linkedin--reducing-proc\nWix writes about managing schema for 2000 (\ud83d\ude2c) microservices by standardizing schema structure with protobuf and Kafka schema registry. Some exciting reads include patterns like an internal Wix Docs approach & integration of the documentation publishing as part of the CI/ CD pipelines. \nhttps://medium.com/wix-engineering/how-wix-manages-schemas-for-kafka-and-grpc-used-by-2000-microservices-2117416ea17b\nThere is much talk about the impact of Machine Learning; however, a stable underlying infrastructure is vital to realize the true potential of Machine Learning. Instacart writes about its usage of Ray and demonstrates how hosting a monolithic service as the computation backend for all distributed ML applications has limitations in scalability, efficiency, and diversity, given the rapidly evolving and highly diversified nature of ML applications.\nhttps://tech.instacart.com/distributed-machine-learning-at-instacart-4b11d7569423\nHeaded to Data Council Austin? Join The Data Stack Show at Scholz Garten on Wednesday, March 29th, for a night of bowling, beers, and brats. You'll have a chance to meet hosts Eric and Kostas in person and nerd out on data with some new friends. \nRegister today\nAirbnb Engineering writes about creating listings categories using machine learning and human expertise. The \"Human-in-the-Loop\" strategy involves training models on a rich dataset of user-generated content, then utilizing human reviewers to validate and refine these models iteratively. Airbnb highlights improving the accuracy of its category predictions and better user experience on the platform with the Human-in-the-Loop approach.\nhttps://medium.com/airbnb-engineering/building-airbnb-categories-with-ml-human-in-the-loop-35b78a837725\nDoorDash writes about switching its feature store from Redis to CockroachDB for efficiency and cost saving. DoorDash highlights the following advantages of CockroachDB over Redis as a feature store.\nSpin up a Redis cluster with the desired number of nodes from the most recent daily backup\nReplay all of the writes from the last day on the new cluster\nSwitch over traffic to the new cluster\nDelete the old cluster\nhttps://doordash.engineering/2023/03/21/using-cockroachdb-to-reduce-feature-store-costs-by-75/\nLyft Engineering introduces Lyft2Vec, an embedding framework representing various entities within the Lyft ecosystem. Leveraging the power of graph-based embeddings, Lyft2Vec captures the relationships and interactions between different entities, such as drivers, riders, and locations. This approach enables Lyft to optimize its services, including dispatching, pricing, and routing. The article outlines the challenges faced, the methodology, and the results achieved, showcasing the potential of Lyft2Vec in enhancing the platform's overall performance.\nhttps://eng.lyft.com/lyft2vec-embeddings-at-lyft-d4231a76d219\nCanva writes about the frequency segmentation at scale using Buy-Till-You-Die(BTYD) model. The blog is very educative for me about measuring the lifetime value of a customer and segmentation on buying behavior. The BTYD model is excellent for building a recommendation engine and marketing personalization. \nhttps://canvatechblog.com/understanding-a-diverse-user-base-with-frequency-segmentation-at-scale-34dc285f0f75\nAdam Brownell writes an in-depth overview of the BTYD model in the blog Customer Behavior Modeling: Buy-til-you-Die Models- A brief intro to the BTYD family, Pareto/NBD, & Pareto/GGG for Predicting Buying Behavior.\nhttps://towardsdatascience.com/customer-behavior-modeling-buy-til-you-die-models-6f9580e38cf4\nAll rights reserved ProtoGrowth Inc, India. We have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/an-engineering-guide-to-data-creation", "title": "Data Engineering Weekly", "content": "All Successful Data-Driven organizations have one thing in common; They have a high-quality & efficient data creation process. Data creation is often the differentiator between the success & the failure of a data team.\nAny business in the modern digital era can be considered a workflow engine. A business process or workflow engine is a software system that enables businesses to execute well-defined steps to complete a user\u2019s intention. It could be either booking a taxi or ordering your lunch; everything is a workflow engine. \u00a0\nLet\u2019s look at the simplified business process of Uber\u2019s ride-sharing. The real-world scenario is much more complex than this, but for the scope of this blog, let\u2019s keep the ride-sharing business process into three simple steps.\nThe riders request a new ride.\nThe ride-share app finds the closest driver and requests to accept the ride.\nOnce accepted, the Rider and Driver connected to complete the journey.\nData engineering starts to add value to the business by capturing events at each step of the business process. The events are then further enriched and analyzed to bring visibility to business operations.\u00a0\nFrom the Event Modeling perspective, In the ride-sharing example, Rider & Driver are called Entities, whereas Request for a Ride & Request for a Driver is called Events. Entities and Events are the two functional units for data creation, and the success of your data organization depends on the stability of the Entity and Event creation architecture.\u00a0\nPlease visit https://github.com/ananthdurai/schemata#schema-classification for a more in-depth understanding of schema classifications. \nThere are three types of architecture patterns in data creation.\nEvent Sourcing\nChange Data Capture [CDC]\nOutbox pattern\nEvent sourcing is a system design pattern that writes the current state of the business process into a journal of records. The journal of records will be immutable, in an append-only format. The journal of records is usually an HTTP server that transports to further downstream consumers to process. \nIn the rider business process, When a rider requests a new ride, the backend system verifies the authenticity of the request & estimates the price. Before finding the driver, it writes a journal event with the following format.\nThe event is a structured data exchange format. It carries all the business context to the downstream consumers and enables them to run multiple analytical processes. \nHowever, Event sourcing comes with a few major limitations.\nIn complex applications, the ride-sharing request can happen via a partner network or back office system where riders call and book a cab or any other possible channel. It leads to a need where a developer should instrument multiple code paths to generate the ride-sharing event. As the developers frequently change the code, it can introduce various bugs in the data generation code. The event may not trigger, have poor instrumentation quality, or have schema incompatibility.\nThe application logic that changes the code path and event-sourcing systems will not be bound to the same transaction boundary. So there will be a case an event might trigger a ride request, but the transactional database may fail the request and vice versa. It leads to an inconsistent state between the downstream systems and the transactional database.\nThe Change Data Capture (CDC) is a technique where instead of applications instrumenting the events, the CDC system monitors and records data modifications, such as insertions, updates, and deletions in the transaction storage and pipe into the downstream systems.\u00a0\nThe CDC system reduces the unpredictable quality & completeness of the events since manual instrumentation is not applied here.\u00a0\nHowever, CDC has also come up with its own set of problems.\nCDC mirrors the transactional data models, a non-ideal data modeling format for the analytical downstream consumers. Let's take the same example of the ride-sharing event. The transactional db might be a model with a list of possible tables.\nrider\nlocation [pickup/ destination]\nprice\nservices\nThe CDC system monitors and continuously streams change data from each table. A downstream streaming middleware should join all the needed streams and create the ride-sharing event for downstream analytical consumption. Stream-stream, combined with a repetitive business logic implementation, is a solution any company wants to avoid. \nOther challenges with CDC include introducing a systematic load on the operation store. There are enough optimization techniques, like reading from isolated replicas and so on, to optimize the infrastructure complexity.\nWhile the CDC is trying to solve the unpredictable quality of events problem, The outbox pattern is trying to address the transactional guarantee of the events.\nThe outbox pattern enables the application developers to write events as part of the transaction while modifying the transactional database. Hence it guarantees consistency between the system of records modification and the events maintained.\u00a0\nThe outbox pattern also helps resolve the causal relationship of events. We can reuse the outbox table timestamp as the event timestamp. Hence any clock drifts in the source system that breaks the causal relationship in the event sourcing method can be averted using an outbox pattern.\u00a0\nA typical outbox table will look like the following.\nAs with any other system, the outbox pattern has its problems too.\nThe relational database support transaction for multiple mutation statements. However, if you use systems like DynamoDB, the transaction support falls under the application or the Data Access Layer. It is expensive to complex to maintain transaction support at the application level.\u00a0\ncache invalidation; distributed financial transactions, search indexing, and similar application integration patterns require distributed transactional support.\u00a0\nIn many cases, transactional support for analytical events may not be a higher priority since completing the business transaction is vital for an application.\u00a0\nAll the issues we discussed regarding the unpredictable quality of events in the event sourcing pattern still apply to the outbox pattern.\nGiven the transactional guarantee depends on the operational data store, the outbox pattern can either support the transaction or not support the transactions. The outbox pattern without transaction support is almost equivalent to event sourcing systems.\nThe fundamental question remains, what is the efficient strategy for event creation? Is it event sourcing or CDC, or outbox pattern?\u00a0\nInstead of thinking of the problem from the technology perspective, Let's think from the business impact perspective. I recently ran this poll to understand the prime pain point in the event sourcing technique.\nThe data engineers say these are the prime concerns in the data creation process.\nLack of Data Modeling\nSchema Breaking Changes\nData Quality\nSchemata build from the ground as a data-modeling-first, data-contract-driven platform. Schemata enables domain-oriented data modeling techniques with a programmatic way to measure how interconnected your data model is.\nSchemata recommends adopting the combination of CDC + (Event Sourcing / Outbox Pattern). \nCapturing Entities and their lifecycle events, such as new rider registration and address change, is easier to source from the CDC pipeline. An Entity change often happens in a single table of the operational system. So it is relatively simpler and reliable to source using the CDC pattern.\nComplex business transactions, on the other hand, can impact multiple tables. Any recreation process from the CDC is expensive. Hence Schemata recommends either the Event Sourcing or the Outbox Pattern for creating events.\nSchemata is a schema modeling framework for decentralized domain-driven ownership of data. Schemata combine a set of standard metadata definitions for each schema & data field and a scoring algorithm to provide a feedback loop on how efficient the data modeling of your data warehouse is.\nYou can find more information about it by visiting \nhttps://schemata.app/.\nIn part 2, We will discuss data creation & the role of data quality. Until then, please share your thoughts in the comments about the Schemata approach to data creation.\nhttps://shopify.engineering/capturing-every-change-shopify-sharded-monolith\nhttps://medium.com/brexeng/transactional-events-publishing-at-brex-66a5984f0726\nhttps://debezium.io/blog/2020/02/10/event-sourcing-vs-cdc/\nhttps://engineering.fb.com/2022/11/09/developer-tools/tulip-schematizing-metas-data-platform/\nhttps://medium.com/meta-analytics/using-log-time-denormalization-for-data-wrangling-at-meta-3b6fc050268a\nhttps://slack.engineering/creating-a-react-analytics-logging-library-2/\nhttps://microservices.io/patterns/data/transactional-outbox.html"}, {"url": "https://www.dataengineeringweekly.com/p/podcast-data-product-oda-reflection", "title": "Data Engineering Weekly", "content": "We are back in our Data Engineering Weekly Radio for edition #121. We will take 2 or 3 articles from each week's Data Engineering Weekly edition and go through an in-depth analysis.\u00a0\nPlease subscribe to our Podcast on your favorite apps.\n\nFrom editor #121, we took the following articles\nOda writes an exciting blog about \u201cData as a Product,\u201d describing why we must treat data as a product, dashboard as a product, and the ownership model for data products.\nhttps://medium.com/oda-product-tech/data-as-a-product-at-oda-fda97695e820\nThe blog highlights six key principles of the value creation of data.\nDomain knowledge + discipline expertise\nDistributed Data Ownership and shared Data Ownership\nData as a Product\nEnablement over Handover\nImpact through Exploration and Experimentation\nProactive attitude towards Data Privacy & Ethics\nhttps://medium.com/oda-product-tech/the-six-principles-for-how-we-run-data-insight-at-oda-ba7185b5af39\n\"Oda builds the whole data product principle & the implementation structure being built on top of the core values, instead of reflecting any industry jargons.\u201d\n\n\"Don't make me think. The moment you make your users think, you lose your value proposition as a platform or a product.\u201d\n\n\"The platform enables the domain; domain enables your consumer. It's a chain of value creation going on top and like simplifying everyone's life, accessing data, making informed decisions.\u201d\n\n\"I think putting that, documenting it, even at the start of it, I think that's where the equations start proving themselves. And that's essentially what product thinking is all about.\u201d\nData Mesh/ Data Product/ Data Contract all the concepts trying to address this problem, and this is a Billion $ $ $ worth of a problem to solve. The author leaves a bigger question, Ownership plays a central role in all these concepts, but what is the incentive to bring Ownership?\nhttps://www.linkedin.com/pulse/some-reflections-talking-data-leaders-peter-bruins/\n\"Ownership. It's all about the ownership.\" - Peter Burns.\n\"The weight of the success (growth of adoption) of the data leads to its failure.\nIs Redshift dying? I\u2019m seeing an increasing pattern of people migrating from Redshift to Snowflake or Lakehouse. Flair wrote a detailed blog on the reasoning behind Redshift to Snowflake migration, its journey, and its key takeaway.\nhttps://craft.faire.com/the-great-migration-from-redshift-to-snowflake-173c1fb59a52\nFlair also opensource some of the utility scripts to make your life easier to move from Redshift to Snowflake\nhttps://github.com/Faire/snowflake-migration\n\"If you left like one percent of my data is still in Redshift and 99% of your data in Snowflake, you're degrading your velocity and the quality of your delivery.\u201d\n\n\n\n\n\n\n"}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-123", "title": "Data Engineering Weekly", "content": "RudderStack Transformations lets you customize event data in real time with your own JavaScript or Python code. Now you can win $1,000 cash by contributing a Transformation to our open-source library.\nhttps://www.rudderstack.com/blog/join-the-transformations-challenge-for-a-chance-to-win/\nIs chatGPT a data product? Is Data a product? What is Data Product, indeed? The author makes an interesting analogy, if you buy your favorite cereal without the box, ingredient details, and other relevant information, do you trust it? I won't trust them. \nThe author defines Data Product as the combination of\nDatasets\nDomain\nAccess\nIt is an exciting time for the data industry as we are increasingly talking about philosophies to adopt data in an organization than technology complexities such as Hadoop, Spark, etc.,\nhttps://sanjmo.medium.com/what-exactly-is-a-data-product-7f6935a17912\nUber writes a comprehensive guide on running incremental ETL using Apache Hudi. The article discusses incremental processing strategy, handling late-arriving data, and backfilling with the design patterns explaining how Apache Hudi simplifies ETL processing. \nhttps://www.uber.com/blog/ubers-lakehouse-architecture/\nData Engineering Weekly talks in detail about adopting functional data engineering principles, and Apache Hudi certainly supports it out of the box.\nWhatnot writes an interesting article about the shift from a loosely coupled system to adopting the Kimbal model centralized core data model generation. The blog discusses implementing Type-2 SCD modeling and strategies to generate surrogate keys and bridge tables to handle many-to-many relationships. \nhttps://medium.com/whatnot-engineering/same-data-sturdier-frame-layering-in-dimensional-data-modeling-at-whatnot-5e6a548ee713\nI keep thinking about the Type-2 SCD and the complexity of the data pipeline. The example Address is a self-contained unit, so it is easier to implement the Kimball-style Type-2 model. What happens in some core models like \u201ccustomers\u201d where more than one dimension change requires tracking? How do you handle these models without compromising scale and usability? Map table vs. using complex data structure? Share your thoughts if you\u2019ve implemented multi-dimensional SCD type-2 changes and how you track it at scale. \nGloat writes about a startup journey adopting a modern data platform with a simplified technology stack.\nDebezium\nKafka\nSnowflake\nAirflow & dbt\nIf anyone is confused with the modern data landscape, the blog is an excellent reminder that you only need a handful of systems to process your data. \nhttps://theblog.workey.co/a-startup-journey-to-a-modern-data-platform-4c6a884f70da\nBeing in the dark is unnerving. Your data team is moving fast, but you can\u2019t be everywhere and test for everything, especially in a small team with limited resources. With a flip of the switch, you can shine a light across all of your data pipelines and tables. Spot bad data before it impacts your stakeholders, costing you time and credibility. Here is an easy, no commitment way to see how you can make a big impact with your data with data observability.\nTry it free\nNetflix recently wrote a series of blogs about its media ML platform. Much of it focuses on model training, evaluation, and scoring. The blog discusses how these ML models integrate with the application to serve users. The blog narrates the use cases for such media ML platforms and discusses the pros & cons of doing on-demand analysis vs. pre-computation. \nhttps://netflixtechblog.com/building-a-media-understanding-platform-for-ml-innovations-9bef9962dcb7\nOn a similar line with the Netflix media ML platform, Expedia discusses the unified ML platform approach to enable innovation across the organization. The blog narrates how Expedia with nine ML systems with no unified way to build and deploy to standardize the infrastructure by adopting build \u2192 optimize \u2192 enhance methodologies. \nhttps://medium.com/expedia-group-tech/unified-machine-learning-platform-at-expedia-group-5aee72606c74\nShopify wrote about its unified ML platform Merlin in the past. Continue expanding Merlin; Shopify writes about Merlin\u2019s online inference system design to unlock real-time prediction services. The platform approach to online inference systems to support no code, low code, and full custom interfaces is an exciting read. \nhttps://shopifyengineering.myshopify.com/blogs/engineering/shopifys-machine-learning-platform-real-time-predictions\nWith Device Mode Transformations, you can transform data sent to downstream integrations running in device mode. When destination integrations are set up in device mode, RudderStack loads that tool's native SDK asynchronously and sends event data directly to the destination from the device itself (i.e., from the browser or mobile app).\nRudderStack Product manager, Badri Veeraragavan, details a few big updates to RudderStack's beloved data transformation feature. New features include Python Transformations (including Libraries and Transformations API), Transformation Templates, and Device Mode Transformations. 75% of RudderStack users already leverage Transformations, and now they're even more powerful.\nhttps://www.rudderstack.com/blog/transformations-move-faster-and-build-data-trust/\nChurn prediction is a vital part of growth engineering to understand the trends in customer churn and deploy mitigation plans to prevent it. The author discusses the problem statement of variable hazard ratios in churn over time and compares churn analysis results of the traditional cox model vs. the time-varying cox model. The article brings back some fond memories as it was the very first ML problem I worked on to predict when an employee will resign from a company :-) Yes, you heard it correctly. \nhttps://medium.com/data-science-at-microsoft/unpacking-churn-with-survival-models-762822132c21\nBuzzFeed writes about adopting generative AI in building products powered by generative AI. I can\u2019t highlight this statement enough.\nAt BuzzFeed, we believe AI will bring a new era of creativity. We think it will open up brand-new content formats, new ideas, and novel ways for content creators to interact with their audience.\nBuzzFeed shares seven learning on integrating generative AI.\nGet the technology into the hands of your employees, especially the creative ones.\nGood and effective prompts are the result of close collaboration between writers and engineers.\nModeration is essential. Build guardrails into your prompt.\nLLMs are not dark magic. Demystifying the technical concepts behind this technology can lead to better application of those tools.\nIntegrating with OpenAI and scaling your usage to thousands of requests per minute is easy, but be prepared for some downtime.\nThe economics of using Generative AI can be tough, especially for ad-supported business models.\nThere are a lot of good tools and resources out there to help you experiment with this technology.\nhttps://tech.buzzfeed.com/lessons-learned-building-products-powered-by-generative-ai-7f6c23bff376\nThere will always be a migration project in data engineering :-) Socure writes another case of migration from Redshift to Snowflake and the deprecation of a few internal systems. The blog narrates the engineering principles they adopted for a large-scale migration and shares some lessons learned along the way.\nhttps://medium.com/the-socure-technology-blog/migrating-large-scale-data-pipelines-493655a47fa6\nWhat is the best orchestration engine for the MLOps? The author compares the choices of Airflow vs. Prefect vs. Kubeflow. The author concludes by continuing with Kubeflow. \nWhat is your choice of Orchestration Engine? Comment on your choice.\nhttps://medium.com/exness-blog/the-best-orchestration-tool-for-mlops-a-real-story-about-difficult-choices-5ee6a087c9e3\nBuilding an efficient on-call culture is critical to bring ownership and operational excellence to operate the data pipeline. The author discusses what data engineers do while on call, the workflow, tools, and complete process associated with running a successful on-call. \nhttps://towardsdatascience.com/how-to-build-an-on-call-culture-in-a-data-engineering-team-7856fac0c99\nI often joke, \u201cThis data catalog tool could be the static website out of dbt docs.\u201d The blog narrates how to build a data catalog without spending money on dbt docs!!! \nhttps://medium.com/hiflylabs/dbt-docs-as-a-static-website-c50a5b306514\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-122", "title": "Data Engineering Weekly", "content": "RudderStack Transformations lets you customize event data in real time with your own JavaScript or Python code. Now you can win $1,000 cash by contributing a Transformation to our open-source library. \nhttps://www.rudderstack.com/blog/join-the-transformations-challenge-for-a-chance-to-win/ \nAt Data Engineering Weekly, We strive to bring the best thought process around building and operating data. However, the newsletter has its limitation. When I read articles for Data Engineering Weekly, there are many instances I say to myself, this is amazing work; I want to talk to the author to learn more from them or discuss it. \nWith this selfish quest for knowing more, Ashwin & I started a new Podcast. We call it Data Engineering Weekly Radio. We take three articles and do an in-depth analysis, and we hope to bring the author of the blogs to discuss more. \nWe will publish all the podcasts in Substack. You can also listen to the podcast in \nApple\nSpotify:\nPlease share your feedback on how we can improve the show further. \nThe author writes about reimagining dbt and suggesting the possible area of improvement in dbt's core product.\nDSLs over Templated Code\nDebuggers [make it easy to debug with code editor breakpoints]\nUnit tests\nI can\u2019t emphasize the importance of DSLs over templated code. The advantage of DSL is that you can make more productivity improvements with compilers such as type safety, debuggers, etc. I expressed a similar thought sometime back.\nPlease reach out if you come across any SQLish DSL for data transformation; I would love to discuss it more, and I hope that will help Pedram to sleep better \ud83d\ude00\nhttps://pedram.substack.com/p/dbt-reimagined\nThere is an increasingly healthy conversation about treating Data as a Product to bring product thinking to Data Asset Creation & Lifecycle management process. Max Illis, a leading thought leader in this space, describes the importance of the Data Product approach and the detailed information checklist to publish, discover and manage a data product. \nhttps://medium.com/@maxillis/on-data-products-and-how-to-describe-them-76ae1b7abda4\nBrex writes an in-depth article about the technical implementation of its CDC pipeline combined with transaction event publishing with an outbox pattern. The blog narrates the architecture to implement an outbox pattern with Debezium, the usage of the outbox router in Debezium, and lessons learned. \nhttps://medium.com/brexeng/change-data-capture-at-brex-c71263616dd7\nNo Data! No Problem, But Partial Data a big problem\nUber highlighted how the partial data caused almost half of their data issues. Uber writes about D3 - an automated system to detect data drift. The blog highlights some common problems with data drift and D3 architecture to detect and alter data drift. \nhttps://www.uber.com/en-US/blog/d3-an-automated-system-to-detect-data-drifts/\nIf implementing data mesh is high on your list of priorities, you\u2019re not alone. As organizations scale their use of data, centralized architectures can prevent data teams from keeping pace with stakeholder demands and system needs. In this guide, learn through strategies deployed by leading data teams that have successfully implemented data mesh.Get The Guide\nPrivacy and access control are the basic components of data engineering. Grab writes about switching access control from a role-based approach to an attribute-based model. The blog highlights some of the practical limitations of role-based access\ntoo many roles in controlling the access\nincrease backlog due to the managerial approval process for granting a role\nStale group membership gives access to members that they should not have.\u00a0\nhttps://engineering.grab.com/migrating-to-abac\nAI/ ML systems came a long way from the McKinsey study of an estimated 88% of machine learning models that were never taken into production in 2017. Feature stores are on the rise, and the author narrates what is feature platform is and the components of a feature platform. \nFeature design\nFeature catalog\nFeature computation engine\nFeature governance \nFeature monitoring\nhttps://medium.com/ibm-data-ai/feature-platforms-a-new-paradigm-in-machine-learning-operations-mlops-24c1ff87b7e1\nWith Device Mode Transformations, you can transform data sent to downstream integrations running in device mode. When destination integrations are set up in device mode, RudderStack loads that tool's native SDK asynchronously and sends event data directly to the destination from the device itself (i.e., from the browser or mobile app).\nRudderStack Product manager, Badri Veeraragavan, details a few big updates to RudderStack's beloved data transformation feature. New features include Python Transformations (including Libraries and Transformations API), Transformation Templates, and Device Mode Transformations. 75% of RudderStack users already leverage Transformations, and now they're even more powerful.\nhttps://www.rudderstack.com/blog/transformations-move-faster-and-build-data-trust/\nCan we apply software engineering principles in managing databases? Though the blog narrates how to adopt CI/CD in managing operational data stores, many ideas expressed in the blog apply to data pipelines. Some of my favorites\nCommit database scripts to version control\nDecouple deployment from data migrations\nKeep changes small\nMake migrations additive\nConsider blue-green deployments\nhttps://hackernoon.com/how-to-manage-databases-with-cicd\nThe fat jar artifact has its latency toll to pay while deploying the Spark job. The obvious choice is to use caching for the libraries to reduce the upload time. Should it be user-level caching or cluster-level caching? LinkedIn writes about dependency caching solutions and why they adopted user-level caching instead of cluster-level. \nhttps://engineering.linkedin.com/blog/2023/reducing-apache-spark-application-dependencies-upload-by-99-\nReverse ETL is an approach to bring data from a central warehouse/lake/lakehouse into real-time operating systems such as Salesforce, Marketo, or Zendesk. The access pattern for Reverse ETL is mostly a bulk fetch and insert approach. The blog narrates how one can optimize SQL Server support reverse ETL workload.\nhttps://medium.com/data-science-at-microsoft/speeding-up-reverse-etl-3af04e069fd1\nDoubleVerify writes about its debt adoption story and how it helped to modernize its data pipelines. TIL about Data Mock Tool (DMT), and looking forward to playing with it. \nhttps://medium.com/doubleverify-engineering/modernizing-data-pipelines-with-dbt-c2941be74b13\nHow far can we go with SQL? The answer is as far as possible. The author demonstrated how to implement online gradient descent in SQL. \nhttps://maxhalford.github.io/blog/ogd-in-sql/\nIf you wonder what is recursive in SQL example, the article below explains a few examples of implementing recursive in SQL. \nhttps://medium.com/swlh/recursion-in-sql-explained-graphically-679f6a0f143b\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-radio-120", "title": "Data Engineering Weekly", "content": "We are back in our Data Engineering Weekly Radio for edition #120. We will take 2 or 3 articles from each week's Data Engineering Weekly edition and go through an in-depth analysis.\u00a0\nFrom editor #120, we took the following articles\nIn this episode, we focus on the importance of data contracts in preventing data quality issues. We discuss an article by Colin Campbell highlighting the need for a data catalog and the market scope for data contract solutions. We also touch on the idea that data creation will be a decentralized process and the role of tools like data contracts in enabling successful decentralized data modeling. We emphasize the importance of creating high-quality data and the need for technological and organizational solutions to achieve this goal.\n\"Preventative data quality rather than reactive data quality. It should start with contracts.\" - Colin Campbell. - Author of the article\n\"Contracts put a preventive structure in place\" - Ashwin.\n\"The successful data-driven companies all do one thing very well. They create high-quality data.\" - Ananth.\nAnanth\u2019s post on Schemata\nIn this conversation, we discuss a framework for data quality assessment called the Action Position framework. The framework helps define what actions should be taken based on the severity of the data quality problem. We also discuss two patterns for data quality: Write-Audit-Publish (WAP) and Audit-Write-Publish (AWP). The WAP pattern involves writing data, auditing it, and publishing it, while the AWP pattern involves auditing data, writing it, and publishing it. We encourage readers to share their best practices for addressing data quality issues.\nAre you using any Data Quality framework in your organization? Do you have any best practices on how you address data quality issues? What do you think of the action-position data quality framework? Please add your comments in the SubStack chat. \nhttps://medium.com/everything-full-stack/action-position-data-quality-assessment-framework-d833f6b77b7\nDremio WAP pattern: https://www.dremio.com/resources/webinars/the-write-audit-publish-pattern-via-apache-iceberg/\nWe discuss the limitations of data catalogs and the author\u2019s view on the semantic layer as an alternative. The author argues that data catalogs are passive and quickly become outdated and that a stronger contract with enforced data quality could be a better solution. We also highlight the cost factors of implementing a data catalog and suggest that a more decentralized approach may be necessary to keep up with the increasing number of data sources. Innovation in this space is needed to improve organizations' discoverability and consumption of data assets.\nSomething to think about in this conversation\n\"If you don't catalog everything and we only catalog what is required for the purpose of business decision-making, does that solve the data catalog problem in an organization?\"\nhttps://www.linkedin.com/pulse/stop-emphasizing-data-catalog-guy-fighel/\n\n"}, {"url": "https://www.dataengineeringweekly.com/p/unlocking-data-stream-processing", "title": "Data Engineering Weekly", "content": "Close your eyes and imagine that you are the proud owner of a small e-commerce website that sells very specific products online. You have a beautiful homepage that showcases your latest products, and testimonials of happy clients, and your customers can easily navigate through your site to find what they're looking for. Your website is easy to use, and your customers can easily add products to their cart, check out, and receive their purchases in just a few clicks.\nWithout you knowing, a famous influencer makes a post about your website: it instantly goes viral. Suddenly, everyone is talking about you on social media, and your website is flooded with traffic. You're getting so many orders that your website, not designed for such peak traffic, becomes unavailable.\nThis is a simple example of what we call the slashdot effect. When you realize what has happened, it is simply too late: the buzz is over. By the time your website is back online - with an improved bandwidth - the potential new customers are gone and you\u2019ve left revenue on the table. You have missed a unique opportunity for your company.What could you have done to address such a spike of connections? Having a more powerful hosting plan, a content delivery network (CDN), or a load balancer, let alone a complete solution like Shopify, is too costly for a small business like yours. The solution would be to do some realtime server monitoring.\nRegularly monitoring your website's performance can help you detect issues early on and take steps to fix them before they become a problem. Another frequent kind of problem is the deployment of a faulty version - you don't have much time to act then. Detecting such an anomaly only 15 minutes earlier can be enough to avert disasters:\n\u201cIf you aim for 99.99% availability, you have approximately 15 minutes of error budget per quarter. The build step of rolling forward may take much longer than 15 minutes, so rolling back impacts your users much less.\u201d (from Google SRE Book)\nNow, let\u2019s go back to our example and say you had such performance monitoring: you monitor your nginx logs and keep track of how much traffic your website has for the last 5 minutes and an alert is sent if an abnormal increase in traffic is observed. Following the buzz, you receive an Slack notification to inform you about the burst of traffic, allowing you to quickly improve the performance of your website. Your website handles the spikes properly and you fully benefit from the buzz, cashing in revenue.\nThat\u2019s the power of real-time server monitoring.\nThe number of connections is an example of a more general problem known as real-time statistical analysis, which computes and maintains different statistics on the latest data points.You could compute some statistics about your server, such as the median session duration during the past week, or do more advanced techniques, such as anomaly detection. Realtime statistical analysis is challenging and traditional techniques such as sliding windows offer an unsatisfying tradeoff between accuracy and computation costs.\nWe see how to lift these hurdles by using an open-source library - Pathway. Pathway is a Python framework for realtime data stream processing that handles updates for you. You can set up your processing pipeline, and Pathway will ingest the new streaming data points for you, sending you alerts in realtime.Let\u2019s dig into realtime statistical analysis!\nIn order to do realtime statistical analysis in streaming systems, such as realtime server monitoring, we only consider the most recent data. Only the most relevant data is kept, i.e. the last ten minutes, and processed in order to detect anomalies or changes in data patterns over time. This portion of the data is called a window. Windows techniques are widely used in stream data processing to analyze and summarize data over a fixed period of time.\nA window is a fixed-size buffer that moves through the data stream at regular intervals, typically in a sliding or overlapping manner. As the window moves through the data stream, it collects and processes a fixed number of data items or a fixed duration of data.\nThe two main windows techniques that are commonly used in stream data processing, are:\n1. Tumbling windows: A tumbling window divides the data stream into fixed-size, non-overlapping time intervals. Each window collects and processes a fixed number of data items or a fixed duration of data, after which the window is closed and a new window is opened.\n2. Sliding windows: A sliding window moves through the data stream in a fixed-size, overlapping manner. Each window collects and processes a fixed number of data items or a fixed duration of data, after which the window is moved forward by a fixed amount.\nFor example, you could use tumbling windows to detect suspicious user activity.For a realtime monitoring, however, you would usually prefer a sliding window over tumbling ones as the latter cut the data in non-overlapping parts: a wrong cut could prevent to detect the spike.In our website monitoring example, you can decide to send an alert when the number of connections during the window reaches a given threshold. You may have the following situation with a tumbling window:\nSuch a cut can fail to discover a spike soon enough, as there was not enough traffic at the end of the second tumbling window to trigger the alert. By the time the alert is launched by the last window, it will be too late. With a sliding window, you can avoid such a situation:\nAs you can see, using a sliding window can lower the probability of missing the beginning of an anomaly.\nSliding windows consider the most recent data at the time of computation: its efficiency will be highly dependent on the timing of the computation. The timing of the start of the window is usually defined by the user, periodically.\nIf you update the sliding window too often, you will waste a lot of resources, making redundant and useless computations: in the extreme case where there was no data point, computing any sliding window is useless. On the other hand, if you wait too long between two updates you will miss alerts just like with tumbling windows.\nChoosing the good timing to compute sliding windows is hard, especially when the data behaves in an unexpected way, which is usually the case when dealing with anomalies.\nWith Pathway, you can easily make sliding windows which are event-triggered: for example the windows can be updated automatically every time a new data point is received.\nNot only the sliding window will be updated whenever a new data point is received, but the computation is not done from scratch, it is updated with the new values: even if the computation is triggered often, its cost is limited.\nLet\u2019s now compute such a sliding window using Pathway.\nYou need two input data streams for your sliding window: one with the events, the new connections to our website for example, one for triggering the computations, containing timestamps. With Pathway, you can easily derive the timestamps datastream from the event one, updating your sliding window whenever a new update is received.\nUsing Pathway\u2019 connectors you can create two tables containing the two input streams, and then compute the sliding window with them:\nYou can use two different input streams, choosing the timestamps of `ts_table` yourself (e.g. every five minutes), but as previously said, you can also easily derive the `ts_table` from the `log_table`. Assuming that all timestamps are stored in a column `ts`, you compute `ts_table` as follows:\nThis will create a table containing the timestamp associated with the latest event in log_table.\nImplementing a `sliding_window` function can be done in Pathway with a couple of lines:\nUsing the sliding window, you can trigger an alert when the number of connections within the last five minutes is higher than a given threshold:\nLet\u2019s put all the pieces together:\nWhenever a new data point is received, each table is updated: old entries are left out during the filter in favor of new ones and whenever the number of kept entries is higher than the threshold the alert value is set to true. \nThe computation on the sliding window is not done from scratch but updated from the previous value, and all of this is automatically handled by Pathway.\nUsing Pathway\u2019 output connectors, you can output the results to your favorite data storages. You may want to send your statistics about your traffic to ElasticSearch or send alerts messages directly to your slack for better reactivity.\nIf you want to see how to redirect nginx logs to Pathway using Filebeat and then forward the alerts to a slack channel, check out our full realtime log monitoring example.\nNow your site reliability engineers can build sophisticated anomaly detection algorithms and not have to worry about looking for traffic deviations in tools like ElasticSearch.\nRealtime statistical analysis is a key part of realtime processing of streaming data.\nSuch analysis is challenging, and existing windows techniques are limited by the fact that they are triggered periodically, without any awareness of the events. This applies to log, IoT, and semi-structured machine generated data. With streaming data, there will be a time when the setting will lead to either a waste of resources or a loss in accuracy.\nWith event-based windows, we have efficient computing on latest data.\nWith Pathway, you can define your pipeline and Pathway will handle the updates for you.\nIf you come up with your cool anomaly detection model, or simply like to dive deeper into your specific use case, we\u2019d love to hear from you: don\u2019t hesitate to join us on our discord.\nAll rights reserved ProtoGrowth Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/podcast-data-engineering-weekly-119", "title": "Data Engineering Weekly", "content": "We are super excited to be back to discussing Data Engineering Weekly Newsletter articles every week. We will take 2 or 3 articles from each week's Data Engineering Weekly edition and go through an in-depth analysis.\u00a0\nOn Data Engineering Weekly edition #119, We are taking three articles.\n#1 Netflix's article about Scaling Media Machine Learning at Netflix\nhttps://netflixtechblog.com/scaling-media-machine-learning-at-netflix-f19b400243\n#2 Alex Woodie's article about Open Table Formats Square Off in Lakehouse Data Smackdown\nhttps://www.datanami.com/2023/02/15/open-table-formats-square-off-in-lakehouse-data-smackdown/\n#3 Plum Living's article about Building a semantic layer in Preset (Superset) with dbt\nhttps://medium.com/plum-living/building-a-semantic-layer-in-preset-superset-with-dbt-71ee3238fc20\nWe referenced David Jayatillake's article about Metricalypse in the show."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-121", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nThe tweet about Data Quality & Data Contract ownership triggers an interesting conversation. I sit with David Jayatillake & Kevin Hu of Metaplane to discuss the ownership and organization dynamics. \nhttps://www.linkedin.com/video/event/urn:li:ugcPost:7036005867741159424/\nThe conversation also leads to interesting conversations in the data-folks Mastro channel. Link to the conversation\nhttps://techhub.social/@datacequia/109966388223034317\nChatGPT might write an essay, Midjourney could create beautiful illustrations, or MusicLM could compose a jingle. On the one hand, they may seamlessly complement human labor, making us more productive and creative; on the other, they could amplify the bias we already experience or undermine our trust in information. Stanford HAI published its perspective on Generative AI in this extensive report. \nhttps://hai.stanford.edu/generative-ai-perspectives-stanford-hai\nMeta writes about four analytical best practices to ensure the most trustworthy and responsible data-driven decisions across the company. The basics of the best practices are to establish Meta\u2019s Ground Truth Maturity Framework [GTMF]\nhttps://medium.com/@AnalyticsAtMeta/four-analytics-best-practices-we-adopted-and-why-you-should-too-a1058ce5f8af\nEasy access to the datasets is 80% of the problem solved in data engineering. Google provides Dataset Search, a dedicated search engine for datasets, powers this feature and indexes more than 45 million datasets from more than 13,000 websites. \nhttps://ai.googleblog.com/2023/02/datasets-at-your-fingertips-in-google.html\nNetflix writes about a unique challenge of its annotation pipeline: the need to support multiple runs of the same annotation tasks. In the date version partition table, we override the partition or swap the version from one bucket location to another; However, Netflix requires these outputs to be searchable and findable as soon the job is finished. The blog narrates how they overcome the challenge with the combination of Cassandra & ElasticSearch\nhttps://netflixtechblog.medium.com/data-ingestion-pipeline-with-operation-management-3c5c638740a8\nIf implementing data mesh is high on your list of priorities, you\u2019re not alone. As organizations scale their use of data, centralized architectures can prevent data teams from keeping pace with stakeholder demands and system needs. In this guide, learn through strategies deployed by leading data teams that have successfully implemented data mesh.Get The Guide\nData Mesh/ Data Product/ Data Contract all the concepts trying to address this problem, and this is a Billion $ $ $ worth of a problem to solve. The author leaves a bigger question, Ownership plays a central role in all these concepts, but what is the incentive to bring Ownership? \nhttps://www.linkedin.com/pulse/some-reflections-talking-data-leaders-peter-bruins/\nIs Redshift dying? I\u2019m seeing an increasing pattern of people migrating from Redshift to Snowflake or Lakehouse. Flair wrote a detailed blog on the reasoning behind Redshift to Snowflake migration, its journey, and its key takeaway. \nhttps://craft.faire.com/the-great-migration-from-redshift-to-snowflake-173c1fb59a52\nFlair also opensource some of the utility scripts to make your life easier to move from Redshift to Snowflake\nhttps://github.com/Faire/snowflake-migration\nRudderStack details three practical ways to make your data stack more cost-effective and your data team more efficient. \nEliminating integration engineering work. \nUsing your data warehouse instead of an expensive CDP. \nUsing APIs, instead of expensive 3rd party services, for data enrichment. \nIt includes details on an interesting use of a data transformation (written in Python), Webhook, and an internal signup API to streamline app signups from their marketing site. A key highlight of the blog\nTheir data team leveraged a cost-effective geolocation API to solve the problem. In a RudderStack Transformation, they passed the user\u2019s IP address to the service and appended the returned region to the payload, which was passed into a custom field in the marketing platform.The marketing team was then able to automatically segment users into regional lists and trigger location-based offers in real-time.\nhttps://www.rudderstack.com/blog/three-architectures-to-make-your-data-stack-more-efficient-in-2023/\nOda writes an exciting blog about \u201cData as a Product,\u201d describing why we must treat data as a product, dashboard as a product, and the ownership model for data products. \nhttps://medium.com/oda-product-tech/data-as-a-product-at-oda-fda97695e820\nThe blog highlights six key principles of the value creation of data.\nDomain knowledge + discipline expertise\nDistributed Data Ownership and shared Data Ownership\nData as a Product\nEnablement over Handover\nImpact through Exploration and Experimentation\nProactive attitude towards Data Privacy & Ethics\nhttps://medium.com/oda-product-tech/the-six-principles-for-how-we-run-data-insight-at-oda-ba7185b5af39\nSure, Storage is Cheap, but how do you define Cheap? I see a pattern where increased attention to optimizing storage cost by applying an efficient compression pattern. Uber has written about Cost Efficiency at Scale in Big Data File Format. Hubspot writes about one such saving from converting from Json log format to Snappy+ ORC. \nhttps://product.hubspot.com/blog/savings-logging-part1\nhttps://product.hubspot.com/blog/savings-logging-part2\nData Council - Austin 2023 is nearing, and I\u2019m excited to meet all the data practitioners in person. Data Engineering Weekly readers can use the DataWeekly20 promo code to get a 20% discount on the ticket price.\nLink to Register: https://www.datacouncil.ai/austin\nPromo Code: DataWeekly20\nAccurately measuring the effect of digital campaigns has been affected by privacy changes initiated by Apple, a decline in third-party cookie data, increased usage of incognito browsing, information loss due to cross-device usage, and multiple touches along the customer journey. The answer, according to Meta, it\u2019s Geo-Testing. Expedia writes about how it runs market segmentation for Geo-Testing at scale, common Geo-Testing challenges, and how to use market segmentation to resolve them. \nhttps://medium.com/expedia-group-tech/market-segmentation-for-geo-testing-at-scale-8d593e0aa755\nEvolving a model from one version to another version or migrating to another target is inevitable in the data pipeline. Indicium writes about how it uses the dbt\u2019s audit_helper package. \nhttps://medium.com/indiciumtech/audit-helper-in-dbt-bringing-data-auditing-to-a-higher-level-3afe0385cd5\nAll rights reserved ProtoGrowth Inc, India. I have provided links for informational purposes and do not suggest endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-120", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nI talked about Schemata, Data Product Management, and Data Contracts in the last few weeks. Ashwin is running an excellent Data Community, \u201cData Heros,\u201d where I interacted with the Data Heros community folks to discuss Schemata & Data Contract.\nIf you\u2019ve not been following the Data Heros community, please do follow; They engage in a highly productive data engineering conversation. I highly recommend following their LinkedIn group for updates.\nhttps://www.linkedin.com/company/data-heroes-community-for-data-folks/ \nI sit with Scott Hirleman on Data Mesh Radio to talk about How We Make Data Contracts Easy, Scalable, and Meaningful. In the conversation, I discussed why collaboration around data is crucial and how data creation is a human-in-the-loop problem. You can hear the full episode here.\nhttps://daappod.com/data-mesh-radio/easy-scalable-meaningful-data-contracts-ananth-packkildurai/\nThe author published the case for a data contract, capturing the current state of the data contract marketplace and potential players in the market. The author points out that Data contracts are a technical implementation, not an organizational one. I believe Data Contract is a technology solution to bring organizational change. It is something like how Kubernetes is a technology solution, at the same time, drives the system architecture to certain characteristics. Data Contract platforms are the same, so this space is wide open, waiting for disruption. \nhttps://uncomfortablyidiosyncratic.substack.com/p/the-case-for-data-contracts\nIt\u2019s time for Matt\u2019s 2023 MAD landscape with 1,416 logos, up from 139 in 2012. the addition of the \u201cFully Managed\u201d category is an exciting space to watch out for. Perhaps merge Data LakeHouse & Data Warehouse in the next edition? The gap is blurring between them. \nhttps://mattturck.com/mad2023/\nWe talked in the past about Data Catalog - A broken promise, and how data catalogs can become quickly outdated and may not reflect the current state of the data assets within an organization. We also talked about how Data Catalog operates in a disjointed workflow leads to usability nightmares. Similarly, the author points out that the semantic layer is a more efficient and dynamic approach to data catalogs. This an interesting observation to keep an eye on. \nhttps://www.linkedin.com/pulse/stop-emphasizing-data-catalog-guy-fighel/\nIf implementing data mesh is high on your list of priorities, you\u2019re not alone. As organizations scale their use of data, centralized architectures can prevent data teams from keeping pace with stakeholder demands and system needs. In this guide, learn through strategies deployed by leading data teams that have successfully implemented data mesh.Get The Guide\nOne of the exciting case studies from Chase is about modernizing its data platform. The five series emphasize any modernization should bring its people along with a sufficient uplifting program. \n600 ETL developers using point-and-click tools could be reskilled to adopt the solution [Code first approach with Spark & Java]\nPart 1: Setting the Stage for Change\nPart 2: Our Pilot Phase and the Beginning of a Modernization Journey\nPart 3: Modernization at Scale \u2014 Starting with People\nPart 4: Accelerating Data Modernization \u2014 Execution Methodology\nPart 5: Lessons Learned (So Far)\nThe author writes about practical difficulties in building data products from the raw data and how Virgin Media O2 adopted Riffing engineering process to navigate it. Riffing is a 5 step process that contains\nWhat is the goal?\nIdentify and study the raw data.\nModeling\nTest and optimize the output\nProductionise into a usable format\nhttps://medium.com/@vmo2techteam/riffing-our-recipe-for-iterating-fast-failing-forward-and-achieving-success-with-data-fd218fda1041\nThe GA4 migration deadline is fast approaching. If you\u2019re still heavily reliant on Google for data collection and reporting, now is the perfect time to center your data analytics strategy around your data warehouse. Join our webinar to learn how you can replace GA with analytics on your data cloud.https://www.rudderstack.com/events/replacing-ga4-with-analytics-on-your-data-cloud/\nMany companies show their technical excellence in their blogs. I\u2019m thrilled to see Funding Circle writes about its Data Engineering culture. A good data team culture is vital to establish data-driven culture across the companies, and I hope many companies will write about its Data Engineering culture. \nhttps://medium.com/funding-circle/data-engineering-culture-fc-445142a51ace\nIf your data engineering team has not yet adopted either the \u201cWrite-Audit-Publish\u201d or \u201cAudit-Publish-Write\u201d pattern, the time to implement the pattern is yesterday :-). The author published a data quality assessment framework for the pipeline patterns, including monitoring & investigation of data quality issues.\nhttps://medium.com/everything-full-stack/action-position-data-quality-assessment-framework-d833f6b77b7\nAncestry writes about using Apache Iceberg and its optimization strategy to update Iceberg tables. The solution mainly focuses on partitioning and compacting the Iceberg tables.\nhttps://medium.com/ancestry-product-and-technology/scaling-ancestry-com-how-to-optimize-updates-for-iceberg-tables-with-100-billion-rows-860285922316\nStandardization & tracking the decision of experimentation is vital for the success of a data-driven organization. Square writes about why one should require an Experimentation template and share their copy for usage.\nhttps://developer.squareup.com/blog/why-you-need-an-experimentation-template/\nTemplate: https://assets.ctfassets.net/1wryd5vd9xez/5ag9t08L5hiRX1dwDDG1Rm/9012733fee27a264a8a10dcb8253083e/Genericized_A_B_template.pdf\nData Council - Austin 2023 is nearing, and I\u2019m super excited to meet all the data practitioners in person. Data Engineering Weekly readers can use the DataWeekly20 promo code to get a 20% discount on the ticket price.\nLink to Register: https://www.datacouncil.ai/austin\nPromo Code: DataWeekly20\nPrompt engineering is a relatively new discipline for developing and optimizing prompts to efficiently use language models (LMs) for various applications and research topics. The repo contains exciting reference materials to learn more about prompt engineering.\nhttps://github.com/dair-ai/Prompt-Engineering-Guide\nThe chatGPT genuinely increased the curiosity about LLM (Large Language Model), with that MetaAI open-sourced LLaMA (Large Language Model Meta AI), a state-of-the-art foundational large language model. Recently I switched to using chatGPT and GitHub co-pilot extensively for my coding, so I\u2019m excited about this space and the innovations. \nhttps://ai.facebook.com/blog/large-language-model-llama-meta-ai/\nThe advancement of AI/ ML techniques and the emerging LLMs create valid concerns over the impact of AI on privacy and its social impact. LinkedIn shares its Responsible AI principles as\nAdvance Economic Opportunity\nUphold Trust\nPromote Fairness and Inclusion\nProvide Transparency\nEmbrace Accountability\nIt will be interesting to learn more about how LinkedIn will monitor and measure the success of these principles and how transparent the findings will be. https://engineering.linkedin.com/blog/2023/linkedin-s-responsible-ai-principles-help-meet-the-big-moments-i\nAll rights reserved ProtoGrowth Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-119", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nNetflix writes about media machine learning infrastructure and media-focused ML infrastructure to reduce the time from ideation to productization for media ML practitioners. The focus is to bring in data in-specific to their media assets and build a feature store. Seeing a pattern similar to Data Mart emerging in ML infrastructure is interesting. Is it the beginning of a domain-specific ML platform? \nhttps://netflixtechblog.com/scaling-media-machine-learning-at-netflix-f19b400243\nDoorDash, as a real-time supply chain optimization problem, is an interesting way to look at their business. DoorDash writes about the ML lifecycle process from ideation to partnering with the customers to verify the efficiency to reduce the dasher\u2019s waiting time. \nhttps://doordash.engineering/2023/02/15/lifecycle-of-a-successful-ml-product-reducing-dasher-wait-times/\nOur day-to-day life increasingly depends on AI & ML systems. We trust AI decision-making better than human decision-making to eliminate human bias. However, we can\u2019t forget that these AI/ ML systems also build by humans with bias. The blog discusses fairness in ML and demonstrates why high accuracy doesn\u2019t mean the algorithm is fair. \nhttps://medium.com/data-science-at-microsoft/measuring-fairness-in-machine-learning-3211b62340b\nIf implementing data mesh is high on your list of priorities, you\u2019re not alone. As organizations scale their use of data, centralized architectures can prevent data teams from keeping pace with stakeholder demands and system needs. In this guide, learn through strategies deployed by leading data teams that have successfully implemented data mesh. Get The Guide\nFoodpanda, in a similar line of application as DoorDash, talks about optimizing menu ranking by applying A/B testing. The blog mostly focuses on how it optimizes the batch pipeline using Airflow and BigQuery.\nhttps://medium.com/foodpanda-data/menu-ranking-422ad21f381e\nShopify writes about its analytical engineering process to build a commerce data model to simplify analytics for non-SQL users. The process applies to any analytical engineering practices that \nstart with the business problem.\nDesign a mock data\nFind the data\nAssess Data Quality and Consistency\nAssess Model freshness\nAssess Model performance\nhttps://shopifyengineering.myshopify.com/blogs/engineering/building-commerce-data-models-with-shopifyql\nThe GA4 migration deadline is fast approaching. If you\u2019re still heavily reliant on Google for data collection and reporting, now is the perfect time to center your data analytics strategy around your data warehouse. Join our webinar to learn how you can replace GA with analytics on your data cloud.https://www.rudderstack.com/events/replacing-ga4-with-analytics-on-your-data-cloud/\nThe Open LakeHouse format is emerging as the defacto storage format for Data Warehouses. The blog compares the features available from Apache Hudi, Iceberg & DeltaLake. The author recommends\n\u201cIf you\u2019re looking for full-featured, more real-time, go with Hudi; if you\u2019re Spark-oriented and very much in the Databricks ecosystem, that choice is obvious. If you\u2019re looking for something with multivendor support right now, go with Iceberg.\u201d \nhttps://www.datanami.com/2023/02/15/open-table-formats-square-off-in-lakehouse-data-smackdown/\nThe semantic layer in the edge to define metrics is gaining adoption as dbt labs acquired Transform recently. Plum Living writes about integrating the dbt semantic layer with Superset and the developer workflow.\nhttps://medium.com/plum-living/building-a-semantic-layer-in-preset-superset-with-dbt-71ee3238fc20\nOptimizing Kafka consumer resources is indeed an exciting problem; Tinybird writes about its optimization problem with Kafka connect, and it uses the rendezvous hashing to balance the consumer workload.\nhttps://www.tinybird.co/blog-posts/kafka-horizontal-scaling\nData Council - Austin 2023 is nearing, and I\u2019m super excited to meet all the data practitioners in person. Data Engineering Weekly readers can use the DataWeekly20 promo code to get a 20% discount on the ticket price.\nLink to Register: https://www.datacouncil.ai/austin\nPromo Code: DataWeekly20\nEvery cloud company offers multiple tools to run analytics, and Google Cloud is no different. The author compares BigQuery, Dataflow, and Spark in Google cloud to measure performance and cost. Dataflow, not surprisingly, costs much more than Spark and BigQuery. \nThank you, Souryhna Luangsay, for submitting this article to Data Engineering Weekly Github.\nhttps://medium.com/cts-technologies/bigquery-spark-or-dataflow-a-story-of-speed-and-other-comparisons-fb1b8fea3619\nMLFlow is an open-source platform for managing the machine learning lifecycle. I found this blog an amazing introduction that talks about how Mlflow can help to build the ML pipeline from training, registering, and serving the model.\nhttps://towardsdatascience.com/end-to-end-ml-pipelines-with-mlflow-tracking-projects-serving-1b491bcdc25f\nTIL about Postgres Filter, a much better readable expression than a case statement. It turns out that Filter not only increases readability but also improves performance.\nhttps://www.crunchydata.com/blog/using-postgres-filter\nPerformance Benchmark: https://blog.jooq.org/the-performance-impact-of-sqls-filter-clause/\nFinally, Transformers have accelerated the development of new techniques and models for natural language processing (NLP) tasks. I found the Transformers-Recipe Github Repo a very informative source to learn more about transformers.\nAll rights reserved ProtoGrowth Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/pathway-unlocking-data-stream-processing", "title": "Data Engineering Weekly", "content": "We have now entered the era of data. Data is everywhere, and people who know how to use the data have become the new stars of the industry, with the proliferation of new data titles: data engineer, data scientist, data analyst, data ops engineer...\nThe reasons for this data boom are two-fold: we can now process the data \u2013both in terms of hardware and software\u2013 and the data is large enough to train models. To increase the value of data analytics, more and more data is generated: new captors are put everywhere, broadcasting all the time. This trend, sometimes dubbed Internet-of-Things, had some unexpected consequences: your fridge and lamps can be connected to the internet, and you can even have a bitcoin miner heater!\nAll these connectors created a new kind of data: streaming data. The data is now organized in data streams: a never ending stream of new data points. The temporal nature of this data makes it extremely useful for data analytics such as prediction. However, it raises a new issue: how can we train a model on dynamic data?\nUsually, we train our models using a batch of data, assuming that the distribution of the data will remain the same.\nwe assume that the data will behave in the future the same way it has in the past: this assumption is impossible for streaming data. \nWe need a new way to keep our models up-to-date and relevant according to the latest data points.\nHow to keep up with changing data? In this article, we will use Pathway. Pathway is a data processing framework that takes care of streaming data updates for you. You can build your processing pipeline (data preprocessing, data modeling, etc.), and then Pathway will ingest the new streaming data points and maintain your models and tables updated in realtime!\nLet\u2019s see how to create a simple linear regression model on data streams!\nA simple linear regression is a linear approach to modeling the relationship between data points using a single explanatory variable. In practice, we have a data stream of data points (xi,yi\u200b), and we want to compute the two parameters (a,b) so that, for each point (xi,yi\u200b), yi\u200b\u200b can be approximated by yi \u2248 a+b*xi\u200b.\nIn a nutshell, we have a set of points, and we want to draw a line that approximates the best of all those points.\nWhile being one of the simplest models, linear regression is also one of the most used: it is frequently used to estimate trends such as sales forecasting or house pricing.\nWe are not going to explain the mathematical details here; if you are interested, you can find all the details in the Wikipedia article.\nSo, what\u2019s wrong with the standard approach? Why can\u2019t we just apply the formula and iterate every time a new point is received?\nBecause the formulas for the slope and the intercept rely on the difference between each xi and the sum of the data points: you need to recompute the sums, which is the same as starting from scratch at every update. This would be time-consuming and prevent the model from being updated in realtime.\nFurthermore, we cannot use only a subset of the data points as the data may be changing. As an example, consider these 10 data points:\nWe could simply do the regression on these points and obtain the red line. But let's take a look at what happens further in time after 30 new points have been added:\nIn red, we have the previous regression, and in green, the one computed on all 40 points: they are clearly diverging!\nThe first model was a clear overfitting and did not really represent the reality of the data. To avoid such overfitting, we need to incorporate all the data we have, and not only a subset!\nPathway is a Python framework for streaming data processing: you create your pipeline as if you were handling static and finite data, and Pathway will manage the updates for you!\nLet\u2019s compute the regression using Pathway. We assume that we have a connector that inputs the data points (xi,yi) into a table t. We need to compute the sum of the xi\u200b, of the xi2\u200b, of the yi,\u200b and of the xi*yi\u200b and the total number of data points received until then. This is done as follows:\nThen we can compute the estimation of a and b:\nAnd that\u2019s it! Whenever a new data point is inserted inside the table t, Pathway will automatically update all the tables, updating the values a and b of the table results_table.\nWe will insert a list of data points around the line y=x (i.e. a=0 and b=1), with a small error on the y values:\u00a0\nBy inserting those values into the table t, we obtain the following values in the table results_table:\nEach row contains the current values of a and b, the logical time of the update, and a flag diff representing whether the row represents an addition or a deletion. An update is represented by two rows: one to remove the old value and one to add the new values. Those two rows have the same logical clock to ensure the atomicity of the operation.\nAs we can see, the obtained values converged to what we expected (a=0 and b=1). These values were updated whenever new values were added in realtime!\nTo learn more about how to connect data streams into Pathway, you can take a look at the full example on Pathway\u2019s website.\nI hope you are convinced that data streams are as challenging as they are an opportunity. This huge amount of data is extremely valuable, but it cannot be processed the same way we process static data.\nPathway simplifies the processing of data streams and the building of ML models over it: you can build your processing pipeline as if you were handling static data, and then the Pathway engine will manage all the updates for you.\nI strongly recommend you not to take everything I said as granted but to test Pathway by yourself \ud83d\ude42\nAll rights reserved ProtoGrowth Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/the-data-founder-story-why-we-founded", "title": "Data Engineering Weekly", "content": "\nMy name is Adi Gelvan, and I co-founded Speedb in November 2020 in Israel with two former colleagues. Speedb is a next-generation KVS storage engine that\u2019s a drop-in replacement for RocksDB, the de facto industry standard. We open-source it to the developer community based on technology delivered in an enterprise edition for the past two years. \nYou can give a Github star for Speedb here: https://github.com/speedb-io/speedb.\nI\u2019d like to begin by making a confession in the spirit of the most important value of open source companies - transparency and openness: \nUntil about 2 years ago, I didn't even know what a storage engine was.\nAll three of us co-founders were working at an enterprise storage unicorn, where I was the CRO. We were experimenting with various settings and performance tuning in the RocksDB key value storage engine, but we quickly found that when the volume of metadata reached more than 100GB, RocksDB could not keep up. My two co-founders, both brilliant technologists, tried sharding to break down the dataset into smaller slices that are more manageable -  but this proved to be a cumbersome approach that resulted in increased complexity. \nI remember the first time that Hilik, Speedb\u2019s chief scientist and co-founder, came to me all excited, saying that he has built what he thought could be the future of storage engines.\nI was pretty embarrassed to tell him: \u201cCool, that\u2019s really great, but what the hell is a storage engine?\u201d \n\u201cAh,\u201d Hilik replied, \u201cyou surely know about LSM trees, right?\u201d \nI looked a bit dazzled and answered, \u201cYeah, I mean, I think so.\u201d (Spoiler alert: I didn\u2019t have a clue, despite a rather long career in the storage industry.) So my first motivation was to avoid feeling like an idiot in my next talk with Hilik. \nThe amount of metadata we had in our former company\u2019s product was growing way beyond our expectations. We realized the only way to have something working in a reasonable time was to embed an enterprise-grade KVS into our product. \nChoosing RocksDB as our storage engine was a pretty straightforward decision for us because:\nIt was developed by Facebook\nIt was forked from Google's LevelDB\nIt is used by thousands of developers and clients worldwide \nIndeed, RocksDB performed smoothly at the beginning. However things became challenging when we started loading more and more metadata into the system. RocksDB is based on a log structured merge (LSM) tree design. LSM trees suffer from write amplification factor (WAF): The more data is written, the number of LSM levels increases and results in poor performance. After investigating this thoroughly, we found that many other RocksDB users have the same problem.\n As we added more data to the system, we began to see more behaviors like I/O hangs, stalls, excessive use of CPU and memory, and other issues that stem from RocksDB\u2019s high WAF. \nPretty soon, it became clear to us: \nReducing Rocksdb\u2019s high WAF can solve many of the performance and scalability issues that resulted from the underline storage engine. \nFurthermore, we could see how the scalability challenge was especially intractable in applications such as analytics, AI and ML - high-growth sectors where cloud-scale ramp ups were common. Yet not one vendor was addressing this very horizontal pain point. \nSo that was exactly our next step. We founded Speedb. Hilik and his team invented a Hybrid Compaction method that reduces the WAF from ~X30 to ~X5. With that, we were ready to go and sell Speedb to enterprise clients who needed a highly scalable embedded KVS and are using RocksDB. \nOur go-to-market (GTM) was a classic enterprise sales model. We signed a strategic alliance with Redis where Speedb is offered as the storage engine for Redis-on-Flash (RoF) customers.  We helped customers to boost metadata memory performance, and replaced RocksDB as the embedded storage engine of some of the world\u2019s most prominent databases, providing them with greater data performance at scale.\nAnd Yet\u2026\u2026\u2026\u2026\n We thought we had it made, developing Speedb as an enterprise-grade storage engine, yet something was missing:\nMany C-level and senior engineering managers don\u2019t even know that their programmers are using Rocksdb\nDevelopers don\u2019t like it when people try to sell them things, and\nDevelopers prefer OSS products. Big time.\n\nWe were in new territory. We thought the goal was to sell proprietary software, and by sell, we meant  keeping our technology secrets to ourselves. Right? \nWrong. \nSix months after starting to sell Speedb to an enterprise client, a smart CEO that I have consulted with about our go-to-market strategy told me: \u201cDo remember, that the world has more money than engineers. No one is going to steal your technology.\u201d Yes, Einat Orr, that was you.\nAt that point, we realized that not being open source is actually holding us back from getting attention from our target audience - developers. In today's world, the developer holds the power to decide what technologies to use via OSS offerings, especially in the data space.\nWe also realized that if we want to put a piece of software in the heart of our clients\u2019 infrastructure, we have to give them the confidence that they know what lies behind it.\nAs we started engaging with developers, we found that there is growing frustration among them due to the following reasons:  \nVersatility - Facebook forked RocksDB from LevelDB, and designed it to fit its own needs and unique use case. In the Facebook use case, every user is considered to be a set of data, and not a very big one. RocksDB was perfectly designed to address this very specific use case. However, it is not suitable for other very common use cases outside Facebook that involve much larger datasets. \nRebase - RocksDB feature set serves mainly the facebook use case. Thus, only certain PR\u2019s that are relevant to Facebook are accepted by the team.\nIn 2013 Facebook made Rocksdb free to use by the open source community ( Apache license), Consequently, RocksDB became widely spread among thousands of enterprises worldwide.\nGiven this state of things, we realized that with by introducing innovation around RocksDB\u2019s scalability, easy of use, performance (at scale), to an open source community, together we can build a next generation KVS storage engine, one that can be versatile enough to fit many unaddressable use cases. We can make it robust and strong with a true vibrant community around It. Combining the benefits of the LSM technology with our own knowledge and the collective wisdom of thousands of engineers, we can make Speedb the storage engine of choice for many developers.\nSounds simple, right? Just put your source code on GitHub, and the open source community will embrace you and start using Speedb across their applications. \nOf course startup life is not all roses and rainbows, and every day brings some aspect of the learning process. We have our open source repo on GitHub, active and growing,  and we have our enterprise version that we keep selling to enterprises who need high performance at scale with support and services. It\u2019s rewarding to build a community, not just a product, and get creative engineers from all over the world to contribute code. You never know what an innocent PR might develop into.\nAs we watch the yin and yang of developer communities with their multiple great ideas on features and functionality, and the response of our engineers, I\u2019m conscious that I\u2019m viewing a beautiful phenomenon where both sides benefit greatly.\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. The data founder story contents provided by the featuring data founders in this article and Data Engineering Weekly is not responsible for any compliance with applicable laws, rules, and regulations."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-118", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nData Engineering Weekly strives to bring our readers the best curation every week. As a Data Engineer, I always want to know what is behind the scene of a product and how efficiently we can use system design to solve a business problem efficiently. We open the Behind the Scene series for startup founders to write in-depth technical articles, system design, and efficient product usage. \nTo begin, we partner with Pathway.com to launch a three-part series about unlocking stream processing, - where Pathway talks about applying linear regression & classification in real-time. Stay tuned for more.\nIf you wish to write the Behind the Scene series, write us ananth@dataengineeringweekly.com. You can also submit the Founder's story and article suggestion. \nData Council - Austin 2023 is nearing, and I\u2019m super excited to meet all the data practitioners in person. Data Engineering Weekly readers can use the DataWeekly20 promo code to get a 20% discount on the ticket price. \nLink to Register: https://www.datacouncil.ai/austin\nPromo Code: DataWeekly20\nAll large data sets are generated over time. Time is almost always an axis in a data set. New orders come in every day. New taxi rides. New logging records. New games are being played. But compute needs will likely not change much over time; most analysis is done over recent data. \nThere is a lot of truth in this statement. Historical data processing is a rare event, where 99% of the computing happens over the last 24 hours of data. It\u2019s true Big Data is dead, but we can\u2019t deny it is a result of collective advancement in data processing techniques. \nhttps://motherduck.com/blog/big-data-is-dead/\nData Testing should be part of the data creation lifecycle; it is not a standalone process. I believe the current data testing platforms can\u2019t support the complex nature of data testing. \nThe Dropbox data team highlights the same problem. It describes how an extended Airflow operator that adopts the Write-Audit-Publish pattern with SQL helps to standardize the data testing strategy. \nhttps://dropbox.tech/infrastructure/balancing-quality-and-coverage-with-our-data-validation-framework\nMy first reaction while reading \u201cHistorically, Mixpanel used to track events at second-level granularity.\u201d, Wait, what? None of the systems is perfect. I admire the Mixpanel team discussing the complexity of changing the timestamp and the system design for it. \nhttps://engineering.mixpanel.com/tracking-events-at-milli-second-granularity-7d1fc7f29e31\nJoin this live session with BARK CTO Nari Sitaraman, & RudderStack Founder Soumyadeb Mitra on 2/15 at 9 AM PT to make sense of the CDP evolution and get practical advice on how to drive competitive advantage as a data leader in 2023.\nhttps://www.rudderstack.com/events/fireside-chat-the-future-of-cdps/\nThe blog comes at the right time when the data community frequently talks about the lost art of Data Modeling. Shopify shares its experience designing tax insight features, the business complexity, and lessons learned. \nhttps://shopifyengineering.myshopify.com/blogs/engineering/complex-data-models-behind-shopify-tax-insights\nAn interesting take on pipeline orchestration engine as a Saga pattern implementation. Picnic writes about how it automates pipeline deployment. The blog definitely added to my curiosity to think more. \nhttps://blog.picnic.nl/deploying-data-pipelines-using-the-saga-pattern-ffc1cbe29cee\nDon't fumble your data strategy in 2023. Learn how other data managers, directors, and other leaders set their teams up for success. See how to drive organizational impact at scale, touching on the technologies, processes, and cultural requirements necessary to succeed in this role.\nGet The Playbook\nAtlassian continues to write about the importance of data privacy laws and what developers need to know about the regulatory requirements. A must-read for data engineering professionals.\nhttps://blog.developer.atlassian.com/data-processing-agreements-dpas-developer-info/\nCross-region (Zone) comes with its penalty of cost and latency in Kafka infrastructure. Etsy writes about resiliency engineering for Kafka infrastructure, adding Zonal resilience in Google Cloud. \nPart 1: https://www.etsy.com/codeascraft/adding-zonal-resiliency-to-etsys-kafka-cluster-part-1\nPart 2: https://www.etsy.com/codeascraft/leveraging-zonal-resiliency-to-improve-updates-for-etsys-kafka-cluster-part-2\nPipelines for data in motion can quickly turn into DAG hell. Upsolver SQLake lets you process fast-moving data by simply writing a SQL query.\nStreaming plus batch unified in a single platform.\nStateful processing at scale - joins, aggregations, upserts\nOrchestration auto-generated from the data and SQL\nTemplates with sample data for Kafka/Kinesis/S3 sources -> S3/Athena/Snowflake/Redshift\nTry now and get 30 Days Free\nI started using chatGPT assistance for my day-to-day coding; It is a huge productive booster, and I don\u2019t think I can go back without it. I\u2019m surprised by how quickly it does the habit building and found this article is a pretty exciting tutorial on building gpt-3 applications. \nhttps://medium.com/data-science-at-microsoft/building-gpt-3-applications-beyond-the-prompt-504140835560\nSpeaking of \u201cBig Data is Dead,\u201d Twitter writes about streamlining the Hadoop cluster operations. Twitter in the past wrote about its move to Google BigQuery; interestingly, Hadoop is still not replaceable internally. \nhttps://blog.twitter.com/engineering/en_us/topics/infrastructure/2023/the-data-platform-cluster-operator-service-for-hadoop-cluster-management\nAny modern databases should support storing and processing semi-structured data & free text search. Expecting a well-defined upfront schema modeling is practically impossible with the variety of data sources we deal with. I found the blog very informative, and it talks about advancements in PostgreSQL to support full-text search. \nhttps://willpostgresliveforever.com/postgres-innovation-full-text-search/\nAll rights reserved ProtoGrowth Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-117", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nGoogle continues to write about its advancement in AI, and this week\u2019s publication talks about the advancement of distributed systems for ML & hardware acceleration. The ML for large-scale production systems highlights the improvement made from the existing heuristic in the YouTube cache replacement algorithm with a new hybrid algorithm that combines a simple heuristic with a learned model, improving the byte miss ratio at the peak by ~9%.\nhttps://ai.googleblog.com/2023/02/google-research-2022-beyond-ml-computer.html\nAs a data engineer, understanding data types is as important as data models and structures. The blog is an excellent overview of problems with floating points and integer data types.\nhttps://jvns.ca/blog/2023/01/13/examples-of-floating-point-problems/\nhttps://jvns.ca/blog/2023/01/18/examples-of-problems-with-integers/\nSpotify writes about its ML infrastructure and talks about the democratization of ML infrastructure with the Ray platform. The blog walkthrough how with a single CLI command, users can create their own Ray cluster with preinstalled ML tools, ready-to-run notebook tutorials, VS Code server for in-browser editing, and SSH access.\nhttps://engineering.atspotify.com/2023/02/unleashing-ml-innovation-at-spotify-with-ray/\nPipelines for data in motion can quickly turn into DAG hell. Upsolver SQLake lets you process fast-moving data by simply writing a SQL query.\nStreaming plus batch unified in a single platform.\nStateful processing at scale - joins, aggregations, upserts\nOrchestration auto-generated from the data and SQL\nTemplates with sample data for Kafka/Kinesis/S3 sources -> S3/Athena/Snowflake/Redshift\nTry now and get 30 Days Free\nThe blog rightly points out that making real-time inferences with machine learning (ML) models at scale is complex. The blog narrates the challenges with real-time decision serving with clear ownership of services and how LyftLearn Serving design solves these problems efficiently. \nhttps://eng.lyft.com/powering-millions-of-real-time-decisions-with-lyftlearn-serving-9bb1f73318dc\nSystem performance significantly impacts the business revenue, especially in e-commerce; for every 100 milliseconds of latency, they lost 1% in sales. Applying data engineering for system performance is one of my favorites, so the blog enjoyed reading this article. The blog highlights the challenges in measuring latency with average and percentile and discusses the alternatives.\nhttps://adrianco.medium.com/percentiles-dont-work-analyzing-the-distribution-of-response-times-for-web-services-ace36a6a2a19\nThere\u2019s certainly more to building a good data engineering strategy in 2023 than 2022\u2019s biggest buzzword. In this report, check out 9 key technologies, cultural shifts, and processes primed to define the new year.\nAccess The Report\nThe blog is an excellent recap of RecSys architecture from the days of Netflix\u2019s three-tier architecture to the latest development in the RecSys architectures. The blog talks about four types of architecture.\nNetflix three-tier architecture\nEugene Yan\u2019s 2 x 2 blueprint\nNvidia\u2019s 4 stage blueprintPermalink\nFennel.ai\u2019s 8 stage blueprintPermalink\nhttps://amatriain.net/blog/RecsysArchitectures\nOne Big Table (OBT) vs. Other Schema modeling techniques is a hot topic in data engineering, which triggers some interesting conversations. The blog narrates the various data modeling techniques for modeling time series data and their pros and cons. \nhttps://www.timescale.com/blog/best-practices-for-time-series-data-modeling-narrow-medium-or-wide-table-layout-2/\nJoin this live session with BARK CTO, Nari Sitaraman, & RudderStack Founder, Soumyadeb Mitra, on 2/15 at 9 AM PT to make sense of the CDP evolution and get practical advice on how to drive competitive advantage as a data leader in 2023.\nhttps://www.rudderstack.com/events/fireside-chat-the-future-of-cdps/\nThe title looks scary \ud83d\ude31 Fear not; the blog narrates a recommender engine design challenge for a food delivery service. The blog highlights Swiggy\u2019s recommender engine system design to recommend food orders, the limitations, and the design constraints associated with the system design.\nhttps://bytes.swiggy.com/building-a-mind-reader-at-swiggy-using-data-science-5a5c38aa6c17\nAfter running roughly 500+ models, 2500+ tests, and 200+ sources into its debt project, Super shares its dbt infrastructure. The blog discusses the proactive & reactive orchestration mechanism, its continuous integration system with lineage and test coverage, and observability on cost per model.\nhttps://medium.com/super/dbt-at-snapcommerce-part-1-orchestration-964c9a87b072\nhttps://medium.com/super/dbt-at-snapcommerce-part-2-continuous-integration-260d4e782eba\nhttps://medium.com/super/dbt-at-super-part-3-observability-c8755109901f\nAn out-of-the-box support for auditing before promoting a data model to a product is critical to fulfilling a data contract. Iceberg in the past writes about its Write-Audit-Publish support; It is exciting to see Apache Hudi supports the pattern from 0.9.0. I can\u2019t find any reference that Delta Lake/ Snowflake supports this pattern. Please share in the comments if you found any reference to it. \nhttps://www.dremio.com/resources/webinars/the-write-audit-publish-pattern-via-apache-iceberg/\nhttps://hudi.apache.org/releases/release-0.9.0/#writer-side-improvements\nAll rights reserved ProtoGrowth Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-116", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nI\u2019m a regular listener of the ItDepends podcast, and I found Sanjeev always bring insightful view from the guest speakers. I've read/watched a few 2023 data predictions but found this conversation more practical, with fewer buzzwords highlighting data practitioners' reality. The conversation emphasizes the importance of governance and even points to a new role, \"Legal Engineering.\"\nThe conversation around data observability points out the growing gap in data observability [aka finding the things] vs. fixing the data quality. The data observability solutions require more attention in the remediation workflow to ensure they don\u2019t end up as a disjointed workflow like Data Catalogs. \nMeta writes about Tulip's adoption story, its data transportation, and the serialization protocol for its data platform. The highlight for me is the debugging tool \u201cloggertail.\u201d The utility of writing the analytics data to a void space (/dev/null) and dynamic subscribe and consume from a command line is a very powerful one.\nhttps://engineering.fb.com/2023/01/26/data-infrastructure/tulip-modernizing-metas-data-platform/\nPart 1: https://engineering.fb.com/2022/11/09/developer-tools/tulip-schematizing-metas-data-platform/\nChatGPT is the talk of the town; I found this article an exciting one to build custom GPT-3-based chatbots. Please try it out and let us know what you build!!!\nhttps://towardsdatascience.com/custom-informed-gpt-3-models-for-your-website-with-very-simple-code-47134b25620b\nPipelines for data in motion can quickly turn into DAG hell. Upsolver SQLake lets you process fast-moving data by simply writing a SQL query.\nStreaming plus batch unified in a single platform.\nStateful processing at scale - joins, aggregations, upserts\nOrchestration auto-generated from the data and SQL\nTemplates with sample data for Kafka/Kinesis/S3 sources -> S3/Athena/Snowflake/Redshift\nTry now and get 30 Days Free\nThe adoption of data products moves beyond empowering business operations to build customer-facing applications. The author writes from a product manager perspective things to consider while adopting data-driven customer-facing data products.\nhttps://pmdata.substack.com/p/incorporate-data-into-your-next-product\nAutoscaling is a curse and a blessing on its own. Autoscale up & down is a highly preferable architecture for the data pipeline to adopt the workload spike at the peak of the hour. The author writes an interesting benchmark result comparing Databricks autoscaling vs. fixed computing.\nIn our analysis, we saw that a fixed cluster could outperform an autoscaled cluster in both runtime and costs for the 3 workloads we looked at by 37%, 28%, and 65%.\nAs with any benchmark results, it varies depending on your workload and configuration, but overall the findings highlight the autoscale infrastructure requires more maturity. \nhttps://medium.com/sync-computing/is-databrickss-autoscaling-cost-efficient-610e6ece4831\nReady to stop fighting bad data and explore end-to-end coverage with Data Observability? Learn the 10 most important things to consider when choosing a data observability platform. Get the new platform guide, and take the next step in your journey to data trust.\nGet The Guide\nCoinbase writes about its unified data ingestion framework SOON!! The blog highlights the complexity around data sourcing from various sources with the mix of CDC and non-CDC events. TIL about Databricks Change Data Feed, and yes Coinbase uses both Snowflake and Databricks :-)\nhttps://www.coinbase.com/blog/soon-for-near-real-time-data-at-coinbase-part-1\nThe Data Contract concept aims to establish the guarantees and expectations around the production-ready pipeline. While defining the expectations and constraints, developer productivity plays a critical role. Miro writes about writing a product pipeline with Airflow with a practical approach to implementing data contracts in the batch pipeline.\nhttps://medium.com/miro-engineering/writing-data-product-pipelines-with-airflow-1ace222f8f5a\nAs Khatabook rapidly scaled users in 2022, Segment\u2019s inferior support and unreasonable pricing became increasingly painful. Join this session with Khatabook\u2019s engineering team to find out why they chose to move to RudderStack and get an overview of the migration process.\nhttps://www.rudderstack.com/events/how-khatabook-migrated-to-rudderstack-from-segment/\nEtsy writes about the changing landscape of its ML platform, from classic gradient-boosted trees to deep learning techniques. The blog shows an example of its search ranking and a walkthrough of how the team improves observability and early feedback tools.\nhttps://www.etsy.com/codeascraft/improving-support-for-deep-learning-in-etsy39s-ml-platform\nThe A/B experiment pipeline is often the last one to finish in a data infrastructure; The window of confidence (the acceptable data sample) is always a question while making a business decision by the learnings from the A/B tests. Dropbox writes about the practical complexity of longer experiments and the usage of machine learning to make an intuitive decision with an example of the Expected Revenue model.\nhttps://dropbox.tech/machine-learning/accelerating-our-a-b-experiments-with-machine-learning-xr\nFlat Pack Tech writes about the re-architecture of its analytical pipeline. It is the first blog I came across that talks about dataform, the dbt alternative that only works with Google BigQuery!!!\nhttps://medium.com/flat-pack-tech/online-analytics-pipeline-re-engineering-14a73792d468\nMehdio shares 10 experiences out of a decade of a data engineering career. The blog dropped some very insightful trends of the past, highlighting past hype, The Cloud will take data engineers\u2019 jobs, The aspiration to become Data scientists, the rush to put Notebooks into production, the modern data stack mess [Oh, my fav one], and Rust!!!\nhttps://mehdio.medium.com/10-lessons-learned-in-10-years-of-data-1-2-4e3a8c358745\nhttps://mehdio.medium.com/10-lessons-learned-in-10-years-of-data-2-2-9cc556840134\nAll rights reserved ProtoGrowth Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-115", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nOne of the promises I made toward the end of 2022 is to publish more of my thoughts and industry observation of data engineering trends. So far, we have published.\nFunctional Programming - A Blueprint\nThe blog retriggers the conversations around data modeling, so many data practitioners reached out and discussed and evaluated their data stack. The response humbles me. Thank you for everyone reaching out and brainstorming about it. \nData Catalog - A broken promise\nA classic blog triggers a few conversations about Data Catalog and its future. A few data practitioners reached out and appreciated triggering a healthy conversation around data catalogs. We\u2019ve seen a similar prediction during the conversation of the Analyst Predictions 2023 and the release of embedded data catalogs such as reCap.\nA few of the upcoming blogs; stay tuned. \nData Quality: Shift Left, Bring Consumers Closer\nEvent Source vs. Outbox Pattern vs. CDC - The challenges and opportunities\nData Contract - An Executive Overview\nData Contract - Why does Everyone Talks about Data Contract now? A historical walkthrough for Data Engineering Leaders about Data Contract\nLast week I had an opportunity to talk about Functional Data Engineering - A Blueprint for adopting functional principles in the data pipeline at the State of Data 2023 Conference. \nWhen I started to talk about Data Contract & Schemata, a few data executives and practitioners approached me and asked, \u201cAnanth, which Data Modeling techniques should we adopt\u201d? Is it Kimball techniques, Data Vault, Activity Schema, or 3NF? The answer is always the classic \u201cit depends.\u201d \ud83d\ude02 \nHowever, then I realized, somewhere in these data modeling concepts, the key principles of Data Engineering, that is\nReproducibility\nRe-Computability\nIf you want to talk more about data modeling and functional principles in data engineering, feel free to pick a slot on my calendar [https://calendly.com/apackkildurai/]\nSlides of the Talk: https://speakerdeck.com/vananth22/functional-data-engineering-a-blueprint-for-adopting-functional-principles-in-data-pipeline\nPipelines for data in motion can quickly turn into DAG hell. Upsolver SQLake lets you process fast-moving data by simply writing a SQL query.\nStreaming plus batch unified in a single platform.\nStateful processing at scale - joins, aggregations, upserts\nOrchestration auto-generated from the data and SQL\nTemplates with sample data for Kafka/Kinesis/S3 sources -> S3/Athena/Snowflake/Redshift\nTry now and get 30 Days Free\nGoogle AI started a series of blog posts highlighting some exciting progress Google made in 2022 and presenting the vision for 2023 and beyond. The first blog post highlights the advancement in language, computer vision, multi-modal models, and generative machine learning models. The blog series is interesting to watch since the advancement of ChatGPT, OpenAI <> Microsoft, and Google triggers the AI Battle. 2023 will be an exciting year for AI research and advancements with refreshed investments from big tech companies. \nhttps://ai.googleblog.com/2023/01/google-research-2022-beyond-language.html\nInfostrux writes about general best practices to structure your dbt environment, with an example of config file structuring and organizing the data flow. \nhttps://medium.com/infostrux-solutions/crafting-better-dbt-projects-aa5c48aebfc9\nOn the list of recommendations, the development environment setup triggers some curiosity. \nWe can generate dev environments by cloning ingest layer of the PROD environment\nOne challenge in Data Engineering is to set up the dev environment since, with the compliance and regulatory requirements, we can\u2019t copy the prod data into the dev. How do you set up your development environment? Please add your thoughts in the comments in the discussion forum \nhttps://www.dataengineeringweekly.com/p/how-do-you-setup-your-development/comments.\nReady to stop fighting bad data and explore end-to-end coverage with Data Observability? Learn the 10 most important things to consider when choosing a data observability platform. Get the new platform guide, and take the next step in your journey to data trust.\nGet The Guide\nActivity Schema focuses on structuring all the business activities in a single time series table, which brings easy to model and understand customer activity across the system. The blog briefly introduces activity schema, pros & cons, and further reads.\nhttps://medium.com/@baruchjacob/maximizing-your-datas-value-using-activity-schema-data-model-c796bea41c4f\nAn exciting blog post of this week with a refreshing idea of templated development for building a data pipeline. The CookieCutter approach finds a fine balance between flexibility and autonomy in building the data pipeline. \nhttps://achievers.engineering/enabling-self-serve-data-platform-with-apache-beam-cookiecutter-d94230e1fef9\nLegacy CDPs charge you a premium to keep your data in a black box. RudderStack builds your CDP on top of your data warehouse, giving you a more secure and cost-effective solution. Plus, it gives you more technical controls, so you can fully unlock the power of your customer data.\u00a0\nTake control of your customer data today.\nWe talked about the functional data engineering principles of building the DateTime partition table, and super thrilled to see the pattern added in dbt with good performance optimization. \nhttps://medium.com/teads-engineering/bigquery-ingestion-time-partitioning-and-partition-copy-with-dbt-cc8a00f373e3\nDV engineering writes about its migration from Luigi to Airflow by taking an example case of file processing DAG. Two key lessons out of the blog.\nFinding a fine balance between parallelism and the effectiveness of a system is a challenge on its own. An easy to create parallel/ concurrent tasks sometimes become a curse.\nA clear and usable UI is a significant differentiator of your system.\nhttps://medium.com/doubleverify-engineering/optimizing-for-dag-and-task-complexity-in-airflow-4fb6501e34d1\nTime is a critical element in the data processing. Understanding time-based window processing techniques is essential if you\u2019re starting a career in data engineering. The blog explains how Flink provides time-based window processing capabilities.\nhttps://www.ververica.com/blog/flink-sql-queries-and-time\nAll rights reserved ProtoGrowth Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/how-do-you-setup-your-development", "title": "Data Engineering Weekly", "content": "The recent blog https://medium.com/infostrux-solutions/crafting-better-dbt-projects-aa5c48aebfc9 triggers some curiosity for me. In particular around the strategies to build the dev environment. The blog mentiones \u201cWe can generate dev environments by cloning ingest layer of the PROD environment\u201d\nOne challenge in Data Engineering is to set up the dev environment since, with the compliance and regulatory requirements, we can\u2019t copy the prod data into the dev. How do you set up your development environment? Please add your thoughts in the comments.\n"}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-114", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nBy far one of the best analyses of trends in Data Management. 2023 predictions from the panel are;\nUnified metadata becomes kingmaker.\nRethink the modern data stack\nSQL is back\nThe definition of data is expanding\nBI/ Analytics get embedded & automated\nA few favorite quotes from the conversation\nData Catalogs are very soon not going to be a standalone product; They are going to get embedded - Sanjeev Mohan.\nCustomers with the data catalogs, whether embedded or not, express three times high satisfaction with their analytical infrastructure - David Menninger.\nI write about how Data Catalogs are failing with the recent article Data Catalog - A Broken Promise. Data Catalogs are moving towards a feature, not a product. We are seeing embedded data catalog tools like reCap emerging to address the concerns.\nThe problem is that what we\u2019re getting to, unfortunately, is what I would call lots of islands of simplicity, but it\u2019s a complex toolchain - Tony Baer.\n\ud83c\udfaf I defined the modern data stack sometime back as;\nBut I like the term \u201cLots of Islands of Simplicity, but it\u2019s a complex toolchain.\u201d\ndbt lab\u2019s recent abrupt pricing change triggers interesting conversations about value creation and the per-user billing model. The author writes an exciting article about switching to Github actions for dbt to save $65,000\nhttps://medium.com/@datajuls/why-i-moved-my-dbt-workloads-to-github-and-saved-over-65-000-759b37486001\nPipelines for data in motion can quickly turn into DAG hell. Upsolver SQLake lets you process fast-moving data by simply writing a SQL query.\nStreaming plus batch unified in a single platform.\nStateful processing at scale - joins, aggregations, upserts\nOrchestration auto-generated from the data and SQL\nTemplates with sample data for Kafka/Kinesis/S3 sources -> S3/Athena/Snowflake/Redshift\nTry now and get 30 Days Free\nWhat will be the impact of having an efficient data & analytical infrastructure; How can it bring you a competitive advantage? The author writes about the success of DoorDash, despite being a late mover into the market and starting merely to supply meals to the Stanford graduate, and how it uses data to beat the competition. \nhttps://findingdistribution.substack.com/p/how-doordash-built-the-greatest-go\nTo validate the previous article on DoorDash\u2019s efficient data & analytical platform usage, DoorDash writes about how it uses ML to save thousands of canceled orders. The article highlights the business need, the challenges of reflecting accurate store opening hours, and how the ML model replaces the heuristic approach.\nhttps://doordash.engineering/2023/01/10/how-doordash-upgraded-a-heuristic-with-ml-to-save-thousands-of-canceled-orders/\nNetflix is another flagship company that successfully powers its business operations & product features. The blog narrates the causal machine-learning approach in designing promotional artwork for their TV shows.  \nhttps://netflixtechblog.medium.com/causal-machine-learning-for-creative-insights-4b0ce22a8a96\nReady to stop fighting bad data and explore end-to-end coverage with Data Observability? Learn the 10 most important things to consider when choosing a data observability platform. Get the new platform guide, and take the next step in your journey to data trust.\nGet The Guide\nThe author captures different data stakeholders' sad states, expectations, and disappointments. Data Modeling increasingly becoming a topic of discussion recently, and I write about Data Modeling with Functional Data Engineering Principles. I like the layered approach expressed by the author and how Data Contracts play a significant role in Data Modeling.\nI recently started working on Schemata [https://schemata.app/], and keep watch this space for some exciting announcements soon!!\nhttps://moderndata101.substack.com/p/optimizing-data-modeling-for-the\nI always find myself very uncomfortable with the naming convention of medallion data architecture. The names hold less meaning to the outcome, but its fancy. However, the medallion architecture brings a clear bucketing of data to align with the organization's delivery strategy from raw data \u2192 filer & clean data, \u2192 business metrics. The author writes a few best practices for managing medallion-style architecture.\nhttps://piethein.medium.com/medallion-architecture-best-practices-for-managing-bronze-silver-and-gold-486de7c90055\nLegacy CDPs charge you a premium to keep your data in a black box. RudderStack builds your CDP on top of your data warehouse, giving you a more secure and cost-effective solution. Plus, it gives you more technical controls, so you can fully unlock the power of your customer data.\u00a0\nTake control of your customer data today.\nCars24 writes a two-part series on upgrading and optimizing the data flow pipeline. The conversation around the Cost of Idle CPU in Snowflake is exciting to me in the blog. The author walked through various strategies, from sync to async job submission and batch job submission strategy. \nhttps://medium.com/cars24-data-science-blog/optimizing-data-flow-cars24-4c0a17b797d1\nhttps://medium.com/cars24-data-science-blog/upgrading-data-flow-pipeline-cars24-1b6b8aea48e\nThe definition of data increases its scope; the author explains the complexity behind managing metadata at data42. The blog establishes the case for centralized data management efforts to collaborate and foster data research with the outside world.\nhttps://medium.com/@lysann_hesske/behind-data42-meta-data-management-299c524407db\nmany machine learning models\u2019 probabilistic outputs cannot be directly interpreted as the probability of an event happening. To achieve this outcome, the model needs to be calibrated. The author explains what calibration is, in which applications it is important and why, and three different methods for calibration.\nisotonic regression\nSigmoid method of calibration\nLogloss metric for calibration\nhttps://medium.com/data-science-at-microsoft/model-calibration-for-classification-tasks-using-python-1a7093b57a46\nParquet is the most popular columnar format used in data lakes. The author explains how to use a simple python script to read parquet files, metadata, and summary stats for each column. \nhttps://towardsdatascience.com/parquet-best-practices-discover-your-data-without-loading-them-f854c57a45b6\nAll rights reserved ProtoGrowth Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-113", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nThe team Seattle Data Guy is running an excellent day-track online conference, \u201cThe State of Data 2023\u201d.  I\u2019m giving a talk on \u201cThe Functional Data Engineering - A Blueprint.\u201d based on my recent article of the same title. Don\u2019t miss out, and click the link below to register.\nhttps://www.eventbrite.com/e/state-of-data-2023-tickets-468776622497\nThe article is an excellent overview of the Data Contract platform and how it brings collaboration among multiple stakeholders in the data creation and value generation process. The author narrates what data products are by the example of Muesli is an excellent analogy. \nhttps://medium.com/@maxillis/on-data-contracts-data-products-and-muesli-84fe2d143e2c\nThe rise of Apache Airflow & dbt makes Jinja templating a must-know toolchain for analytical engineering. The article is an excellent intro to Jinja templating with dbt to get started.\nhttps://blog.devgenius.io/excelling-at-dbt-jinja-macros-for-modular-and-cleaner-sql-queries-part-1-2-55e29d4b29e2\nhttps://blog.devgenius.io/excelling-at-dbt-jinja-macros-for-modular-and-cleaner-sql-queries-part-2-2-88949c1af46c\nOptimizing the ML workflow is a vital goal for MLOps, as any good developer platform. Microsoft's data science team writes about their observation of the ML workflow with a layered approach.\nData Science code layer \nSpecification layer\nThe orchestration layer\nhttps://medium.com/data-science-at-microsoft/a-layered-approach-to-mlops-d935beefca2e\nPipelines for data in motion can quickly turn into DAG hell. Upsolver SQLake lets you process fast-moving data by simply writing a SQL query.\nStreaming plus batch unified in a single platform.\nStateful processing at scale - joins, aggregations, upserts\nOrchestration auto-generated from the data and SQL\nTemplates with sample data for Kafka/Kinesis/S3 sources -> S3/Athena/Snowflake/Redshift\nTry now and get 30 Days Free\nTechnology advancement leaves traces along the way, and it is important to look back to understand the evolution pattern. It not only helps us understand the pattern but also makes us realize the Black Swan events, which we might very well miss. The author writes an excellent walk back the memory lane of Big Data now and then. \nhttps://towardsdatascience.com/2003-2023-a-brief-history-of-big-data-25712351a6bc\nNormConf is an online tech conference about things that matter in data and ML but don\u2019t get the spotlight. I enjoyed watching some of the thought-provoking talks from the leading industry experts. The author narrates the NormConf experience with a summary of some of the excellent talks. \nhttps://medium.com/the-prefect-blog/what-i-learned-from-normconf-2022-f8b3c88f0de7\nFull Conference YouTube Playlist\nhttps://www.youtube.com/playlist?list=PLYXaKIsOZBsu3h2SSKEovRn7rGy7wkUAV\nReady to stop fighting bad data and explore end-to-end coverage with Data Observability? Learn the 10 most important things to consider when choosing a data observability platform. Get the new platform guide, and take the next step in your journey to data trust.\nGet The Guide\nNetflix writes about the natural evolution of its Asset Management Platform into a data processing pipeline. It is exciting to read some of the common characteristics of the asset management platform, such as schema validation, versioning, access control, sharing, and triggering configured workflows, which naturally pave the path for the data processing pipeline. \nhttps://netflixtechblog.medium.com/data-reprocessing-pipeline-in-asset-management-platform-netflix-46fe225c35c9\nSometimes, we won\u2019t have the luxury of processing all the data to compute the success metrics and tend to rely on sampled success metrics. How do we separate noise over the success signal over time? Shopify writes about how Monte Carlo simulation helps to separate signal from noise.\nhttps://shopifyengineering.myshopify.com/blogs/engineering/monte-carlo-simulations-sampled-success-metrics\nLegacy CDPs charge you a premium to keep your data in a black box. RudderStack builds your CDP on top of your data warehouse, giving you a more secure and cost-effective solution. Plus, it gives you more technical controls, so you can fully unlock the power of your customer data.\u00a0\nTake control of your customer data today.\nExpedia writes about 2023 tech trends and what they mean for the travel industry. The key trends from the predictions.\nTech gets (hyper) personal\nPlatforms become more open and accessible\nHuman-centric design is in the spotlight\nAI might feel the frost\nThe (hyper) personalization is an excellent trend to watch; We see patterns of internal recommendation apis to serve various personalization product features. \nhttps://medium.com/expedia-group-tech/ai-personalization-and-openness-exploring-the-definitive-tech-trends-of-2023-d5ba45875c80\nI often feel the impact of Presto in data engineering is very underappreciated. Presto most likely helped systems like Snowflake to win the perception problem\nTwilio shares its experience running Presto on AWS with some excellent optimization techniques.\nhttps://prestodb.io/blog/2022/12/28/presto-at-twilio.html\nTalk\nI recently came to know CSV has schema specification standards. Maybe I\u2019m slow to discover this, but when I found it, I was super excited for an unknown reason. I\u2019m still trying to process it \ud83d\ude0a Nonetheless, I enjoyed reading the draft version of the specification, which combines both the schema and validation specifications.\nhttps://digital-preservation.github.io/\nAll rights reserved ProtoGrowth Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions.\n"}, {"url": "https://www.dataengineeringweekly.com/p/recap-a-data-catalog-for-people-who", "title": "Data Engineering Weekly", "content": "The following article is a republication of an original article that first appeared at https://cnr.sh/essays/recap-for-people-who-hate-data-catalogs.\nAuthor: Chris Ricommni\nRecap Github: https://github.com/recap-cloud/recap [Don't forget to star it]\nI\u2019m excited to release Recap, a dead simple data catalog for engineers written in Python. Recap makes it easy for engineers to build infrastructure and tools that need metadata. Unlike traditional data catalogs, Recap is designed to power software.\nRecap is a data catalog for machines\u2013a metadata service. It supports neither the same users nor use cases as traditional catalogs. Humans use traditional data catalogs. Machines and software using Recap. Traditional catalogs focus on data discovery and curation. Recap focuses on metadata that software needs\u2013schema, access controls, data profiles, indexes, and queries. Recap could be part of a traditional data catalog, something you could use to build a data catalog, but it isn\u2019t a data catalog.\nRecap is something I always wanted at my last job. My data engineering team wrote a lot of scripts to automate new data pipeline creation, create data warehouse views, manage user access controls, detect sensitive information (PII), and locate data quality issues. Our work required metadata, which we scraped from BigQuery, MySQL, Airflow, and other systems in an ad-hoc way. Each scraping implementation was brittle and costly to maintain.\nSuch an experience is hardly unique. Data quality, data contract, data discovery, compliance, governance, and ETL tools all need metadata\u2013row counts, cardinality, distribution, max, min, number of nulls, and so on. Even query engines use similar metadata for planning and optimization.\nDevelopers have two ways to get at this metadata: collect metadata directly in their tools or fetch metadata from a data catalog, which does the scraping.\nIn-tool metadata collection\u2013by far the most common\u2013is inefficient and error prone. Different systems scan over the same data over and over again without coordination. Duplicate scans increase cloud costs and decrease upstream application performance. Developers must also implement each individual source\u2019s metadata API and understand its nuances.\nI haven\u2019t found many examples of the second approach\u2013fetching metadata from a data catalog. Catalogs cover dozens of use cases and are used by entire organizations. Such a wide surface area leads to a complex product and bloated operational footprint. We actually tried to get a catalog up and running at WePay; it did not go well.\nYet, for all the use cases catalogs cover, they still lack many features that engineers want: a decent CLI, a nice library, a small operational footprint, deep and robust data statistics, and a composable architecture. Engineers need these features because they\u2019re using metadata to build software, not locate data. Recap addresses these needs.\nRecap\u2019s UI is a CLI and REST API. It runs with no web front-end, no scheduler, no orchestrator, and no external system dependencies. Recap can also be run as a Python library, so it\u2019s easy to integrate with data engineering and analytics engineering tools. The REST service works with infrastructure in other languages and allows Recap to be run centrally. The code is modular and pluggable, so developers can extend it where needed.\nRecap is a young project. It needs tests, for one thing. I want to add streaming (Kafka), file system (S3, GCS), and data lake crawlers (Iceberg, Hudi). Airflow, Prefect, and Dagster integrations would be nice. And, of course, more data analyzers to get more statistics. I\u2019m open-sourcing Recap now to solicit feedback because I think Recap is already useful.\nStar Recap\u2019s repository, then gets started with Recap\u2019s documentation. You can find me on Twitter @criccomini and Mastodon @criccomini@data-folks.masto.host. Let me know if you\u2019re interested in integrating Recap with your software. I look forward to hearing from you!\nThanks to Josh Wills, Jacob Matson, Ananth Packkildurai, Sarah Catanzaro, Chrix Finne, and Chad Sanderson for early draft feedback."}, {"url": "https://www.dataengineeringweekly.com/p/data-catalog-a-broken-promise", "title": "Data Engineering Weekly", "content": "Data catalogs are the most expensive data integration systems you never intended to build.\u00a0Data Catalog as a passive web portal to display metadata requires significant rethinking to adopt modern data workflow, not just adding \u201cmodern\u201d in its prefix.\nI know that is an expensive statement to make\ud83d\ude0a To be fair, I\u2019m a big fan of data catalogs, or metadata management, to be precise. I even authored a special edition capturing metadata tool\u2019s history at Data Engineering Weekly.\nAfter overseeing a couple of data catalog implementations in recent times, it made me pause and started to question my belief. The question is essentially two-fold.\nWhy is it so expensive in terms of the level of effort to roll out a data catalog solution?\nDespite the initial energy from the stakeholders, why does the usage of Data Catalogs keep declining? \nIs that the experience unique to me? So I seek data community thoughts about Data Catalog in a poll on LinkedIn.\nIf you discount a few data catalog vendor votes from the poll, 26% shrinks to 20%. So it\u2019s not me; 80% of people think Data Catalog is not a prime-time data workflow system but a handy tool that is sometimes somewhat useful. \nThe result reaffirms my experience with the Data Catalog, and also it triggers more curiosity in me to understand why so? To understand better, Let\u2019s step back and examine the data catalog of pre-modern-era and modern-era1 Data Engineering.\nLet\u2019s call the pre-modern era; as the state of Data Warehouses before the explosion of big data and subsequent cloud data warehouse adoption. Applications deployed in a large monolithic web server with all the data warehouse changes go through a central data architecture team. \nA couple of important characteristics of a Data Warehouse at this time\nThe ETL tools and Data Warehouse appliances are limited in scope. There are not many sources to pull the metadata.\nThe footprint of people in an organization directly accessing the Data Warehouse is fairly limited; getting access to query the Data Warehouse directly is a privilege and a specialized skill. \nHadoop significantly reduced the barrier to storing and accessing large volumes of data. The cloud Data Warehouses & LakeHouse systems further accelerate the ease of access to the data. It also opens up data for multiple use cases such as A/B testing2, AI/ML3, Growth Engineering4, and Data-Driven product features5, etc., \nAll combined, two important characteristics changed in the modern data infrastructure.\nData Warehouses now ingest data from multiple data sources6, which was not possible before, giving unprecedented insights.\nThe ease of access and multiple use cases expose data warehouses to multiple organizational stakeholders and specialized tools to get the job done. \nAt a high level, we can define modern data engineering as Fn(work) in your favorite tool.\nWhat does that mean? Let\u2019s expand a bit more to demonstrate the current state of the data catalog with the modern data stack. \nAs you can see, We still adopt the age-old metadata integration strategy to build data catalogs. It makes rolling out the data catalogs.\nExpensive and time-consuming\nIt creates a disjointed workflow which makes folks rarely use the tool\nAnthony Algmin writes an interesting perspective reflecting many of the things we sketched in the article with a title, Make An Impact: The Data Catalog is a 1980s Solution for 2020\u2019s Problems. The author nailed the fundamental problem with the Data catalog.\n\u201cThere\u2019s a bigger problem (*with Data Catalogs). It is that we now need to make potential data catalog users aware that the data catalog exists and then train them how to use it! A system that neither creates the data nor does the analysis\u2014not exactly what most sane people want to spend a lot of time learning. A big strike two!\u201d\nThe author proposes two ways the Data Catalog can evolve further.\nLose the Interface and embedded into the data creation tools\nExpand from Data Catalog to Knowledge Engine - Aka not just a passive web portal, but integrate into the data creation process, aka Data Contract platform.\nI don\u2019t think data catalogs are going away soon, but the data catalog tools should acknowledge the underlying system dynamics changes. We can\u2019t design a system that works for 20-year-old infrastructure. \nWhat do you all think about the future of Data Catalogs? I look forward to your comments. \nhttps://vwo.com/blog/cro-best-practices-booking/\nhttps://research.netflix.com/research-area/recommendations\nhttps://engineering.gusto.com/what-is-growth-engineering/\nhttps://slack.engineering/recommend-api/\nhttps://airbyte.com/connectors\n"}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-in-year-2022", "title": "Data Engineering Weekly", "content": "The holidays bring joy and memories. It is always a joyful memory for me every week when I pen down (or key down \ud83e\udd37\ud83c\udffd\u200d\u2642\ufe0f) every edition of Data Engineering Weekly. I want to take a holiday break for this week's edition, and instead, I want to reflect on our journey in 2022. \n2022 has been a remarkable year in terms of subscriber growth. We crossed 5-digit subscribers a long back with 126% YoY growth this year. \nAs we keep growing, we always keep the engagement metrics. Our newsletter \u201copen rate\u201d throughout the year is close to 50%, a healthy engagement rate according to Substack. \nIt is also a good time for us to review what worked and what didn\u2019t work with Data Engineering Weekly. You might be surprised when I quote it \u201cData Engineering Products.\u201d  Data Engineering Weekly taught me how consistent value creation could build a sustainable business. I think it is fair to call it a product. \nOur signature weekly curation of data engineering articles continues to grow. We are fortunate to back by industry-leading companies like RudderStack, Firebolt, MonteCarloData, Soda, and Upsolver. \nWe strive to remain vendor neutral and constantly refuse the addition of vendor promotional content in our newsletter. Every week we shortlist 18+ articles and filter articles by content quality for our readers. We follow simply two principles.\nNo vendor promotional articles\nNo clickbait - There are numerous articles with clickbait. The Data Engineering Weekly newsletter may get good engagement featuring the clickbait, but we constantly filter it out. \nWe launched the \u201cReaders Article Review\u201d feature via Github. The feature encourages readers to share their views about the articles they enjoy reading. I thought this would be an IMDB for Data Engineering articles.  We\u2019ve not made the impact we hoped for; We got a few contributions, some via Github and some via LinkedIn. The articles are mostly a reference to their company blog rather than a 3rd party review. I\u2019m still thinking about structuring this feature more in the coming weeks. \nOne of the biggest hurdles in the entrepreneurship journey is getting the first initial set of customers and getting your name into the market. Everyone should have an equal opportunity to succeed, and we can build something beyond our ability. \nFounder\u2019s story attempts to provide the Data Engineering Weekly platform for entrepreneurs to introduce their founding story to our readers. So far, we have featured ten incredible founding stories in our newsletter. I hope we will see more founding stories in 2023. \nIn 2022, I attempted two times to launch the podcast; in both times, I failed to release the podcast product into our readers' hands (or ears?). With those two failed attempts, I learned a ton, and in 2023 I'm going to retry again to see if we can bring the Data Engineering Weekly podcast.\nBut why do you want to run a podcast? Well, it is mostly for my selfish reason. Whenever I read interesting articles, I want to meet the article's author, discuss more context behind their architecture, and learn their thought process for designing such a system. I'm sure you want the same, and I hope the Data Engineering Weekly will bring you that experience soon. \nWe will be launching soon \u201cTech Series with Guest Authors.\u201d It will be deeply technical content where the authors explain the under-the-hood implementation of how system design works. \nWe are already in discussion with companies like Pathway.com to feature articles. We are opening up to everyone from mid-Jan. If anyone wants to launch their tech series, usually limited to 3 week's edition, don't hesitate to get in touch with me.\nThe wonderful and daunting truth about writing is that one can always ... ALWAYS ... improve. I remind myself that we can always improve no matter how hard we work on something. Tell me, how can we improve Data Engineering Weekly together?\nThank you for all your kind support. Let\u2019s learn together, and Let\u2019s grow together.  I wish you all a prosperous new year 2023!!!\nhttps://github.com/ananthdurai/dataengineeringweekly#contributing-guide\nhttps://github.com/ananthdurai/dataengineeringweekly#how-can-i-contribute-founder-story"}, {"url": "https://www.dataengineeringweekly.com/p/functional-data-engineering-a-blueprint", "title": "Data Engineering Weekly", "content": "Data modeling has been one of the hot topics in Data LinkedIn. Hadoop put forward the schema-on-read strategy that leads to the disruption of data modeling techniques as we know until then. We went through a full cycle that \u201cschema-on-read\u201d led to the infamous GIGO (Garbage In, Garbage Out) problem in data lakes, as noted in this What Happened To Hadoop retrospect. \nWe must walk through memory lane to understand why functional data engineering is critical. Let\u2019s reference what the data world looked like before the Hadoop era. \nAny blog is incomplete if it does not include a Gartner prediction, so let\u2019s start with one. Garner predicted in 2005 around 50% of Data Warehouse projects would fail. Why is it so? Because the data warehouse is considered back office work, barely integrated with the product strategy. A simple addition of a column requires multiple approval workflows and a project.\nThe survey published by AgileData back in 2006 stat 66% of respondents indicated that development teams sometimes go around their data management (DM) groups. 36% of developers found the data group too slow to work with.\nSource: http://agiledata.org/essays/culturalImpedanceMismatch.html [I know- pie chart \ud83d\ude03]\nTowards the end of 2010, we saw software engineering adopt devops principles and agile development methodologies. The \u201cbuild fast, break things\u201d era is reciprocal to \"Let's have an architecture meeting to add a new column.\u201d  Once again, the Agile data blog captures the growing gap in the advancement of software engineering and how the data management gap increases the cultural mismatch. \nSource: http://agiledata.org/essays/culturalImpedanceMismatch.html\nThe cultural mismatch is largely still in play. Data is still struggling to find its feet in the organization, and articles like How to prove the value of your data team convince the leadership team. \nThe close we get to bringing the software engineering principles to data engineering, the greater we see the impact of data engineering in the industry. \nMaxime Beauchemin wrote an influential article, Functional Data Engineering \u2014 a modern paradigm for batch data processing. It is a significant step to bring Software Engineering concepts into Data Engineering. The principle utilizes the advancement from Hadoop.\nCloud object storage like S3 makes the storage a commodity. \nThe separate Storage & Compute, so both can scale independently. Yes, human life is too short for scaling storage and computing simultaneously. \nFunctional data engineering follows two key principles\nReproducibility - Every task in the data pipeline should be deterministic and idempotent\nRe-Computability - Business logic changes over time, and bugs happen. The data pipeline should be able to recompute the desired state.\n In the article, Max writes about how the functional data engineering concepts drive the design principles of Apache Airflow. Let\u2019s see how we can implement the principles in Lake House systems like Apache Hudi, Apache Iceberg & Delta Lake.\nThe foundation of the implementation is the Schema classification. We can broadly classify the schema as \nEntity - mutable data with a primary key. An Entity represents the business objects like the user, product, etc.,\nEvent - immutable data with a timestamp. An Event represents a business activity like user A added product X at timestamp Z into the shopping cart.\nThe founding principle of functional data engineering is everything is a time partition table. The time partition table maintains a full copy of the current state of an entity for each time partition. \nLet\u2019s take an example of a user entity table. A typical data modeling will look as follows.\nA typical object storage file structure will look as follows. [Taking S3 as an example]\nThe events, on the other hand, are immutable in nature and temporal in nature. A user_activity table will look as follows.\nA typical object storage file structure will look as follows. [Taking S3 as an example]\nEntity data pipelines can run in two modes.\nIncremental Snapshot\nFull Snapshot\nWe often don\u2019t have the luxury of getting a full snapshot of an entity. A change data stream like CDC or event sourcing gives the incremental view of the entity as it is changing. \nThe incremental snapshot goes through an extra couple of pre-steps before taking the date-time table partitions.\nLoad the incremental data into a landing area with a date-time partition.\nMerge the incremental snapshot into the base table. The base table is the initial bootstrap of a full table snapshot.\nthe full snapshot is a relatively simpler approach\nTake the deep clone of the source data\nWrite a date time version table into the data warehouse\nThe date partition table streamlines the data pipeline process; at the same time, the entity's current state requires querying for ad-hoc analytics. Every time asking the end users to add a ds partition in the query filter will cause more confusion. A simple logical view on top of the latest partition greatly simplifies the accessibility. \nEvents are append-only, removing the complication of merging or taking full snapshots of dimensions from the source.\nOne of the significant advantages of the functional data engineering paradigm, it requires less modeling upfront and enables time travel for both entities and events. \nThe data observability platform detected a data quality issue or a bug in pipeline computation? The functional data engineering pattern allows us to rerun from the interception of the bug to reproduce the data artifacts.  \nLet\u2019s say the business wants to change the ARR (Annual Recurring Revenue) model? no problem. We can create an ARR_V2 [version 2] pipeline and recompute from the beginning of the date partition. It will instantly give the advantage of providing a historical view of changing business context. \nThe functional data engineering principles are not a replacement for any of the data modeling techniques. It makes sense to model your data mart for a domain such as Kimball or data vault. The functional data engineering principles allow us to recompute or re-generate the model if we get something wrong during the data modeling. \nAs Maxime Beauchemin quoted in another influential article The Downfall of the Data Engineer\nData engineering has missed the boat on the \u201cdevops movement\u201d and rarely benefits from the sanity and peace of mind it provides to modern engineers. They didn\u2019t miss the boat because they didn\u2019t show up; they missed the boat because the ticket was too expensive for their cargo.\nWe barely started modernizing data engineering as we are seeing the increased adoption of the formal role of product managers in data. We have a long way to go, and I\u2019m excited to see all the developments in the data engineering space. \nIf you want to discuss further functional principles in data engineering, Please DM me on LinkedIn or book a slot in my calendar. \nhttps://www.linkedin.com/in/ananthdurai/\nhttps://calendly.com/apackkildurai\nhttps://docs.databricks.com/sql/language-manual/delta-merge-into.html\n https://hudi.apache.org/docs/writing_data/\nhttps://iceberg.apache.org/docs/latest/spark-writes/\nhttps://blog.devgenius.io/what-is-data-versioning-and-3-ways-to-implement-it-4b6377bbdf93\nhttps://maximebeauchemin.medium.com/functional-data-engineering-a-modern-paradigm-for-batch-data-processing-2327ec32c42a\nhttps://medium.com/free-code-camp/the-rise-of-the-data-engineer-91be18f1e603\nhttps://maximebeauchemin.medium.com/the-downfall-of-the-data-engineer-5bfb701e5d6b\nAll rights reserved ProtoGrowth Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-112", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nMcKinsey publishes the state of AI in 2022 with the last five years\u2019 review. A few highlights\n63 percent of respondents say they expect their organizations\u2019 investments to increase in AI over the next three years.\nToday, the biggest reported revenue effects are found in marketing and sales, product and service development, and strategy and corporate finance, and respondents report the highest cost benefits from AI in supply chain management\nThe tech talent shortage shows no sign of easing, threatening to slow that shift for some companies. A majority of respondents report difficulty in hiring for each AI-related role in the past year, and most say it either wasn\u2019t any easier or was more difficult to acquire this talent than in years past\nhttps://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai-in-2022-and-a-half-decade-in-review\nOne important characteristic of the data infrastructure is that the more recent the data more frequent the access. Given the characteristic, are we having a \u201cBig Data\u201d problem? Can we spin off a machine with all the data stack and run through the analysis? The author writes an exciting blog, Modern data stack in a Box!!\nhttps://duckdb.org/2022/10/12/modern-data-stack-in-a-box.html\nAirflow is probably one of the Top 5 breakthrough data technology in the last ten years. The author narrates the competitive landscape in the orchestration engine today by comparing some of the pros and cons of Airflow as its stands today. \nhttps://dataengineeringcentral.substack.com/p/why-is-everyone-trying-to-kill-airflow\nDataframe is mainstream data abstraction now, and as the popularity increases, the innovation around the tools to run efficiently increases. Looking at the test results, Polars implementation performs much better than Apache Spark.\nhttps://www.confessionsofadataguy.com/dataframe-showdown-polars-vs-spark-vs-pandas-vs-datafusion-guess-who-wins/\nPipelines for data in motion can quickly turn into DAG hell. Upsolver SQLake lets you process fast-moving data by simply writing a SQL query.\nStreaming plus batch unified in a single platform.\nStateful processing at scale - joins, aggregations, upserts\nOrchestration auto-generated from the data and SQL\nTemplates with sample data for Kafka/Kinesis/S3 sources -> S3/Athena/Snowflake/Redshift\nTry now and get 30 Days Free\nWayve, the autonomous driving technology based on computer vision and machine learning, writes about its end-to-end deep learning model for self-driving cars. I found the tech forum from Scale AI very informative about the various approaches in self-driving car efforts.\nhttps://www.infoq.com/news/2022/12/wayve-deep-learning-model/\nWhether we like it or not, most data engineering and modeling challenges will be handling semi-structured data in the coming years. \nSaaS companies like Salesforce and Zendesk are increasingly processing and emitting sem-structure data. We have already seen systems like Apache Pinot; Apache Druid improves their JSON support. The Percona blog walkthrough JSON support in the relational databases. \nhttps://www.percona.com/blog/json-and-relational-databases-part-one/\nMinimize data downtime, and maximize data trust. As data becomes increasingly important to modern companies, it\u2019s crucial for it to be trusted and accessible. Learn how to stop missing incidents and spending precious engineering hours maintaining static tests and learn how data pipeline monitoring can help take your team to the next level by accessing this new \"Data Pipeline Monitoring 10\" guide.\nGet The Guide\nI enjoy reading Etsy blogs about A/B testing, TIL about the winner\u2019s curse in the experimentation, and the blog narrates how Etsy approaches to mitigate the winner\u2019s curse.\nhttps://www.etsy.com/codeascraft/mitigating-the-winners-curse-in-online-experiments\nWill the semantic layer induce more challenges than the problem it solves? The author explains the problem with customer mapping. Who is a customer? The question remains the same but will have a different answer from marketing, sales, and products. The author gives a fresh perspective to the semantic layer!!\nhttps://diginomica.com/we-need-real-semantic-layer-something-missing\nLegacy CDPs charge you a premium to keep your data in a black box. RudderStack builds your CDP on top of your data warehouse, giving you a more secure and cost-effective solution. Plus, it gives you more technical controls, so you can fully unlock the power of your customer data.\u00a0\nTake control of your customer data today.\nThe blog is an exciting one giving a peak into the private capital ventures approach to finding startup investment strategy. The blog doesn\u2019t leave any traces of the data sources they consume, but curious \nWhat are the data sources the private venture capital firms depend on? Let me know in the comments or DM me on LinkedIn\nhttps://motherbrain.ai/disrupting-private-capital-using-machine-learning-and-an-event-driven-architecture-a966c66ac93a.\nPossibly one of the most brilliant pieces of engineering I read this year\nKudos to the Monzo data team. The blog narrates bringing a platform approach to dbt, lessons learned, tracking back, and pragmatic hacking into dbt core to build the extension framework; A great joy to read. \nI hope we will see dbt-core support the extension framework out of the box\nhttps://monzo.com/blog/2022/12/15/building-an-extension-framework-for-dbt.\nShopify writes three more practical tips for optimizing Apache Flink. TIL about the Hybrid Source support from Apache Flink and the role in Backfilling. I recently had to design for a similar problem and vaguely arrived at a similar strategy, but I thought it might be complicated. Seeing Shopify implement it gives much hope to explore the option further. Thank you, Shopify data team.\nhttps://shopifyengineering.myshopify.com/blogs/engineering/optimizing-apache-flink-tips-part-two\nFirst part: https://shopify.engineering/optimizing-apache-flink-applications-tips\nThe blog is a good summarization of the searching and ranking problem domain. The author narrates techniques to adopt finding the best matching document [search] and order them [rank]. TIL about Pointwise, Pairwise, and Listwise learning methods. \nhttps://medium.com/data-science-at-microsoft/search-and-ranking-for-information-retrieval-ir-5f9ca52dd056\nAll rights reserved ProtoGrowth Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-111", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nIt is a nervous week at the desk as I'm writing the 111th edition of Data Engineering Weekly. If you're a cricket follower, you know what I'm talking about. Yes, we hit the Nelson. If you're unfamiliar with cricket and Nelson, here is an interesting history behind the number 111. \n2022 has been a remarkable year and one of the most memorable years for me personally. It's the year's end, so there is plenty of 2023 predictions in Data Engineering. Should I write another prediction? Probably not (I can hear the sigh of relief from all of you). Instead, I will write three back-to-back articles from my experience in recent data engineering projects. \nThere has been an uptick in discussion about data modeling in recent years. Maxime Beauchemin wrote an influential article, Functional Data Engineering \u2014 a modern paradigm for batch data processing. I recently worked on sketching data architecture from scratch, and the article summarises data pipeline patterns I put in to gather to adopt the functional principles. \nI've been a big fan of data catalogs for a long time. After actively observing a couple of data catalog implementations, I started questioning my beliefs. The article is a reflection of my thoughts and experience with data catalogs. Is Data Catalog a product or a feature? \ud83e\udd14\nData Quality is close to my heart, and I continue studying various social & organizational dynamics about quality in other domains. The Data Quality tools available in the market focus on Quality Control but not much focus on Quality Assurance. I think we barely touched the surface of Data Quality Management. \n\nWith that, Let\u2019s jump on to this week\u2019s top articles.\nA collective understanding of the impact of the experiments is essential to understand the overall business impact of the changes made across all teams. Etsy writes about using holdout groups to estimate the collective impact of individual experiments. \nhttps://www.etsy.com/codeascraft/understanding-the-collective-impact-of-experiments\nExperimentation is cultural; either you believe in an experiment, or you don't. The article from LinkedIn reminds the same by stating the journey from Why Experimentation is so Important for LinkedIn to Approach to Research and A/B Testing.\nhttps://engineering.linkedin.com/blog/2022/our-approach-to-research-and-a-b-testing-\nInstacart writes the summary article from its Distinguished Speaker Series by Professor\u00a0Hongning Wang\u00a0of the University of Virginia. Talks at Google are one of my sources of informative talks, and kudos to the Instacart team for sharing the same.\u00a0\nhttps://tech.instacart.com/personalizing-recommendations-for-a-learning-user-ed170a197f2e\nInstead of building an AI model from scratch, developers can use pre-trained models and customize them to meet their requirements. How exciting it is!! Nvidia writes about some of the sources of pre-trained AI models and their applications. \nhttps://blogs.nvidia.com/blog/2022/12/08/what-is-a-pretrained-ai-model/\nPipelines for data in motion can quickly turn into DAG hell. Upsolver SQLake lets you process fast-moving data by simply writing a SQL query.\nStreaming plus batch unified in a single platform.\nStateful processing at scale - joins, aggregations, upserts\nOrchestration auto-generated from the data and SQL\nTemplates with sample data for Kafka/Kinesis/S3 sources -> S3/Athena/Snowflake/Redshift\nTry now and get 30 Days Free\nZero Trust is a security framework requiring all users, whether in or outside the organization\u2019s network, to be authenticated, authorized, and continuously validated for security configuration and posture before being granted or keeping access to applications and data. Grab writes about how it implemented Zero trust infrastructure for Kafka.\nhttps://engineering.grab.com/zero-trust-with-kafka\nLumen shares a few practical tips to run Flink in production, reflecting a few core themes to scale the streaming infrastructure.\u00a0\nMulti-instance (one cluster per job) infrastructure scale is better than the multi-tenant (one cluster for all jobs) cluster.\nAutomate the CI/ CD pipeline and observability, so it's relatively simpler to scale.\nhttps://medium.com/lumen-engineering-blog/our-journey-with-apache-flink-part-1-operation-and-deployment-tips-5c23e1b96bf7\nMinimize data downtime, maximize data trust. As data becomes increasingly important to modern companies, it\u2019s crucial for it to be trusted and accessible. Learn how to stop missing incidents and spending precious engineering hours maintaining static tests and learn how data pipeline monitoring can help take your team to the next level by accessing this new \"Data Pipeline Monitoring 10\" guide.\n Get The Guide\nAutoTrader writes about its advert scoring feature with prior weightings to give adverts with few observations score that was fair, smoothly changing with increasing data. Thank you, Mark Crossfield, for contributing the article to Data Engineering Weekly. \nhttps://engineering.autotrader.co.uk/2022/10/28/scoring-adverts-quickly-but-fairly.html\nIt is an excellent article about marketing attribution and various attribution models. The author discusses the pros and cons of the Last Touch Attribution Model, First Touch Attribution Model, Linear Attribution Model, and Multi-Touch Attribution Model (Markov Chain)\nhttps://tech.trivago.com/post/2022-12-06-marketing-attribution-evaluating-the-path-to-purchase/\nLegacy CDPs charge you a premium to keep your data in a black box. RudderStack builds your CDP on top of your data warehouse, giving you a more secure and cost-effective solution. Plus, it gives you more technical controls, so you can fully unlock the power of your customer data.\u00a0\n Take control of your customer data today.\nThe multi-cloud fragmented data infrastructure is a nightmare for innovation. The Plexure team writes about how it simplifies the architecture using Databricks and Prefect from Azure & AWS services. \nhttps://medium.com/task-group/reshaping-data-engineering-at-plexure-5897bf398b2b\ndbt Exposure is one of my favorite features that helps codify a model's downstream usage. It may need to be more scalable to define the upstream, but it is a helpful feature to deliver last-mile insights with the data visualization tools. HomeToGo writes about one such experience on integrating dbt exposure and Apache Superset. \nhttps://engineering.hometogo.com/how-hometogo-has-connected-superset-dashboards-to-dbt-exposures-to-improve-data-discoverability-3d0add162e4a\nAn opinionated tool/ platform that drives behavioral and process change is the secret sauce of many successful companies. In a way, the title is misleading, but the essence of the blog is to build tools to drive process changes, not a general-purpose tool. \nhttps://angadsg.medium.com/data-tooling-is-not-the-problem-processes-and-people-are-da25973caa2f\nStandardization of naming and glossary is often a significant hurdle in data management\u2014the author proposes to take inspiration from the Schema.org approach of the sharable definition of a dataset across organizations. Data Contracts are a critical missing piece to achieve this since the shared definitions must integrate with the developer's workflow instead of a separate workflow.  \nhttps://medium.com/@Tonyseale/building-your-connected-data-catalog-634674b41770\nAll rights reserved ProtoGrowth Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions.\n"}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-110", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nBefore we start this week, I\u2019m sorry to disappoint you all: Zero-ETL is nothing, but someone dumps the data into your S3 bucket instead of doing it yourself. You still require cleaning it up. Have fun!!!!\nWhat is the role of data models in modern data platforms, and how have they changed in recent years? The author narrates why the data models are still important for managing data assets' structure, content, and relationships but also need to keep agility in mind to bring business velocity. The article highlights the challenges of maintaining data models in a world where SQL data warehouses are no longer the primary data platform. The author discusses the need for richer metadata to support complex data lineage and evolving privacy requirements.\nhttps://medium.com/@moving-the-needle/reinventing-data-models-keystone-for-modern-data-platforms-132d8283acbc\nWe are navigating a challenging economy which brings focus on optimizations a lot. Given the market condition, what would be a leading trend in data for 2023? Where would the companies spend their $$$?\nThe author gives seven predictions. My take on this \nThe prediction is spot on with the cost optimization, but #1 (cost optimization) & #2 (specialization) conflict. The cost optimization favors more generalized than specialized, so it will be interesting to see how it will turn out.  \nI agree with #3 (central data platform team remains) and #6 (data warehouse and data lake difference blur); it will be amazing if #4 ( > 51% ML application in production) becomes true.\nOn #5, I have a vested interest in Data Contract with Schemata, So hell yeah.\nOn #7, I'm a bit pessimistic about it, given the massive fragmentation in the data infrastructure today with the modern data stack. \nhttps://towardsdatascience.com/whats-next-for-data-engineering-in-2023-7-predictions-b57e3c1bf2d3\nThe application development came a long way in standardizing the interoperability of services. COM/DCOM, CORBA, WSDL to REST Api & rpc frameworks gRPC. The journey created a developer tooling around it and economics around companies like Swagger & Postman. The author narrates the need for Data like the API. We started Schemata on a similar mission, so a big yes. It's time we treat our data like API. \nhttps://joemonti.org/its-time-we-treat-our-data-like-an-api-2a5723b3830b\nThe metrics is a valuable tool for simplifying complex business information and helping to understand how the business is doing. Should we create more metrics to understand the business? The author narrates why choosing a small number of high-quality metrics reduces unnecessary noise and improves decision-making. \nhttpJetBlue'singpoint.substack.com/p/you-have-too-many-metrics\nPipelines for data in motion can quickly turn into DAG hell.  Upsolver SQLake lets you process fast-moving data by simply writing a SQL query. \nStreaming plus batch unified in a single platform.\nStateful procInfluxDB'sscale - joins, aggregations, upserts\nOrchestration auto-generated from the data and SQL\nTemplates with sample data for Kafka/Kinesis/S3 sources -> S3/Athena/Snowflake/Redshift\nTry now and get 30 Days Free\nIt's AWS re: invent time, and AWS did tons of product updates on AI/ ML & Analytics tools. The author ranked the favorites from the announcement. It's more curiosity than excitement for me to see how Athena supports Spark's announcement. I like the idea of \"serverless Spark applications\". What is your favorite announcement? Please comment. \nhttps://c-nemri.medium.com/my-favorite-ai-ml-analytics-aws-re-invent-2022-announcements-b5744c68d5f8\nWhat is all your pipeline is a collection of CTE (Common Table Expression) which occasionally persist data? What if CTE can run in parallel and does a speculative execution, reuse/ rewrite for optimal usage? \nI recently shared the thought and am excited to see Meta\u2019s blog on static analysis of SQL queries. Though it is not exactly what I described, the possibility of a static analyzer on SQL is exciting. \nThere is a need for a SQL orchestration engine that is \u201cPipeline aware\u201d and brings optimization and type safety to data engineering. Let\u2019s call it \u201cdbt next\u201d \ud83d\ude09\nhttps://engineering.fb.com/2022/11/30/data-infrastructure/static-analysis-sql-queries/\nThe data team at JetBlue Airways, a leading carrier in the United States, is responsible for powering insights for the entire organization\u2019s operational and customer service activities. Learn how JetBlue\u2019s data engineering and data science teams leverage Monte Carlo and Snowflake together to accelerate data analysis and drive business value.\nData Engineering Weekly Readers can Save Your Seats by clicking the link.\nDeveloping a test environment is one of the hardest parts of data engineering. Netflix writes about Dataflow and how it supports generating sample workflow with the mocked data to boost developer productivity. \nhttps://netflixtechblog.com/ready-to-go-sample-data-pipelines-with-dataflow-17440a9e141d\nTrace analytics picking momentum in the observability to better understand causal analysis of system failures. There is a lot of similarity between funnel analytics and trace analytics. Is Trace an appropriate data structure for funnel analysis than dimensional modeling? It is something to explore further and delighted to see the release of TraceQL from Grafana.  \nhttps://grafana.com/blog/2022/11/30/traceql-a-first-of-its-kind-query-language-to-accelerate-trace-analysis-in-tempo-2.0/\nJoin RudderStack and InfluxDB\u2019s Director of Analytics, Mona Sami, on Wednesday, December 7th, to learn how the InfluxDB team used RudderStack to establish their data warehouse as a single source of truth.\nhttps://www.rudderstack.com/events/how-influxdata-eliminated-data-silos-in-weeks-with-rudderstack/\nThe server-side event as a communication model suits us well when we have an application design for precomputed & predetermined delivery model. Shopify writes about the system design of Black Friday shopping live visualization. \nhttps://shopifyengineering.myshopify.com/blogs/engineering/server-sent-events-data-streaming\nThe Expedia data platform team writes about unifying data lakes across multi-region using AWS Lake Formation and Glue, which allows federated cross-region data lakes spanning multiple geographic regions in the cloud. This new solution allows teams to access the data without data replication, improving scalability and reducing data latency.\nhttps://medium.com/expedia-group-tech/unify-data-lakes-across-multi-regions-in-the-cloud-61119db325f9\nThoughtworks writes the best practices to implement effective machine learning, and one of the key aspects of it shift-left the data quality via contracts!!!\n\ud83d\udcaf Shift Left, bringing consumers close to the source via Data Contract, is the key to an effective data pipeline.\nhttps://www.thoughtworks.com/insights/blog/machine-learning-and-ai/effective-ml-part-II\nA primary function of the data team is to build a feedback loop for the product performance to improve efficiency and measure the business impact. The Helpshift data team writes an exciting blog about how it runs the performance analysis of its chatbot product with Spark. \nhttps://medium.com/helpshift-engineering/generating-chatbot-performance-insights-using-spark-sql-at-helpshift-6cf15e905604\nAll rights reserved ProtoGrowth Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-109", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nThe Thanksgiving break gives me enough time to catch up on a few podcasts. I'm a fan of the \"SaaS Developer Community\" podcast and Benn's writing, and I can't miss any conversation about Data Contracts.\ud83d\ude0e\nMy thoughts on this conversation, Benn captured very well the overall goal of the Data Contract and the skepticism around it. I have a long list of thoughts on this conversation, which might need a blog post on its own. I want to address one comment in the conversation.\n\u201cDeveloper\u2019s Job is to ship the application code, not to make your dashboard looks good\u201d\nI agree that shipping the application code is the priority. But What is an \u201cApplication Code\u201d? Let\u2019s take an example of Slack features, \u201cCompose a DM,\u201d Channel Selection,\" Invite Members,\u201d or \u201cInvite Reminder\u201d? Machine Learning powers every application feature listed above. Maybe Slack is 1% of the company implementing data engineering effectively to drive the product feature, but that is the point of implementing data contract and shifting left for an efficient data creation process.\nIf you think Data is only for unknown dashboards & back office needs, and data is not part of your product strategy, Sure, you don\u2019t need Data Contract. But if you want to be that 1% of the company that differentiates the product experience and business operation with data, you need to focus on implementing Data Contracts.\nI've seen many data predictions for successive years, but I'm always a fan of folks writing a look back at what happened in the industry to light up the future trend. Possibly one of the best reads I have had recently in Data Engineering, the author highlights three emerging patterns in Data engineering. \nSystems Tend Towards Production\nSystems Tend Towards Blind Federation\nSystems Tend Towards Layerinitis\nhttps://ian-macomber.medium.com/data-systems-tend-towards-production-be5a86f65561\nDid Meta successfully privatize world peace? \ud83e\udd14 \nMeta writes about CICERO \u2013 the first AI to achieve human-level performance in the popular strategy game Diplomacy. CICERO demonstrated this by playing on webDiplomacy.net, an online version of the game. CICERO achieved more than double the average score of the human players and ranked in the top 10 percent of participants who played more than one game!!!\nhttps://ai.facebook.com/blog/cicero-ai-negotiates-persuades-and-cooperates-with-people/\nAirbnb takes customer service from a simple customer service response template to the AI text generation model. The real-time agent assistant model is an exciting read. \nhttps://medium.com/airbnb-engineering/how-ai-text-generation-models-are-reshaping-customer-support-at-airbnb-a851db0b4fa3\nThe data team at JetBlue Airways, a leading carrier in the United States, is responsible for powering insights for the entire organization\u2019s operational and customer service activities. Learn how JetBlue\u2019s data engineering and data science teams leverage Monte Carlo and Snowflake together to accelerate data analysis and drive business value.\n Save Your Seat\nMyntra writes about its near-real-time streaming platform built on top of Kafka, Flink & Spark. It is a great overview of streaming infrastructure characteristics. \nhttps://medium.com/myntra-engineering/quicksilver-near-real-time-platform-at-myntra-9e8edf6ede91\nData & Machine Learning are increasingly powering the applications and driving user experience. Airbnb writes one case about building Airbnb, building travel categories with ML and human-in-the-loop.  \nhttps://medium.com/airbnb-engineering/building-airbnb-categories-with-ml-and-human-in-the-loop-e97988e70ebb\nIn this piece RudderStack CEO, Soumyadeb Mitra, makes the case for a new approach to the customer data platform\u2014the headless CDP. He defines the headless CDP as a tool with open architecture, purpose built for data and engineering teams, that makes it easy to collect customer data from every source, build your customer 360 in your own warehouse, then make that Data available to your entire stack.\nhttps://www.rudderstack.com/blog/it-s-time-for-the-headless-cdp/\nFlink SQL made significant advancements in unifying the batch and the real-time computation. The blog captures the history of Flink SQL, its current state, and the challenges ahead of it. The stream-stream join is still expensive to operate; I\u2019m excited to see the future progress of Flink SQL and how it can simplify operating streaming infrastructure. \nhttps://www.ververica.com/blog/apache-flink-sql-past-present-and-future\nI thought Apache Atlas was largely forgotten at this stage; Line writes an exciting blog about its usage of Apache Atlas for data lineage. Too many data lineage visualizations can also confuse the users, and it is exciting that the Line data team highlighted the edge case and how it solved it. \nhttps://engineering.linecorp.com/en/blog/data-lineage-on-line-big-data-platform\nFeature Snippets are a vital technique to elevate the search & discovery experience for the users. AutoTrader writes about the system design of its customer segmentation to drive the Feature Snippet in its search experience. \nhttps://engineering.autotrader.co.uk/2022/11/23/enabling-real-time-personilsation-with-our-in-house-customer-data-platfom.html\nShould we keep dbt monorepo in an organization or split it as multiple repos? The build systems like Bazel and Pants encourage monorepo, but that comes with operation and implementation costs. The author narrates how the dbt package helps to minimize code duplication and encourages multi-repo patterns. \nhttps://techwithadrian.medium.com/dbt-repository-to-split-or-not-to-split-909d366d0998\nEvery engineer has their own horror stories about their work with CSV files. We can write N number of blogs on Why You Don\u2019t Want to Use CSV Files, But CSV format is widely used in data science and the simple human-readable format that is widely known and understood. The simplicity of CSV is its drawback; one such drawback is the lack of a type system. The author narrates how Apache Arrow infers types while reading the CSV file. \nhttps://thisisnic.github.io/2022/11/21/type-inference-in-readr-and-arrow/\nAll rights reserved ProtoGrowth Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-108", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nGoogle published Data Cards, a dataset documentation framework aimed at increasing transparency across dataset lifecycles. Data Cards include the following: \nupstream sources \nData collection and annotation methods \nTraining and evaluation methods\nIntended use of the dataset \nDecisions affecting model performance\nThe data cards approach is fascinating, especially As machine learning (ML) research moves toward large-scale models capable of numerous downstream tasks. A shared understanding of a dataset\u2019s origin, development, intent, and evolution becomes increasingly essential for responsible and informed development.\nhttps://ai.googleblog.com/2022/11/the-data-cards-playbook-toolkit-for.html\nThe short YouTube video gives a nice overview of the Data Cards.\nI found this article a bit late, but an exciting read for this week. We often think of AI/ ML as a complex data processing problem, but it doesn\u2019t make any use until it is exposed to an end user or an application. So what makes a user interface intelligent? The author walks through what is intelligent in the UI and what it does for users. \nhttps://uxdesign.cc/what-makes-user-interfaces-intelligent-9f63b27ca39\nData as a product is a trending phrase and has begun mainstream adoption integrating with the organization's product strategy. But what are the types of product strategies? The author classifies the data products as\nData Platform as a product\nData Insight as a product\nData Activation as a product\nhttps://pmdata.substack.com/p/types-of-data-products\nThe Data team's prime mission is to educate and empower data-driven product & business operations across an org. The most critical work of data happens outside the data team. The author narrates a few practical tips for creating success with people outside the data team. \nhttps://mikkeldengsoe.substack.com/p/purple-people-outside-data\nWith Upsolver SQLake, you build a pipeline for data in motion simply by writing a SQL query defining your transformation.\nStreaming and batch unified in a single platform\nNo Airflow - orchestration inferred from the data\n$99 / TB of data ingested | transformations free\nStart Your 30 Day Trial\nGartner Predicts 25% of People Will Spend At Least One Hour Per Day in the Metaverse by 2026. I\u2019ve no idea how it will play out yet, but as a data engineer, what the Metaverse means to us? The author narrates what an industrial metaverse is, its key components, and some practical applications of an industrial metaverse. \nhttps://medium.com/data-science-at-microsoft/industrial-metaverse-a-software-and-data-perspective-d09950a453f6\nData privacy and regulatory requirements mostly won\u2019t feature in a developer blog, and I found pleasantly surprised with this article from Atlassian. Kudos to the author and the Atlassian team. The blog narrates the European Commission\u2019s updated version of the European Standard Contractual Clauses (EU SCCS) and how to prepare to handle the privacy laws.\nhttps://blog.developer.atlassian.com/learn-how-to-prepare-for-new-european-data-privacy-requirements/\nIn this piece RudderStack CEO, Soumyadeb Mitra, makes the case for a new approach to the customer data platform\u2014the headless CDP. He defines the headless CDP as a tool with open architecture, purpose built for data and engineering teams, that makes it easy to collect customer data from every source, build your customer 360 in your own warehouse, then make that data available to your entire stack.\nhttps://www.rudderstack.com/blog/it-s-time-for-the-headless-cdp/\nIn a data pipeline, there is always a conflict between correctness (Trust) and speed (Velocity). The trade-off plays a critical role in a critical system like Experimentation. The author narrates balancing the velocity and confidence in online Experimentation. \nhttps://doordash.engineering/2022/11/15/balancing-velocity-and-confidence-in-experimentation/\nI'm delighted to see more Netflix engineering blogs coming out in recent days talking about the impact of AI/ ML in media production. The blog narrates one such application that uses video quality with neural networks. I don't know if Netflix will be a thing in the next ten years, but the impact it will make on media production integrated with the advancement of AI/ ML will be significant. \nhttps://netflixtechblog.com/for-your-eyes-only-improving-netflix-video-quality-with-neural-networks-5b8d032da09c\nAre you considering investing in a data quality solution? Before you add another tool to your data stack, check out our latest guide for 10 things to consider when evaluating data observability platforms, including scalability, time to value, and ease of setup.\nAccess You Free Copy for Data Engineering Weekly Readers\nMyntra writes about its data processing framework - Janus. The blog narrates the requirements and motivation to design Janus and critical pieces of the architecture, such as data catalogs, pipeline modeling, pipeline deployment, and pipeline execution. I'm a little more curious to understand the design in detail to see the data catalog as an integral part of the pipeline design. \nhttps://medium.com/myntra-engineering/janus-data-processing-framework-at-myntra-980ba8cb15a5\nStructured data from source systems can significantly reduce data management's complexity; however, it is not uncommon to encounter source systems without such capabilities. The author narrates why Google BigQuery's schema auto-detection system fails them, which leads to building a custom schema detection tool.\nhttps://medium.com/99dotco/bigquerys-schema-auto-detection-does-not-work-perfectly-like-we-want-it-to-so-we-build-our-own-93a5f1a1f67\nMany real-time pipelines are micro-batch pipelines. I often tell my team real-time & batch systems are data processing pipelines with different window functions. I found the blog exciting as the first blog I've seen using Snowflake for a near-real-time pipeline that replaces OLAP systems like Apache Druid. \nhttps://medium.com/gumgum-tech/replacing-apache-druid-with-snowflake-snowpipe-74c8d7c9b9c3\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-107", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nAdopting a new skill is always challenging, but that is where we grow as a programmer. Innovation happens at the intersection of applying learning from one domain to another. I'm a software engineer; how can I transition to a Machine Learning engineer? The author shares the experience of one such transition. \nhttps://vickiboykis.com/2022/11/10/how-i-learn-machine-learning/\nNumerous heterogeneous services make up a data platform, such as warehouse data storage and various real-time systems. The schematization of data plays a vital role in a data platform. Meta writes about its internal implementation of the Schema management system at scale. \nhttps://engineering.fb.com/2022/11/09/developer-tools/tulip-schematizing-metas-data-platform/\nData Contract is a hot topic in data engineering. It moved from the speculation to the data engineers understanding the benefit of it and asking when we can get the implementation soon. \nI met many data leaders about Data Contracts, my project Schemata, and how the extended version we are building can help them create high-quality data. The conversation is mostly should adopt a carrot-or-stick approach.\nThe author walks through various strategies a data contract platform can adopt to simplify the adoption.\nhttps://pmdata.substack.com/p/a-pms-thoughts-on-data-contracts\nIf you are a modern data leader and interested in adopting Data Contract or talking to understand what it is, say hi on LinkedIn\nhttps://www.linkedin.com/in/ananthdurai/\nCan ML replace creative content generators, or can it be an excellent assistant to take creativity to a new height? Netflix writes about its ML platform to assist its media production. \nhttps://netflixtechblog.com/new-series-creating-media-with-machine-learning-5067ac110bcd\nWith Upsolver SQLake, you build a pipeline for data in motion simply by writing a SQL query defining your transformation.   \nStreaming and batch unified in a single platform\nNo Airflow  - orchestration inferred from the data \n$99 / TB of data ingested | transformations free\n Start Your 30 Day Trial\nA robust event-tracking system is critical for an efficient data management platform. Wealthfront writes about the end-to-end system design of its event tracking system on top of Avro. \nhttps://eng.wealthfront.com/2022/11/07/event-tracking-system-at-wealthfront/\nOS-C is establishing an Open Source collaboration community to build a data and software platform that will dramatically boost global capital flows into climate change mitigation and resilience. The author narrates how OS-C adopted Data Contract and federated data governance strategy to help fight against climate change. \nhttps://towardsdatascience.com/making-climate-data-easy-to-find-use-and-share-5190a0926407\nIt's easy to overlook all of the magic that happens inside the data warehouse. Here, Brian Lu details the core concepts of dimensional data modeling to give us a better appreciation of all the work that goes on beneath the surface. He covers the speed vs. granularity tradeoff, highlighting the denormalized table and why it's today's technique of choice, and he offers some clarity on how to think about the benefits of immutability.\nhttps://www.rudderstack.com/blog/why-you-should-care-about-dimensional-data-modeling/\nEtsy writes about its journey from gradient boost decision tree-based search ranking to a neural ranking model. The blog highlights the need for a longer training window time for the neural ranking model compared to the decision tree model and the need for high-quality backward compatible historical events. \nhttps://www.etsy.com/codeascraft/deep-learning-for-search-ranking-at-etsy\nUber writes about its Uber Fright architecture highlighting how it archives data freshness, latency, reliability, and accuracy. The design is a good testimony to Apache Pinot's performance with index optimization techniques like JSON, sorted columns, and Star-tree to accelerate the query's performance. \nhttps://www.infoq.com/news/2022/11/uber-freight-analysis/\nAre you considering investing in a data quality solution? Before you add another tool to your data stack, check out our latest guide for 10 things to consider when evaluating data observability platforms, including scalability, time to value, and ease of setup.\nAccess You Free Copy for Data Engineering Weekly Readers\nIf you continue working on a problem, you can soon discover the pattern to abstract the problem. The author shares one such experience where all the A/B experiment analytics can build on top of the basic SQL queries.\nhttps://medium.com/@foundinblank/using-sql-to-summarize-a-b-experiments-d30428edfb55\nAirflow made some leaps of improvement with TaskGroups and dedicated SQLColumnCheckOperator and SQLTableCheckOperator. This blog is an excellent overview of incorporating a data quality check with Airflow.\nhttps://medium.com/@astronomer.io/how-to-keep-data-quality-in-check-with-airflow-f7856443149a\nMcDonald's writes its always-on reporting system to empower executive reporting, sales decomposition, and scenario forecasting. The blog narrates the design of the data collection, modeling & visualization layers.\nhttps://medium.com/mcdonalds-technical-blog/enabling-advanced-sales-decomposition-at-mcdonalds-559a7311ac23\nAWS DMS and DynamoDB Stream simplify, enabling a Change Data Capture [CDC] pipeline from zero to one. Swiggy writes its adoption of CDC with Schema evolution and reconciliation engine to handle the late-arriving & unordered data. \nhttps://bytes.swiggy.com/architecture-of-cdc-system-a975a081691f\nProbably this is the first time I have heard the term infrastructure renovation. Coinbase writes about its internal Kafka platform features that support multi-cluster management, streaming SDK, ACL support, and enablement of Kafdrop, a web UI for viewing Kafka topics and browsing consumer groups. \nhttps://www.coinbase.com/blog/kafka-infrastructure-renovation\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/an-open-letter-to-data-ninjas-yes", "title": "Data Engineering Weekly", "content": "Not too long ago, I worked for a famous messaging platform; You may be typing on it as you read this blog. I took a break from data engineering and focused on system monitoring and observability for some time. Little did I know I would be building data pipelines to measure system efficiency instead of measuring business efficiency.\u00a0\nAs the famous saying goes;\nOnce a data engineer, always a data engineer.\u00a0\nI worked with many incredible humans in reliability engineering; It\u2019s fun, and there is always tons to learn every day. I learned state-of-the-art incident management processes and the quest for operational excellence to run large-scale distributed systems. There I saw the \u201cReliability Ninjas.\u201d\nIncidents are brutal and stressful; I\u2019ve seen companies paying to be on-call or incentives to take a day off after stressful incidents. We have all seen it, and we have all gone through it.\u00a0\nHowever, I have seen a set of people; I call them fondly \u201cThe Reliability Ninjas.\u201d When the pager alerts, they are enlightened. It is their call, time to roll up their sleeves, time for a quick expresso, and yay, it is the incident time.\u00a0\nReliability Ninjas; Let\u2019s admit it; You love incidents.\nIs it 2010? I don\u2019t remember the exact year. I consulted an online media company focusing on building a search engine for curated recipe content. The chefs enter their recipes in an internal content management system (CMS), and our job is to build a search & recommendation platform. Soon we ran into a search efficiency problem. The CMS provides a simple free text box for the Chefs to enter the ingredients, measurements, and other metadata. There is no structure, making it harder for us to bring relevancy to the search.\u00a0\nWe embarked on a project to apply NLP to extract the structure; At that time,\u00a0 I asked the data engineering leader,\u00a0\nMe: \u201cHey, You know what; the data creation is in our control. Why can\u2019t we provide a simple UI to enter the ingredient, quantity & unit of measure? Why do we need an NLP?\u201d\u00a0\nThe answer I got: \u201cAnanth, if we do so, where is the technology challenge for us?\u201d \nIt is where, I met the first Data Ninja, and it never stopped from there.\nI saw the data ninjas unfurled with the Data Contract conversations. Some blame the people for not entering the ingredients to make it a people problem. Some say, Buckle up, data engineers; let the upstream send all data in whatever format. We got the dbt superpower to make things work; ignore the Snowflake billing.\u00a0\nListening to all these conversations, After a decade, I still wonder why we can\u2019t introduce simple \u201cdata contract\u201d tooling to enable high-quality data creation to capture the ingredients. Am I worthy enough Data Engineer?\nIs being a Data Ninja bad? No, not at all. However, One vital characteristic I observe from the Reliability Ninja is their actions after the incident. The Reliability Ninjas go through a look-back analysis, understand why the incident happens, and figure out a way to prevent it from happening again.\u00a0\nI still remember the mantra; It\u2019s okay to fail. Just don\u2019t fail twice the same way.\nData Ninjas, on the other hand, double down on building patches for leakage since It is a matter of building another dbt model. We lost the data team's meaning on every data patching and restricted ourselves from delivering dashboards and building dbt models.\nThe Data Team exists to provide an end-to-end data solution for an organization, starting from the origination of the data itself. The quest to build a high-quality end-to-end data system is the difference between successful data-driven organizations and the rest of the organizations.\nNow it is time for you to ask which persona you are. Are you a Reliability Ninja or Data Ninja? Let me know in the comments.\nWhen I wrote Schemata, this was at the top of my mind. As a data leader, should I focus on teaching the product engineers about data engineering or build tooling to abstract it? How should I build systematic feedback to enable executive buy-in for data creation?\u00a0\nI will be writing more about this in the coming weeks; Meanwhile, if this is something you can resonate with and want to discuss more, \nBook me a slot here: https://calendly.com/apackkildurai \nLinkedIn: https://www.linkedin.com/in/ananthdurai/\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-106", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nLast week I switched from Twitter to Mastodon. Thanks to David Jayatillake for setting up data-folks.masto.host. Honestly, I was a bit suspicious, but to my surprise, it a far better than I expected. I\u2019ve more high-quality engagement with the data folks than Twitter without distraction. If you\u2019re a data professional, please join at data-folks.masto.host. I\u2019m at ananth@data-folks.masto.host. I\u2019m following most of the data professionals, so you can easily build your network from my following list.\nLast two weeks, a few data folks reached out to me about Data Contract and what it is. The term \u201cContract\u201d is always a source of confusion.  People think of \u201cContract\u201d in a traditional term as static and bureaucratic. I often use the term \u201cSchema Ops\u201d for this very reason. Here is my definition of a Data Contract\nA data contract/ Schema Ops is not static or a one-time task. The data contract flow originated from the data producer. As the adoption grows, the consumers start amending expectations and expect enrichment on their contracts. A data contract is a continuous and collaborative system because the business context and requirements won\u2019t be static.\nI plan to write a series of blogs on Schemata and Data Contract in the coming weeks. I know I told you this before, so George R. R. Martin kindly stepped in for me to give the update for my promised blog posts.\nIn-House notifications are a significant lead generator for online commerce. Uber writes about the complexity of the problem statement and how it adopted the linear program (linear optimization) to achieve the best outcome.\nhttps://www.uber.com/en-US/blog/how-uber-optimizes-push-notifications-using-ml/\nMeta writes about a similar system of improving notification with ML. The blog discusses the tradeoff between the user experience and the CTR model for notification and the adoption of a causal inference model for notification management systems. \nhttps://engineering.fb.com/2022/10/31/ml-applications/instagram-notification-management-machine-learning/\nLooping in real-time user interaction events with the recommendation engine can significantly improve the user experience. Pinterest writes one such system for their Homefeed and how it leverages real-time user actions in the recommendation to boost Homefeed engagement volume. \nhttps://medium.com/pinterest-engineering/how-pinterest-leverages-realtime-user-actions-in-recommendation-to-boost-homefeed-engagement-volume-165ae2e8cde8\neBay writes about its adoption of the weighted z-test, which can combine readouts (including p-values, lift, CI, etc.) from multiple independent experiments for the same hypothesis. I\u2019m looking forward to reading more on this topic to learn more.\nhttps://tech.ebayinc.com/engineering/increase-a-b-testing-power-by-combining-experiments/\nConsidering investing in a data quality solution? Before you add another tool to your data stack, check out our latest guide for 10 things to consider when evaluating data observability platforms, including scalability, time to value, and ease of setup.\nAccess You Free Copy for Data Engineering Weekly Readers\n A fascinating read of the week about the Explore-Exploit dilemma in the ranking model. \nThe problem in the context of Trivago as Exploitation means showing users accommodations that have historically performed well. Exploration means showing accommodations that have never been shown to the user, with the hope of finding those that will perform better than those currently shown.\nTrivago concludes that one can overcome this by combining classical approaches to exploration with model-based approaches to systematically identify the most promising inventory in the unknown pool.\nhttps://tech.trivago.com/post/2022-11-04-explore-exploit-dilemma-in-ranking-model/\nMachine Learning increasingly occupies important decisions in our lives, from credit scores to loan approval to where to eat and shop. But How well do we know the Machine Learning models? \nMachine Learning (ML) model explainability is analyzing and surfacing the inner workings of a Machine Learning model or other \"black box\" algorithms to make them more transparent. \nThe blog narrates how Azure InterpretML service can help to understand the ML models' predictions better.\nPart 1: https://medium.com/data-science-at-microsoft/how-well-do-you-know-your-machine-learning-models-part-1-of-2-35979512ceba\nPart 2: https://medium.com/data-science-at-microsoft/how-well-do-you-know-your-machine-learning-models-part-2-of-2-c36e8184bab4\nLearn how HealthMatch built a HIPAA-compliant data stack with Customer.io and RudderStack to reduce reliance on developers for messaging use cases. After only a week of implementation time, their team launched a targeted SMS campaign with the new stack that drove $130k in revenue within 24 hours. Register today and join live on Wednesday 11/9, at 12PT / 3ET.\nhttps://www.rudderstack.com/events/how-healthmatchio-used-customerio-and-rudderstack-to-launch-their-new-business-model-in-24-hours/\nContinuing our quest to learn more about the ML model, The author writes about how the Decision Tree works. Though Decision Trees look simple and intuitive, there is nothing straightforward about how the algorithm decides on splits and how tree pruning occurs. I learned a ton from this article. \nhttps://towardsdatascience.com/decision-trees-explained-entropy-information-gain-gini-index-ccp-pruning-4d78070db36c\nIt is indeed an excellent year for software system design for ML; as you noticed in this week's edition, most of the article discusses ML system design. The author compiled some exciting papers on MLOps and is looking forward to reading more of these papers. \nhttps://datadrivenbabe.substack.com/p/this-has-been-such-an-excellent-year\nAs a data practitioner, It is inevitable to expose to a stream of requests from various stakeholders. How should one approach it systematically to elevate data function and improve customer satisfaction? The author gives some valuable strategies on the same. \nhttps://pedram.substack.com/p/the-eternal-suffering-of-data-practitioners\nThere is always flakiness in adopting any solutions that require further optimization. Inventa writes about such optimization challenges with dbt cloud's CI/ CD system and how it optimized it. TIL about Slim CI, and looking forward to reading more about it. \nhttps://medium.com/building-inventa/how-we-slimmed-down-slim-ci-for-dbt-cloud-6a944e7574e2\nIdentity and access management is a critical need for the data infrastructure. There is a need for a lightweight solution in this space, and delighted to see Permifrost from Gitlab. Yousign's team writes about how it adopted Permifrost with its infrastructure. \nhttps://medium.com/yousign-engineering-product/snowflake-rbac-implementation-with-permifrost-3d30652825ad\nOf all criticism about Hadoop, Hive, and its ecosystem, one thing it got correct is the Hive metastore. Every data processing engine has one metadata store to integrate. The cloud data warehouses and LakeHouse systems have broken that promise ever since, and it is a constant struggle to sync metadata across different systems. \nZapr talks about one such challenge with Hive metastore and Glue catalog and its approach to bringing efficiency. \nhttps://kpskarthick1.medium.com/how-we-enhanced-productivity-of-zaprs-data-platform-and-saved-costs-5ab5f3a42aa8\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-105", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nThe results are out for our poll on the current state of the Data Catalogs. The highlights are that 59% of folks think data catalogs are sometimes helpful. It means Data Catalogs still not find their place in the data workflow, where 15% of folks are unsatisfied with them. Only 1/4th of the people are happy with the data catalogs [well, 20% of the responders are the data catalog vendors, so]\nIn the coming weeks, I will reach the responders to gain more insights and share more as I learn. Thank you to everyone who participated in the poll. \nI'm excited about the Schemata & Data contract because it perfectly stitches catalogs, data quality, and glossary. The current state of these systems is inherently passive systems. We saw in the Data Catalog poll how far it has to go to be helpful and active within a data workflow. \nThe author narrates the data contract, why it is the foundation for building data mesh, and how it glue data catalogs, glossary and data quality systems. \nhttps://towardsdatascience.com/data-contracts-the-mesh-glue-c1b533e2a664\nIt is almost the end of 2022, and it's time for 2023 predictions. I found the nine predictions exciting and spot on in most cases. My thoughts,\nI'm 100% on WASM or similar technology becoming an integral part of the data ecosystem.\u00a0\nI'm a bit skeptical about #5, where Notebook capture Excel. It is reciprocal of the other two predictions where in-memory & WASM are gaining popularity. I expect spreadsheets to gain more adoption with these advancements than Notebooks.\nThere is no mention of data management in general, but mainly of usage and operational factors. Nothing groundbreaking will happen on data management in 2023, but I expect a little momentum behind data management towards the end.\u00a0\nhttps://pitch.com/public/1115d675-9e2e-4c03-ab3c-aa91fb0bf4be/d6c34fc4-d7c7-40b8-837d-e09c9c6e187f\nThe generative AI is on everyone\u2019s lips. In the past few years, the power of the foundation model has stretched our imagination of what is possible: the model can be trained on a surrogate task and later apply the learned knowledge from that task to many other downstream tasks. I found the blog helpful in understanding the generative model\u2019s historical development and the path forward.\nhttps://irissun.substack.com/p/anything-and-everything-about-generative\nConsidering investing in a data quality solution? Before you add another tool to your data stack, check out our latest guide for 10 things to consider when evaluating data observability platforms, including scalability, time to value, and ease of setup.\nAccess You Free Copy for Data Engineering Weekly Readers\nThe self-service is not free and brings challenges, as the LinkedIn data team narrates.\n1)\u00a0multiple similar datasets often led to inconsistent results and wasted resources, \n2) a lack of standards in data quality and reliability\u00a0made it hard to find a trustworthy dataset among the long list of potential matches, and \n3) complex and unnecessary dependencies among datasets led to poor and difficult maintainability.\nLinkedIn writes about how it builds \u201cSuper Tables\u201d [Is that another name for the semantic layer?] to solve some of the challenges with self-serving. \nhttps://engineering.linkedin.com/blog/2022/super-tables--the-road-to-building-reliable-and-discoverable-dat\nThe value of the data is directly proportional to the recency of the data. Can we use an in-memory database to increase the speed and the value generation? DuckDB is gaining much attention on this promise, and the Dagster team writes about its experimental data warehouse built on top of DuckDB, Parquet, and Dagster. \nhttps://dagster.io/blog/duckdb-data-lake\nIt's easy to overlook all of the magic that happens inside the data warehouse. Here, Brian Lu details the core concepts of dimensional data modeling to give us a better appreciation of all the work that goes on beneath the surface. He covers the speed vs. granularity tradeoff, highlighting the denormalized table and why it's today's technique of choice, and he clarifies how to think about the benefits of immutability.\nhttps://www.rudderstack.com/events/how-shippit-achieved-a-unified-view-of-customers-with-snowflake-and-rudderstack/\nThe first critical step to bringing data-driven culture into an organization is to embed the data collection and analytical requirement part of the product development workflow. The author narrates how the analytical requirement document can help to define a better data strategy. \nhttps://sarahsnewsletter.substack.com/p/the-analytics-requirements-document\nDeezer writes about Flow moods, an emotional jukebox recommending personalized music at scale. The approach to learning the presence of certain types of instruments, the choice of tempo, complex harmonies, and loudness to construct a song's musical mood is an exciting read. \nhttps://deezer.io/how-deezer-built-the-first-emotional-ai-a2ad1ffc7294\nExpedia writes about its approach to categorizing customer feedback using unsupervised learning. Part of me wonders, shouldn\u2019t it be a solved problem where the cloud providers can give these solutions out-of-the-box? \nhttps://medium.com/expedia-group-tech/categorising-customer-feedback-using-unsupervised-learning-8608c1e62d48\nData ingestion is a heterogenous system with multiple sources with its data format, scheduling & data validation requirements. The modern data stack is trying to address this problem in a silo; the org eventually has to tie everything to make it work. ABN AMRO shares its case study on how it built a metadata-driven data ingestion platform to preserve lineage, quality, and scheduling.\nhttps://medium.com/abn-amro-developer/building-a-scalable-metadata-driven-data-ingestion-framework-7c43779277d4\nProfiling and auditing the workflow is essential for operating the data pipeline at scale. In the past, I try to use the \"airflow.log\" table and the \"profiling\" feature to achieve the same. Surprisingly, the Airflow profile and the logs table are not widely popular. I'm excited to see Apache Hop added it as an integrated feature with the information logging framework. \nhttps://bartmaertens.medium.com/a-new-execution-information-logging-in-apache-hop-92534f8a306c\nHow to think about starting Ethereum blockchain analytics? The author explains how to dump the history of blockchains into S3. Til about https://github.com/blockchain-etl/ethereum-etl. \nThank you, Anton Bryzgalov, for contributing the blog to the Data Engineering Weekly. \nhttps://betterprogramming.pub/how-to-dump-full-ethereum-history-to-s3-296fb3ad175\nPlease find the instruction here if any of our readers would like to contribute to DEW.\nhttps://github.com/ananthdurai/dataengineeringweekly#how-can-i-contribute-articles\nSimplicity is the art of excellent engineering. The title is self-explanatory about what it is doing, and I found this exciting hack. Kudos to Tom Klimovski. \nhttps://medium.com/@tom.klimovski_90944/serving-dbt-docs-on-gitlab-static-pages-3365416c8b22\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-104", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nHello Data Friends, Welcome to another edition of Data Engineering Weekly. First, I\u2019m thrilled to see this poll from AirByte. 48.5 of Data Engineers say they read Data Engineering Weekly to keep up with the data engineering landscape. Thank you all for your kind support. \u2764\ufe0f\u2764\ufe0f\u2764\ufe0f\nThe top of my mind for this week is Data Catalog. I'm one of the early advocates for Data Catalogs and am excited about the possibility of Data Catalogs. The Data Engineering Weekly even published a special Metadata Edition focusing on the historical development of the Data Catalog. \nhttps://www.dataengineeringweekly.com/p/data-engineering-weekly-21-metadata\nIt is almost two years since we published the metadata edition, but I keep thinking back. Does Data Catalogs live up to the promise? Hence I published an open poll on LinkedIn to find out.\nhttps://www.linkedin.com/posts/ananthdurai_dataengineering-datacatalog-activity-6989772780862873600-YFmA/\nWe will talk more about Data Catalog in the coming weeks. Meanwhile, share your thoughts about Data Catalog in the poll & comments section. Oh, a humble request to the Data Catalog vendors, Please abstain from the poll \u2764\ufe0f \nIt has been an eventful last week for dbt with Coalesce conference.  I missed attending in person this time but caught of tech talks via live streaming. My reaction to the conference,\nYou can watch all the recordings of the talk here\nTwo significant announcements at the dbt conference\nPython language support in dbt core\npublic preview of the semantic layer\nThe author narrates an in-depth view of the dbt semantic layer.  \nhttps://blog.rittmananalytics.com/the-dbt-semantic-layer-data-orchestration-and-the-modern-enterprise-data-stack-78d9d9ed5c18\nWe debated a lot of bundling vs. unbundling. Is all-in-one data stacks the future? The article from Ben came timely as dbt unveils the semantic layer to play the hub of the analytical ecosystem. The author compares five available all-in-one data platforms and discusses their pros & cons. \nhttps://medium.com/coriers/the-next-generation-of-all-in-one-data-stacks-f46069ad10fd\nDon't miss a chance to get candid with your data peers on the hottest topics in data, learn about 2023 trends, and hear from the biggest names in data and analytics about the ideas and technologies pioneering our industry. Featuring, founders and data leaders from dbt Labs, Fivetran, The New York Times, GitLab, Fox Corporation.\nData Engineering Weekly readers, Get Your Free Ticket!!!\nRecSys is a leading conference focusing on industrial recommender engine implementation. Crtieo published the key takeaway for the 2022 RecSys conference. TIL about AI Mediated Communication and its impact. \nhttps://medium.com/criteo-engineering/highlights-of-recsys-2022-c136a9b6fbd0\nNetflix writes about Maestro, it\u2019s workflow orchestrator that can schedule and manage workflows at a massive scale. The design is a fantastic system design read on how to build a scalable orchestration engine. It is one of the very few systems I wish open-sourced soon. \nhttps://netflixtechblog.com/orchestrating-data-ml-workflows-at-scale-with-netflix-maestro-aaa2b41b800c\nRegardless of how data is being used, it is critical that the information is trusted. The practice of data reliability engineering has gained momentum recently to address that question. Soda Checks Language helps support the efforts of data teams with the corresponding Soda Core utility that acts on this new DSL. In this Data Engineering Podcast by Tobias Macey episode, Tom Baeyens explains their reasons for creating a new syntax for expressing and validating checks for data assets and processes, as well as how to incorporate it into your own projects.\nData testing and Data Observability are vital components to keep the data quality in a complex data pipeline. Checkout.com writes about how it uses dbt tests, Monte Carlo, and Data dog to test & monitor the data pipeline. \nhttps://medium.com/checkout-com-techblog/testing-monitoring-the-data-platform-at-scale-e22d9cf433e8\nKumu writes about its ML platform journey with the end-to-end Machine Learning Platform lifecycle. The ML platform is overwhelmingly complex, and the Kumu team suggests focusing on three basics to scale.\u00a0\nCode Maintainability\nAutomated Tests and Deployment\nDeployment Governance\nhttps://medium.com/@karlitodata/ml-engineering-at-kumu-turning-models-into-products-b2b4faeb2b40\nJoin us live on October 25th for a free deep-dive webinar featuring Nitt Chuenprateep, Business Systems and Data Manager at Shippit. Learn from the experts as they share the secrets of Shippit\u2019s success and how they successfully utilized Snowflake and RudderStack to become warehouse-first. Don\u2019t miss this opportunity to gain expert advice on how to build your ideal data stack and achieve a unified view of your customers. https://www.rudderstack.com/events/how-shippit-achieved-a-unified-view-of-customers-with-snowflake-and-rudderstack/\nWe often joke that Data Team is the backend of the backend. Gaining visibility in an org is the first significant challenge any data team will face to influence a data-driven culture. Shopify writes an exciting blog narrating how to structure the data team to maximize the influence in an org. \nhttps://shopifyengineering.myshopify.com/blogs/engineering/how-to-structure-data-teams\nWhen developing new products, the big question we seek to answer is, \u201cDoes this product have product-market fit?\u201dAnalytics plays a central role in addressing this question. The Meta team writes an exciting blog how to approach PMF (Product-Market Fit) through analytical engineering. \nhttps://medium.com/@AnalyticsAtMeta/analytics-and-product-market-fit-11efaea403cd\nExperimentation plays a vital role in analytical engineering. Should one buy expensive software to run experimentation? Is it complex to build an in-house experimentation platform? The author writes a simple enough hack to start experimentation without waiting to build a platform or buying experimentation software. \nhttps://medium.com/deliberate-data-science/experimentation-platform-in-a-day-c60646ef1a2\nDuckDB, an in-process database management system, has gained good traction recently. Selectively mixing DuckDB with Panda's workload improves the data join performance significantly. The author writes a step-by-step guide on using dbt and DuckDB. \nhttps://rmoff.net/2022/10/20/data-engineering-in-2022-exploring-dbt-with-duckdb/\nBye Bye Google Studio!! Google going all in on Looker as a default BI layer for Google BigQuery. Google also announced support for unstructured data analytics & Big Query ML pipeline integration with Vertex AI.\nhttps://medium.com/google-cloud/10-key-take-aways-from-google-cloud-next22-d5def84a3cf4\nFast.ai published its course content on From Deep Learning Foundations to Stable Diffusion. Since the introduction of Stable Diffusion, it gains a lot of momentum all the way to the possibility to render the Cinema 4D scene natively. I\u2019m looking forward to take this course.\nhttps://www.fast.ai/posts/part2-2022-preview.html\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-103", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nWe missed the last couple of weeks, but hey, folks - we are back. Data Contract has been a hot topic in the last couple of weeks. Chad published an engineering guide to a data contract, Jake published the contract-powered platform, David published three-part series on Data Contracts, and Yali Sassoon published why data contract is a good idea. \nI guess this tweet summarizes all, and this is my general reaction to the case against the data contract. \nI thought about it a lot and realized how poor naming could significantly impact the adoption curve in mainstream companies. Data Contract implies it is slow and bureaucratic. I understand why since a simple addition of columns in the traditional data warehouses can take months to roll out due to the org structure. \nI think it is essential to name it to reflect its purpose. Hence I call it \"Schema Ops.\" after inspiration from the success of reliability engineering. \nData collection and preparation occupy the vast majority of the Machine Learning workload. DoorDash writes about five typical data quality gotchas in the ML pipeline, including missing and invalid values, outliers, defaulting, and sampling errors. The open-source Data Quality Report is an exciting project to keep an eye on. \nhttps://doordash.engineering/2022/09/27/five-common-data-quality-gotchas-in-machine-learning-and-how-to-detect-them-quickly/\nDeveloper productivity is vital to win the digital business, and Intuit is taking that principle in AI/ ML development. Intuit writes about Numaproj, an open-source collection of Kubernetes-native developer tools for real-time data analysis and AIOps. \nhttps://medium.com/intuit-engineering/numaproj-driving-dev-velocity-with-real-time-analytics-aiops-on-kubernetes-fba62e00eecf\nAirbnb writes about its migration story of Spark 3 + Iceberg. The highlight is how far Airbnb's data infrastructure has evolved from HDFS to S3, and now Iceberg reminds data infra requires continuous improvement & investment. I've not tried Spark 3 AQE (Adaptive Query Execution), but exciting to see performance benefits in the article. \nhttps://medium.com/airbnb-engineering/upgrading-data-warehouse-infrastructure-at-airbnb-a4e18f09b6d5\nHear from some of the biggest names in data and analytics about the ideas and technologies pioneering our industry. Featuring keynote speakers Nate Silver, founder of FiveThirtyEight, Daniel Kahneman, Nobel Prize-winning economist, Ali Ghodsi, CEO of Databricks, and founders and data leaders from dbt Labs, Fivetran, The New York Times, GitLab, Fox Corporation, and other companies pioneering the way forward for reliable data.\nData Engineering Weekly readers, Get Your Free Ticket!!!\nWhat if we can\u2019t utilize Spark 3 & AQE? Regardless of the Spark versions, the article gives an excellent overview of the basic structure to keep in mind while building data pipelines with Apache Spark. \nhttps://medium.com/dbs-tech-blog/accelerating-big-data-processing-with-spark-optimisation-1f2f5dad03ea\nBig data processing generates too big of logs to process and index. Uber writes about how efficiently compress and index Spark logs using CLP integrated with the Log4J appender. CLP(Compressed Log Processor) is a tool capable of losslessly compressing text logs and searching the compressed logs without decompression.\nCLP Paper: https://www.usenix.org/system/files/osdi21-rodrigues.pdf \nCLP Github: https://github.com/y-scope/clp\nhttps://www.uber.com/blog/reducing-logging-cost-by-two-orders-of-magnitude-using-clp/\nRegardless of how data is being used, it is critical that the information is trusted. The practice of data reliability engineering has gained momentum recently to address that question. Soda Checks Language helps support the efforts of data teams wtih the corresponding Soda Core utility that acts on this new DSL. In this Data Engineering Podcast by Tobias Macey episode, Tom Baeyens explains their reasons for creating a new syntax for expressing and validating checks for data assets and processes, as well as how to incorporate it into your own projects.\nDeveloper velocity to improve feature engineering is the focus for many companies to iterate fast and build ML applications. Along the line of Airbnb's Zipline and Uber's Michelangelo Palette, Snap writes about Robusta, its internal feature automation framework.\nhttps://eng.snap.com/speed-up-feature-engineering\nThere is much literacy for building recommendation engines, and there is Kaggle competition. Operating a recommendation engine at scale is another challenge. Netflix writes an exciting blog narrating best practices for operating recommendation engines in production.\nhttps://netflixtechblog.medium.com/recsysops-best-practices-for-operating-a-large-scale-recommender-system-95bbe195a841\nJoin this webinar with RudderStack and BlastX to learn all about the differences between Universal Analytics and GA4 and get a detailed rundown of the different GA4 implementation options. The session will cover server-side vs. client-side tracking and offer a practical framework to help you determine the best deployment method for your business.\nhttps://www.rudderstack.com/events/exploring-options-for-ga4-cloud-measurement/\nYou wonder why I included a product announcement from AWS. I found AWS Cache a fascinating development to have a native file cache on top of S3. It will be interesting to see how emerging in-process query engines like DuckDB and native file system cache on top of S3 can change the query layer. \nhttps://aws.amazon.com/blogs/aws/amazon-file-cache-a-high-performance-cache-on-aws-for-your-on-premises-file-systems/\nLyft writes about the evolution of its streaming pipeline architecture on top of Apache Beam. The blog narrates how the initial version started with cron jobs and the continuous improvement to simplify pipeline creation. \nhttps://eng.lyft.com/evolution-of-streaming-pipelines-in-lyfts-marketplace-74295eaf1eba\nThe blog narrates the current state of the standalone data lineage tools in the market. There is much momentum behind the data catalogs. The minimal number of options in data lineage makes me wonder if data lineage is still a separate category or subset of the scheduler or catalog. \nhttps://medium.com/bliblidotcom-techblog/data-lineage-state-of-the-art-and-implementation-challenges-1ea8dccde9de\nCTE, dbt, Snowflake, and performance impact seems to have repeated theme in recent times. The author demonstrated another example of how an additional layer of CTE can cause a significant performance impact.   \nhttps://techwithadrian.medium.com/the-hidden-risk-of-using-ctes-53b241e256b2\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-102", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nData Contract is the most discussed topic recently in the data world. Benn highlighted that he agrees that disagreement is a problem but disagrees that we need an agreement to solve it. \nIn Benn\u2019s article,\nData contracts make exactly that trade. They replace a brittle technical system with a negotiating table. And the more that contracts depend on one another, the more people will want to be involved. I don\u2019t know if that kills innovation, but it\u2019s at least an annoying set of conversations that most people don\u2019t want to have.\nI'm afraid I have to disagree with this assessment. Change Management is all we do in software engineering. \nCode Review, PRD, RFC, Sprint Planning; Everything is a negotiation in software engineering. Does it kill innovation? No, in fact, it accelerates industrial-scale innovation. So why the special treatment for Data Engineers?\nhttps://benn.substack.com/p/data-contracts\nIt brings a question; Hey, Ananth. I\u2019m a Data Engineering Leader. When should I focus on Data Contracts? I made a Magic Quadrant for you.\nPing me on LinkedIn. Curious to know your thoughts: https://www.linkedin.com/in/ananthdurai/\nChad talks about what it takes to build a production-grade data pipeline. The article focus on\nCollaborative design\nContracts\nExpectations\nMonitoring\nChange Management\nhttps://dataproducts.substack.com/p/the-production-grade-data-pipeline\nDoes ELT is way more heavily rent-seeking than ETL? Did we shift right too far to do the data transformation? The author discusses Fivetran and dbt as an example of the ELT model. \nhttps://medium.com/@laurengreerbalik/how-fivetran-dbt-actually-fail-3a20083b2506\nThe onboarding process is easily the best time to learn about organizational culture. An effective onboarding process demonstrates strong empathetical and inclusive engineering practices. The author writes about the experience of data team onboarding processes. \nhttps://medium.com/coriers/onboarding-for-data-teams-100e041a012c\nHave you ever totally overrun your monthly budget for an analytics environment overnight? Here are a few thoughts on how we prepare ourselves for what lies ahead in the public cloud and in the economy. In this post, we look at factors to consider when building a data warehouse. Our goal is to point out the potholes you are most likely to hit from a cost perspective and what you can do to avoid them.\nhttps://www.firebolt.io/blog/cloud-data-warehouse-costs-look-before-you-leap\nBottom Up innovation is the best way to fuel and iterate a company's growth. Intuit writes about six steps to drive grassroots innovations. Seeing 74% of innovation paper submissions from IC (Individual Contributors)Engineers is impressive. \nhttps://medium.com/intuit-engineering/how-to-drive-grassroots-ai-innovation-tap-into-a-diversity-of-ideas-f2e1ed6258e6\nSQLLite is reaching the browser. I can\u2019t wait to try analytics on edge.\nThe author discusses the SQLite architecture, transaction guarantees in SQLite, and what is ahead of SQLite in the near future. \nhttps://muratbuffalo.blogspot.com/2022/09/sqlite-past-present-and-future.html\nMax Schultze, Data Engineering Manager at Zalando, and Prof. Dr. Arif Wider, Professor of Software Engineering at HTW Berlin, share their experience in bringing forward the practical side of data mesh from an engineer's perspective and answer challenging questions that tackle some of the common misconceptions of putting data mesh into practice.\nhttps://directory.libsyn.com/episode/index/id/24095136\nLooking back at history and comparing the current state is always good. It is an exciting time for data engineering with the significant investment and progress in storing and querying data. The author compares the days of Hadoop/ HDFS to the LakeHouse architecture and progress made in data infrastructure. \nhttps://rmoff.net/2022/09/14/data-engineering-in-2022-storage-and-access/\nTransformers become standards in NLP tasks such as machine translation, text summarization, question answering, etc. The author published the transformers' categorization based on a survey of efficient transformers.\u00a0\nComputational complexity\nSpectral complexity\nRobustness\nPrivacy\nApproximation\nModel compression\nhttps://medium.com/data-science-at-microsoft/efficient-transformers-survey-of-recent-work-75022cddc86a\nIn this piece, Ben Rogojan outlines your options for solving data integration challenges as your company grows: building a scalable framework or architecting a stack with the right tools. Check it out for some practical advice on which approach to take.\nhttps://www.rudderstack.com/blog/better-customer-data-integration-management-for-growing-teams\nSlack writes about its unified end-to-end machine learning infrastructure to generate recommendations. The article highlights some product experiences where machine learning provides a rich experience. The article is a classic example of how to use ML to drive product features and growth. \nhttps://slack.engineering/recommend-api/\nNetflix switched from account sharing okay in 2016 to crack down on account sharing in 2022. I believe the article is the first gist of how fraud detection work behind the scene.  \nhttps://netflixtechblog.medium.com/machine-learning-for-fraud-detection-in-streaming-services-b0b4ef3be3f6\nEvery piece of infrastructure is driven by the basic principle of an organization toward achieving business goals. Picnic writes about its principles for building an internal data science platform. \nhttps://blog.picnic.nl/mlops-principles-to-build-picnics-data-science-platform-851cbe2e8045\nFaire writes about its real-time ranking feature, challenges in monitoring real-time ranking model evaluation metrics in near real-time, and anomaly detection on critical metrics. The blog is an excellent read on how to build a reactive system to improve operational efficiency. \nhttps://craft.faire.com/monitoring-machine-learning-systems-at-faire-6d5f8337e9e7\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-101", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nI found this description interesting and how far the industry came up from defining who is a data scientist to a full stack data scientist. :-) \nhttps://shopifyengineering.myshopify.com/blogs/engineering/what-is-a-full-stack-data-scientist\nWhat is the metrics layer? The blog establishes the need for the metrics layer and walks through the metrics layer development timeline. The author compares the current state of the metrics layer products Looker, dbt & LightDash. \nhttps://pedram.substack.com/p/what-is-the-metrics-layer\nIs Snowflake's approach to query every query as a full table scan? Maybe not. The author explains how Snowflake does partition pruning, Query rewriting, predicate pushdown, column pruning, and join query optimization. \nhttps://teej.ghost.io/understanding-the-snowflake-query-optimizer/\nThe more the tools get used, the shortcoming of tools will surface. It is a perfect place to improve the system. The author highlights some design constraints from dbt on Snowflake and Postgres.\nhttps://blog.devgenius.io/dbt-snowflake-and-time-traveling-4253fb703f44\nThis time on The Data Engineering Show, Maxime Beauchemin \u2013 The guy behind Airflow, Superset, and Preset, tells the bros about his recipe for smart data-driven companies. Choosing the right system and services is key for a successful start and can help you avoid the chaos of having too many tools spread across multiple teams.\nhttps://www.firebolt.io/blog/how-preset-built-a-data-driven-organization-from-the-ground-up-podcast\nEmbeddings make it easier to do machine learning on large inputs like sparse vectors representing words. Instacart writes about how it uses embeddings in search relevancy.\nhttps://tech.instacart.com/how-instacart-uses-embeddings-to-improve-search-relevance-e569839c3c36\nThe OLAP engines are a perfect fit but also underutilized in operational analytics. LinkedIn writes an excellent case study of Real-time Operational Metrics Analysis (ROMA) using Apache Pinot.\nhttps://engineering.linkedin.com/blog/2022/real-time-analytics-on-network-flow-data-with-apache-pinot\nIn this piece, Ben Rogojan outlines your options for solving data integration challenges as your company grows: building a scalable framework or architecting a stack with the right tools. Check it out for some practical advice on which approach to take.\nhttps://www.rudderstack.com/blog/better-customer-data-integration-management-for-growing-teams\nTwitter writes about its Data Quality Platform on top of Great Expectation. The blog narrates how the system integrated with the Airflow orchestration process.\nhttps://blog.twitter.com/engineering/en_us/topics/infrastructure/2022/data-quality-automation-at-twitter\nLike Customer 360 view for retailers, the end-to-end lineage is the holy grail for the data engineers. Can we define data lineage in one view? The blog narrates three types of lineage views from the Back Market team's perspective.\nDataset lineage\nJob Lineage\nRun lineage\nhttps://engineering.backmarket.com/our-understanding-of-data-lineage-c72f5718abd0\nMax Schultze, Data Engineering Manager at Zalando, and Prof. Dr. Arif Wider, Professor of Software Engineering at HTW Berlin, share their experience to bring forward the practical side of data mesh from an engineer's perspective, and answer challenging questions which tackle some of the common misconceptions of putting data mesh into practice.\nhttps://directory.libsyn.com/episode/index/id/24095136\nThe glance team writes about the infrastructure design for the live stream viewer's computation. The push vs. pull design is an exciting read. \nhttps://engg.glance.com/computing-live-stream-viewers-count-in-real-time-at-high-scale-ef813bc1b9cb\nWhat is trust in AI vs. trustworthy AI? Why is it essential to understand and incorporate from the beginning of your AI journey? The author narrates the critical difference by reviewing basic statistical applications to help data scientists build trust in their models. \nPart 1: https://medium.com/data-science-at-microsoft/trust-in-ai-versus-trustworthy-ai-why-is-it-important-part-1-of-3-af28195b7612\nPart 2: https://medium.com/data-science-at-microsoft/trust-in-ai-versus-trustworthy-ai-why-is-it-important-part-2-of-3-77e5edebe898\nPart 3: https://medium.com/data-science-at-microsoft/trust-in-ai-versus-trustworthy-ai-why-is-it-important-part-3-of-3-c9aef3cb2478\nLooker's Malloy is an exciting project to watch. The blog gives an excellent overview of the project. As the author quotes, Could Malloy finally be the language that replaces SQL? It's a nigh impossible task, but I am very much hoping that it succeeds. Let's keep watching this space.\nhttps://carlineng.com/?postid=malloy-intro#blog\nA well-defined Looker project can significantly simplify the data pipeline complexity. The author gives some practical design solutions for the Looker project.  \nhttps://www.spectacles.dev/post/fix-your-lookml-project-structure\nHow does the table format of the LakeHouse systems like Delta Lake work? The author took a sneak peek at the writing and reading part of Delta Lake. \nPart 1: https://www.waitingforcode.com/delta-lake/acid-file-formats-writing-delta-lake/read\nPart 2: https://www.waitingforcode.com/delta-lake/table-formats-reading-delta-lake/read\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/brain-kidney-cancer-research-and", "title": "Data Engineering Weekly", "content": "The best conversation happens at home, especially when you have a different domain of expertise. Different perceptive and life experience always brings innovative thoughts and help to refine your ideas. I capture my lunch conversation with my wife about data contracts and hope it will be helpful for you. She is a cancer biologist, hence the reference to brain, kidney, and cancer research. Stay with me; it has some interesting relevance.\u00a0\nWhat is the data contract that you're talking all over?\nA data contract is an agreement between the data producer and consumers of the data that abstractly describes data to be exchanged. (i.e) The explicit description of expectations around data model, data quality, and data availability.\u00a0\nI don\u2019t understand what you\u2019re trying to say. It is all jargon to me.\u00a0\nLet\u2019s take a scenario in your lab. You\u2019re running an experiment on mice, and you want to share your data about certain drugs on the mice with your colleague. How will you share the data?\nWe usually share in an excel sheet.\nHow will your colleagues know what data you\u2019re collecting to ensure it is helpful for their research?\nWe exchange notes now and then; sometimes, we implicitly make some assumptions about the structure of the data.\nHmm, isn\u2019t it dangerous? What happens if the assumption goes wrong?\nOh Yes, That happens all the time. We were working on an experiment to collect a Drug reaction on a mouse\u2019s kidney and brain. Measuring the weight of the kidney is an essential and obvious parameter for the experiment. We thought the person doing the kidney study knew the obvious, and we were under the impression that the measurements were being collected. When we exchanged the measurement at the end of the experiment, we found no data for the weight of the kidneys. We had to wait another six months to re-run the experiment.\u00a0\ud83d\ude22\nExcellent, Oh, I mean, well, that is not good. Who did it?\ud83e\udd15 Well, you see, human errors are common. It\u2019s no one\u2019s fault; this is where the data contract comes into play.\u00a0\nSuppose you write down what measurement you\u2019re collecting, how each experiment is interconnected, who is responsible for which measurement, and data quality expectations; you could have prevented it. We call this process Data Contracts.\u00a0\nGot it. It makes sense. But won\u2019t it create a physiological fear? My colleagues may fear that others will blame them if something goes wrong. It will create destructive team dynamics.\u00a0\nIt is a valid concern. The software industry underwent a similar transformation to the DevOps culture for software development. Many companies successfully adopted this model. We hope the Data Contract can influence these cultural changes.\u00a0\nI\u2019m going back to your \u201cmissing-the-weight-of-the-kidney\u201d measurement incident. Even if a single measurement is missing, you could still derive a decision. I\u2019m trying to play a devil\u2019s advocate to say Data Contract is not a bigger deal. Can you still make a directionally correct decision using partial measurement?\u00a0\nYou can, but you must apply your prior human knowledge as one variable to make the decision. It depends on your domain expertise that can impact the result in any variance. At this point, you are not making a data-driven decision from the experiment; you\u2019re simply looking at the data and making assumptions. It might work in your business context. Maybe the cost of the mistake might be low, but certainly not in cancer research, where people\u2019s life at risk.\u00a0\nYes, that makes sense.\nData Contract is fantastic, and I can imagine how much it is helpful. But what is Schemata that you\u2019re talking about recently?\nSchemata is for the data practitioners in an organization. Developers may roll out a change that can break the data pipeline and lead to a chaotic experience for the data analyst & data scientist. Schemata enable a dynamic and rapidly evolving data contract framework that provides a collaborative experience for the data producer and consumers to define ownership, set expectations, and live happily ever after.\u00a0\nThat sounds interesting. Good luck with that.\u00a0\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-100", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nWhat is production? Benn raised a question if we can\u2019t define it, we can\u2019t meaningfully build it. The best answer to that is,\nThe thread triggered many interesting discussions around the need for production, and I found David Jayatillake came up with an excellent question.\nhttps://benn.substack.com/p/what-is-production\nData Production, Data is a Product, and all these higher level concepts ultimately build on top of \"Data Contracts.\" I wrote my version of the Data Contract initiative called \"Schemata,\" a collaborative, decentralized data contract management system. \nData Contracts will be the next big step in the modern data stack to empower data as a strategic advantage. \nBut what is Data Contract? Where to start? Mehdi writes an excellent guide about data contracts. \nhttps://towardsdatascience.com/data-contracts-from-zero-to-hero-343717ac4d5e\nThe Data Mesh principle builds on four core principles, domain-oriented decentralized data ownership & architecture, data as a product, self-serve data infrastructure as a platform, and federated computational governance. The founding element in Data Mesh is no surprise is the Data Contracts. BlaBlaCar writes about Dos & Don'ts while adopting Data Mesh. \nhttps://medium.com/blablacar/dos-and-don-ts-of-data-mesh-e093f1662c2d\nCan an organization deliberately create data to power machine learning and advance analytical use cases? The current state is that every ML product goes through the discovery process, which finds missing data/sources, resulting in a data extraction process. I agree with the author's view, and as an industry, we have a long way to go to build what I call an \"Analytical Ready Data Asset Creation.\"\nhttps://datacreation.substack.com/p/organizations-need-to-deliberately\nBuilding a new data warehouse is a daunting challenge. It requires massive investments into both the query engine and surrounding cloud infrastructure. Mosha Pasumansky and Benjamin Wagner wrote a paper about it which explains how Firebolt engineers built a query engine on top of existing projects and invested heavily into differentiating features.\nhttps://www.firebolt.io/content/firebolt-vldb-cdms-2022\nMcdonald's writes about how it enforces data contracts between the producer and consumer. The domain-based sharing is an exciting approach to bringing multi-tenant event streaming to isolate failure modes. \nPart 1: https://medium.com/mcdonalds-technical-blog/behind-the-scenes-mcdonalds-event-driven-architecture-51a6542c0d86\nPart 2: https://medium.com/mcdonalds-technical-blog/mcdonalds-event-driven-architecture-the-data-journey-and-how-it-works-4591d108821f\nIKEA writes about its knowledge graph system and the layers of knowledge graphs. The three-layer approach focuses on\nConcepts: represent the business concepts of what the company does.\nCategories: represents a controlled vocabulary or terminology used within the business.\nData: represent the product or unique selling unit of a business.\u00a0\nhttps://medium.com/flat-pack-tech/ikeas-knowledge-graph-and-why-it-has-three-layers-a38fca436349\nFor reliable data pipelines-as-code, check out the fresh, easy-to-follow, step-by-step guide to setting up and integrating open-source data quality checks with Apache Airflow.\nhttps://www.astronomer.io/guides/soda-data-quality/\nInstacart writes about lessons learned from its journey to build real-time machine learning applications highlighting critical challenges with the infrastructure. I think the real-time feature store is a unique problem waiting to be disturbed. \nhttps://tech.instacart.com/lessons-learned-the-journey-to-real-time-machine-learning-at-instacart-942f3a656af3\nSimulation analysis is an exciting area of study, and delighted to read about how Grub enables automatic rule backtesting with historical data. The replay system to simulate the rule changes or any proposed rule change is exciting to read. \nhttps://engineering.grab.com/automatic-rule-backtesting\nIn this piece, Ben Rogojan outlines your options for solving data integration challenges as your company grows: building a scalable framework or architecting a stack with the right tools. Check it out for some practical advice on which approach to take.\nhttps://www.rudderstack.com/blog/better-customer-data-integration-management-for-growing-teams\nWe've seen thoughts on Cloud Data Warehouse cost, from Why Snowflake so expensive? to How Snowflake fails, the customer's love for Snowflake. I thought this article better narrates the misconception from marketing to the reality of operating the Cloud data warehouse system. I like how the author constructively narrates critical aspects of Snowflake and the suggestion to improve. \nhttps://www.linkedin.com/pulse/snowflake-performance-challenges-solutions-part-1-slim-baltagi/\nThe emerging LakeHouse systems narrow the gap between the data warehouses and data lakes systems on top of object stores. Historically the two-state system with a mix of the data lake and data warehouse act as a bridge to balance the scale and speed. Claimforce writes an exciting blog that narrates its journey to unify data lake and data warehouse with delta lake. \nhttps://medium.com/claimsforce/lakehouse-the-journey-unifying-data-lake-and-data-warehouse-bef7629c143a\nOne of the shortcomings in Claimforce\u2019s article highlights the lack of support from Athena (AWS version of Presto) with Delta Lake. Bytedance talks about how Presto works with Apache Hudi, another leading LakeHouse system.\nAccess Control is a challenging part of data management at scale. Uber writes about how it did achieve finer-grained encryption with Apache Parquet in the past.\nOne Stone, Three Birds: Finer-Grained Encryption @ Apache Parquet\u2122\nThe blog narrates the challenges and performance benchmarks of adopting parquet encryption in Presto.\nhttps://prestodb.io/blog/2022/07/10/presto-parquet-column-encryption.html\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-99", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nMany companies are swinging the pendulum towards cost optimization, given the current economic condition. I ran a LinkedIn poll to find how much time the data engineers spend on cost optimization. Almost 1/3 of the engineers spend more than 20% of their time optimizing the cost. \nThe increased focus on cost brings a lot of criticism to the cloud data warehouses and the transformation frameworks like dbt. Are we expecting too many things from the DB? Benn discusses on this blog Snowflake & data warehouse cost.\nhttps://benn.substack.com/p/how-snowflake-fails\nThe awareness of Data Contracts is on the rise, and many companies are adopting it using an Excel sheet or GitHub repo or via a schema registry. It also brings some confusion for many folks. \nHistorically, a centralized team changed the schema, which made the data warehouse slow and bureaucratic. There is a skepticism that data contracts (rightfully so) may fall into the same trap. However, there is no way one person in an organization can hold the entire data model. \nA data contract should be decentralized, dynamically evolving, and collaborative. Schemata is one such attempt to bring decentralized, dynamic & collaborative data contracts. \nChad & Ethan had an exciting conversation on the data contract this week.\nMy take on data contracts\nMeta writes about Velox, its open-source unified execution engine for the data workload. The project is an exciting one to watch and how it gets adopted.\u00a0\nIt is moving away from the default JVM-based execution engines with Spark & Presto yet provides full query compatibility.\nThe dataframe libraries can represent the execution plan as the Velox plan, which open the possibility of having a unified execution engine for both dataframe and SQL workloads.\u00a0\nhttps://engineering.fb.com/2022/08/31/open-source/velox/\nAn excellent summarization of key takeaways from Gartner data & analytical summit 2022. The highlight on delivering the right insights at the right time, which I suppose we will keep trying to solve. \nhttps://towardsdatascience.com/key-takeaways-from-gartner-data-analytics-summit-2022-ec908f9599df\nRobert Harmon from Firebolt responded to Tristan Handy, dbt Labs, and David Jayatillake, Metaplane, following their sharp review of Firebolt. He explains where the platform is currently at, what we're doing around mutability, and what we're building RIGHT NOW\nhttps://www.firebolt.io/blog/hey-david-and-tristan-this-is-where-firebolt-is-at\nAnother excellent summarization blog on data conferences, where the ML6 team wrote about the Apache Beam Summit highlights and recommended rewatch sessions.\nhttps://blog.ml6.eu/our-takeaways-and-sessions-to-rewatch-from-beam-summit-2022-fecee83d892e\nOne of the exciting articles I read over last week is about building an aircraft radar system in JavaScript. The blog is an excellent read for aspiring data engineers on how to think about data collection from a data source and derive insights to make a difference. \nhttps://charliegerard.dev/blog/aircraft-radar-system-rtl-sdr-web-usb/\nLearn how to integrate the transformational power of dbt with Soda's data incident management in this hands-on workshop, presented by Bastien Boutonnet. See how, with dbt + Soda, data teams can augment the tests that they run in dbt to manage data reliability and quality incidents, whether at the dataset or record level. And a whole lot more...including how to store dbt test results over time in Cloud Metrics Store to test and validate data and alert the right people at the right time before there is a downstream impact. Register now to listen, learn, and like on September 6.\nhttps://www.eventbrite.com/e/data-incident-management-with-soda-and-dbt-tickets-348553873017\nThough it is a bold claim, lineage is vital for operating data. But How can we create a single interface that everyone can consume and understand the data lineage? The author discusses the layered approach from Google Maps and what data lineage visualization tools can learn from it. \nhttps://medium.com/data-monzo/the-many-layers-of-data-lineage-2eb898709ad3\nCoupang writes about its data platform for its food delivery business, Eats. The blog narrates all three types of data pipeline architecture depending on the data processing window. \nNon-Real-Time\nNear-RealTime\nReal-time\nThe blog is an excellent characterization of the data pipeline, demonstrating how the architecture design varies for each category. \nhttps://medium.com/coupang-engineering/eats-data-platform-empowering-businesses-with-data-3cc00fa9968d\nRudderStack is now HIPAA compliant and ready to sign BAAs with customers. Here, you'll learn how RudderStack makes security and compliance easy for healthcare data teams with its warehouse-first approach and features such as in-flight data transformations.\nhttps://www.rudderstack.com/blog/announcing-hipaa-compliance\nReliable labeled data is vital for efficient ML-based systems. Trivago shared its experience and practical tips on designing and tweaking data annotation projects to produce high-quality label data. \nhttps://tech.trivago.com/post/2022-09-01-powering-ml-based-systems-with-reliable-data-annotation/\nInterviewing is always stressful, and it is exciting to see companies like NuBank sharing practical tips to ace data science interviews. \nhttps://building.nubank.com.br/data-science-interview-pratical-tips/\nFinding the gaps/ missing value is the 99% case for the root cause analytics. The author echoes the same pattern and explains techniques and approaches to find the gaps with SQL.\nhttps://medium.com/@jberry_33001/finding-gaps-with-sql-4f62982f797d\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-98", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nAn excellent reflection article sharing experience running dbt after a year. dbt model grown from 0 to 700 dbt models is impressive growth. The growth aligns with the strategy the team adopted, Messy to start and clean afterward. \nThere are two approaches to handle the demand: build dbt models carefully while meeting all best practices (more proper), or build them fast and messy and worry about tech debt later (as long as the data is correct).\nhttps://medium.com/@imweijian/lessons-learned-after-1-year-with-dbt-a7f0ccf85b12\n\u201cProper data modeling will take less time and money to add new features or new capabilities to the analytics. Your data will increase over time, even if your business is stable. But proper data modeling can help you avoid proportional growth of costs.\u201d\nManyChat, at the same time, writes about its experience putting data models as a catalyst for growth. \nBalancing the growth with clean/ reusable datasets with a cost concern will be a hot data problem to solve next.  Top-Down data modeling or Bottom-Up tech debt handling, Data Modeling plays a significant role in the health of data for a company.\nhttps://medium.com/manychat-team/data-modeling-today-launching-cost-effective-analytics-for-manychat-764d305f287b\nI heard the first time a company moved from Snowflake to Teradata, citing unpredictable costs. It surprised me since I assume the legacy data warehouses are reaching the final days, as the author pointed out. \nhttps://clouddb.substack.com/p/the-final-days-of-the-legacy-data\nThe Twitter thread sparked some exciting conversation, where Bobby Neelon shared some practical tips to optimize Snowflake\nContinuing the Snowflake cost debate, Wise engineering writes a timely article on three main elements in your Snowflake bill to understand & optimize for cost efficiency. \nSomeone should launch a Udemy course: Understanding Snowflake billing \nhttps://medium.com/wise-engineering/3-main-elements-in-your-snowflake-bill-45331ab7b224\nFirebolt is the cloud data warehouse for builders of next-gen analytics experiences.Combining the benefits and ease of use .of modern architecture with asub-second performance at a terabyte scale, Firebolt helps data engineeringand dev teams deliver data applications that end-users love.\nhttps://www.firebolt.io/\nThrowing everything into a data lake is a long-gone architecture approach, as the modern LakeHouses brings Data Modeling back to the mainstream. Walmart writes an excellent article on handing SCD-2 (Slowly Changing Dimension) with Apache Hudi.\nhttps://medium.com/walmartglobaltech/implementation-of-scd-2-slowly-changing-dimension-with-apache-hudi-465e0eb94a5\nSQL everywhere has become the norm, from transactional databases to data warehouses to streaming databases. Several noSQL systems formally adopted SQL-ish syntax as an interface. The article reflects the same sentiment about why SQL has become the second programming language everyone needs to know. \nhttps://spectrum.ieee.org/the-rise-of-sql\nIf you\u2019re a data engineer and you want to become a better data engineer, or you\u2019re switching from software engineering to data engineering...I think a great thing that you can do is code reviews and open-source to the tools that you\u2019re going to be using. You\u2019re going to understand how they\u2019re built. You\u2019re going to see how the different pieces interact. And of course, you can do this by writing code in the tools that you\u2019re using as well.\nhttps://sodapodcast.libsyn.com/ep-011-meet-holden-karau-author-and-open-source-engineer-at-netflix\nSQL provides multiple ways to express and examine the set operation between two or more entities. The author writes an experiment to understand various ways to frame queries for Find me all the things in set \"A\" that are not in set \"B.\"\nhttps://www.crunchydata.com/blog/rise-of-the-anti-join\nVariability and unpredictability with late-arriving features make the real-time machine learning pipeline challenging. The author writes about various infrastructure strategies to build streaming-first infrastructure for real-time machine learning. \nhttps://www.infoq.com/articles/streaming-first-real-time-ml/\nThis article from Ben Rogajan explores the challenges of data integration in a world where more teams need access to more data for more complex use cases, and it outlines the pitfalls of attacking data integration without a thoughtful strategy.\nhttps://www.rudderstack.com/blog/why-business-applications-create-data-integration-debt\nUber writes an exciting blog about the generation of Uber Freight's carrier scoreboard analytical system. The blog narrates the iteration from querying MySQL DB to establishing a real-time streaming system using Apache Pinot & Flink. \nhttps://www.uber.com/blog/uber-freight-carrier-metrics-with-near-real-time-analytics/\nEtsy writes an exciting article about its observability approach for the Machine Learning Platform. The blog narrates the motivation to build a centralized ML observability platform, challenges, and a high-level design.  \nhttps://www.etsy.com/codeascraft/towards-machine-learning-observability-at-etsy\nAdopting Machine Learning to power the product feature to enrich the user experience is always a delight to read. Instacart writes one such experience sharing how it uses ML to enrich autocomplete.\nhttps://tech.instacart.com/how-instacart-uses-machine-learning-driven-autocomplete-to-help-people-fill-their-carts-9bc56d22bafb\nAs a data engineer, you can't escape from Data Governance. What is Data governance? The author explains Data Governance with a checklist to narrate the process, roles, and people involved et al., \nhttps://medium.com/@corymaklin/data-governance-checklist-152a3a691002\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-97", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nThe performance benchmark often takes center stage in the LakeHouse & Data Warehouse comparisons. Rich feature support & developer friendliness are often unspoken and left to individuals to compare. OneHouse writes an excellent feature-by-feature comparison of open-source Apache Hudi with Delta Lake and Apache Iceberg.\nhttps://www.onehouse.ai/blog/apache-hudi-vs-delta-lake-vs-apache-iceberg-lakehouse-feature-comparison\nReusability in the data pipeline eliminates duplications and improves the consistency of the data assets. Dagster writes about how it enables the data assets and operations reusability. It would be great to add how this reusable component converts into the target runtimes like Snowflake et al.,\nhttps://dagster.io/blog/roman-roads-assets-ops\nThe event-driven architecture often encounters the dual-write problem where the producer mutates the state in the relational databases and sends an event notification about the changes. It brings a typical two-phase commit complexity to maintain the consistent state of the system. The author writes about how the Outbox pattern with Debezium can potentially solve this problem. \nhttps://www.infoq.com/articles/change-data-capture-debezium/\nAn excellent summarization of the Data/AI summit talk 2022 on running Spark on Kubernetes. You can watch the talk here\nhttps://medium.com/@coderstan/apache-spark-on-kubernetes-lessons-learned-from-launching-millions-of-spark-executors-databricks-9187890f0dc3\nFirebolt is the cloud data warehouse for builders of next-gen analytics experiences.Combining the benefits and ease of use .of modern architecture with asub-second performance at a terabyte-scale, Firebolt helps data engineeringand dev teams deliver data applications that end-users love.\nhttps://www.firebolt.io/\nAdobe writes some practical tips to improve the operational efficiency of the Kafka infrastructure. The blog narrates some valuable metrics to measure the dynamics of producer-consumer performances. \nhttps://medium.com/adobetech/wins-from-effective-kafka-monitoring-at-adobe-stability-performance-and-cost-savings-a3ecb701ee5b\nLate 2021, Stitch Fix open source Hamilton is a general purpose micro-framework for creating python functions. Stitch Fix writes an exciting blog on implementing the data quality check part of the Hamilton workflow. TIL about Pandera data validation library. \nhttps://multithreaded.stitchfix.com/blog/2022/07/26/hamilton-data-quality/\nData reliability needs its language - a language that is specific enough to address the problems of data engineers who often find themselves firefighting data issues when reports, dashboards, or machine learning models break, yet specific enough to address the problems that data teams face, and accessible enough for non-engineers to use. (I\u2019m)Possible? Check out Soda Checks Language.\nhttps://www.soda.io/resources/introducing-a-new-domain-specific-language-for-data-reliability\nAre the low-code/ no-code ML tools can beat manual analysis? Can a relative novice use these tools effectively and accurately? Are these tools are cost-effective than hiring an expert data scientist? The author runs through a simulation to find out. It is exciting, and read for yourself to find the conclusion.\nhttps://arstechnica.com/information-technology/2022/08/no-code-wrapped-our-ml-experiment-concludes-but-did-the-machine-win/\nAnomaly detection is a critical function in the observability & reliability of the system. eBay writes about how it incorporates a machine learning approach on Prometheus to deliver an intelligent alerting & monitoring system.\nhttps://tech.ebayinc.com/engineering/sherlock.io-an-upgraded-machine-learning-monitoring-system/\nThis article from Ben Rogajan explores the challenges of data integration in a world where more teams need access to more data for more complex use cases, and it outlines the pitfalls of attacking data integration without a thoughtful strategy.\nhttps://www.rudderstack.com/blog/why-business-applications-create-data-integration-debt\nSimplicity and cost in the modern data stack are often indirectly proportional to each other :-( It's common to hear about the expensive billing of the cloud data warehouses. The author explains why Snowflake is expensive and what it can do better. \nhttps://blog.devgenius.io/why-is-snowflake-so-expensive-92b67203945\nEndeavor writes about its ML platform built on Prefect, dbt & Snowflake. The overarching principles to build the platform, the dbt model organization & model export ops are some exciting reads. \nhttps://medium.com/@endeavordata/machine-learning-platform-at-endeavor-93ba88b66986\nThe historical version or the slowly changing dimensions increases the complexity of a data management system. Slowly changing dimensions types came in the era of storage scarcity. What is the advantage of slowly changing dimension techniques vs. time travel? Find out more in the blog.\nhttps://afroinfotech.medium.com/time-travel-versus-slowly-changing-dimension-type-2-a37ac1538e0d\nBenchmarking a stateful system to find the cluster capacity is critical for capacity planning and supporting the company's future growth. As the author rightly pointed out, relying on the vendor benchmark is unreliable. Riskfields writes about using the OpenMessaging Benchmarks (OMB) to gain benchmark insights. \nhttps://medium.com/riskified-technology/know-your-limits-cluster-benchmarks-ecc6c3c77574\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/introducing-schemata-a-decentralized", "title": "Data Engineering Weekly", "content": "I\u2019m thrilled to write about Schemata, a decentralized schema modeling framework for data contracts.\u00a0Oh, wait, all the jargon, what is it? Let me take you all on the Schemata journey.  You can find the source code and the documentation here.\nGitHub Repo: https://github.com/ananthdurai/schemata\nLet\u2019s first admit it, The era of running the \u201cDataWarehouse in a Box\u201d is history now. The cloud data warehouses and the modern data stack significantly simplified the infrastructure complexity and eased access to the data. The Data Warehouses are more developer friendly to ask questions, transform the data, and derive insights.\nThe simplification of data processing also brings a problem; as my good friend Tam\u00e1s N\u00e9meth quoted, \u201cThe TV Remote Under the Couch Problem.\u201d\nYou want to watch the TV, and you misplace the remote under the couch. If you find it, you\u2019re lucky. If not, you ask if anyone has seen the remote. If nothing workout, you buy a new remote. It may be a single click \u201cBuy Now\u201d in most cases.\u00a0\nThe modern data stack is like your missing remote under the couch. If we don\u2019t find the required data, writing your dbt transformation job is relatively quick but has a significant cost associated with it. We can\u2019t keep buying the TV remote, as it will degrade the Benefit-Cost Ratio (BCR).\nThe TV Remote under the couch problem creates a far more dangerous consequence. It creates a Garbage In, Garbage Out model.\u00a0\nUntil Schemata, there is no systematic way to measure the integrity of the data model. We keep building new data models with no feedback loop to balance the cost and integrity of the data assets. It creates the Garbage-In Garbage-Out model.\u00a0\nThe GIGO problem is like a virus on organizational knowledge management systems, a significant business differentiator for many companies.\u00a0\nData Lake (or LakeHouse) becomes the defacto architecture pattern to source events and produce analytical insights. The Data Lake inherently creates a producer-consumer relationship between the product feature team and the data engineering team. As Data Lake grows, the complexity of data management grows. Let\u2019s take an everyday data flow in a typical data management.\u00a0\nThe data producer generates data for the product feature they develop and sends it to the data lake. (Either as a ProtoBuf/ Avro/ Thrift if you\u2019re lucky or Json format if you like data adventure)\nThe consumers down the line have no domain understanding of the producer and struggle to understand the data lake data.\nThe consumers then connect with the data producer to understand the data to the producer\u2019s domain expert. The domain expert may not have the context, or human knowledge may not be available.\n\nThe Data Lake becomes a technical debt rather than a strategic advantage as it becomes trash storage rather than data as an asset.\u00a0\nSchemata focus on treating data as a product. The feature team that works on the product feature has the domain understanding of the data, not the data's consumer. Schemata enable the data ownership to the feature team to create, attach metadata, catalog the data, and store it for easier consumption.\nThe data curation and the cataloging of the data at the data creation phase bring more visibility and make it easier for consumption. The process also eliminates the human knowledge silo and truly democratizes the data. It helps the data consumers not worry about the data discovery and focuses on producing value from the data.\nTraditionally upfront data modeling comes with a cost. A centralized data architecture/ modeling team often coordinates with multiple teams to design an enterprise data modeling. It is hard for one individual human to hold the entire company's data architecture in their head. The data modeling tools don't reflect the current state of the data modeling. Decentralized data modeling is the only scalable approach, and Schemata enables the bottom-up crowdsourcing data modeling approach to democratize data access in an organization.\nThe decentralized data modeling principle brings a unique collaborative approach to managing the data asset's lifecycle. It brings all the proven devops principles like ownership, accountability, collaboration, automation, continuous improvement, and customer-centric action to data management.\nData is inherently social in nature.\u00a0\nThe significant challenge of decentralized data management is that the lack of connectivity among the data will degrade the usability of the data. Schemata is an opinionated data modeling framework that programmatically measures the connectivity of the data model and assigns a score to it. We call this Schemata Score.\nObservability metrics like SLO & Apdex Score inspired the formation of Schemata Score. A lower Schemata Score means lesser data connectivity of a data model. It allows the teams collaboratively fix the data model and bring uniformity to the data.\nSince the incarnation of Hadoop & MapReduce, the data engineering community has significantly focused on commoditizing data transformation. All the Hadoop abstractions like Hive, Pig, Crunch, et al. building on top of Hadoop to further simplify the data transformation. Apache Spark to dbt the data engineering community made a significant leap forward by simplifying the data transformation.\u00a0\nHowever, during this time, we made zero to no improvement on data modeling & data contracts to build the proper knowledge management system. Data Catalog acts like a search index platform for the data assets. There are many exciting breakthroughs in the data discovery space, but they are not good enough to build a systematic feedback loop and knowledge graph.\nIs Schemata the savior? Honestly, I don\u2019t know the answer, and I\u2019m looking forward to hearing from you all on this. But I know that a Schemata-like feedback loop is missing in the modern data stack.\u00a0\nGitHub Repo: https://github.com/ananthdurai/schemata\nWant to discuss Schemata? \nSay Hi Ananth on LinkedIn: https://www.linkedin.com/in/ananthdurai/\nSay Hi Ananth on Twitter: https://twitter.com/ananthdurai"}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-96", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nUber writes the second part of its internal ML education program focusing on content delivery, usage observability, marketing & reach. The blog post is a good reminder that platform engineering is not only about building abstraction and driving standardization but more about marketing & selling the abstraction to the internal customers.  \nhttps://www.uber.com/blog/ml-education-at-uber-program-design-and-outcomes/\nfirst part: https://www.uber.com/blog/ml-education-at-uber/\nPayPal writes about its strategy to adopt data mesh principles. The blog acknowledged there is no standard implementation yet, but established a business case why PayPal needs DataMesh principles in their data strategy. It is an exciting space to observe from PayPal. \nhttps://medium.com/paypal-tech/the-next-generation-of-data-platforms-is-the-data-mesh-b7df4b825522\nThe talk gives a much more systematic case for DataMesh.\nInfoQ released its 2022 AI, ML & Data Engineering Trends report. The Resource Managers like Yarn and stream processing now moved to the late adopted stage. There is tons of exciting new entrant Knowledge Graphs, AI pair programmer (like Github Copilot), and Synthetic Data Generation.\nhttps://www.infoq.com/articles/ai-ml-data-engineering-trends-2022/\nDoorDash writes about its real-time data infrastructure on top of Kafka, Flink & Pinot. The blog narrates how a producer proxy for Kafka helped to scale the pipeline, Flink SQL abstraction, the usage of Kafka schema registry, and data warehouse integration. \nhttps://doordash.engineering/2022/08/02/building-scalable-real-time-event-processing-with-kafka-and-flink/\nFirebolt is the cloud data warehouse for builders of next-gen analytics experiences. Combining the benefits and ease of use .of modern architecture with a sub-second performance at a terabyte-scale, Firebolt helps data engineering and dev teams deliver data applications that end-users love.\nhttps://www.firebolt.io/\nAirbnb writes about the design of its Access Management system. The blog narrates various focus points of the design, system guarantees, and the impact of the system after the rollout. \nhttps://medium.com/airbnb-engineering/airbnbs-approach-to-access-management-at-scale-cfa66c32f03c\nOnline experimentation plays a central role in product development. Etsy writes about how it uses the Interleaving Experimentation Test to capture the user's preference at the individual level rather than comparing average behaviors of two groups seeing distinct experiences with typical AB tests. \nhttps://www.etsy.com/codeascraft/faster-ml-experimentation-at-etsy-with-interleaving\nData reliability needs its own language - a language that is specific enough to address the problems of data engineers who often find themselves firefighting data issues when reports, dashboards, or machine learning models break, yet specific enough to address the problems that data teams face, and accessible enough for non-engineers to use. (Im)Possible? Check out Soda Checks Language.\nhttps://www.soda.io/resources/introducing-a-new-domain-specific-language-for-data-reliability\nAstrafy writes a 3 part series on building dbt pipeline on Google Cloud. The blog focuses on dbt in Google Cloud architecture, dbt versioning, data quality, orchestrating with Google cloud composer, and data ops. \nPart 1: https://www.astrafy.io/articles/dbt-at-scale-on-google-cloud-part-1\nPart 2: https://www.astrafy.io/articles/dbt-at-scale-on-google-cloud-part-2\nPart 3: https://www.astrafy.io/articles/dbt-at-scale-on-google-cloud-part-3\ndbt established a case for CTEs (Common Table Expression) are passthrough, and the performance impact is negligible as modern data warehouse optimizers recognize this pattern. The blog narrates how that is not the case with Snowflake by comparing the imported CTE with referencing the base table directly in CTE. The results yield a reduced build time from over 30 minutes to less than 10 minutes, roughly $600 saving in a table. \nhttps://medium.com/@AtheonAnalytics/snowflake-query-optimiser-unoptimised-cf0223bdd136\nReference in the article:\nhttps://discourse.getdbt.com/t/ctes-are-passthroughs-some-research/155\nhttps://discourse.getdbt.com/t/why-the-fishtown-sql-style-guide-uses-so-many-ctes/1091\nJoin this live webinar on August 17th for a deep dive on identity resolution in the warehouse with Allbirds Staff Data Engineer, Chandra Gangireddy. Chandra will detail the architecture and end-to-end data flow they use to break down data silos, build customer profiles, and operationalize customer data throughout the company.\nhttps://www.rudderstack.com/events/how-allbirds-solves-identity-resolution-in-the-warehouse-with-dbt-labs-snowflake-and-rudderstack\nLMAX Disruptor is one of the best libraries in java to build a bounded queue with lock-free enqueues. Hubspot writes about how LMAX Disruptor helped to build a fast, thread-safe tracking library. \nhttps://product.hubspot.com/blog/hotspot-tracking-library\nLMAX Disruptor Paper: LMAX Disruptor: High performance alternative to bounded queues for exchanging data between concurrent threads\nGames24x7 shares its experience running ksqlDB and tuning for optimal performance. Tuning state storage, horizontal vs. vertical scaling and GC optimization reveal the internal functioning of ksqlDB.\nhttps://medium.com/@anupsdtiwari/ksqldb-in-data-engineering-at-games24x7-9c66b7cf5aa0\nYelp writes about its attempt to capture the Spark lineage from metadata description files. \nI'm curious whether there is any automatic way to capture the Spark lineage than manual input, perhaps AST parsing of Spark logical plan. Please comment if you know any library that parses Spark's logical plan for lineage. \nhttps://engineeringblog.yelp.com/2022/08/spark-data-lineage.html \nZ-Order indexing is a popular index optimization implemented in many data management systems. Cloudera writes about Z-Order implementation in Impala and how it improves query efficiency. \nhttps://blog.cloudera.com/speeding-up-queries-with-z-order/\nReferences:\nZ-order indexing for multifaceted queries in Amazon DynamoDB\nPerformance Tuning Apache Spark with Z-Ordering and Data Skipping in Azure Databricks\nHudi Z-Order and Hilbert Space Filling Curves\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/the-data-founder-story-from-mclaren", "title": "Data Engineering Weekly", "content": "Hey there, I\u2019m Tomas, one of the Quix founders.\nAfter several years of hard work, Quix has become a data processing platform that allows engineers and scientists to transform and deliver data as it\u2019s created. It\u2019s designed around a message broker, differentiating it from traditional data engineering platforms designed around databases. The platform is commonly used to automate digital experiences that produce a large volume of real-time data and works best when aggregations aren\u2019t enough.\nHere are how those years of hard work went, taking my co-founders and me from a high-speed racing company that used data to build a data company of our own.\nAround 2018, Peter Nagy, Patrick Mira Pedrol, Mike Rosam and I began building systems that most Formula 1 teams now use to stream and process over a million data points per second in real-time. Formula 1, where a team\u2019s ability to connect engineers to streaming data directly correlates to their performance on the track, is the ultimate live-data environment.\nAt McLaren, we also consulted on projects to apply real-time ML in the sport, automotive, transportation and healthcare industries. We helped clients use real-time data to improve fan engagement, develop electric vehicles faster, predict tire failures in vehicle fleets, and optimize patient postoperative outcomes.\nAcross these experiences, we kept seeing the same pattern: the rapid adoption of message broker technologies increasing the value of data-driven operations in every industry, together with the explosion of people with Python skills, the language of machine learning. The two trends should be a match made in heaven, but there was a problem.\nMachine learning on streaming data in real-time is an order of magnitude more complex than ML on batch data. And while our data scientists could quickly develop ML models off-line, it took us years to operationalize machine learning on streaming data infrastructure.\nWe\u2019ve since met with many companies working on real-time machine learning projects, and they all have similar problems. ML-first companies like Uber and Airbnb invest hundreds of millions to develop bespoke internal solutions, while huge legacy organizations in every industry pay suppliers eye-watering sums for never-ending digital transformation projects. And while broker technology is becoming more accessible, operationalizing real-time ML on streaming data remains entirely out of reach to all but the top 1% of ML-first companies. We founded Quix to solve this problem by helping any Python developer work with streaming data. Quix is a developer-first platform with the message broker at the core. It accelerates the development of real-time data-driven products by providing all the infrastructure, APIs and SDK that Python developers need to stream, process and store data without support from any IT, DataOps or DevOps people.\nIn 2020, in the depth of the first lockdowns, we went out on our own to build a generic platform that anyone developing streaming data apps could use. We wanted to decrease the amount of time and money companies poured out to harness the power of stream processing while enabling individuals to access the same technological benefits.\nWe\u2019re focused on removing the time, hassle and investment of setting up infrastructure and integrating different technologies so that any organization \u2014 regardless of size, funding or industry \u2014 has the ability to process stream data in real-time with machine learning models and automate applications with the same quality and reliability as the world\u2019s leading ML companies.\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. The data founder story contents provided by the featuring data founders in this article and Data Engineering Weekly is not responsible for any compliance with applicable laws, rules, and regulations."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-95", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nUber wins by scaling machine learning. We recognize org-wide that a powerful way to scale machine learning adoption is by educating. That\u2019s why we created the Machine Learning Education Program.\nEducating internal customers is a vital part of a platform. An Engineering org is a mesh of various skills, and not everyone reads Data Engineering Weekly or goes to all the cool Data & ML conferences. Uber writes an excellent article on ML education and its approaches from the engineering principles perspective. \nhttps://eng.uber.com/ml-education-at-uber/\nThe Amplify team published an excellent collection of articles to help build modern data teams. The collection includes building a data org, data strategy, the focus of a data team, and establishing a career in data et al., \nhttps://amplifypartners.com/moderndatateamshub/\nOh wow! Shot Fired. Possibly one of the best articles articulating clearly the problems with Airflow. \nSo what is my problem with Airflow? My problem is that Airflow was not designed to address these problems \u2014 it lacks the ambition we need, even while occupying the critical pedestal as the foundational execution engine.\nIn fact, Airflow is already displaced. Airflow qua Airflow is already obsolete, and it happened right within the Airflow ecosystem. It\u2019s called Astronomer.\nI had a similar experience with the Astronomer team while explaining most of the problems mentioned by the author and sketched why dbt is Airflow's missed opportunity. I wrote the gist of it here \nhttps://www.dataengineeringweekly.com/p/bundling-vs-unbundling-the-tale-of. \nThe response I got; Our customer struggles to spin off and maintain Airflow, and we are going after the cloud infrastructure. From a business aspect, I agree & understand the strategy. However, I can't stop thinking; is commercialization killing the mission of an open source system?\nhttps://stkbailey.substack.com/p/airflows-problem\nAnother classic narration from Benn about the current state of the data landscape and the promising land of what it could be. \nFor the last few years, most data startups have followed Peter Thiel\u2019s advice: Avoid competition. This positioning, however, can\u2019t last forever. Eventually, as startups grow, companies that see one another as polite partners will start to jockey for the same space.\nI can\u2019t second this thought enough. \nhttps://benn.substack.com/p/powder-keg\nFirebolt is the cloud data warehouse for builders of next-gen analytics experiences.Combining the benefits and ease of use of modern architecture with asub-second performance at a terabyte-scale, Firebolt helps data engineeringand dev teams deliver data applications that end-users love.\nhttps://www.firebolt.io/\nCost optimization is a critical engineering function in system design. The author explains the strategies and sourcing data to optimize the Databricks cloud cost. \nhttps://medium.com/@progeorgek/databricks-usage-and-cost-analysis-e974e380916a\n99.co writes an excellent overview of the infrastructure setup of Airflow, dbt & Kubernetes with code examples. \nhttps://medium.com/99dotco/the-evolution-of-transformation-layer-architecture-in-99-group-dbt-airflow-and-kubernetes-cb46900f3662\nGet hands-on with the new open-source framework to test and monitor data as-code, across every data workload, from ingestion to transformation to production. Easy to set up, read, and maintain. Try out and install Soda Core to see how to stop firefighting data issues, maintain reliable pipelines, and deliver high-quality, reliable data products. Access the docs here,\nhttps://docs.soda.io/soda-core/overview-main.html\nConfluent writes an excellent article about the approaches one can take to apply unit & integration testing for Apache Kafka pipelines. \nhttps://www.confluent.io/blog/apache-kafka-ci-cd-with-github/\nRunning the Apache Spark workload on spot instances can reduce the infrastructure cost up to 60-90% compares to on-demand instances. The author writes some tips & design strategies to design Spark applications spot instances ready. \nhttps://blog.dataminded.com/make-spark-resilient-against-spot-interruptions-on-kubernetes-a2d6403399b0\nJoin this live recording of The Data Stack Show with Continual CEO, Tristan Zajonc and Tecton Engineering Manager, Willem Pienaar to explore the democratization of ML, how it's impacting businesses today, and where things are headed next.\nhttps://datastackshow.com/events/the-future-of-machine-learning/\nThe CIDR 2021 paper on Lakehouse Lakehouse: A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics is an informative read on modern Lakehouse design. The author writes an excellent summary of the paper narrating the historical development before the Lakehouse systems.\nhttps://0x0fff.com/lakehouse/\nIs Data Quality impacts the business operation? The author walks through various maturity stages of data adoption in an organization and the framework for measuring the impact of data quality in each maturity cycle. \nhttps://towardsdatascience.com/a-framework-to-understand-how-low-quality-data-hurts-business-performance-386c10c4fe1e\nCriteo writes about its reporting infrastructure built on top of Apache Kafka, HDFS, and Vertica. (\ud83e\udd14 is that what folks call pre-modern data stack?). The blog narrates Critieo's approach to data quality, data modeling, and data partition approaches. \nhttps://medium.com/criteo-engineering/reporting-data-at-criteo-how-to-measure-at-scale-b315b0d8d78a\nI\u2019m an active user of the Tabnine code completion plugin in IntelliJ, and I\u2019m sure many with Github copilot. Google writes about how it combines ML and SE to develop a novel Transformer-based hybrid semantic ML code completion for internal Google developers.\nhttps://ai.googleblog.com/2022/07/ml-enhanced-code-completion-improves.html\nThe Harvard University CS109A- introduction to Data Science lecture materials, Python notebooks, and video lectures are now free for download.\nhttps://harvard-iacs.github.io/2019-CS109A/pages/materials.html\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-94", "title": "Data Engineering Weekly", "content": "It\u2019s time to make sense of today\u2019s data tooling ecosystem. Check out rudderstack.com/dmds to get a guide that will help you build a practical data stack for every phase of your company\u2019s journey to data maturity. The guide includes architecture and tactical advice to help you progress through four stages: Starter, Growth, Machine Learning, and Real-Time. Visit RudderStack.com/dmds today to DROP the modern data stack and USE a practical data engineering framework.\nDiscussion between Aditya Parameswaran & Sesh Seshadri\nWalmart recently wrote about the DataBathing framework to transpile the SQL to Spark Dataframe. It made me curious to research more, and Rajesh Parikh shared his insights and this excellent discussion about pandas and SQL. The vision of layered architecture with pluggable runtime supporting SQL & DataFrame API is tempting. Apache Beam attempts a similar strategy but primarily focuses on unifying both batch & real-time computing. \nWe got the next two posts going to be on the same topic - Coincident \ud83e\udd14\nStaying with SQL & Pandas, Databricks announces the support for Python UDF in SQL. The macros and UDFs are trying to empower the declarative nature of SQL, but is that enough? Please share your thoughts in the comments or on Twitter @data_weekly.\nhttps://databricks.com/blog/2022/07/22/power-to-the-sql-people-introducing-python-udfs-in-databricks-sql.html\nWe got SQL to Python dataframe transpiler & Python UDFs, where Malloy approaches from the Data Modeling perspective to describe data relationship & transformation. The author writes the first look of Malloy, walking through the SQL generation, ease of use, and BI integrations. \nhttps://datamonkeysite.com/2022/07/22/first-look-at-google-malloy/\nThanks for reading Data Engineering Weekly! Subscribe for free to receive new posts and support my work.\nNetwork analytics of downstream impact reveal unique insights, but how hard is it? What if there is a 1:n relationship downstream, and it is time-sensitive? LinkedIn writes about the CAMEL framework for downstream impact analytics that handles 1:n relationship & time factors. \nhttps://engineering.linkedin.com/blog/2022/measuring-downstream-impact-on-social-networks\nGet hands-on with the new open-source framework to test and monitor data as-code, across every data workload, from ingestion to transformation to production. Easy to set up, read, and maintain. Try out and install Soda Core to see how to stop firefighting data issues, maintain reliable pipelines, and deliver high-quality, reliable data products. Access the docs here,\nhttps://docs.soda.io/soda-core/overview-main.html\nApache Airflow is a widely adopted data engineering orchestration engine and has also gone through multiple criticisms. The author walks through the current sentiment and features of Airflow. \nhttps://medium.com/coriers/should-you-use-apache-airflow-e71c6cf7c0c4\nStaying with Airflow, we can\u2019t stop thinking, should one need an Airflow-like orchestration engine if I use only dbt? The author narrates the scenario where the organization-wide orchestration engine requires. \nhttps://davidsj.substack.com/p/going-with-the-airflow-part-1\nFor the readers, it is also a good reference to refresh the evolution of Airflow & dbt.\nFirebolt is the cloud data warehouse for builders of next-gen analytics experiences.Combining the benefits and ease of use of modern architecture with asub-second performance at a terabyte-scale, Firebolt helps data engineeringand dev teams deliver data applications that end-users love.\nhttps://www.firebolt.io/\nAirbnb's Minerva blogs influenced many conversations around the metrics layer. dbt talked about its vision of the metric system, and there was some exciting discussion about the metric layer & metadata.\nAirbnb talks about the second generation of Minerva (2.0) system design, narrating the challenges with the initial version of Minerva. \nMinerva\u2019s previous blogs for reference:\nHow Airbnb Achieved Metric Consistency at Scale\nHow Airbnb Standardized Metric Computation at Scale\nUber writes about its next-generation A/B testing infrastructure with the system design goal. The system design focuses on user productivity & flexibility to support a variety of experimentation. \nhttps://eng.uber.com/supercharging-a-b-testing-at-uber/\nModel-Centric vs. Data-Centric is an exciting system development in ML infrastructure. Shopify writes about its Data-Centric ML approach to build Shopify's Inbox classification system.\nhttps://shopifyengineering.myshopify.com/blogs/engineering/shopify-inbox-message-classification-model\nJoin RudderStack live with the Seattle Data Guy, Ben Rogojan, and Max Werner, Owner of Obsessive Analytics Consulting, to learn about the four stages of The Data Maturity Journey. You'll come away with practical architectures you can use to drive better decision-making at every stage of your company\u2019s growth.\nhttps://www.rudderstack.com/video-library/the-data-maturity-journey\nAdobe writes an excellent overview of Kafka consumer\u2019s internal focusing on Fetcher, Consumer group, Rebalancing & Partition Assigner.\nhttps://medium.com/adobetech/exploring-kafka-consumers-internals-b0b9becaa106\nIf you\u2019re starting to explore ML, The cheatsheet is an excellent guide.\nhttps://stanford.edu/~shervine/teaching/cs-229/cheatsheet-unsupervised-learning\nThe ML course note repo shares notes for AI, ML & NLP related course\nhttps://github.com/dair-ai/ML-Course-Notes\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-93", "title": "Data Engineering Weekly", "content": "It\u2019s time to make sense of today\u2019s data tooling ecosystem. Check out rudderstack.com/dmds to get a guide that will help you build a practical data stack for every phase of your company\u2019s journey to data maturity. The guide includes architecture and tactical advice to help you progress through four stages: Starter, Growth, Machine Learning, and Real-Time. Visit RudderStack.com/dmds today to DROP the modern data stack and USE a practical data engineering framework.\nEveryone talked about ML, but what they needed was data\nAn excellent article captures the author's observation on industrial-scale ML engineering. The author stresses the importance of access to the data & compute with simple abstractions is the key for ML. It is an excellent read on the next significant challenges of ML. \nhttps://medium.com/@profjoeyg/how-machine-learning-became-useful-5732c3419c81\nData Engineering practices & A/B testing have broader applications in reliability engineering. While working for the Slack observability team, I realized a simple Apdex score computation on the HTTP error code is a significant feedback loop to improve the reliability of the systems. Airbnb writes an excellent blog narrating how it uses AB testing & data science to safeguard changes in production. \nhttps://medium.com/airbnb-engineering/how-airbnb-safeguards-changes-in-production-9fc9024f3446\nCriteo writes about its homegrown scheduler Cuttle and a self-service web application called BigDataflow as an abstraction for Cuttle. The design centered around building a simpler task abstraction. By inferring the DAG of tasks statically, the platform implements all the things workflow management systems like task execution, data retention & backfilling. \nPart 1: https://medium.com/criteo-engineering/scheduling-data-pipelines-at-criteo-part-1-8b257c6c8e55\nPart 2: https://medium.com/criteo-engineering/scheduling-data-pipelines-at-criteo-part-2-8b0da38ff3a4\nRiskfield writes about its self-serve streaming infrastructure using Spark streaming. The first blocker for a platform is to minimize the manual-intensive tasks that can create unavoidable bureaucracy. It's admirable that the author called it out explicitly in the blog. \nhttps://medium.com/riskified-technology/spark-streaming-as-a-service-53420d8a857b\nFirebolt is the cloud data warehouse for builders of next-gen analytics experiences. Combining the benefits and ease of use of modern architecture with a sub-second performance at a terabyte-scale, Firebolt helps data engineering and dev teams deliver data applications that end-users love.\nhttps://www.firebolt.io/\nI observed that data maturity shrinks as an organization grows a few months back. The author establishes the same case and shares a few techniques to handle the complexity. \nhttps://mikkeldengsoe.substack.com/p/data-team-size\nA marketing campaign can reach the consumer via multiple channels. The last-click conversion measurement can misrepresent the influence of a marketing channel. LinkedIn writes about measuring incremental impacts using the Bayesian Structural Time Series (BSTS) model approach to measure the causal effect of an intervention. \nhttps://engineering.linkedin.com/blog/2022/measuring-marketing-incremental-impacts\nShippeo writes about its CDC infrastructure using Debezium, Kafka & Snowflake. The blog includes excellent insights on data format choice, Postgres reliability, and observability metrics. \nhttps://medium.com/shippeo-tech-blog/debezium-to-snowflake-lessons-learned-building-data-replication-in-production-a5430a9fe85b\nJoin RudderStack live with the Seattle Data Guy, Ben Rogojan, and Max Werner, Owner at Obsessive Analytics Consulting, to learn about the four stages of The Data Maturity Journey. You'll come away with practical architectures you can use to drive better decision making at every stage of your companies growth.\nhttps://www.rudderstack.com/video-library/the-data-maturity-journey\nData Engineering Weekly frequently features an article about the social impact of AI. The XAI (explainable AI) plays a vital role in bringing transparency to the AI system. The two-part blog from GovTech Singapore explains the applications of XAI and an overview of XAI methods.\nPart 1: https://medium.com/dsaid-govtech/towards-a-comparable-metric-for-ai-model-interpretability-part-1-d55d4bae8a58\nPart 2: https://medium.com/dsaid-govtech/towards-a-comparable-metric-for-ai-model-interpretability-part-2-423e4fc2b232\nPolicygenius writes about implementing the data governance policy for Tableau. The blog highlights the classic dashboard management problem and explains how a decentralized data-driven approach helped them to scale the data governance strategy. \nhttps://medium.com/policygenius-stories/how-we-implemented-a-tableau-governance-strategy-59c055727433\nI hear significantly less about the Azure services for data engineering. The blog is an excellent first insight into the lesson learned from Azure Data Factory. \nhttps://medium.com/@guangx/lessons-learned-from-azure-data-factory-4778eca0fc25\nJellysmack Labs writes about its Airflow usage in running production-ready data science jobs. The focus on DAG quality and uniformity in declaring a DAG & task is an exciting read. \nhttps://medium.com/jellysmacklabs/how-jellysmack-pushed-data-science-jobs-orchestration-to-a-production-ready-level-e92dc4786413\nStaying with the uniformity of the jobs, Walmart writes about DataBathing, a framework to transpile the SQL to Spark Dataframe calculation flow code. The blog claim the performance increased from 10 to 80% with DataBathing. The blog mentioned that there would be a follow-up blog post on why such a transformer is required. I'm more curious to read the follow-up. \nhttps://medium.com/walmartglobaltech/databathing-a-framework-for-transferring-the-query-to-spark-code-484957a7e049\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/the-data-founder-story-singularity", "title": "Data Engineering Weekly", "content": "Hey everyone! I'm Yingjun Wu, the founder of Singularity Data \u2014 an early-stage startup innovating next-generation database systems. Our product is RisingWave, a cloud-native SQL streaming database for modern real-time applications. With RisingWave, users can easily extract knowledge from streaming data by issuing powerful and efficient queries using standard SQL language.\nThis is the story of how our journey began.\n\"There's a way to do it better \u2013 find it.\" \u2013 Thomas A. Edison\nYou can't tackle new problems with old solutions; therefore, you must always look for fresh ways to innovate. And that is what a Ph.D. degree entails: a voyage of discovery and invention. I began working on database systems during my Ph.D. at the National University of Singapore. Those were the formative years that shaped my journey of entrepreneurship.\nI've always tried to do things differently since I was a child. Getting a Ph.D. was all about honing that ability of creativity \u2014 coming up with new ideas and putting them into action. However, I quickly realized that academia has its own set of constraints. All of these unique concepts don't always reach the light of day. They aren't implemented in real time. Simply put, new ideas emerge, but breakthroughs don't happen in terms of application / reaching users.\nHence, I joined IBM research, a wonderful blend of academic research and industry, to put my ideas into action and see changes in real time. IBM Research provided me with access to amazing brains, a wealth of valuable resources, and, above all, DB2. DB2 is a world-class database system that was developed about 40 years ago. I collaborated closely with the product development team. During brainstorming sessions, breakthrough ideas would emerge, but the execution would be limited because the focus had to be on the product. Following IBM, I joined AWS and worked on developing Redshift, one of their most important products. AWS is the world's most frequently used cloud platform, and Redshift is the most widely used cloud data warehouse. I had a great time working there \u2014 a great learning experience.\nBut, alas, innovation proved to be a difficult task once again. All were attempting to repair or improve various issues by employing tried-and-tested technology. I wanted to do something new. I knew one hundred percent that real-time data was becoming increasingly important to users. And something new could have revolutionized the field and genuinely benefit users. I firmly believe that we must continue to innovate. We won't be able to sell things if we continue to rely on outdated technology. So, I wanted to combine my findings and create something useful for today's generation: a next-gen model.\nLook, you can have brilliant ideas, but you won't put them into action unless there's a genuine demand. People increasingly demand real-time analytics and want to shift their streaming database to the cloud to save money and improve efficiency. Based on these two reasons, I decided to create something new: a streaming database that is not just streaming but also cloud-based. I came up with the concept, and it was time to implement it. With this notion in mind, I talked to some prominent personalities and startup founders in the Bay Area. We talked about it, and I asked them for their thoughts. I was hoping to get feedback on whether or not my ideas were feasible.\nLong story short, I received assistance from several startup founders (whom I will not name). Those multi-hour chats have been invaluable to me. Singularity Data was formed in early 2021, after much deliberation, planning, and execution. RisingWave was developed by a collaboration of professional database researchers and practitioners from businesses including AWS Redshift, Microsoft Azure, Snowflake, LinkedIn, and Uber.\nStream processing has a huge demand. An increasing number of firms are looking into modern streaming systems to see how they may revolutionize their operations. But unfortunately, many stop along the way, blaming the exorbitant expense of implementing streaming technology. There are two critical reasons for this:\nUnlike traditional databases (such as MySQL and PostgreSQL), which utilize SQL as their interactive interface, most, if not all, streaming systems require users to master a set of platform-specific programming interfaces (most likely in Java) to modify the data. For non-tech people, learning streaming technologies becomes a near-impossible task. Data is represented differently in streaming systems than it is in databases. Users must create sophisticated data extraction logic to transfer data between streaming systems and databases.\nSeveral prominent streaming systems use open-source software. Docker images and comprehensive deployment scripts are readily available. But, open-source does not imply that something is free or inexpensive. Streaming workloads can change dramatically depending on demand. Businesses must purchase a big cluster of computers to withstand worst-case scenarios. The cost of setting up and maintaining a streaming system is sometimes substantially more than buying devices. In fact, putting together a team of engineers to work late to run the system can be a genuine pain.\nWe put all of our efforts into democratizing stream processing at Singularity Data. RisingWave is a cloud-native streaming database that makes stream processing simple, affordable, and accessible to everybody.\nRisingWave is a distributed streaming database with an interactive interface based on ordinary SQL. It uses the PostgreSQL dialect and can be integrated into the PostgreSQL environment without requiring any code changes. RisingWave treats streams like tables, allowing users to declaratively and elegantly create complicated queries over streaming and historical data. Users can also focus solely on their analytical query logics with RisingWave, rather than learning Java or system-specific low-level APIs.\nRisingWave is a cloud-based application. RisingWave can fully utilize cloud platforms' elastic resources, thanks to its cloud-native architecture. RisingWave, as a fully managed service, deploys, manages, and scales in the cloud on its own, without human intervention. RisingWave automatically assembles multiple tiers of compute and storage resources in the cloud to accomplish the performance target at a minimal cost once users establish their service-level agreement (SLA). RisingWave is a serverless platform, meaning that users only pay for what they use and don't have to pay unless they use it. We're continually improving the service to keep RisingWave inexpensive for small businesses.\nA great product originates from the pooled wisdom of a vibrant and open community. Instead of depending on the expertise of a limited number of professionals to produce RisingWave, we collaborate with the open-source community to design and implement it. We chose to release RisingWave kernel under the Apache License 2.0, a permissive free software license. The RisingWave community is open to all. Here's how you can get involved:\nParticipate in the development of the project roadmap.\nDeploy the distributed streaming database in your cloud service.\nContribute and provide feedback to the community.\nThe RisingWave community is collaborative, and we're excited to work with other communities to construct the new real-time data infrastructure stack. RisingWave is in the early stages of development, but we're growing rapidly. We have over 70 contributors worldwide, and we examine and integrate more than 100 Pull Requests every week. We still have a long way to go to get recognized as a \"fully-fledged\" system. But, hey, we've covered quite a lot in the last few months. Check out our recently launched roadmap (our first roadmap).\nThis is our story. And to know more, check the RisingWave GitHub repository, join our Slack community, follow us on Twitter, and stay tuned with RisingWave!\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. The data founder story contents provided by the featuring data founders in this article and Data Engineering Weekly is not responsible for any compliance with applicable laws, rules, and regulations."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-92", "title": "Data Engineering Weekly", "content": "It\u2019s time to make sense of today\u2019s data tooling ecosystem. Check out rudderstack.com/dmds to get a guide that will help you build a practical data stack for every phase of your company\u2019s journey to data maturity. The guide includes architecture and tactical advice to help you progress through four stages: Starter, Growth, Machine Learning, and Real-Time. Visit RudderStack.com/dmds today to DROP the modern data stack and USE a practical data engineering framework.\nAuthors: Shivani Kapania, Oliver Siy, Gabe Clapper, Azhagu SP, Nithya Sambasivan\nAn exciting weekend read about the AI Authority and the societal perception of it. (e.g.) 79.2% of people in India prefer AI to validate their loan applications over human validation. AI assigned legitimized power to influence people's actions without presenting adequate evidence of the system's capability. \nAI impacts the social-economic factors of a large population, yet the data community rarely talks about explainable AI and its user experience. A few internet companies talk about explainable AI, but it feels like marketing. I have never seen their product reflects or gives me an explainable AI experience. \nThe question I want to leave with the Data Engineering Community is that we are too busy talking about zombie dashboards yet never about their social impact. Are we on the wrong side of history? \nhttps://research.google/pubs/pub51146/\nIf the paper is a bit long, the tech talk is an excellent summary of the paper.\nAuthors: Dominik Kreuzberger, Niklas K\u00fchl, Sebastian Hirschl\nWhy was the ML project never taken off from the prototype? Why is it operationally challenging to run an ML infrastructure? The paper is an excellent summary of the current state of MLOps. \nhttps://arxiv.org/pdf/2205.02302.pdf\nConference, reading, and interaction with the data practitioners are great ways to question our perspectives. The author writes an excellent perspective-changing summary of the Hadoop world to Cloud data warehouses with DataAISummit. \nhttps://hysterical.substack.com/p/reflecting-on-dataaisummit2022\nOne of the curious announcements in the DataAISummit is Spark Connect. The idea of Spark everywhere is interesting. Databricks published more details on Spark Connect this week.  \nhttps://databricks.com/blog/2022/07/07/introducing-spark-connect-the-power-of-apache-spark-everywhere.html\nYou can find the proposal document for Spark Connect here,\nhttps://docs.google.com/document/d/1Mnl6jmGszixLW4KcJU5j9IgpG9-UabS0dcM6PM2XGDc/edit#\nFirebolt is the cloud data warehouse for builders of next-gen analytics experiences. Combining the benefits and ease of use of modern architecture with a sub-second performance at a terabyte-scale, Firebolt helps data engineering and dev teams deliver data applications that end-users love.\nhttps://www.firebolt.io/\nConferenceogue is an excellent way to capture the essence of a conference from the data practitioner's perspective. The author writes about the Tecton's apply() 2022 Conference focuses on industry trends, production use cases, and open-source libraries out of the conference. \nhttps://data-notes.co/what-i-learned-from-tectons-apply-2022-conference-86f3bf87f80e\nData Contracts are an emerging need to manage large-scale data infrastructure. The author shared vital learnings from implementing Data Contracts with GoCardless. \nOver the last ten years, We significantly commoditized data asset creation by simplifying the data processing tools. However, we have yet to democratize the exchange of business context, which the data represent among various stakeholders. Schemata.app is an attempt to solve the data contract where we recently added dbt support. More about Schemata is coming soon in a series of blog post. \nhttps://barrmoses.medium.com/implementing-data-contracts-7-key-learnings-d214a5947d5e\nA detailed guide to building the Machine Learning Stack\u2014an architecture to help you take your first steps into the world of ML and move from historical analytics to predictive analysis. The ML stack is phase three of RudderStack's Data Maturity Journey framework.https://www.rudderstack.com/blog/what-is-the-ml-stack\nData shuffle in the MapReduce paradigm brings reliability, efficiency, and scalability issues. LinkedIn, in the past, wrote about its shuffle service Magnet, which merged into the Spark 3.20 release. [SPARK-30602 & SPARK-33235]\nUber writes about its Spark shuffle service with Remote Shuffle Service (RSS). The blog narrates how RSS works and its efficiency in solving reliability & scalability issues. \nhttps://eng.uber.com/ubers-highly-scalable-and-distributed-shuffle-as-a-service/\nAn excellent guide on using Spark's streaming query listener to find the discrepancy between the Spark checkpoint and Kafka commit offset. \nhttps://medium.com/myheritage-engineering/guide-to-streamingquerylistener-in-pyspark-streaming-f3bbfe56a774\nCriteo writes about its disinformation model to tackle advertisements on websites spreading disinformation about the ongoing conflict in Ukraine. TIL about the Global Disinformation Index [https://www.disinformationindex.org]\nhttps://medium.com/criteo-engineering/using-ai-and-nlp-to-tackle-advertisement-spreading-disinformation-on-the-conflict-in-ukraine-59b6745a0c43\nPossibly one of the thought-provoking articles I read this week in data analytics. The author questions the role & responsibility of a product data scientist highlighting the grey areas of A/B testing and the neutral results. \nhttps://developer.squareup.com/blog/success-metrics-for-product-analytics/\nA great compilation of the open source data lineage tools. I\u2019m not aware of half of the tools in the article :-) Time to read more about these open-source lineage tools.\nhttps://blog.devgenius.io/5-best-open-source-data-lineage-tools-in-2022-f8ef39a7d5f6\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-91", "title": "Data Engineering Weekly", "content": "It\u2019s time to make sense of today\u2019s data tooling ecosystem. Check out rudderstack.com/dmds to get a guide that will help you build a practical data stack for every phase of your company\u2019s journey to data maturity. The guide includes architecture and tactical advice to help you progress through four stages: Starter, Growth, Machine Learning, and Real-Time. Visit RudderStack.com/dmds today to DROP the modern data stack and USE a practical data engineering framework.\nThe Databricks AI & data summit 2022 brings some exciting talks and announcements. The improvements with the delta sharing, similar to Snowflake & Redshift Data sharing, is an exciting development overall by the cloud data stores. \nThanks for reading Data Engineering Weekly! Subscribe for free to receive new posts and support my work.\nMatt wrote an excellent summary of key takeaways from the Databricks AI summit. \nPart 1: https://medium.com/@matt_weingarten/data-ai-summit-takeaways-part-i-ed1da1aa8125\nPart 2: https://medium.com/@matt_weingarten/data-ai-summit-takeaways-part-ii-2f940eee2487\nOne critical development in the announcement is Spark Connect, a thin \u201cthin client\u201d to enable Spark query capability on low-compute devices. Stan writes an excellent summary of Spark Connect.\nhttps://medium.com/@coderstan/understanding-spark-connect-reynold-xins-keynote-on-data-ai-summit-2022-6b9e1b5e1fd9\nApache Airflow released its 2022 survey capturing the developer\u2019s thoughts about the current state of Apache Airflow. The data-driven scheduling & signal-based scheduling seems to be the most wanted features. I wish more open source systems publish their user survey to bring more openness.\nhttps://airflow.apache.org/blog/airflow-survey-2022/\nIs Modern Data Stack(?) solving the right problem? The author raised an interesting question: Can it solve people's problems?\nMy take on this: I think it is a bit tricky to answer. From a typical system thinking perspective, The tools provide a feedback loop [e.g.] How slow is the query running? What is the frequently used Dashboard? Every company's workflow & culture is different. It's up to the data team to use the feedback loop and create a balancing action on the workflow. The workflow defines the culture of an org, and I don't think there will be one vendor that can provide an out-of-the-box solution to solve this problem. \nhttps://locallyoptimistic.com/post/people-first-data-stacks/\nThe author writes an excellent overview of thinking about building the data team. Start with the right objective [vision & mission], and build a team structure to enable the execution, the tech stack & hiring strategies. \nhttps://towardsdatascience.com/things-i-wish-i-knew-when-i-was-building-a-data-team-efcb43591204\nFirebolt is the cloud data warehouse for builders of next-gen analytics experiences. Combining the benefits and ease of use of modern architecture with a sub-second performance at a terabyte-scale, Firebolt helps data engineering and dev teams deliver data applications that end-users love.\nhttps://www.firebolt.io/\nThe ML Model evaluation metrics aim to find how well the model will perform on unseen data. The author discusses various evaluation metrics one could use for the model evaluation.\nhttps://www.kdnuggets.com/2020/05/model-evaluation-metrics-machine-learning.html\nData Drift occurs when the dataset used to train your model does not mimic the data you receive in production. An unexpected and undocumented change to the data structure, semantics, and infrastructure can potentially cause Data Drift. The article discusses various testing strategies to identify Data Drift. \nhttps://evidentlyai.com/blog/data-drift-detection-large-datasets\nA detailed guide to building the Machine Learning Stack\u2014an architecture to help you take your first steps into the world of ML and move from historical analytics to predictive analysis. The ML stack is phase three of RudderStack's Data Maturity Journey framework. https://www.rudderstack.com/blog/what-is-the-ml-stack\nThe importance of data modeling has been highlighted often recently. Halodoc writes about its data modeling strategy and the choice of a Hybrid Data model with a combination of Dimensions, Facts, and denormalized structure.\nhttps://blogs.halodoc.io/data-model-adoption-at-halodoc/\nZomato writes about its Trino infrastructure and how it enables data analytics across the org. The experimentation around the scaling needs and design to handle skewed traffic is an exciting read. \nhttps://www.zomato.com/blog/powering-data-analytics-with-trino\nBazaar writes about its adoption story of Apache Pinot. The blog overviews Apache Pinot\u2019s architecture and integration with the data tools ecosystem.\nhttps://medium.com/@fizzabid96/why-apache-pinot-is-worth-the-hype-bazaars-success-story-1f90b5212fe4\nDisaster recovery is a critical aspect of data reliability engineering to bring business continuation. AWS writes about strategies for disaster recovery designs with Amazon EMR on Amazon EC2 for Spark workloads.  \nhttps://aws.amazon.com/blogs/big-data/disaster-recovery-considerations-with-amazon-emr-on-amazon-ec2-for-spark-workloads/\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions.\nThanks for reading Data Engineering Weekly! Subscribe for free to receive new posts and support my work."}, {"url": "https://www.dataengineeringweekly.com/p/the-data-founder-story-tukan", "title": "Data Engineering Weekly", "content": "This is the story of how\u00a0me\u00a0and my two co-founders\u00a0Ilan\u00a0and\u00a0Miguel\u00a0took a data journalism blog and turned it into a SAAS business.\nThe year is 2018 and we are wrapping up college courses needed to graduate from university. We continue to hear from a multitude of professors that \u2018cleaning data will keep the bread in our mouths\u2019 and would enter the labor market as glorified data cleaners disguised as licensed mathematicians.\nIlan and I were running a statistical analysis on the statistical significativeness of a national referendum in Mexico. When we were completing our experiments we realized that our data was wrong. After the initial panic subsided we took to manually inspecting our pipelines and trying to figure out what went wrong.\nIt turned out that one of the sources labeled state data differently than another and upon merging was mistakenly matched. We implemented a 'hotfix' of the geographic values that the electoral institute used compared to those used by the national institute of statistics and re-ran our data pipelines.\nThis experience would mark the first of many encounters manually parsing web data to fit into existing data models. The three of us that would eventually launch Tukan worked in different industries and yet were creating ETL jobs to fix a variety of the same data quality issues.\nWe were distraught calculating how much time and effort we could expect to spend cleaning data. Surely there is a better way for talented professionals to budget their time.\nAfter realizing this was a shared pain point, we could not ignore the calling to solve. .\u00a0TUKAN\u00a0is a Mexican startup with the intent to standardize the world's public data.\nAfter a brief stint in data journalism, we took up the challenge of building a data product and hired our first software developers to help us with our MVP. These first hires allowed us to launch our data catalog that grants users access to standardized public data including regulatory, financial, macroeconomic, demographic, and many more.\nOur data catalog enables TUKAN users to integrate web data from continuously changing and cumbersome data sources.\nTUKAN\u2019s initial launch landed a handful of clients. We are fortunate to focus on the maintenance and further expansion of aggregating diverse and scattered data points into the data catalog. We are rebuilding our search module by integrating active metadata into our search fields to facilitate data discovery. Furthermore, we are focusing on our ability to monitor external data sources for changes and outages to be able to quickly respond to a changing data ecosystem.\nTUKAN\u2019s built-in hierarchy of data table attributes allows our users to not just query individual entities but compare them to their parent attributes. For example, being able to compare an individual bank\u2019s profit loss with the average of the entire commercial banking sectors\u2019 within our application in one single query. Our tools are making operational analytics ubiquitous to stakeholders from multiple industries.\nTake our\u00a0product tour, explore our\u00a0data catalog\u00a0or sign up for a\u00a0demo\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. The data founder story contents provided by the featuring data founders in this article and Data Engineering Weekly is not responsible for any compliance with applicable laws, rules, and regulations."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-90", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nA scalable orchestration engine is the holy grail of a data pipeline. dbt writes an exciting blog detailing the redesign of its cloud orchestration engine. The use of round-robin load balancing is an interesting choice for a multi-tenant scheduler since various tenants will have their priority to maintain. Airflow did a PriorityQueue as a workaround and Yarn with a hierarchical queue. It's an incredible effort by the team to increase the reliability from only 21% of runs being \"on time\" at the beginning of the year to 99.99%. \nhttps://www.getdbt.com/blog/rebuilding-dbt-scheduler/\nI happened to see this article pretty late; nonetheless, it is an excellent comparison of SCD of Kimball methodology to how we could handle the SCD today. \nhttps://www.holistics.io/blog/scd-cloud-data-warehouse/\nBuilding trust within your stakeholders is the first step to building trust in the data. The author writes excellent do's and don'ts as a data analyst and the stakeholders to establish the trust. \nhttps://medium.com/@mikldd/stakeholders-the-most-important-relationship-for-analysts-e078f3ea0c60\nStaying with building trust in data; An efficient execution framework is essential to facilitate data empowerment across an organization. The author introduces JTBD (Jobs To Be Done) framework that can help data teams prioritize the exemplary work and be more impactful in their day-to-day. JTBD framework focused on five major areas of data functions\nData Activation\nMetrics Management\nProactive Insight Discovery\nDriving Experimentation\nInterfacing with the Data.\nhttps://locallyoptimistic.com/post/building-more-effective-data-teams-using-the-jtbd-framework/\nBut Data Apps is still a loosely defined term, and there\u2019s a lot of debate and confusion about what it really means, and how it differs from traditional dashboarding and embedded analytics. Boaz Farkash shares his point of view on the subject.\nhttps://www.firebolt.io/blog/embedded-analytics-vs-data-apps\nData management is moving to a healthy space where folks started to talk more about the quality and integrity of the data over the volume of the data. The blog highlights various dimensions of data quality, such as data profiling, data quality measurement, data cleansing & repair, and advanced capabilities such as entity resolution & collaborative metadata management. \nhttps://gradientflow.com/data-quality-unpacked/\nIs it too many tools to manage a data infrastructure? One could not stop wondering why we got three formats under \"Open Table Format.\" The blog rightly called the future of metastore is still in the dark. The blog is an excellent summary of the state of data engineering in 2022. \nhttps://lakefs.io/the-state-of-data-engineering-2022/\nIs SQL a silver bullet for all data analytics problems? The blog narrates the complexity of SQL in computing the funnel analysis. I remember a few conversations on tracing as a better choice for funnel analytics, and curious to know what is coming out of Motif Analytics.\nhttps://motifanalytics.medium.com/everything-is-a-funnel-but-sql-doesnt-get-it-c35356424044\nStaying with Is SQL good enough territory, Shopify writes a timely article about ShopifyQL, an SQLish domain language for ECommerce analytics. Perhaps it is the beginning of domain-specific SQL-ish DSLs? \nhttps://shopifyengineering.myshopify.com/blogs/engineering/shopify-commerce-data-querying-language-shopifyql\nA detailed guide to building the Growth Stack\u2014an architecture to centralize every data point into a comprehensive source of truth and activate that centralized data in downstream tools. The growth stack is phase two of RudderStack's Data Maturity Journey framework.https://www.rudderstack.com/blog/what-is-the-growth-stack\nWe often quote counting as the most challenging problem in data engineering, but why? Why do we still struggle to answer basic questions like how many customers are still active or what exactly is the company\u2019s churn? The author walks through the practical complexity of designing systems that seem simple as churn computing. \nhttps://medium.com/coriers/why-are-we-still-struggling-to-answer-how-many-active-customers-we-have-191b2b3c09a0\nMicrosoft writes about forecasts for multiple univariate time series in a scalable manner, focusing on multi-horizon forecasts instead of single horizon forecasts. The article compares the result of TFT (Temporal Fusion Transformer) with the optimized Prophet approach published earlier. \nhttps://medium.com/data-science-at-microsoft/scalable-time-series-forecasting-fee61da75923\nInstacart writes about its ML platform Griffin, an extensible platform that supports diverse data management systems and integrates with multiple machine learning tools and machine learning workflows. Airflow combined with AWS Sagemaker and a cloud data warehouse becomes a defacto ML platform over the bespoke vendor solutions.\nhttps://tech.instacart.com/griffin-how-instacarts-ml-platform-tripled-ml-applications-in-a-year-d3d4dcae3690\nCanva publishes an excellent data ingestion journey from database snapshots to CDC (Change Data Capture) to ingest business process changes. The blog narrates how Snowplow and S3 used to integrate continuous data ingestion to Snowflake. \nhttps://canvatechblog.com/service-aligned-data-platform-architecture-6b5a6fc366c4\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-89", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nWe can frame many real-world machine learning and data analytics problems as graph problems. It's a constant question in me, yet we do not leverage graph modeling as much and treat it as an independent entity. Airbnb writes an exciting blog describing the benefit of using graphs for machine learning. \nhttps://medium.com/airbnb-engineering/graph-machine-learning-at-airbnb-f868d65f36ee\nLinkedIn writes about its data health monitor architecture to track data freshness and schema changes to detect the health of the data. It will be interesting to see the architecture evolves from \u201cmonitoring-after-the-fact.\u201d to a preventive solution\nhttps://engineering.linkedin.com/blog/2022/towards-data-quality-management-at-linkedin.\nForecasting key business metrics quarterly, weekly, or daily helps the business monitor performance, make business decisions, and improve our product offerings. Spotify writes about its forecasting infrastructure and lessons learned. \nhttps://engineering.atspotify.com/2022/06/how-we-built-infrastructure-to-run-user-forecasts-at-spotify/\nGrab writes about its automated experimentation analytics to automate the basic analytics. The separation of metrics configuration and computation, dataset classification of bronze and gold datasets, and star schema modeling practices are exciting to read. \nhttps://engineering.grab.com/automated-experiment-analysis\nBut Data Apps is still a loosely defined term, and there\u2019s a lot of debate and confusion about what it really means, and how it differs from traditional dashboarding and embedded analytics. Boaz Farkash shares his point of view on the subject.\nhttps://www.firebolt.io/blog/embedded-analytics-vs-data-apps\nThe streaming analytics from the transactional database to the analytical applications is always challenging. The infrastructure to stitch and maintain is expensive to maintain. Etsy writes about one of its challenges of building an analytical app using the change data capture. \nhttps://www.etsy.com/codeascraft/using-real-time-streaming-to-power-etsy-offsite-ads\nGradient Flow publishes the distributed computing roles and needs for each stage of the ML lifecycle to demonstrate the overlap of AI & distributed computing. \nhttps://gradientflow.com/distributed-computing-for-ai-a-status-report/\nThe rapid expansion of additional data sources and the expansion of the use cases brings challenges to the modern cloud data infrastructure. The blog narrates the challenges and why the data product approach solves these emerging issues. \nhttps://petrjanda.substack.com/p/a-path-towards-a-data-platform-that\nIs data modeling dead? The author narrates the importance of data modeling and why it\u2019s hard to adopt in modern data technologies. The traditional approach of a centralized data modeling team won\u2019t scale, and the author calls for rethinking data modeling.\nhttps://dataproducts.substack.com/p/the-death-of-data-modeling-pt-1\nA detailed guide to building the Growth Stack\u2014an architecture to centralize every data point into a comprehensive source of truth and activate that centralized data in downstream tools. The growth stack is phase two of RudderStack's Data Maturity Journey framework. https://www.rudderstack.com/blog/what-is-the-growth-stack\nAirflow summit hosts some excellent talks in data engineering, and the author summarizes the conference talks here.\nhttps://potiuk.com/airflow-summit-2022-the-best-of-373bee2527fa\nZalando writes about how to version the Airflow DAGs on a single server through isolated pipeline and data environments to enable more convenient simulation and testing.\nhttps://engineering.zalando.com/posts/2022/06/accelerate-apache-airflow-testing-through-dag-versioning.html\nAn excellent overview of how the SaaS companies handle the customer churn prediction and the models and approaches using machine learning to predict the customer churn. \nhttps://www.kdnuggets.com/2019/05/churn-prediction-machine-learning.html\nHomeToGo writes about integrating dbt metadata with Superset making the metadata available for easier consumption by the data consumers. The approach to push the metrics definition from Superset to a more source-controlled solution and using dbt manifest as a source of truth is fascinating. \nhttps://engineering.hometogo.com/how-hometogo-connected-dbt-and-superset-to-make-metadata-more-accessible-and-reduce-analytical-2223af539cc6\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-88", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nLast week sit with Joe Reis & Matt Housley about data contracts and domain ownership. I talked about Schemata, the first open-source \u201cdata contract\u201d framework. I believe the data contract & data sharing is the next big wave in data engineering. You can find more details about Schemata in schemata.app\nStaying on Data Contracts, where Schemata.app solving the semantic layer of the data contracts, GoCardless writes about the data transportation using the Outbox Pattern. \nhttps://medium.com/gocardless-tech/data-contracts-at-gocardless-6-months-on-bbf24a37206e\nAn excellent blog from Debezium on implementing CDC with Outbox pattern.\nhttps://debezium.io/blog/2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern/\nShopify writes about lessons learned from running Airflow at scale. Shopify runs 10,000 DAGs with an average of 400 concurrent tasks at any given point and 150,000 DAG runs per day!. The manifest file approach is exciting and reminds me of Slack's \"SlackDAG,\" an implementation of Airflow DAG that helped solve ownership problems and enforce the best practices as part of the CI/ CD process.\nhttps://shopifyengineering.myshopify.com/blogs/engineering/lessons-learned-apache-airflow-scale\nBut Data Apps is still a loosely defined term, and there\u2019s a lot of debate and confusion about what it really means, and how it differs from traditional dashboarding and embedded analytics. Boaz Farkash shares his point of view on the subject.\nhttps://www.firebolt.io/blog/embedded-analytics-vs-data-apps\nDefining a well-regulated event field at the source will reduce the significant burden on the ETL system. Vimeo writes about its journey of building a scalable event tracking library. The blog narrates the pros & cons of \u201cattribute-based\u201d tracking vs. \u201cuser-action-based\u201d tracking. \nhttps://medium.com/vimeo-engineering-blog/the-evolution-of-event-data-collection-at-vimeo-part-1-the-fatal-attraction-era-9eae5f67e1bc\nData Engineering has come a long way from click & drop tools to robust data frameworks to programmatically author data pipelines. It opens up adopting software engineering best practices. Expedia compares the data engineering tools/frameworks with the software architectural pattern.\nhttps://medium.com/expedia-group-tech/software-architectural-patterns-in-data-engineering-5d3bf22106a0\nLearn how today\u2019s best data engineering and analytics leaders are staying ahead of the competition in our complete guide.\nDownload the modern data leader\u2019s playbook\nAn efficient experimentation framework is vital for the safe and faster iteration of the product. DoorDash writes about Dash-AB, a centralized library for statistical analysis. \nhttps://doordash.engineering/2022/05/24/meet-dash-ab-the-statistics-engine-of-experimentation-at-doordash/\nStaying with the importance of the culture of experimentation, Netflix had an internal Causal Inference and Experimentation Summit. I thought this was amazing and could apply to other parts of the critical infrastructures. Netflix shares a sneak peek of the event in this blog post with a few selected talks.\nhttps://netflixtechblog.com/a-survey-of-causal-inference-applications-at-netflix-b62d25175e6f\nReal-time demand forecasting in a supply chain system is always challenging. Grubhub writes about its demand forecasting data infrastructure and design principles. \nhttps://bytes.grubhub.com/forecasting-grubhub-order-volume-at-scale-a966c2f901d2\nJoin RuddersStack for a fireside chat on the future of analytics with Hex co-founder, Barry McCardel, and Transform co-founder, Nick Handel. They'll talk about bridging the gap between data and business functions, discuss the current state of analytics, and examine the next challenges to be tackled in data analytics.\nhttps://www.rudderstack.com/video-library/future-of-analytics-on-the-modern-data-stack \nFeature store becomes an essential part of the data infrastructure. MoMo writes about the need for a feature store and its evaluation of open-source feature stores. The blog narrates how MoMo developed ML workflow, ingestion, and data quality management.  \nhttps://tech.info-momo.com/mlops-at-momo-feature-store-e38e59da272e\nA well-thought-through onboarding process boosts the developer's productivity and establishes an inclusive engineering culture. The author shared thoughts on approaches to onboard new data engineers. Please comment how is data engineering onboarding looks like in your team. \nhttps://medium.com/@erickreyesr22/this-is-how-i-onboarded-more-than-10-data-engineers-and-got-excellent-reviews-and-feedback-c569bcfca8f9\nPyScript generated a lot of excitement in the data community. I found this is an excellent tutorial on how to use PyScript\nhttps://www.freecodecamp.org/news/pyscript-python-front-end-framework/\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-87", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nLet\u2019s start this week with this thought.\nThere are some fantastic lively discussions on data deletion, the need for it, and the cultural aspect of the teams. The discussion leads to a few exciting reads on my list.\n***IEEE March 2022 Edition is discussing data errors and privacy. ***\nThough the ML lifecycle has several phases, it is all part of the developer workflow. A single interface to manage the data/ ML lifecycle yields significant productivity gain. The LinkedIn blog on the One-Stop MLOps portal is an excellent reminder of developer productivity in the data workflow. \nhttps://engineering.linkedin.com/blog/2022/one-stop-mlops-portal-at-linkedin\nAs data adoption grows, the next few years' data engineering challenges will be figuring out data contract frameworks and efficient data sharing internally and externally among different org. I started my attempt to solve this problem with Schemata.\nRead more about Schemata, and I'm looking for contributors to Schemata.\nAdidas writes its adoption design of data mesh on a similar principle to define contracts between the data producer & consumer. \nhttps://medium.com/adidoescode/adidas-data-mesh-journey-sharing-data-efficiently-at-scale-c50ee671fbd7\nLior Solomon, VP of Data Engineering at Vimeo shares his own experience on The Data Engineering Show: What made him recently build a new data ops team? How do you operate a data stack that supports 85 billion events per month and 2 PBs of data? What does Fatal Attraction have to do with all of this?\nhttps://www.firebolt.io/blog/how-vimeo-keeps-data-intact-with-85b-events-per-month\nInfrastructure upgrades are always challenging and require disciplined engineering practices. HubSpot writes an exciting article describing the engineering practices for upgrading its data infrastructure. \nhttps://product.hubspot.com/blog/updating-data-infrastructure\nStaying on upgrades, Lyft writes about its Trino upgrade story. The article is an exciting read to understand Trino internals and efficient flame graph usage in performance debugging. \nhttps://eng.lyft.com/trino-open-source-infrastructure-upgrading-at-lyft-83f26b099fa\nMalloy is a query language of data, an alternate to SQL. It is an exciting framework that I'm actively following, and looking forward to trying it out. The blog is an exciting read to understand the evaluation and design thinking of building a framework.  \nhttps://medium.com/@michaeltoy/designing-malloy-0-introduction-88b8809d75d0\nhttps://medium.com/@michaeltoy/designing-malloy-1-the-syntactic-shell-7216bcc9ffdf\nLearn how today\u2019s best data engineering and analytics leaders are staying ahead of the competition in our complete guide.\nDownload the modern data leader\u2019s playbook\nData Quality & testing frameworks play a vital role in establishing data contracts and healthy data exchange between the data producer & consumer. The author writes an excellent comparison blog on data quality & testing frameworks. \nhttps://servian.dev/data-quality-and-testing-frameworks-316c09436ab2\nThe metric layer is picking momentum with the dbt metric layer and the introduction of MetricFlow. The advantage of discoverable, sharable & reusable metric definition is significant in streamlining the analytical processing. The author discusses various metric query patterns and the logical layer to integrate the metric layer. \nhttps://prakasha.substack.com/p/the-metrics-layer-has-growing-up?s=r\nTwitter started their on-prem to Google BigQuery migration. The blog narrates the realized gain with the cloud migration and how the automated data ingestion framework reduced the initial productivity and onboarding hiccups. \nhttps://blog.twitter.com/engineering/en_us/topics/infrastructure/2022/scaling-data-access-by-moving-an-exabyte-of-data-to-google-cloud\nHere, RuddersStack examines the technical limitations behind the push to unbundle the CDP and assesses whether unbundling is the appropriate way to overcome these limitations.\nhttps://www.rudderstack.com/blog/the-future-of-customer-data-platforms-to-bundle-or-not-to-bundle\nRovio writes about its adoption of Druid and its need to write its custom Druid ingestion framework. In the past, I had tons of trouble with the Druid middle manager & Zombie tasks of unsuccessful segment commits. It is exciting to see Rovio's design without the Druid middle managers. \nHow did we find out about Druid? We came across some existing BI tools that supported pivot charts: Turnilo and Superset.\nI find this interesting quote in the blog; it highlights the importance of community integrations.\nhttps://medium.com/@Rovio_Tech/unlocking-interactive-dashboards-at-rovio-with-druid-and-spark-40f8fe6a0b05\nKafka or variation of Kafka protocol implementation becomes a defacto component in the data infrastructure. Adobe writes an educative blog that narrates the internals of Kafka producers. \nhttps://medium.com/adobetech/exploring-kafka-producers-internals-37411b647d0f\nIdempotency and event ordering guarantees are critical properties to understand in the event-driven architecture. The blog narrates these properties and walks through a few examples. \nhttps://www.cockroachlabs.com/blog/idempotency-and-ordering-in-event-driven-systems/\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-86", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nIt's a busy week in the dbt land, with Pedram Navid's blog post detailing the lack of clarity or transparency on the roadmap for dbt. It triggers exciting conversation in the data Twitter that leads to The response you deserve! It's a healthy sign of a robust community-driven system design in progress. \nhttps://pedram.substack.com/p/we-need-to-talk-about-dbt\nI know I'm probably late to talk about it, but \"Python in the browser\" is an exciting development. My First Impression Trying Python on Browser is an excellent follow-up on trying PyScript. \nhttps://engineering.anaconda.com/2022/04/welcome-pyscript.html\nA data catalog is essential for a collaborative analytical solution, but how do you choose one? Sarah writes about the spectrum of available data catalog solutions and their pros & cons. \nhttps://sarahsnewsletter.substack.com/p/choosing-a-data-catalog?s=r\nTwitter shared an exciting blog about the Twitter conversation with Wordle as a case study. It's a good reference article on telling stories with data analytics. \nhttps://blog.twitter.com/engineering/en_us/topics/insights/2022/understanding-twitter-conversations--a-wordle-case-study\nLior Solomon, VP of Data Engineering at Vimeo shares his own experience on The Data Engineering Show: What made him recently build a new data ops team? How do you operate a data stack that supports 85 billion events per month and 2 PBs of data? What does Fatal Attraction have to do with all of this?\nhttps://www.firebolt.io/blog/how-vimeo-keeps-data-intact-with-85b-events-per-month\nThe latency requirements bring unique challenges in adopting the prediction services. DoorDash writes about its client-side Caching to improve Feature Store performance by 70%.\nhttps://doordash.engineering/2022/05/03/how-we-applied-client-side-caching/\nThe importance of the data model is often an undervalued process. Data modeling is challenging since most of the process depends on individual experience and opinion, but following standard techniques like Data Vault can bridge the gap. Snowflake writes an exciting blog on data vault techniques in Snowflake. \nhttps://www.snowflake.com/blog/data-vault-technique-immutable-storage/\nStaying with the importance of data modeling, why should one care about data modeling? The author narrates the various aspects of a well-defined data model. \nhttps://towardsdatascience.com/what-is-well-modeled-data-for-analysis-28f73146bf96\nData maturity is rapidly becoming a matter of survival, but the modern data stack can be overwhelming. Here, RudderStack provides a helpful framework that places the tools of the modern stack in the context of a 4-stage journey to help you build the right stack at every stage.\nhttps://www.rudderstack.com/blog/a-practical-guide-to-the-modern-data-stack-the-data-maturity-journey\nIntuit writes about the challenges of too many features in the feature selection process and how Data X-ray data quality solutions help them. The blog narrates Data X-rays' automated data quality analysis of feature attributes analysis, feature selection analysis & feature pruning analysis. \nhttps://medium.com/intuit-engineering/data-x-ray-automated-data-quality-analysis-tool-streamlines-feature-selection-process-for-machine-9c4a93e76cb6\nWhatnot writes about tuning its data platform for speed and scale, focusing on three founding principles. \nBuild Modules, Not Monoliths\nDomains Own Their Data\nAutomate Platform Processes\nhttps://medium.com/whatnot-engineering/tuning-whatnots-data-platform-for-speed-and-scale-28d2b5993b42\nLearn how today\u2019s best data engineering and analytics leaders are staying ahead of the competition in our complete guide.\nDownload the modern data leader\u2019s playbook\nPinterest writes about HNSW (Hierarchical Navigable Small World graphs) streaming filters on top of its in-house search engine Manas. The streaming filtering abstracts away implementation details of how filtering is executed and relieves the client from the burden of over-fetch tuning.\nhttps://medium.com/pinterest-engineering/manas-hnsw-streaming-filters-351adf9ac1c4\nHow does data analytics translate into the day-to-day as a product manager? The article is excellent data for the product managers.\nhttps://engineering.backmarket.com/data-for-product-managers-part-1-2-fd2967333c00\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-85", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nMonzo writes about its Machine Learning stack built on the three principles.\nAutonomy\nFlexibility\nReuse over a rebuild.\nThe blog is an exciting read with the mix of Google Cloud for data science work and AWS for production deployment.\u00a0\nhttps://monzo.com/blog/2022/04/26/monzos-machine-learning-stack\nNetflix writes about Axion, its fact store that stores the large volume of high-quality data leveraged to compute offline. The blog is an exciting read pointing out the importance of high-quality data for Machine Learning applications and the long-standing challenges of accessing a small subset of data in the data lake systems. \nhttps://netflixtechblog.com/evolution-of-ml-fact-store-5941d3231762\nMeta writes about SQL notebook, combing the power of notebook and SQL. The blog narrates some of the enforcement challenges with CTE and how it uses the notebook-style cell reference design to make the code more modular. \nhttps://engineering.fb.com/2022/04/26/developer-tools/sql-notebooks/\nEtsy writes about the evolution of its recommendation engine over the years. The blog narrates the journey from a static pre-computed journey to a platform-centric approach. \nhttps://www.etsy.com/codeascraft/building-a-platform-for-serving-recommendations-at-etsy\nWatch how Firebolt's cloud data warehouse for engineers delivers sub-second analytics at the data lake scalehttps://www.firebolt.io/resources/cloud-data-warehouse-demo\nSentiment in a conversation is more complex than a movie or product review. The conversation sentiment rapidly changes with the context of the conversation for both the speakers. PayPal writes about how it approaches conversational sentiment analysis. \nhttps://medium.com/paypal-tech/words-all-the-way-down-conversational-sentiment-analysis-afe0165b84db\nThe LakeHouse capabilities are evolving fast, and it's hard to keep track of all the features. We've seen a few lake format comparison studies, and the author rightly pointed out a few corrections. \nUnlike the ugly benchmark war by a few systems in the past, I like the approach from Vinoth Chandar on collaboratively building the future of LakeHouse. \nOpen Call for the LakeHouse Formats\nIt will be excellent if these systems come together and build the OpenLakeHouse format. The data quality, data visualization, and data discovery systems can benefit from shared metadata from an OpenLakeHouse format. Historically these metadata are only used for query optimizers for the closed-loop system. With the LakeHouse format, we genuinely have an opportunity to build an ecosystem around metadata.  \nhttps://bytearray.io/corrections-in-data-lakehouse-table-format-comparisons-b72eb63ece32\nData maturity is rapidly becoming a matter of survival, but the modern data stack can be overwhelming. Here, RudderStack provides a helpful framework that places the tools of the modern stack in the context of a 4-stage journey to help you build the right stack at every stage.\nhttps://www.rudderstack.com/blog/a-practical-guide-to-the-modern-data-stack-the-data-maturity-journey\nPinterest writes about its optimizing strategy adopted in its logging infrastructure. The journey from adopting a round-robin strategy to writing to the Kafka partitions to the RandomPartition approach & static partition write approach is an exciting read. \nhttps://medium.com/@Pinterest_Engineering/optimizing-pinterests-data-ingestion-stack-findings-and-learnings-994fddb063bf\nIt reminds me of our earlier design of Slack's logging pipeline Murron. \nModernDataStack tries solving each niche data engineering challenge; however, it brings its problem of introducing a disjointed data workflow. The tweet summarizes the same.\nFindHotel reflects a similar pain point in its data ecosystem and writes how it integrates reliability metadata with Looker.\nMost of these tools store the results of the reliability tests in a database and expose them in a custom front-end application.\nHowever, the need to access another tool can become a problem. Data consumers are usually familiar with the BI tool. Having them open the BI tool in one tab, the data reliability tool in another tab, and perhaps other tools (data catalog, etc.) in different tabs, may have a bad impact on adoption and usage.\nhttps://blog.findhotel.net/enriching-looker-with-reliability-metadata-8a4aff6667cb\nData Storytelling is vital to influencing the decision-making process, and Spotify writes about strategies to adopt data storytelling. \nhttps://shopifyengineering.myshopify.com/blogs/engineering/data-storytelling-shopify\nLearn how today\u2019s best data engineering and analytics leaders are staying ahead of the competition in our exclusive guide.\nDownload the modern data leader\u2019s playbook\nA good data modeling process empowers the business to make data-driven decisions, enables curated self-service, and ensures scalability, reliability, and shared context. The Analytical Engineering Roundup recently wrote about Data Modeling for Collaboration. Mothership writes about its journey of adopting the data modeling process.\nhttps://medium.com/mothership/analytics-engineering-at-mothership-8d061b66bec3\nSwiggy writes about its end-to-end detection system for detecting anomalies for critical business metrics. Adopting an expert system module to add human input to detect rain or other local events is an exciting approach. \nhttps://bytes.swiggy.com/an-end-to-end-system-to-detect-and-explain-anomalies-in-operational-metrics-448bc74c700e\nThe data engineering world can't escape from the bundling and unbundling debate!! Hightouch writes The CDP as we know it is dead: Introducing the Unbundled CDP, and mParticle followup with Will the CDP be unbundled?\nThis complexity arises from the low-quality event tracking system, as pointed out.\nhttps://medium.com/@mkatz0630/will-the-cdp-be-unbundled-6e8308b2e0e1\nData visualization plays a vital role in experiment evaluation. Disney writes about automated visualization at the ad-hoc and platform levels to simplify the experiment evaluation. \nhttps://medium.com/disney-streaming/the-fine-art-of-visualizing-experiment-results-95a687b2bb0e\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/the-data-founder-story-shipyard", "title": "Data Engineering Weekly", "content": "\ud83c\udfd7 Data Founder: Blake Burch\n\ud83c\udfe2 Product: Shipyard\nHey everyone! I\u2019m Blake Burch, the data co-founder at Shipyard. We\u2019re a modern data orchestration platform that serves as the quickest way for data teams to launch, monitor, and share data workflows. With fully-hosted infrastructure, built-in security, dynamic scalability, and optional low-code templates, we\u2019re breaking down the barriers of entry so anyone on a data team can deploy resilient data pipelines in a matter of minutes.\nThis is the story of how our journey began.\nI started my journey in data by not touching data at all. In 2014, I was your everyday account manager at PMG, a digital advertising agency. We were a small crew of roughly 30 individuals serving high-profile brands like Travelocity, Sephora, OpenTable, and J.Crew. My days consisted of researching new keywords, creating new ads, and adjusting bids and budgets to maximize performance. All the work was Excel based and, as someone passionate about automation, I eventually started getting fed up with the manual work.\nI knew there had to be a way to take those repetitive daily tasks and cut myself out of the picture. I worked with the development team to automate data ingestion from advertising APIs to a database (Redshift at the time). Once in the database, I taught myself SQL (thanks SQL Zoo) so I could replicate the data I was manually pulling into Excel every day.\nOnce I had that logic in place, I needed to figure out how to execute that SQL programmatically and push updates directly to each advertising platform. It was time to learn Python. With my budding programming skills, I started creating scripts to perform my daily account management work and quickly realized I couldn't keep running things on my local machine. I needed a way to automate and monitor their performance.\nThat\u2019s where Eric Elsken, my co-founder comes in.\nEric was one of our best engineers and a frequent go-to when anyone needed something automated at the company. At the time, he was building out a solution that helped our analytics team automate Tableau extracts for our client dashboards. Soon after, he was tasked with automating web scraping for our client\u2019s websites. And then he had me pestering him with my solutions. It was in that moment he thought, \u201cWhat if instead of building these one-off use cases and being the company cron manager, I build a UI that lets anyone at the agency automate, monitor, and share any script they want?\u201d. So he did just that.\nWith the birth of a new platform, my scripts were automated for one client and started driving crazy growth. I\u2019m talking 300%+ more conversions on million dollar budgets. That growth, of course, spurred the need for every client to run these scripts with slight tweaks. That use case led us to building templates directly in the platform.\nThrough this growth cycle, I had the opportunity to build the Data Engineering, Data Innovation, and Data Activation teams from the ground up. Our main goal was to mix 1st party data with 3rd party advertising platform data to scale omni-channel digital marketing efforts like crazy using algorithms and automation. We pushed our new internal platform to the limits, systematically building out the same data sets for every client so we could rapidly experiment and implement winning solutions across clients with minimal effort. These solutions included:\nReacting to Social Influencer Trends in Real Time\nHourly Forecasting for Holidays\nUsing Live Stores Visits + User Location Modifiers to influence Store Traffic\nDynamic Ad Creation from Promotional Calendars\nQA Notification Bots for Account Best Practices\nScoring Inventory Based on Competitive Pricing\nAs we continued to expand our use cases across the organization, we realized there were three big gaps across our clients.\nFrom my experience, most data work is ubiquitous, no matter the tool or the industry. Despite this, hundreds of engineering hours gets wasted writing the same basic scripts to perform tasks like downloading and uploading data from common services in an efficient manner. I\u2019ve seen a few ways to get around this issue:\nSome teams manage their repetitive code by sharing snippets that can be copied and pasted around the organization. This makes the code easy to implement, but even easier to get out of sync when updates need to be made.\nMore advanced teams build out internal packages that can be imported into other scripts and updated from a central location. While this makes management easier, it\u2019s still difficult to verify where the package is being used and what impact an update will have.\nIn the end, most teams don\u2019t have a solution in place, so everyone codes from scratch every time.\nIn many ways, the existing data orchestration technology is prohibitively technical for the average data practitioner.\nTo get started, you have to sift through hours of documentation to learn how to either create a proprietary DAG file or how to mix proprietary platform logic with the business logic in your code.\nAfter building proof-of-concept workflows, these tools can take weeks of DevOps knowledge to get a production environment set up. Once servers are live, you\u2019ll inevitably run into scalability issues in a few months due to increased memory usage, unplanned query volume, or general package version conflicts. While the latter issue can be solved by teaching the data team Docker, that just adds one more layer of complexity.\nAs a result of this difficulty, I saw many data teams spending a majority of their time gathering, cleaning, and troubleshooting data rather than actually acting on it. That\u2019s why you see articles where 73% of company data goes unused or 87% of data projects never make it to production.\nA lot of data teams operate a black box where few people are aware of every data touchpoint and how the data gets used. Teams rely on fragmented tools to run data ingestion, transformation, report delivery, model deployment, and reverse ETL at very specific scheduled times without regard for the steps that need to occur before and after.\nThis results in three common issues:\nSomeone changes the underlying definition of a dataset which negatively impacts other data team member\u2019s models, dashboards, or automated scripts. This is isn\u2019t malicious - it\u2019s just the nature of not knowing what end products each data set is supporting.\nOne process fails, resulting in bad data that nobody notices until a business user finds it days or weeks later. At that point, there\u2019s a mad scramble to try and pinpoint when and where the issue occurred, all while business users loses trust in the data.\nData pipelines are artificially slow due to self-imposed bottlenecks of running fragmented systems one after the other at specific times.\nAfter seeing the value that the technology was able to create for our internal data team and our clients, we decided to spin off PMG\u2019s internal orchestration technology into its own separate entity - what today we call Shipyard. When we spun off, PMG was around 250 people with even bigger clients (Apple, Gap Inc., TikTok). These clients all relied on 98 solution templates and 1.8k unique data workflows that ran on an hourly or daily basis to power marketing campaigns.\nWhile we knew our technology was battle tested by some of the largest brands, we still had to overcome the hurdle of turning an internal technology into something that anyone could pick up and use. After 6 months of interviews, testing, and redesigns, we had our first iteration of the product in 2020 that directly addressed the problems we saw in the market.\nOut of the gate, we built 60+ low-code templates to perform simple actions like downloading data, uploading data, executing queries, or sending messages against all of the major tools a data team might use (BigQuery, Snowflake, S3, GCS, Slack, Email, Fivetran, dbt, etc.). All you need to do is provide your credentials and fill out a form to get started. Since we know that the data ecosystem values transparency, we designed our templates as open source Python CLIs so teams can see what\u2019s happening under the hood and make adjustments where needed.\nSince we were in the weeds ourselves, we know that our templates that will only get you 80% of the way there. Once you inevitably have a proprietary process that needs to be repeated, you can build your own template in Shipyard for others to use. Code can be synced directly with a Github repository and branch of your choice to keep everything up-to-date. Our advantage over creating an importable package is that you gain visibility into every time a template is added to a workflow, helping you track performance and usage at a high level. Plus, the interface makes it easy for anyone to run your code with slight tweaks without needing to code themselves.\nTo combat the complexity we noticed, we designed Shipyard with the goal of helping data practitioners build and deploy new data workflows in 10 minutes or less. We accomplish this by making the actual build process as flexible as possible:\nWe\u2019re a fully-hosted and serverless, scaling dynamically as you automate more workflows.\nYou can build workflows with a drag-and-drop visual editor or with a YAML editor.\nWorkflows can be as simple or complex as needed with options for branching, converging, and conditional logic.\nWorkflows can be a mix of our low-code templates, proprietary low-code templates that you build, or your own custom code written in Python, Node, or Bash.\nEvery task in a workflow is containerized so language and package conflicts will never occur.\nWorkflows can be triggered manually, on a schedule, or with event-driven webhooks.\nIn the event of errors, we\u2019ll handle automatic retries and send notifications to the appropriate team.\nAll of these efforts combined let us help data professionals focus on what they\u2019re best at - building solutions with data.\nBy breaking down the barriers to entry, everyone from the Data Analyst to the Data Engineer can build data workflows together in a single unified platform. Every interaction can be connected together, giving your team the full end-to-end picture of how data is used.\nWith detailed notifications, metadata, and logging, you gain visibility into the specific location of any error for further investigation. Plus, with the visual overview of how your data workflow is designed, you\u2019ll know exactly what downstream actions your changes might affect and who built them. This gives teams the confidence to make updates without fear of unknown breakages.\nFor example, let\u2019s say that your dbt transformation steps break for finance data tables. Shipyard can automatically send proactive alerts to both the data team and the finance team so everyone is aware of the issue. Since every touchpoint is connected, downstream financial forecast models, reports, and dashboards will be halted in place so the bad data never gets delivered.\nToday, we\u2019re expanding our templates to cover more tools in the modern data stack while simultaneously improving the developer experience. We service high-growth data teams at companies in almost every industry. This year, we\u2019re planning on doubling our team size with key hires to invest in our product development and marketing.\nWant to build data workflows like a 10x engineer? You can get started for free and try out everything that Shipyard has to offer.\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. The data founder story contents provided by the featuring data founders in this article and Data Engineering Weekly is not responsible for any compliance with applicable laws, rules, and regulations."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-84", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nI gave a talk about the emerging trends in data engineering last October at the CrunchConf. The video got published now. \nSpeaker Deck : https://speakerdeck.com/vananth22/back-to-the-future-emerging-trends-in-data-engineering\nMeta writes about Looper, An AI platform to support the complete machine learning lifecycle from model training, deployment, and inference all the way to evaluation and tuning of products.\nHello, again Bundling vs. UnBundling\nA coupling of things stands out in the blog,\nIt is a declarative AI system, which means that product engineers only need to declare the functionality they want. The system fills in the software implementation based on the declaration.\nWhile other AI platforms often perform inference offline in batch mode, Looper operates in real-time.\nhttps://ai.facebook.com/blog/looper-meta-ai-optimization-platform-for-engineers/\nCustomers, competitors, and the economy's direction are unpredictable in their own way. Experimentation is vital for testing the product change to build evidence to drive significant decisions. Lyft writes an exciting blog on the challenges of supporting the culture of experimentation.\nhttps://eng.lyft.com/challenges-in-experimentation-be9ab98a7ef4\nThe scheduler is a core part of data transformation. dbt writes about the scalability challenges with dbt and the recent improvements. I'm looking forward to part 2 of this to understand dbt cloud scheduler more!!.\nhttps://www.getdbt.com/blog/a-good-problem-to-have/\nIn this blog, we argue that performance is actually not about performance at all! We\u2019ll contextualize real-world customer needs for data warehouse performance, and we\u2019ll even make a bold prediction about the future of data warehousing (preview - it\u2019s all about the new CDW).\nhttps://www.firebolt.io/blog/future-of-performance-is-not-about-performance\nZalando writes about the architecture and tooling behind its ML platform. The ZFlow on top of the AWS step function and the custom web interface on top of\u00a0Backstage\u00a0looks interesting.\u00a0\nhttps://engineering.zalando.com/posts/2022/04/zalando-machine-learning-platform.html\nDoorDash writes about its expansive merchant selection to onboard high-value merchants to ensure the selection in every market matches customer demand. The model strategy to train the customer preference to the merchant onboard looks interesting, but I wonder how the team maintains algorithm fairness? Any potential AI bias can lead to social imbalance, but the blog does not mention how it handles algorithm fairness. \nhttps://doordash.engineering/2022/04/19/building-merchant-selection/\nData quality issues are universal, and dealing with them at scale is toil. Join The Data Stack Show on Wednesday at 10 PT for a live recording with some of the brightest minds working to solve the problem. Leaders at Bigeye, Great Expectations, Lightup, and Metaplane will discuss why data quality is so challenging and how to fix it.\nhttps://datastackshow.com/live-data-quality/\nBlinkit writes about its usage of Redash and narrates the challenges of running the SQL dashboarding tools and how Blinkit effectively solved them. \nhttps://lambda.blinkit.com/evolution-of-redash-at-blinkit-fb50a64770bf\nBuilding trust in data in an organization is the most crucial function of a data team. The author compares the broken window theory with the data testing function. \nhttps://mikkeldengsoe.substack.com/p/broken-windows\nA perfect labeled data is often hard to achieve with cost and the human effort involved. Yet, label data is critical for the supervised learning task. The author discusses the approaches to take when there is not enough labeled data in a three-part series. \nLearning with not Enough Data Part 1: Semi-Supervised Learning\nLearning with not Enough Data Part 2: Active Learning\nLearning with not Enough Data Part 3: Data Generation\nLearn how today\u2019s best data engineering and analytics leaders are staying ahead of the competition in our exclusive guide.\nDownload the modern data leader\u2019s playbook\nAn exciting article from booking.com discussing the danger of tracking users who can't be in the treatment category (called\u00a0overtracking) affects the variance of the experimentation metrics and dilutes the treatment effect, making its detection harder.\nhttps://booking.ai/overtracking-and-trigger-analysis-how-to-reduce-sample-sizes-and-increase-the-sensitivity-of-71755bad0e5f\nMany companies adopt the product over project strategy and treat the internal platform as a product. The author discusses the role of a product manager in building ML-based products. \nhttps://meryam.substack.com/p/whats-the-role-of-an-ml-pm\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/the-data-founder-story-joining-as", "title": "Data Engineering Weekly", "content": "\ud83c\udfd7 Data Founder: David Jayatillake\n\ud83c\udfe2 Product: Avora\nThis is the story of how I became a Cofounder at Avora, an AI-powered business observability platform. Avora enables businesses to respond, quickly and with context, to unexpected changes in their metrics.\nI've been a Data practitioner for about 12 years, working across industries such as eCommerce and FinTech. I've worked heavily in Analytics Engineering and Analytics, but broadly in Data.\nDuring my time at Worldpay, I led a small team focused on building a system to predict our main metrics. This system was also able to explain why our metrics changed vs the past or forecasting. It was a long and expensive process and involved me building a team to build, deploy and maintain it. Whilst it was successful and well-used, it wasn't generic, but was highly focused on a specific part of the payments system. A generic version of this system could be hugely valuable in Analytics: this is Root Cause Analysis.\nThis team was the first analytical team I built. I have since led and built multi-discipline data teams at Elevate Credit, Lyst and Ruby Labs.\nWhilst at Elevate Credit, I attended Snowflake's first ever in-person conference in London. One of the presentations I attended was by Avora, around their then full-stack BI solution.\nAt the time I felt it could be of real benefit to Elevate Credit, as we had a very small data team with limited resources. The modern data stack did not exist yet, with even Fivetran being a scrappy startup at this point.\nI managed to catch up with the presenter from Avora, and exchanged contacts to learn more later. This led to Avora coming in to Elevate to pitch, with Ricky Thomas, my now cofounder, leading. The boardroom was hostile, with people casting aspersions about whether Avora could work! Ricky managed to pitch well about the platform and team and we later signed with Avora.\nAs a customer of Avora, I was always impressed by the team and delivery. I was glad to have brought them in during my time at Elevate. Ricky and I stayed in touch and regularly discussed new features and trends in the industry.\nI started advising Avora soon after this, on product features and UX. I was one of the first to see the Anomaly Detection (AD) and Root Cause Analysis (RCA) products pre-launch. For the last couple of years, I've advised Avora on further enhancements to these new products, which are the basis of the NewCo I have joined.\nAvora has needed a metrics layer, in order to provide AD and RCA on metric trends. This has been an in-house metrics layer that is simple but powerful for customer needs; allowing for metric relationships and quick time to value. At the end of 2021, I had been guiding them towards adopting the dbt metrics layer. I believed then, and even more firmly now, that shared and standardised metrics layers will be widely adopted in the future. The potential for serving a wider customer base, and reducing onboarding friction, is a huge opportunity for Avora.\nIn parallel to my advisory role, I also acted as a customer reference for Venture Capital funds. This has turned out to be very useful, given, as a cofounder, I'm currently speaking to VCs to raise a seed round!\nAt the end of last year, it wasn't the right time to join Avora for either me or Avora. My personal circumstances changed at the start of March, and with good customer acquisition and signs of product-market fit, the same was true for Avora!\nI have joined as cofounder to lead us towards:\nintegrations with metrics layers\ntechnical partnerships in the Modern Data Stack\ncommercial partnerships\ncontent and community engagement\nproduct-led growth\nI believe there is a missing tactical layer in how data serves the strategic apex:\nAt the strategic layer, we now have tools enabling self-serve metrics and curated analytical research. This is difficult to automate without the advent of AGI! It's this consultative work that really fulfils humans in a data team, and empowers them to impact their org.\nAt the tactical layer of the apex, we have an opportunity to augment decisions that need to be made faster. These need human input, but with the advantage of AI-powered context and monitoring. This is where Avora fits in.\nAt the operational layer, decisions need to be made in their thousands and millions daily as well as possibly in real-time.\nAvora is a great opportunity for me to be a cofounder, with a level of safety in having an experienced cofounder alongside me. More importantly, I really believe the time is right for our products to shine, and that they are sorely needed:\nThe Modern Data Stack is enabling a level of data maturity that wasn't possible even two years ago\nData consumers are hungry for richer actionable insights\nThe scale of complexity and change in data mean data teams are struggling using BI tools, to do the workload Avora automates\nThere is a real problem in the industry in terms of a shortage of engineers and data people to build internal teams\nI believe our products help fulfil some of these needs. We have the foundational team to win, and delivering on our roadmap will help our customers win even bigger with data.\nDon't take it from us! You can try Avora for free, no-strings-attached, for 14 days. Sign up on our website."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-83", "title": "Data Engineering Weekly", "content": "Live on April 20th, the Director of Analytics from Joybird, a La-Z-Boy company, joins RudderStack, Snowflake, and Iterable to detail how his team retooled its data stack and reduced their time spent building integrations and managing data pipelines by 93%.\nhttps://www.rudderstack.com/video-library/joybird-retools-their-customer-data-stack-using-rudderstack-snowflake-and-iterable/\nData is the new oil, but how much are companies paying to extract the value out of the data? The author compares the data practitioners' salaries in FAANG companies.\nhttps://medium.com/@mikldd/data-salaries-at-faang-companies-in-2022-29d5b56b2428\nEvery time we flirt with some NoSQL alternative, we rebound to SQL. However, is SQL the appropriate abstraction for the emerging semantic & metrics layer? The author raises some interesting questions about the next-gen business query language comparing dbt & Malloy.\nhttps://benn.substack.com/p/has-sql-gone-too-far\nIs the data warehouse broken? Does the immutable data warehouse the cure? Though the article titled the immutable data warehouse, I feel the focus point is the contract-driven collaborative data engineering between the data producers and the consumers. \nhttps://towardsdatascience.com/is-the-modern-data-warehouse-broken-1c9cbfddec3e\nFollowing up on the previous article, this is an interesting rebuttal to the \u201cdata warehouse is dead\u201d argument. There have been several rather significant attempts at exterminating and bypassing a data warehouse from the data lake to date mesh. The author writes a historical walkthrough of attempts and points out the data warehouse's resiliency. \nhttps://www.linkedin.com/pulse/buying-data-warehouse-rip-bill-inmon/\nHow do you perform operations on an array of arrays? Or on multiple correlated arrays in a table? Do you keep on UNNESTing, or is there a more easy-to-use, elegant approach to querying it? If anyone can help you navigate the world of arrays with ease \u2013 all using SQL and without the need to define new types of objects \u2013 it\u2019s Octavian Zarzu. \nhttps://www.firebolt.io/blog/sql-thinking-in-lambdas\nLakeHouse architecture is evolving and getting fast adoption. The article compares the lakehouse systems Apache Hudi, Apache Iceberg & Databricks Delta lake. The recommendation is as follows.\nUse Apache Iceberg if your primary pain point is the Hive metastore\nUse Apache Hudi if you need high flexibility in handling mutable & immutable data.\u00a0\nIf you're a Databricks/ Spark shop, use Delta lake.\u00a0\nhttps://lakefs.io/hudi-iceberg-and-delta-lake-data-lake-table-formats-compared/\nBenn Stancil quoted the metric layer as the missing piece of the modern data stack. Fast forward, we got multiple commercial and open-source metric layer systems emerging. The author writes a comparative study of the metrics layers. Please suggest to the sheet if you spot any inaccuracy.\nhttps://medium.com/@vfisa/an-overview-of-metric-layer-offerings-a9ddcffb446e\nOrganizations spend 40% of their time tackling data downtime. Learn how to tackle data trust with best practices from today's data leaders.\nDownload the Big Book of Data Observability\nEvery technology & process breaks at scale. How to create a continuously sustainable ML platform? DoorDash writes about three principles to follow.\nDream Big, Start Small\n1% better every day\nCustomer Obsession\nhttps://doordash.engineering/2022/04/12/3-principles-for-building-an-ml-platform/\nPreparing and managing features has been one of the most time-consuming parts of operating our ML applications at scale. Feature stores are emerging as the solution to manage the ML feature data. LinkedIn open sources its feature store named Feathr. \nhttps://engineering.linkedin.com/blog/2022/open-sourcing-feathr---linkedin-s-feature-store-for-productive-m\nThe recommender system has a wide application; many publications focus on the recommender model, but these scores aren\u2019t enough to serve users\u2019 recommendations in most real-world contexts. The author writes about 4-stages of the recommender system.\nThe talk on the same is an interesting watch. \nhttps://medium.com/nvidia-merlin/recommender-systems-not-just-recommender-models-485c161c755e\nRudderStack explores how the unique technical challenges that come along with customer data make lakehouse architecture a natural fit for its storage and processing.\nhttps://www.rudderstack.com/blog/how-does-the-data-lakehouse-enhance-the-customer-data-stack\nThe metrics layer, semantic layer, or denormalized flat table; what does that all mean to the visualization layer? Preset writes an exciting blog comparing query-centric, semantic-layer-centric & data-centric visualization and how Apache Superset approaches the data-centric visualization. \nhttps://preset.io/blog/dataset-centric-visualization/\nUber writes about its usage of Presto to query Kafka. The article narrates the challenges of running Presto Connectors for Kafka and some of the guardrails to limit the network congestion from the Presto queries. Adhoc querying the streaming data sources underinvested in most companies, and thrilled to see the Presto improvements for Kafka. \nAn interesting talk to watch on the same topic.\nhttps://eng.uber.com/presto-on-apache-kafka-at-uber-scale/\nS3 is the defacto data storage for bulk data processing in the AWS cloud. AWS writes about a few tuning techniques to optimize EMR data processing on S3. \nhttps://aws.amazon.com/blogs/big-data/best-practices-to-optimize-data-access-performance-from-amazon-emr-and-aws-glue-to-amazon-s3/\nAlluxio implements a virtual distributed file system that allows accessing independent large data stores with compute engines like Hadoop or Spark through a single interface. Alluxio writes about how running external systems like Zookeeper causes inconsistency and its adoption of Raft implementation from Apache Ratis.\nhttps://www.alluxio.io/blog/from-zookeeper-to-raft-how-alluxio-stores-file-system-state-with-high-availability-and-fault-tolerance/\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/the-data-founder-story-i-founded", "title": "Data Engineering Weekly", "content": "\ud83c\udfd7 Data Founder: Josh Temple\n\ud83c\udfe2 Product: Spectacles\nThis is the story of how Dylan Baker and I founded and bootstrapped Spectacles, continuous integration, and DevOps suite for Looker developers and admins. 45% of Looker changes have unseen errors, so the best data teams rely on Spectacles to check their work (over 10k Pull Requests worth of work!) and ship bug-free code, faster.\nThey say that choosing a co-founder is the most important decision you'll make when starting a business. Choosing a co-founder is kind of like getting married. There's a legally binding contract, hundreds of high-stakes decisions, and sometimes you get texts from them in the middle of the night. So why did I decide to start a company with someone I'd only spoken with for 30 minutes at a bar? Sounds like the start of the Tinder Swindler, right?\nLet's rewind to the spring of 2019, when \"Co-vid\" sounded more like a bad Netflix competitor than a world-altering pandemic.\nI was sitting in the audience for a panel discussion at a Looker meetup in NYC. Dylan Baker, a highly regarded Looker consultant, was on stage, lamenting, \"I would pay someone to build this, my clients would pay someone to build this. Please, somebody build this!\"\nThis got my attention. If you'd like to start a company someday, here's a hint: if someone tells you they'll pay for you to solve their problem, listen up! And if they'll pay you now to build it later, even better.\nDylan was venting about a frustrating experience that developers in Looker encounter every day: the drudgery of manually testing dimensions and measures from the Explore page to make sure they don\u2019t have SQL errors. As a data analytics lead and Looker developer, I'd been doing a lot of this. And I had an idea for how we might automate away this tedious task.\nI nervously approached Dylan at the happy hour after the meetup. I said, \"I think we could build that tool you described!\" We talked about how automated testing, or continuous integration, was taking off in the software engineering community, but hadn't been adopted much by data teams. We talked about how Looker was investing more and more in their API and platform strategy.\nSoon, the genesis of an idea emerged: a continuous integration tool for Looker that automatically tests LookML for SQL errors and content validation issues.\nWe agreed we'd submit an abstract to Looker's upcoming JOIN conference. Would continuous integration for Looker be compelling to anyone else?\nWe were thrilled when Looker accepted our abstract a few months later. But then reality set in: we actually needed to build this tool. Unlike the founder fable where two college kids sit across from each other in a dorm room, coding for 36 hours straight with nothing but cold pizza and Mountain Dew to sustain them, Dylan and I lived nearly 3,500 miles apart\u2014me in NYC, and Dylan in London.\nToday, most of us are familiar with the barriers and blessings of remote work. But in those dreamy, pre-pandemic days, working remotely on nights and weekends to develop a product across GitHub Pull Requests and Slack messages felt exciting and futuristic. This was true remote, globalized work! Technology without borders!\nBetween code reviews and pairing sessions, Dylan and I began to get to know each other better. We compared fantasy football draft algorithms, we debated the correct spelling of \"organisation,\" we reviewed the Hamilton soundtrack. I learned that Dylan has no self control when it comes to buying website domains. And slowly but surely, we became less like LinkedIn connections and more like friends.\nWe came up with the name Spectacles after browsing unclaimed names for Python packages. As humongous nerds, we liked the pun that Spectacles could help you \"Look\" (see: Looker) better. Naming things is the hardest problem in engineering, and we found this to be true. All of our other ideas didn't pass muster (\"Looker Lawyer\" being a particularly egregious example).\nThe title slide from our JOIN 2019 presentation.\nThat fall, we flew to San Francisco for Looker's annual conference with nothing but a dream and a CLI in our pocket. At each networking event and happy hour, we pitched people on Spectacles. I couldn't stop smiling after one of Looker's first engineers said to me, \"I've been telling people we should do this in Looker for years!\"\nBut my heart sank as I walked to our designated spot for the presentation\u2014a small room with about 30 seats, tucked away in a far-flung corner of the convention center. I remember thinking to myself, \"I guess our abstract title, 'Develop LookML Like a Software Engineer: Continuous Integration for Looker' didn't exactly dazzle the conference organizers.\" (A few years later, I can only hope I've gotten better at marketing and headline writing).\nDespite our best efforts to prematurely bomb our talk attendance with a boring title, people turned out in droves to listen. As our start time neared, there was standing room only, and there were rows of people lining up outside the door to listen in. I turned to Dylan and said, \"Okay, this is clearly a topic people care about. There might be something here...\"\nAfterwards, we collected a list of emails of people who were interested in testing Spectacles. Many of these teams became alpha customers, and are still loyal customers today. We agreed we needed to make Spectacles a reality, not as a command-line utility, but as a fully-fledged software product.\nOver the past three years, Dylan, myself, and Tom (our 3rd co-founder), built on the original Spectacles CLI to create an easy-to-setup SaaS application that integrates with Looker to validate LookML and squash bugs before they can escape to production.\nToday, Spectacles gives hundreds of Looker developers at 50+ companies peace of mind as they ship high-quality LookML. We've worked with teams of two and teams of 100 to solve countless LookML issues.\nLooking back, it's been a wild ride. Dylan moved to Toronto, then back to London, Tom moved to Stockholm. Even though there's an Atlantic Ocean between us, I'm lucky to have them as partners. Spectacles is a testament to the power of remote entrepreneurship. Should you be cautious about starting a company with an Internet friend? Certainly! But the data space is growing faster than ever and is filled with wonderful people with brilliant ideas. The term \"modern data stack\" already feels pass\u00e9\u2014what a time to be alive!\nDon't take it from us! You can try Spectacles for free, no-strings-attached, for 14 days. Sign up on our website. Or check out this 2-minute video on how Spectacles works.\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. The data founder story contents provided by the featuring data founders in this article and Data Engineering Weekly is not responsible for any compliance with applicable laws, rules, and regulations."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-82", "title": "Data Engineering Weekly", "content": "Live on April 20th, the Director of Analytics from Joybird, a La-Z-Boy company, joins RudderStack, Snowflake, and Iterable to detail how his team retooled its data stack and reduced their time spent building integrations and managing data pipelines by 93%.\nhttps://www.rudderstack.com/video-library/joybird-retools-their-customer-data-stack-using-rudderstack-snowflake-and-iterable/\nLet\u2019s start this week with an informative Twitter thread from Wes Kao.\nWhat is the next stage of disruption in the data ecosystem? The authors predicted Self-serving AI/ Ml infrastructure, \"Data Contracts,\" decoupling data infrastructure where the distinction between the data warehouse and data lake is becoming increasingly obscure. Streamlining became mainstream adoption and the standardization of data practices. \nI'm pretty bullish on \"Data Contracts,\" which will significantly impact the data landscape. It's an exciting time to be in data engineering. \nhttps://www.linkedin.com/pulse/modern-data-stack-looking-crystal-ball-apoorva-pandhi/\nThe promise of the metrics layer as a semantic abstraction between the storage & compute to define the metrics once and reuse them across the data ecosystem is groundbreaking. Transform took its first step in that mission by open-sourcing MetricFlow, an open-source metrics framework. \nhttps://blog.transform.co/product-news/introducing-metricflow-your-powerful-open-source-metrics-framework/\nGithub: https://github.com/transform-data/metricflow\nThe barrier to entry for building & operating the real-time infrastructure with high reliability is still rigid. This week, Singularity Data's open-source RisingWave, a cloud-native streaming database, continues the shower of the open-source announcement. \nhttps://singularity-data.com/blog/risingwave-A-Cloud-Native-Streaming-Database/\nGithub: https://github.com/singularity-data/risingwave\nShopify writes about Merlin, its Machine Learning platform on top of open-source technologies such as Kubernetes, Ray, Airflow, Oozie & Jupyter Notebook. It's exciting to see the increasing adoption of Ray, which is something I'm looking to try this month. \nhttps://shopifyengineering.myshopify.com/blogs/engineering/merlin-shopify-machine-learning-platform\nWhat does a tech stack that always needs to be at the forefront of technology look like? Roy Miara from Explorium talks about building data products for the audience that can\u2019t be fooled \u2013 data engineers.\nhttps://www.firebolt.io/blog/building-data-products-for-data-engineers\nThe long-tail events are always challenging in computing & prediction. DoorDash writes about how the long-tail prediction impacts the delivery estimates and uses gamma distribution to improve the model performance. \nhttps://doordash.engineering/2022/04/06/using-gamma-distribution-to-improve-long-tail-event-predictions/\nI'm thrilled to read the LinkedIn post about an explainable AI-driven recommendation system. The goal of the analytics function is to give a context-rich narration to empower decision making, not just metrics & dashboard. I have written about it in the past.\nWhenever I raise the question, it is always the job of the data catalog as a documentation solution. I wouldn't be surprised if the next generation of startups focused on explainable, news feed-driven business analytics. \nhttps://engineering.linkedin.com/blog/2022/the-journey-to-build-an-explainable-ai-driven-recommendation-sys\nLast month we went through multiple discussions of bundling and unbundling, and sooner we started to see a few M&A in the data ecosystem as a sign of consolidation. However, the core of the problem is an efficient & programmatic approach to managing data assets and asset lifecycle. Dagster writes about its approach to software-defined assets and how it accelerates data management efficiency. \nhttps://dagster.io/blog/software-defined-assets\nA typical AI project is a cross-functional effort from the incubation to operating in production. The author discusses the skills required to be an AI Technical Product Manager (TPM).\nhttps://medium.com/data-science-at-microsoft/the-role-of-a-technical-program-manager-in-ai-projects-8f1ff41905b0\nJoin 3 virtual keynotes and 3 city stops, to learn how data leaders are tackling the biggest challenges in data, from building more reliable stacks to hiring top talent for your team.\nRegister Now: The Impact Tour\nSome great practical tips on deploying deep learning applications in production start with an iterative development model, leveraging domain-specific feedback loops, a human-in-the-loop review process, etc. \nhttps://medium.com/aquarium-learning/lessons-from-deploying-deep-learning-to-production-9b7a3576881d\nThere is always a trade-off between correctness and speed in a data pipeline. The author explains why data quality is more than SQL queries and the trade-off between operational reliability and data quality. \nhttps://medium.com/coriers/why-building-data-reliability-systems-is-hard-e0bf25c5ee36\nData engineering is changing as technologies advance, and new roles emerge. Alex Dovenmuehle breaks down the differences between analytics engineers and data engineers, and he outlines the benefits of the two roles working together.\nhttps://www.rudderstack.com/blog/analytics-engineering-vs-data-engineering\nUber writes about essential components to enable security features on a Kafka cluster and how it secures it. The incremental rollout of the security features for 500+ Kafka topics and the performance tuning of the Kafka clusters are some of the exciting system design reading. \nhttps://eng.uber.com/securing-kafka-infrastructure-at-uber/\nHudi\u00a0is a rich platform to build streaming data lakes with incremental data pipelines on a self-managing database layer while being optimized for lake engines and regular batch processing. Halodoc writes an exciting blog sharing its experience in adopting and optimizing the Apache Hudi lakehouse infrastructure.\u00a0\nhttps://blogs.halodoc.io/key-learnings-on-using-apache-hudi-in-building-lakehouse-architecture-halodoc/\nCost optimization is always top of the mind for most data teams. Redpanda publishes the performance of Graviton 2 using Redpanda as a target application, with results that show Arm at about a 20% price/performance advantage over Intel instances for a high-throughput message ingestion scenario.\nhttps://redpanda.com/blog/aws-graviton-2-arm-vs-x86-comparison/\nAn outstanding data-driven finding of what skillsets the data engineers require: the author extracted 550 United States data engineer jobs from indeed.com and did some quick analyses using job description, location, and salary range.\nhttps://www.linkedin.com/pulse/what-skills-do-you-need-become-data-engineer-peng-wang/\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/the-data-founder-story-the-subtle", "title": "Data Engineering Weekly", "content": "Kalin Naidoo\nSubtle.how\nMy name is Kalin Naidoo, and I am a data founder.\nI am a database developer by trade and have been working as a database developer since 2014. I am creating a trusted workplace application for people of diverse backgrounds leveraging available disclosed data.\nWith my professional domain expertise in database development, I think it will be something of value with sourcing the disclosed workforce data and standardizing it for Subtle customers to be able to compare companies against each other. I also have personal domain experience.\nAt a previous job I experienced racial discrimination. My manager at the time would pick on myself and another diverse coworker for taking a 5-minute coffee break in the morning and began asking me to do things, not in my job description.\nI felt uncomfortable saying no, and to be safe; I started saving emails from him where he was asking for me to work out of regular work hours. He asked me to work off the books and told me he would give me back time later in the month. I waited until I was there for an entire year so it wouldn't look bad on my resume and gave my two weeks notice. \nThe day I put in my two-week notice, I spoke to HR and gave them all of my emails. I told how uncomfortable he made the diverse staff under him, and I was escorted out of the office.\nI realized that day, that HR works for the company, not the employees. I know people need what I am making because I am one of those people.\nA study done by a UK company called the vault highlighted the problem as a trust gap. They found that roughly (37%) of US office workers believed their organization would brush aside at least one form of workplace misconduct if it were likely to impact profit or reputation. What was more troubling was (30%) of US HR and compliance DMs surveyed believed their organization would look to ignore at least one form of workplace misconduct if it was likely to impact profits or reputation.\nLinks:\nhttps://f.hubspotusercontent20.net/hubfs/4979575/Whitepapers/The Trust Gap Report.pdf\nSubtle is a mobile application and social network that helps consumers, employees, and employment seekers of diverse backgrounds measure corporate authenticity towards diversity, equity, and inclusion by using the latest news, disclosed workforce data, and employee reviews.\nA centralized repository where users can get all related and relevant data in one place. Our goal is to remove the fluff, normalize the data and allow consumers to compare companies to each other and get a rough estimate of the companies to stack up against each other and how they have done against the leaders in industry and sector.\nWe offer a high-level summary of authenticity and access to underlying datasets as to why. EEO1 insights allow customers to see if people of color are leaving at higher rates and if demographic is being promoted to managerial roles at higher or lower rates.\nThe EEO1 report is a demographic report. All companies with 100 employees have to report to the Equal Employment Opportunity Commission (EEOC), which breaks the company\u2019s demographics down to 10 job categories and eight racial demographics.\nIt is something companies fought to keep from disclosing in the past but have recently begun to disclose it on their own after the murder of George Floyd. Subtle is a community where consumers can lean on each other for advice and share lived experiences.\nSubtle is a resource + tool to help people of diverse backgrounds navigate and succeed in corporate America. But more than that, Subtle is a social movement. There is a change in mindset that\u2019s taking place, a greater recognition that we can do better.\nIt\u2019s very important for us to take the momentum that has been created as a society, as a country, and say let\u2019s use this to finally have an impact.- Obama after the George Floyd murder.\nOur big audacious goal is to make corporate disclosures of EE01 report standard practice for all companies.\nOne of the reasons I know people need what we are making is because of the moment we are living in. With Covid and the Great Resignation, people are finally sick of being mistreated. They are over being discriminated against at work and have decided there are better opportunities than the ones they are living with now.\nAn MIT study found that toxic culture in the workplace was driving the great resignation and that it was not just about not a pay increase. Another reason why the moment is now is that before the murder of George Floyd, it was taboo to discuss race and discrimination, especially in the workplace.\nLinks:\nhttps://www.rev.com/blog/transcripts/barack-obama-speech-transcript-on-george-floyd-death-protests\nhttps://sloanreview.mit.edu/article/toxic-culture-is-driving-the-great-resignation/\nDoing research on corporations and the company culture is difficult and time-consuming.\nThe frustrating part is that it's still not easy to navigate diversity and discrimination on the current solutions like Glassdoor, Teamblind, and Fishbowl. These websites cover racial discrimination but more so as an afterthought.\nBefore I take a job, I do my research on the company. I usually start on the company\u2019s website and look for their diversity and inclusion page to see if they care about diversity. Then I check the reviews to see what people are saying about the company but usually, there are not too many race-related reviews.\nGlassdoor failed me when I researched the company I mentioned above. I recently read a story from the Washington Post that helped me understand I was not alone, and this research was commonplace for people of color. It described the lengthy process people of color started to vet a company before accepting a position.\nLinks:\nhttps://www.washingtonpost.com/business/2021/02/18/millennial-genz-workplace-diversity-equity-inclusion/\nWith Subtle, we are creating a one-stop-shop that captures each part of the research phase a person with a diverse background might take.\nFirst, we gather existing corporate news a person would find when googling a company during the research phase.\nSecond, we gather the diversity and workforce metrics an employment seeker would find when going to a company's website. This part of the process can be tricky because it\u2019s hard to tell how the company being researched is doing against others in the space without a baseline.\nFinally, we create a place for employees and past employees to leave reviews on companies they used to work for. Employment seekers can navigate this page and find relevant reviews that could impact their experience at their future place of work. Previously, said employment seeker would check Glassdoor reviews, but if the reviews didn\u2019t explicitly say racism or discrimination, they could be hard to find.\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. The data founder story contents provided by the featuring data founders in this article and Data Engineering Weekly is not responsible for any compliance with applicable laws, rules, and regulations."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-81", "title": "Data Engineering Weekly", "content": "RudderStack Provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\n\nThe tweet triggered some exciting conversations, where George Xing pointed me to one of his blogs about building data-driven organizations. The blog discusses the broad challenges of adopting the data in an organization. Are we making decisions differently based on data? What are the challenges to including data in the decision-making process? How do operationalize a better decision-making process? \nhttps://georgexing.substack.com/p/what-it-means-to-be-data-driven\nhttps://georgexing.substack.com/p/why-organizations-fail-to-make-data\nhttps://georgexing.substack.com/p/building-data-driven-organizations\nI write about the disjoined data workflow with model and task approach of data orchestration with Bundling Vs. UnBundling: The Tale of Airflow Operator and dbt Ref. \nI'm happy to see the conversation started in the dbt community even before the article in this thread. https://discourse.getdbt.com/t/representing-non-sql-models-in-a-dbt-dag/2083.\nThe author summarizes the need for running tasks and the model and points to a promising tool fal.I\u2019m excited to see what is coming out of the fal team.\nhttps://andyreagan.medium.com/non-sql-in-your-dbt-pipeline-c7cef2091619\nLyft writes about democratizing distributed computing through Kubernetes Spark and Fugue. Exciting approaches to describing cluster resources as a code, reusable transformation logic with Fugue, and ephemeral clusters for running Hive queries. \nThe programming model to define the resource usage with the application is an exciting approach. \nIt will be fantastic to see the orchestration engine taking the next step in integrating serverless computing, where the application can define CPU, memory, or even Cost to run a computation. \nhttps://eng.lyft.com/how-lyftlearn-democratizes-distributed-compute-through-kubernetes-spark-and-fugue-c0875b97c3d9\nAn excellent weekend read Pathways with a single-controller programming model for richer computing patterns. Like the LyftLearn in the previous blog, it is exciting to see the emerging pattern of defining the computing need in the user code.\nhttps://arxiv.org/abs/2203.12533\nSubstituting code with SQL can oftentimes result in complex and long SQL statements that are not performing as fast as expected. If your query is running slow and you want to understand why - you\u2019ve come to the right place.\nhttps://www.firebolt.io/blog/5-steps-to-debug-your-complex-sql-queries-in-firebolt\nA challenge of running the data analytics org is the help desk style request known as can you pull the data real quick!. The author explains why one can't simply pull the data quickly, highlighting real-world data problems. \nhttps://motifanalytics.medium.com/why-cant-you-pull-data-real-quick-318f90024712\nProduction upgrade without downtime is always challenging. Pinterest writes about its YARN cluster upgrade discussing multiple upgrade patterns. \nhttps://medium.com/pinterest-engineering/large-scale-hadoop-upgrade-at-pinterest-a23a112deb73\nJoin 3 virtual keynotes and 3 city stops, to learn how data leaders are tackling the biggest challenges in data, from building more reliable stacks to hiring top talent for your team.\nRegister Now: https://www.impactdatatour.com\nFeature flagging is the core part of the modern software development & release process. Gusto writes about building a feature to empower the product managers to launch experiments and the data scientists to iterate on models to improve users engagement.\nhttps://engineering.gusto.com/end-to-end-feature-development/\nAirbyte is a CDC SaaS application like Debezium for DB changes. How to enable real-time analytics on SaaS CDC pipelines? The author writes an exciting step to integrate Airbyte, Kafka, and Apache Pinot. \nhttps://medium.com/event-driven-utopia/build-a-real-time-data-analytics-pipeline-with-airbyte-kafka-and-pinot-c9ff3c42dcf2\nPrincipal Engineer, Ranjeet Mishra, details how the foresight of RudderStack's founding engineers made it easier to solve some of the unique technical challenges involved in building Reverse ETL.\nhttps://www.rudderstack.com/blog/how-rudderstack-core-enabled-us-to-build-reverse-etl\nBeat writes about its dbt adoption and lessons learned along the way. The author narrates things to improve on dbt and the steps to migrate the existing workflow to dbt. \nhttps://build.thebeat.co/data-build-tool-dbt-the-beat-story-a5c09471cf66\nBatch computing came a long way from the MapReduce programming model, which triggered innovative file formats, table formats, and query engines. The author takes a brief historical view of how different segments evolved in this space. \nhttps://www.linkedin.com/pulse/recent-history-batch-data-sam-crowder/\nOutbox pattern is well known and widely discussed integration pattern. The blog narrates the hidden technical challenges of adopting the Outbox pattern while implementing Change Data Capture with Debezium.\nhttps://medium.com/yotpoengineering/outbox-with-debezium-and-kafka-the-hidden-challenges-998c00487ae4\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/the-data-founder-story-time-to-zingg", "title": "Data Engineering Weekly", "content": "Sonal Goyal \nZingg.ai & https://github.com/zinggAI/zingg\nIt has been a week of open source with Zingg. Though I had imagined the process of putting it out as a lot of work, it amounted to finally changing the git repository setting to public. AND that was it! All the hard work of the past few months and years (..more on that later) was as simple as a LinkedIn post. I had been nervous till then, making sure every bit of the documentation was proper...that the build was working..oh did we cover all the flavors of Spark? My overwhelming worry has been the documentation - when you have been working long enough on something, a lot of things seem obvious. It is only when you open it up that you realize so much more of what could be better.\nI followed up the LinkedIn post with announcements in various LinkedIn groups I was part of. And shares on Reddit and Hacker News. And a long (really long) article in Towards Data Science. The plan for the next few weeks is to spread the word, get people to know about it and (hopefully) some people will find it useful. I am very very glad that in the first week, people noticed Zingg. And some of them wrote back!\nIn my interactions past week, people told me of a broken link (oops!), the need for a step-by-step guide, python API, docker, information on the algorithms and on how we compared to Tamr or fuzzywuzzy.\nBesides these, most conversations were around the motive - why me, why Zingg, why now, why OSS? In this post, I want to elaborate on these.\nI have been working in the data field for a long long time - both on the warehouse and on the datalake sides of things. As a boutique consulting, we would often get tasked with integrating data from multiple internal systems and building up the dashboards and the models. Sometimes the data also needed enrichment. It was way back in 2013 when we got stuck on a project, and we failed miserably in our first attempt. The work needed us to integrate customer records from 3 different Oracle systems. There were no common unique identifiers, no standard definition of the customer as well as lots of variations in the data for it to match exactly. Existing tools did not fit into our stack and were way too expensive. They also looked like black boxes with a long learning curve.\nHow tough could it be? - the programmer in me thought. I took a sample of 25,000 records from the 2 datasets and wrote a small algorithm to match them, which crashed my computer! I had no basics in entity resolution at that time and did not understand how hard it could be to match everything with everything. I changed my approach and rewrote it. Let me throw a bit of hardware to this, I thought. So we built something on hadoop with some kind of fuzzy matching using open libraries, a refinement of my single machine algorithm..specifically tailored for the attributes we had and barely managed to get it running. We shifted to Spark, and it worked better. We were able to deliver something that saved us, but I wasn\u2019t too proud.\nA few weeks later we again stumbled upon this problem in a different project. This time we had to enrich internal first-party customer data with third-party data sourced externally. I was wiser, or so I thought. This time I read up a lot of research, Arxiv was my friend! And I was glad that I had not been a sloppy programmer, albeit an ignorant one! Top machine learning researchers were building systems on record linkage, and most conferences had a paper or two on it. It was an important AND a hard problem. Clearly, I had a lot to catch up on\u2026\nThis project was successful, and I was able to write something that worked decently, scaled ok, and worked with different types of entities, though with varying levels of changes to the program. But it was a start! I talked about our work at Spark Summit, then got on with other work, as is usual in data consultancies. But I did fall in love with the problem, I must admit!\nIt was in 2015 that a Hong Kong-based event management company approached me. They had seen my Spark Summit talk and were looking to deduplicate their customer records. So far, they had been doing this deduplication manually. It was a big drain on their time and resources and messing up all the customer analytics they wanted to do. They did not want to buy an expensive MDM or Identity Resolution tool, and the open source libraries they tried had not worked for them. I was excited, but there were two problems. Their data was in Chinese and Japanese and English. And they wanted something that would work upto a million records, which was larger than our earlier datasets.\nOne thing I learned which was eye-opening was that they wanted to get the problem solved, in any way it could be, and have a repeatable process to do so. \nThankfully they had some training data which we could use. It was a stretch for us, but it worked. And I couldn\u2019t be happier!\nThere were many things I wanted to build in the algorithms to take it to the next level. But growing work and other priorities pushed me in another direction. I also saw that people were struggling at the base layers of extraction and loading more. Matching could only happen once that was settled. It made sense to go slow. But I kept speaking about it and having conversations about what we had built or what people were facing. The problem kind of never left me.\nThings have changed drastically over time. Thousands of companies use Snowflake. Databricks is rising and rising ( and raising :-)). We now have good and established practices around extraction, loading AND transfomation. Analytics is a profession!\nThe modern data stack has arrived!\n(Though I do think it is missing Zingg, what good is data without the relationships? :-)\nAgree/Disagree/Have opinions? Comment below)\nEventually, when I took a pause to think about what I really want to do and which way I want to go, this problem was the highest on my list. I just see it everywhere - how can you build customer 360 if the data is not unified? How can you do customer lifetime value or segment customers or personalize their messaging when you have multiple records of the same customer? What is Anti Money Laundering if the households and links between customers are not established? Oh and what about all those company names and their details which show up in every vendor system or B2B enterprise without ever getting reconciled?\nAnd so in the last year, I started off afresh and wrote Zingg from the grounds up.\nNow to the question of open source. I have been consuming open-source software all my life. Right from Java in 1998! It feels great to give something back in my own way. I also feel that open source Zingg is more powerful than closed source because then it is up to the community to put it to use in more ways than I have seen and could ever anticipate. It is also a promise to work closely with different people and collaborate, something I am very sincerely hoping will happen!\nWe will have to watch the next few months on how much that promise gets materialized, but it is a start. Wish me luck!\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. The data founder story contents provided by the featuring data founders in this article and Data Engineering Weekly is not responsible for any compliance with applicable laws, rules, and regulations."}, {"url": "https://www.dataengineeringweekly.com/p/launching-the-data-founder-story", "title": "Data Engineering Weekly", "content": "There was a mixed reaction to the Data Council - 2022 conference. Though I\u2019ve mixed experiences, I enjoyed \u201cLightning Talk\u201d on the second day. I love the idea of providing a platform for engineers and founders to share their thoughts.\nFrom the incubation of Data Engineering Weekly, the readership increased significantly. We are close to hitting the 10,000 reader base shortly. It makes me think about what Data Engineering Weekly can contribute back to the community.\u00a0\nEvery data startup came out to solve specific data pain points we came across at some point as data practitioners. What better platform than Data Engineering Weekly to tell your data founder story to the data practitioners community?!\nData Engineering Weekly is proud to launch the Data Founder Story. Tell us your story to our readers.\nYou\u2019re just thinking of implementing your ideas, seed funder, or company listed on the public stock exchange; it doesn\u2019t matter. As long as you\u2019re solving the data problem, you\u2019re the data founder.\u00a0\nYou\u2019re someone who wants to tell your data story, the pain points as a data practitioner to inspire fellow data practitioners.\u00a0\nWrite your story\nOpen a PR in https://github.com/ananthdurai/dataengineeringweekly [We are all coders at heart anyway :-) ]\nWe will review and feature it as a separate edition of the newsletter.\nWe believe writing is a complete form of expressing our thoughts. Interviewing someone will force into a structured process of storytelling. We want to give the data founders the freedom to express themself.\u00a0\nData Engineering Weekly is the widely read newsletter in the data community. You can reach out to 1000s of data practitioners with your story and idea.\nI heard from VCs reading the Data Engineering Weekly!!\nData Engineering Weekly is a perfect platform to find collaborators and contributors to your projects & ideas.\nSoftware is a reflection of our thoughts. You\u2019re inspiring the thoughts of Data Engineering Weekly readers, thereby shaping the data industry.\u00a0\n\nData Engineering Weekly & I were thrilled to read all the data founders\u2019 stories. Please feel free to ping Twitter: @data_weekly for more information."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-80", "title": "Data Engineering Weekly", "content": "RudderStack Provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nIt's been an eventful week at the Data Council conference in Austin. I plan to reflect on my experience later this week, so stay tuned. One of the big news on the conference day is that Astronomer acquired DataKin, the company behind the open-source Marquez project. I predicted the consolidation on data lineage and orchestration engines, but I never thought this would happen this fast. \nhttps://techcrunch.com/2022/03/23/astronomer-ready-for-its-next-mission-after-datakin-acquisition-213m-series-c/\nWhy is data lineage so crucial in data management? The author gives an overview of what a comprehensive data lineage can bring into data management.\nhttps://towardsdatascience.com/how-should-we-be-thinking-about-data-lineage-541ca5ab83d0 \nCompanies invest a lot in analytics - but are these investments valuable? The study found that using a descriptive dashboard increased their weekly revenues by 4%-10%.\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=3745748\nStaying on data and business decisions; Data storage and computation become less expensive and commoditized, but how does the data bridge a business decision? Are we asking the right business question? TIL about Decision Intelligence framework for data-informed decisions.\nhttps://www.lorienpratt.com/a-framework-for-how-data-informs-decisions/\nI'm looking forward to learning more about this in the coming weeks. I found this short video a thought-provoking one.  \nHow do we measure the success of a data product? The engineering approach measures the data freshness, pipeline speed, and the model's accuracy. Shopify wries an informative blog that narrates why measuring the success from the business goal and customer perspective is vital for the success of a data product. \nhttps://shopifyengineering.myshopify.com/blogs/engineering/a-data-scientist-s-guide-to-measuring-product-success\nPlay The Big Data Game \u2013 Because even a simple query can send you on an unexpected journey...\nhttps://www.firebolt.io/big-data-game\na16z's Future released Data50, the top data startup with a funding and location analysis. I'm thrilled to see all the Data Engineering Weekly sponsors Rudderstack, Monte Carlo, & Firebolt featured in the Data50 startup list.\nhttps://future.a16z.com/data50/\nRudderstack's Warehouse Actions is now RudderStack Reverse ETL. The rebranded product launched with new features to make your data engineering workflows easier, including enhanced processing and scheduling, Visual Data Mapper, and Custom SQL Models. The product complements RudderStack's Event Stream offering, sharing all 150+ integrations and, in most cases, its Transformation and Data Governance capabilities.\nhttps://www.rudderstack.com/blog/announcing-rudderstack-reverse-etl\nBuy vs. Build is always an ongoing architectural decision in an organization. I've seen folks underestimate the \"cost to integrate\" off-the-shelf solutions. The author captured the challenges in validating and integrating MLOps and DataOps products. I have written about the emerging patterns of data sharing in data engineering weekly [Omicron Paradigm: Architectural patterns for the Infinite Data Logistic]. It is an exciting data engineering challenge to solve.\nhttps://djpardis.medium.com/data-sharing-and-transfer-challenges-2e87e18a1167\nAggregation is the standard best practice for analyzing time series data, but it can create problems by stripping away crucial context. The author narrates the consequence of uninformed data aggregation. \nBut every time you aggregate, you make a decision about which features of your data matter and which ones you are willing to drop. Informed aggregation simplifies and prioritizes. Uninformed aggregation means you\u2019ll never know what insights you lost.\nhttps://stackoverflow.blog/2022/03/03/stop-aggregating-away-the-signal-in-your-data/\nSpotify writes about how it uses properties of the Poisson bootstrap algorithm and quantile estimators to reduce the computation complexity for efficient bootstrap confidence intervals. \nhttps://engineering.atspotify.com/2022/03/comparing-quantiles-at-scale-in-online-a-b-testing/\nLast week we saw Spotify moving away from Luigi to Flyte. Lyft writes about its incubation of Flyte and the difference between Airflow. However, I can't stop wondering why a new system instead of adding the features in Airflow! Nonetheless, it is excellent to see event-driven dependency management rather than the polling approach in Airflow. \nhttps://eng.lyft.com/orchestrating-data-pipelines-at-lyft-comparing-flyte-and-airflow-72c40d143aad\nI've not seen many engineering blogs talking about the developer workflow after an alert or incident in the data pipeline. DataOps is my favorite part of data engineering, and glad to see Miro's developer workflow of DataOps. \nhttps://medium.com/miro-engineering/our-journey-to-data-engineering-monitoring-c14d6ff20351\nKIP-500 probably widely read Kafka RFC, and Confluent writes an excellent summary of replacing ZooKeeper with KRaft.\nhttps://www.confluent.io/blog/why-replace-zookeeper-with-kafka-raft-the-log-of-all-logs/\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-79", "title": "Data Engineering Weekly", "content": "RudderStack Provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up free to test out the tool today.\nThis week, let\u2019s start with this handy mini-introduction to seven python libraries for machine learning. \nThe data community had an interesting conversation around bundling vs. unbundling. I had a fun conversation with Gorkem Yurtseven, Prukalpa, and Nick Schrock. \n, and here is the exciting poll result. \nStanford HAI writes about the current state of AI, highlighting the diversity challenges, booming private investment, and AI patent fillings. \nhttps://hai.stanford.edu/news/state-ai-9-charts\nLuigi, one of the pioneers in the data orchestration engine leaving its home. Spotify writes about its plan to move to Flyte over Luigi. \nIs that official EOL for Luigi? \nhttps://engineering.atspotify.com/2022/03/why-we-switched-our-data-orchestration-service/\nHandling the mutability of the data is a critical aspect of the data infrastructure. The LakeHouse framework adopts incremental update as a core design principle. LinkedIn writes about Opal design and how it facilitates building a mutable dataset in the data lake. \nhttps://engineering.linkedin.com/blog/2022/opal--building-a-mutable-dataset-in-data-lake\nPartial data unavailability is a cause of ML model failure in most cases. Twitter writes about an efficient, scalable approach for handling missing features in graph machine learning applications. \nhttps://blog.twitter.com/engineering/en_us/topics/insights/2022/graph-machine-learning-with-missing-node-features\nChange Data Capture with the transactional outbox pattern, and Debezium becomes a standard approach for event sourcing. Grab writes about its design of real-time data ingestion.\nhttps://engineering.grab.com/real-time-data-ingestion\nWhen it comes to Reverse ETL, business use cases typically get all the attention. Here, RudderStack focuses on how reverse ETL makes data engineering easier. They drive the point home with an example from their own data engineering team that involved using the Google Click ID (gclid) to get enriched conversions into Google Ads.\nhttps://www.rudderstack.com/blog/making-data-engineering-easier-operational-analytics-with-event-streaming-and-reverse-etl\nI write about the curious case of the AWS EMR pricing model highlighting the EMR surcharge pricing model. Stitch Fix writes about other operational issues such as non-adoptive provisioning, Spark multi-version support, scattered observability, and the lack of configuration agility. Stitch Fix is mitigating the complexity of running EMR on EKS; \nI think it is time to rethink Yarn for the cloud. I don't think running one scheduler over the other is an optimal solution. \nhttps://multithreaded.stitchfix.com/blog/2022/03/14/spark-eks/\nCost optimization is top of the mind for many companies; AWS writes about approaching performance & cost optimization with tips for right-sizing Kafka cluster, network throughput, and monitoring continuous optimization. \nhttps://aws.amazon.com/blogs/big-data/best-practices-for-right-sizing-your-apache-kafka-clusters-to-optimize-performance-and-cost/\nWhatNot writes about its adoption of the modern data stack. We highlighted the tale of Airflow operator vs. dbt, and Whatnot article reflects the same.\u00a0\nSecond, for our data transformation layer, we rely on two tools:\u00a0Apache Airflow\u00a0and\u00a0DBT. For any data replication work (where we transform data\u00a0before\u00a0it is loaded into the data warehouse), we write the logic in Python that transforms and loads the data. We also use Airflow to orchestrate training machine-learning models.\nIt will be exciting to see how orchestration and data transformation co-exist or merge at some point. One thing I'm not clear in the blog is that there is \"No dimensional data model for now\"? Does that mean throwing JSON data into the data warehouse or not adopting the confirmed dimensions style data modeling?\u00a0\nhttps://medium.com/whatnot-engineering/building-a-modern-data-stack-at-whatnot-afc1d03c3f9\nStaying on the modern data stack, ManyPets writes about its data stack, highlighting its choice of tools. A couple of key themes\nAirflow + dbt as two orchestration engines for data transformation is a common approach.\u00a0\nBefore dbt, we stored our modeling queries as SQL files and ran them as tasks in an Airflow job. We had to manually add dependencies between the tasks and we used Python\u2019s string formatting to allow some basic code reuse. Initially, I\u2019m now embarrassed to say, I didn\u2019t think we needed dbt to help manage this. As we grew further though it became clear that we did and now I can\u2019t imagine life without it!\nData lineage & discovery tooling doesn\u2019t play a significant role in the beginning stage of the modern data stack.\n\nhttps://medium.com/data-manypets/how-manypets-implemented-the-modern-data-stack-35877715c0da\nWe sometimes joke about data engineering as the backend of the backend. The author narrates three stages of the data team\u2019s maturity model\u2014a highly recommended read on how to mature the data function in an organization.\nReactive stage\nProactive stage\nInfluential stage.\nhttps://scientistemily.substack.com/p/reactive-proactive-influential\nTIL about re-data, an open-source data reliability framework for observing the dbt projects. The author narrates an introduction to use re-data with the dbt project. \nGithub: https://github.com/re-data/re-data\nhttps://towardsdatascience.com/prevent-data-loss-with-this-free-dbt-package-a676c2e59c97\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-78", "title": "Data Engineering Weekly", "content": "Data Council published the Austin 2022 schedule here. The data engineering weekly readers can get a 20% discount using promo code DataWeekly20\nhttps://www.datacouncil.ai/austin\nData Analytics and Data Science is the last mile solution to amplify the value of the data. As the author points out, collecting data is a unique opportunity to learn many staple technologies in data. \nhttps://www.georgeho.org/data-collection-is-hard/\nIf you're a history buff, this might excite you. DeepMind writes about Ithaca, the first deep neural network that can restore the missing text of damaged inscriptions, identify their original location, and help establish the date they were created.\nhttps://deepmind.com/blog/article/Predicting-the-past-with-Ithaca\nNature Paper: Restoring and attributing ancient texts using deep neural networks\nithaca GitHub: https://github.com/deepmind/ithaca\nWordle became my favorite family bonding activity, and I was thrilled to see the data analytics. As expected, four is the most common guess count, with frequency decreasing rapidly on either side. There is some fantastic product design principle hidden in that insight for sure. \nhttps://observablehq.com/@rlesser/wordle-twitter-exploration\nMetrics always require (human) interpretation to be actionable. Tests are immediately actionable by automated processes.\nSalesforce writes about why a metric-centric strategy won\u2019t scale and the paradigm shift on test-centric ML with Evaluation store. \nhttps://engineering.salesforce.com/einstein-evaluation-store-beyond-metrics-for-ml-ai-quality-4ec2f5504421\nHistorically, access control is thought of as a separate layer on top of data storage. The tabular format such as Iceberg, DeltaLake & hudi formats opens up the possibility of implementing fine-grained access control co-exist with the storage. It's exciting to read Uber's implementation of access control, encryption, and retention on top of Apache Parquet. \nhttps://eng.uber.com/one-stone-three-birds-finer-grained-encryption-apache-parquet/\nA Dependency Confusion attack or supply chain substitution attack occurs when a software installer script is tricked into pulling a malicious code file from a public repository instead of the intended file of the same name from an internal repository.\nHow evident is it? Here is an article talks about the implementation of it.\nDependency Confusion: How I Hacked Into Apple, Microsoft and Dozens of Other Companies\nPinterest writes about how it approaches dependency confusion. \nhttps://medium.com/pinterest-engineering/addressing-python-dependency-confusion-at-pinterest-e0a0609c8e9\nWhen it comes to Reverse ETL, business use cases typically get all the attention. Here, RudderStack focuses on how reverse ETL makes data engineering easier. They drive the point home with an example from their own data engineering team that involved using the Google Click ID (gclid) to get enriched conversions into Google Ads.  \nhttps://www.rudderstack.com/blog/making-data-engineering-easier-operational-analytics-with-event-streaming-and-reverse-etl\nAntifragile principles give a new perspective in system design on top of resiliency and feedback loop. The article may be a good starting point for a conversation among the data community about Chaos engineering & Antifragility. Picnic writes an exciting article about its Antifragile principles in the data warehouse. The seven principles are,\nSticking to simple rules\nAvoiding naive interventions that do more harm than good in the long term\nBuilt-in redundancy and layers (no single point of failure)\nEnsuring that everyone has a stake\nExperimenting and tinkering \u2014 taking lots of small risks.\nKeeping our options open\nNot reinventing the wheel \u2014 looking for habits and rules that have been around for a long time.\nhttps://blog.picnic.nl/7-antifragile-principles-for-a-successful-data-warehouse-574b655f0bc6\n Benchling writes an interesting article about its search infrastructure from ElasticSearch to Postgres and again to ElasticSearch. The article's theme is about the hardness in maintaining a consistent view of two disjointed infrastructures is a pain. \nhttps://benchling.engineering/a-look-at-the-evolution-of-benchlings-search-architecture-c4d5327452c\nOne of the exciting features I like about Apache Pinot is picking and choosing an indexing strategy for each column. The article is an excellent in-depth explanation of RangeBitmap and how the range indexes in Apache Pinot works. \nhttps://richardstartin.github.io/posts/range-bitmap-index\nThe developer community divides between low code/ no code vs. code only approach. In the AEW (Analytical Engineering Weekly), Tristian made a significant point.\nIf we say that the only appropriate way to participate in certain activities is to do so via writing code, then we are inherently excluding the majority of humanity.\nMy thought on this,\nWe treat programming as elite work, but every job requires some level of automation. A farmer could automate their job with a no-code solution to predict crop growth and quality. Since we keep the barrier of entry to such a system, the farmer has to rely on a middle man that we call a marketplace platform. Democratization of the technology removing the barrier to entry in programming is vital to maintain the balance of the society. The advantage of the code-only approach is the testability and repeatability. Version control & code review are one way to implement. \nhttps://sarahsnewsletter.substack.com/p/no-code-is-the-future\nYou\u2019ve heard about Reverse ETL. Here\u2019s your chance to learn all about the tooling from the folks who are creating it. Join Hosts Eric and Kostas for a live recording of The Data Stack Show on March 9th to get insights from experts at Census, Hightouch, and Workato.\nhttps://datastackshow.com/livestream-registration-reverse-etl/\nTalabat writes about Perspectives, its internal metric aggregation framework. The Configuration as a Code for metrics generation is becoming popular (e.g., dbt metrics layer). It will be a curious case study to see when the descriptive DSL like SQL becomes too much of a barrier to entry that pushes the momentum behind the Configuration as a Code. \nhttps://medium.com/talabat-tech/perspectivestalabats-data-aggregation-framework-c8fb3ba6d08\nOperating a streaming platform is always challenging, and Shopify writes some excellent tips to optimize the Apache Flink applications. \nhttps://shopifyengineering.myshopify.com/blogs/engineering/optimizing-apache-flink-applications-tips\nRedhat writes an exciting article comparing running one Kafka cluster to rule them all vs. running multiple Kafka clusters. The multi-tenant vs. multi-instance is an exciting system design debate, and I'm always in favor of the multi-instance model in the cloud environment. \nhttps://developers.redhat.com/articles/2022/03/10/which-better-single-kafka-cluster-rule-them-all-or-many\nAll rights reserved Pixel Impex Inc, India. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/bundling-vs-unbundling-the-tale-of-92d", "title": "Data Engineering Weekly", "content": "In the first part (Bundling Vs. UnBundling: The Tale of Airflow Operator and dbt Ref), we established a case for the importance of having a unified DAG view of the data ecosystem.\u00a0\nNow, what is the consequence of not having a unified view? Let\u2019s walk you through the data lifecycle journey.\u00a0\nThere are a few recent comparisons of developing data products with microservices. Data contract approach widely spoken and getting adopted. The data contract typically establishes a contract between the data producer and the downstream consumers. It is a lot similar to the microservices approach. Well, not entirely true.\u00a0\nIn the Microservices, the contract agreed with a gRPC or REST interface. The producer takes known input and produces contractually agreed output to the consumer. The QoS (Quality of Service) bounds with the service response, and all the consumers have the same view about QoS. We can say a table or metrics equivalent to a microservice.\nHowever, a downstream consumer connects with multiple tables in the data pipeline to produce a derived table(fact or metrics). The consumer views the data quality through the lens of aggregated views from multiple tables. The terms of the data quality from a consumer often reflect the consumer domain, and each consumer will have their definition of the data quality. There will be a 1:N relationship between producers and consumers. \nA table/ metric is quality enough for one consumer but not for the other consumer. The worst part of it, because the data computation is pipelining in nature, the data quality issues may not be visible to the immediate downstream consumer.\u00a0Hence the question: Who owns the data quality? Is it the data producer or the data consumer?\nWe approach data quality in two strategies.\nData testing typically involves checking Null values, data distribution, uniqueness, known invariants, etc.,\nWe can\u2019t predict all the failure modes, and the data downtime happens all the time. Data observability is an end-to-end observability strategy to detect.\u00a0\nThe data quality methods are beyond the scope of this blog. The blog Data Observability vs. data testing: Everything you need to know is a good overview. Dbt test, Great Expectations, AWS deque, are popular tools for data testing. There are pretty solid vendors in the data observability space.\u00a0\nSimilar to the CAP theorem, there is a trade-off among Speed, Volume & Correctness in a data pipeline. You can get only two out of three in your pipeline system design.\u00a0\nLet us walk through how the data quality tools integrate with the data pipeline and the trade-off between speed and correctness. There is broadly two data pipeline pattern to support the data testing.\nIn the Staging-First approach, the producer writes the table in a staging environment and run through all the data quality check. If any test fails, and if it is a blocking failure, an incident is notified to the data producer to fix it.\u00a0\nIn the Production-First approach, the producer writes the table in the production environment and notifies all the consumers. The consumers may or may not choose to run the data quality verification.\u00a0\nIn both approaches, the following property of the data pipeline remains the same.\nShould we block the pipeline for all the consumers to finish their data quality check?\nHow do we measure the downstream impact of a pipeline failure?\nHow can we identify which upstream causes the pipeline failure?\nHow do we recover? What is the scheduling story for backfilling & error correction?\nWith the data observability tooling, the time delta between data production and error detection is high and often non-blocking.\u00a0\nWithout the data orchestration engine having a full lineage view, it becomes more painful to operate the data pipeline. As the number of data assets grows, the complexity of the data ecosystem grows and falls on its success.\u00a0\n\nI highlighted the Airflow fallacies in my talk, Operating data pipeline using Airflow @ Slack.\n Though it is called Airflow fallacies, it is indeed the \u201cdata pipeline fallacies.\u201d A couple of critical points to highlight;\nThe upstream task success is reliable.\nThe task remains static after the success state.\nData Mutation is inevitable in the data pipeline. It could be from an incomplete view from the upstream, a data quality & observability issue from downstream models, or GDPR and other compliance reasons.\nWith the fragmentation between the task & model, the orchestration engines don\u2019t have a complete lineage view of the data system. The data lineage is at present hidden in a separate data discovery system. The backfilling & data correctness often requires jumping around multiple tools. Often a custom scheduler requires to manage data retention and lifecycle. The vital piece is \u201cdata lineage for all this to happen.\u201d \nData Orchestration, Data Quality (testing, observability) & Data lineage are part of a task/ model lifecycle.\u00a0If you can take one takeaway from the blog, this is it. Establishing trust in data will be a distant dream without a unified view of the data lifecycle.\nIn a reflection on the previous post in the Analytics Engineering Roundup, Tristan raised some great questions.\nQuestion: How much of the problem you\u2019re describing here would go away if dbt Core natively supported non-SQL languages? Could we go back to just having a single DAG (now in dbt) or are there other barriers?\nAnswer: If dbt Core can gain a single DAG view and with a pluggable architecture, the complexity of enterprise asset management will reduce. It is more of a throwback question for the dbt community. What is build stands for in dbt now and in the future?\nQuestion: Do you think the \u201ctask execution\u201d and the \u201cmodel\u201d are actually different? Or can the concept of a dbt model be extended to handle what has historically been written as a task with minimal friction?\nAnswer: I think task execution is a subset of model execution. The critical distinction is that a task can produce a null model.\u00a0\nQuestion: As we think about how to push dbt in this direction, what do you think should be top of mind for us that we may not be considering?\nAnswer: It can potentially introduce complexity to the existing simplicity in the incremental and materialized model. Keeping the simplistic opinionated abstraction and transferring dbt to a true \u201cdata operating system\u201d will be challenging.\u00a0\u00a0\u00a0\n\u200b\u200b\nWell, Let me introduce my startup\u2026\u2026\u2026 Lol No.\u00a0\ud83d\ude04\ud83d\ude04\ud83d\ude04\ud83d\ude04\nOne of the best things that happened with the Modern Data Stack is more momentum and intelligent people solving data engineering challenges. \nWe thrive on solving data at an industrial scale, similar to the cotton industry and the industrial revolution. How can we produce consistent data insights similar to cotton production? A true challenge ahead in this decade.\u00a0\nWe can take many paths to solve this.\nMaybe the orchestration engine can have the end-to-end lineage and manage the data lifecycle.\nMaybe the data lineage and discovery system can trigger a task/ model. Adding a new model is equivalent to adding a new edge in the lineage graph.\nMaybe the observability tool provides a robust feedback loop to manage the data lifecycle.\u00a0\nIf the problem statement resonates with you and you are looking for collaboration, I\u2019m happy to lend my time in whatever way possible.\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-77", "title": "Data Engineering Weekly", "content": "Data Council published the Austin 2022 schedule here. The data engineering weekly readers can get a 20% discount using promo code DataWeekly20\nhttps://www.datacouncil.ai/austin\nThe explosion in data collection has led to challenges in storing enormous amounts of data, particularly for archival data. The Harvard researchers introduce a new container for long-term storage: dye!!\nhttps://www.datanami.com/2022/02/14/harvards-new-data-storage-is-to-dye-for-avoids-dna-storage-pitfalls/\nEstablishing a faster feedback loop is vital in developing the recommendation engine. LinkedIn writes about the usage of Samza SQL and Apache Pinot to build near real-time personalization. \nhttps://engineering.linkedin.com/blog/2022/near-real-time-features-for-near-real-time-personalization\nOn a similar note with LinkedIn's previous blog, eBay writes about the maturity phases of the recommendation engine. The blog narrates the architecture style adopted by eBay from batch-only, batch & near real-time to a near-real-time (NRT) system. \nhttps://tech.ebayinc.com/engineering/building-a-deep-learning-based-retrieval-system-for-personalized-recommendations/\nSpotify writes about the adoption of its experimentation in its search product. Any product will go through its technology adoption lifecycle(\u00a0Chasm theory), yet we rarely talk about it. Spotify narrates the adoption curve and the importance of starting and maintaining the momentum in adoption.\u00a0\nhttps://engineering.atspotify.com/2022/02/search-journey-towards-better-experimentation-practices/\nSpotify\u2019s New Experimentation Platform (Part 1)\nSpotify\u2019s New Experimentation Platform (Part 2)\nIn the last two decades, the industry has attempted to reinvent the alternative for SQL with no success. The lack of a software library and the limitation in distributing SQL are some of the significant shortcomings of SQL. It's is an exciting conversation on SQL, software libraries, and dbt. \nhttps://future.a16z.com/sql-needs-software-libraries/\nA good collection of tools before starting working on machine learning & AI at the industrial scale engineering.\u00a0\nQuestion to the readers:\u00a0What would be the top 9 tools you wish you had learned before entering data/ analytical engineering?\u00a0Please tweet back to @data_weekly.\nhttps://towardsdatascience.com/nine-tools-i-wish-i-mastered-before-my-phd-in-machine-learning-708c6dcb2fb0\nHere, RudderStack provides a detailed overview of the architecture, data, and modeling required to assess the contribution to conversion in multi-touch customer journeys.\nhttps://www.rudderstack.com/blog/from-first-touch-to-multi-touch-attribution-with-rudderstack-dbt-and-sagemaker\nThe ML & data landscape is fragmented, where each tool tries to solve niche problems. The author narrates the current state of the MLOps and a few predictions. The author predicts\u00a0Increasing consolidation around end-to-end platforms,\u00a0similar to the conversation in the data landscape with bunding & unbundling.\u00a0\nhttps://www.mihaileric.com/posts/mlops-is-a-mess/\nThe author shared the notes from the Tecton conference. I didn't have a chance to go through the full notes, but there is tons of learning. Thanks, James, for sharing your notes. \nhttps://data-notes.co/what-i-learned-from-attending-tecton-apply-meetup-2022-4b7be87e2f17\nData lineage is a critical connector to establish end-to-end observability and explainability of the analytical pipeline. Monte Carlo writes about the importance of the column-level lineage of the SQL pipeline and the design journey to establish observability. \nThere are a lot of talks and effort on \"Explainable AI\" did we achieve \"Explainable Analytics.\" Is there any tool a salesperson can use to understand the business logic of ARR computation without understanding SQL? Found anything, please tweet @data_weekly\nhttps://www.infoq.com/articles/field-level-lineage-modern-data-systems/\nA backward-incompatible schema change is painful, and I still remember fixing a Thrift incompatible change to make sure any backfilling in the future does not break. Expedia writes an exciting blog that narrates how to handle Avro incompatible schema changes. \nhttps://medium.com/expedia-group-tech/handling-incompatible-schema-changes-with-avro-2bc147e26770\nTIL about PyCaret, an open-source, low-code machine learning library and end-to-end model management tool built-in Python for automating machine learning workflows. \nhttps://www.kdnuggets.com/2021/04/multiple-time-series-forecasting-pycaret.html\nYou\u2019ve heard about Reverse ETL. Here\u2019s your chance to learn all about the tooling from the folks who are creating it. Join Hosts Eric and Kostas for a live recording of The Data Stack Show on March 9th to get insights from experts at Census, Hightouch, and Workato.\nhttps://datastackshow.com/livestream-registration-reverse-etl/\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-76", "title": "Data Engineering Weekly", "content": "Data Council published the Austin 2022 schedule here. The data engineering weekly readers can get a 20% discount using promo code DataWeekly20\nhttps://www.datacouncil.ai/austin\nLet\u2019s start this week\u2019s edition with this excellent thread on the challenges of collecting SaaS metrics and how to approach solving them.\n2021 is an exciting year for data startups. The blog is a compilation of fund rounds raised by data startups. \nhttps://adat.blog/2022/02/fundraising-by-data-companies-in-2021/\nHow does the data-informed product loop look? The author narrates its lifecycle.\nHave a strategy \nTranslate that into models\nAdd minimally viable measurement.\nIdentify leverage points\nExplore options \nRun experiments\nhttps://cutlefish.substack.com/p/tbm-852-the-data-informed-product\nVortexa writes about its selection process of choosing the analytical tool and why they decided on Metabase. \nhttps://medium.com/vortechsa/choosing-an-analytics-tool-metabase-vs-superset-vs-redash-afd88e028ba9\nThe blog is an excellent overview of the data quality tool's landscape. It is a good reference article if you're in the process of choosing a data quality tool.\nhttps://sarahsnewsletter.substack.com/p/choosing-a-data-quality-tool\nStaying on the data quality story, Vimeo writes about monitoring data quality with Monte Carlo. The CI/CD flow for the data quality is check is an exciting read; I'm curious to read more about the feedback loop for error correction.\nhttps://medium.com/vimeo-engineering-blog/monitoring-data-quality-at-scale-using-monte-carlo-934577e45ab0\nWhat changed and how much changed is the first question we ask when looking at the data. The blog narrates the explanatory algorithms for finding the difference between two datasets. The author implemented an open-source version of the paper Diff and explained how that could help solve the problem.\nDiff Paper: http://www.bailis.org/papers/diff-vldb2019.pdf\nhttps://blog.marcua.net/2022/02/20/data-diffs-algorithms-for-explaining-what-changed-in-a-dataset.html\nMany companies still struggle to answer even basic questions with their data. These data modeling best practices from RudderStack will help you build a well-defined core data layer, enabling teams to answer harder questions while ensuring a better experience for every end-user.\nhttps://www.rudderstack.com/blog/data-modeling-in-the-warehouse-for-data-engineers\nThe QCon talks about the Netflix data pipeline now available. The workflow support for the event-driven and the scheduled trigger is an exciting approach for an orchestration engine.\nThe metrics layer is an exciting development, and curious to see how it progresses. The author shared the experience of trying the dbt metrics layer with a concern; there is a lot of YAML!!\nhttps://stkbailey.substack.com/p/kicking-the-tires-on-dbt-metrics\nDream 11 writes about its feature store Data Feast. The choice of HBase as a feature store is interesting. TIL about RonDB, and looking forward to reading more on it. \nhttps://blog.dream11engineering.com/data-feast-a-highly-scalable-feature-store-at-dream11-69b8ed5289fd\nPyTorch announced TorchRec, a PyTorch domain library for Recommendation Systems. TorchRec library provides common sparsity and parallelism primitives, enabling researchers to build state-of-the-art personalization models and deploy them in production.\nGithub: https://github.com/pytorch/torchrec\nhttps://pytorch.org/blog/introducing-torchrec/\nMetaflow, an orchestration engine for ML pipeline, popularized deploying notebooks in production. Outbounds introduce Notebook Cards, which allow data scientists to use notebooks to visualize and debug production workflows and help bridge the MLOps divide between prototype and production.\nhttps://outerbounds.com/blog/notebooks-in-production-with-metaflow/\nThe article is an excellent compilation of Avro schema evolution with practical advice. It is a comprehensive guide to educate users on Avro schema evolution to simplify managing schema changes.\nhttps://medium.com/expedia-group-tech/practical-schema-evolution-with-avro-c07af8ba1725\nYou\u2019ve heard about Reverse ETL. Here\u2019s your chance to learn all about the tooling from the folks who are creating it. Join Hosts Eric and Kostas for a live recording of The Data Stack Show on March 9th to get insights from experts at Census, Hightouch, and Workato.\nhttps://datastackshow.com/livestream-registration-reverse-etl/\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/bundling-vs-unbundling-the-tale-of", "title": "Data Engineering Weekly", "content": "I started working on the data pipeline at the early stage of Hadoop/ Bigdata when Big Data was a buzzword. Apache Oozie (anyone remembers Oozie?) is a go-to tool to orchestrate the data pipeline, where you have to hand-code workflow in an XML file(not surprisingly, the file name is workflow.xml).\nApache Airflow has improved the data pipeline massively. The ability to programmatically author a data pipeline, the concept of DAGs, and on top of that, a fantastic usable UI for the first time is an order of magnitude better than the previous tools.\nAirflow\u2019s initial release was June 2015, six months after Snowflake launched its commercial offering. The cloud data warehouses barely started to take off at that point. The orchestration engine mainly focuses on running Hadoop Map Reduce, Pig, Crunch & Hive jobs. We\u2019ve seen fragmented big data processing frameworks at that point before Apache Spark unified the SQL and fluent data processing model.\u00a0\nAirflow adopted the task (Airflow operators) as a functional computing unit. Airflow provided sensors and task dependency to capture the dependency. The Airflow DAG model is an excellent abstraction to define the task dependency; however, the visibility of the task lineages is limited to a single DAG. If you want to run through backfilling, we had to develop a dedicated tool to understand the overall dependency.\u00a0\nA few companies adopted the \u201cOne DAG\u201d model to bundle all the tasks in one pipeline, where at the Slack data team, we build a DAG parser to construct a unified DAG view [see my talk: Operating Data Pipeline using Airflow @ Slack - 2018]\nAs the raw data transformed into more structured tables, we\u2019ve seen a pattern where Hive SQL was adopted as a de-facto standard to build the data model. SQL won the data processing abstraction. The task dependency proved to be hard to manage since the unit of transformation logic is expressed as the model in SQL, and finding the task dependency for each Hive table is a hectic job. Airflow provides HivePartitionSensor & SQL Sensors, yet it doesn\u2019t solve the underlying problem of task dependency for a model framework.\u00a0\nBy 2019, Cloud data warehouses got adopted widely. The task execution was not adopted well with the cloud datawarehouse systems and entered dbt. The most important function in dbt is ref(). The dbt uses these references between models to build the dependency graph automatically.\nThe reference model dramatically simplifies the data pipeline, and dbt becomes the defacto tool for the data pipeline. It is important to note that Airflow also supported other popular features like the Jinja template and dbt documentation. I captured my thoughts on dbt here.\nThe task & model execution unit results from the diverse data processing frameworks and the need to support various business use cases. SQL is well suited for data analytics/ BI, where ML & raw data processing uses more of a fluent interface.\u00a0\nAs a consequence of the separate execution unit, both the task orchestration engine and model execution engine can\u2019t have an end-to-end data lineage of an organization. It paved the path for specialized data lineage/ metadata systems. \nThe data engineering weekly wrote a special edition for capturing the history of metadata systems.\nThe handshake between the task unit & model unit increasing became complicated. We started to see the rise of data observability tools emphasizing data contracts and data certifications for defining specific data quality standards and SLA.\u00a0\nThe unbundling of the data platform is a consequence of a separate execution model. The orchestration engines have less and less context on the org-wide data management state.\u00a0\nAt the same time, we should also acknowledge that data orchestration, data quality, lineage, and model management are significant problems on their own. The individual tools are trying to solve specific problems; however, looking from an overall architecture perspective results in duct tape systems. It makes the dream of data an asset is still an aspirational/ unrealistic goal for many companies.\u00a0\n\u00a0\nThe data community often compares the modern tech stack with the Unix philosophy. However, we are missing the operating system for the data. We need to merge both the model and task execution unit into one unit. Otherwise, any abstraction we build without the unification will further amplify the disorganization of the data.\u00a0 The data as an asset will remain an aspirational goal.\nWe need to merge both the model and task execution unit into one unit. Otherwise, any abstraction we build without the unification will further amplify the disorganization of the data.\u00a0\nYou can ask, What is the other organizational chaos you see in your experience? Let me answer that in my next blog by answering a simple question.\nWho owns data quality? Let\u2019s find out in the next part.\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-75", "title": "Data Engineering Weekly", "content": "Data Council published the Austin 2022 schedule here. The data engineering weekly readers can get a 20% discount using promo code DataWeekly20. I\u2019m excited to attend the Data Council in person and hope to meet you all.\nhttps://www.datacouncil.ai/austin\nIt is an exciting week at the data land. Gorkem Yurtseven started the conversation with an excellent write-up,\u00a0The Unbundling of Airflow.\u00a0The blog ends with\nA diverse set of tools is unbundling Airflow, and this diversity is causing substantial fragmentation in the modern data stack. Like everyone else, I also predict some consolidation of these tools in the coming years.\nDagster writes about its mission of Rebundling the Data Platform.\u00a0\nHaving this many tools without a coherent, centralized control plane is lunacy and a terrible end state for data practitioners and their stakeholders.\nI have a few thoughts to share on this, which I plan to write a blog about. I do believe in the consolidation of the tools soon. dbt is iterating its metadata management layer [Leveraging dbt metadata in data management], Atlan stepping into data exploration and visualization space. \nI often hear comparisons of the modern data stack with the Unix philosophy. Who is the Unix terminal of the modern data stack is the billion-dollar question!! I believe the race has already started, which is excellent news for the data practitioners.\nhttps://blog.fal.ai/the-unbundling-of-airflow-2/\nhttps://dagster.io/blog/rebundling-the-data-platform\nObservability into the orchestration engine is vital for operating the data pipeline reliably. Prefect writes about Orion logging, A Pythonic logging system designed to maximize observability with a minimum of effort.\nhttps://medium.com/the-prefect-blog/logs-the-prefect-way-a9e6923185fb\nStaying with the workflow orchestration, Pinterest writes about its migration of internal workflow orchestration engine Pinball to Apache Airflow.\nhttps://medium.com/pinterest-engineering/spinner-pinterests-workflow-platform-c5bbe190ba5\nApache Airflow introduced Flight SQL, a new client-server protocol developed by the Apache Arrow community for interacting with SQL databases. This tweet summarizes how significant this development is,\nhttps://arrow.apache.org/blog/2022/02/16/introducing-arrow-flight-sql/\nKevin Kho writes about Fugue, an open-source abstraction layer that provides a seamless transition from a single machine to a distributed computing setting. The article narrates the inconsistency between Pandas and PySpark, how Fugue can help to bridge the gap and increase developer productivity.\nhttps://towardsdatascience.com/introducing-fugue-reducing-pyspark-developer-friction-a702230455de\nThe median data to engineers ratio for the US companies I looked at is 1:7 compared to 1:4 for the European companies. And the design to engineers ratio is 1:9 for both groups.\nThere are many surprises in this study. Analytics companies are in fact not that analytical!!!. Are US companies automating more data engineering functions, or EU leap ahead in adopting data practitioners? I tend to believe the latter but will be curious to know the operating principles of the companies. \nhttps://towardsdatascience.com/data-engineers-and-designers-how-us-compares-to-europe-e1ce6f0a8908\nI switched between backend engineering and data engineering in my career. What excites me in data engineering is the uniqueness of thinking from the business perspective on what data points require to run a business. \nA simple question like, What is the count of unique users itself can reveal how the business operates. \nThe author writes an exciting article narrating why data engineers must have domain knowledge and an approach to acquire it.\nhttps://medium.com/pipeline-a-data-engineering-resource/why-data-engineers-must-have-domain-knowledge-and-how-to-gain-it-e9228ff3350d\nSalesforce writes about the importance of mutability in the Big Data ecosystem and an overview of its Activity Platform. We have seen the rise of LakeHouse architecture like Apache Hudi, Iceberg, and DeltaLake supports the mutability of the data. Modern OLAP engines like Apache Pinot embraces the mutability of the data. \nMutability is not bad as long as the transaction is limited to a bounded context., and it's time to embrace the mutability.\nhttps://engineering.salesforce.com/embracing-mutable-big-data-bf7106c2064d\nMicrosoft writes an exciting blog that provides an overview of some critical elements in Natural Language Understanding (NLU). The blog is an excellent overview to get the big picture of Natural Language Processing.\nPart 1: https://medium.com/data-science-at-microsoft/natural-language-understanding-whats-the-purpose-of-meaning-part-1-of-2-18a370a763\nPart 2: https://medium.com/data-science-at-microsoft/natural-language-understanding-whats-the-purpose-of-meaning-part-2-of-2-cfac532103d4\nLast November, we have seen the infamous performance benchmark warfare between Snowflake and Databricks. The blog is a curious read because the Back Market operates Delta Lake, Snowflake, and Google Big Query!!!.\nhttps://medium.com/back-market-engineering/from-delta-lake-to-bigquery-ac2cee830b24\nFoodpanda shares some great insights on Google BigQuery pricing, best practices to monitor the cost, and the utilization of reservation slots to reduce the cost by 45%.\nhttps://medium.com/foodpanda-data/how-foodpanda-reduced-45-of-our-bigquery-cost-with-reservations-slots-2c79e1d37e4\nawesome-dbt is an excellent collection of dbt resources with sample projects. Thank you, Son N. Nguyen, for sharing the repo.\nhttps://github.com/Hiflylabs/awesome-dbt\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-74", "title": "Data Engineering Weekly", "content": "The current data stack design skews towards serving a well-defined dashboard. The curious question often falls under Adhoc request that triggers the long tail of manual exploration. The author writes an excellent case for tuning data stacks to answer curious questions. Invoking curiosity through adjacency is an excellent read. \nMy take on this,\nThe data analytics world can learn a lot from the content platforms like YouTube and TikTok and influencer marketing. The data world has to live with the zombie dashboard apocalypse until then.\nhttps://prakasha.substack.com/p/how-to-design-your-data-stack-for\nContinuing on the quest for overcoming the dashboard apocalypse, Twitter writes an exciting blog narrating the Qurious [Great coincident with the previous blog on data stack for curiosity!!] app architecture,  its natural language query system with Slack interface!!!\nhttps://blog.twitter.com/engineering/en_us/topics/insights/2022/next-generation-data-insights-using-natural-language-queries\nThe Business Intelligence & Data Warehouse came a long way from Teradata to Snowflake. The consistency across the metrics is still challenging, giving the path to metrics layer/ store/ platform or Headless BI (Ha, we need naming consistency for the metrics layer first!!!). The author gives an excellent overview of what metrics store is.\nhttps://medium.com/kyligence/understanding-the-metrics-store-c213341e4c25\nShopify writes an exciting blog about its playbook for scaling machine learning. Identifying the downstream and optimizing machine learning for the business outcomes is an excellent model for anyone starting machine learning from scratch.\nhttps://shopifyengineering.myshopify.com/blogs/engineering/shopify-playbook-scaling-machine-learning\nUber writes about the system design of its deep learning system to predict the arrival time. The design of a general ETA prediction service across all Uber's businesses is an exciting read.\nhttps://eng.uber.com/deepeta-how-uber-predicts-arrival-times/\nOne of the challenging tasks of data engineering is to create a staging environment that mimics close to the production. The previous attempt is like a random event generator, or a web event simulator is not optimal. I believe anonymized production data with a sampling technique is optimal for the staging environment. It's great to see eBay write about its staging system design on the same line.\nhttps://tech.ebayinc.com/engineering/creating-high-quality-staging-data-with-a-nosql-data-migration-system/\nThere are many data predictions; this is an excellent summarization of the trends to watch for 2022. I believe operational and real-time analytics will play a vital role in data engineering, and it is great to see the author reflect the same.\nhttps://medium.com/validio/5-data-trends-in-2022-4035c099aac2\nDistance measure algorithms are vital in recommendation systems and similarity classifiers. The author did a fantastic job explaining the available distance measure algorithms, advantages, and use cases.\nhttps://towardsdatascience.com/9-distance-measures-in-data-science-918109d069fa\nPinterest writes about MemQ, a scalable cloud-native Pub-Sub system in the past. It's great to see MemQ open-sourced, and AWS writes on how it works with the AWS ecosystem.\nhttps://aws.amazon.com/blogs/storage/memq-by-pinterest-an-efficient-scalable-cloud-native-publish-subscribe-system/\nAccounting for the unobserved parts of the journey in marketing attribution is always challenging. Can we use probabilistic techniques to answer the unknown? The author writes a compelling case for marketing attribution using the Hidden Markov Model.\nhttps://jwithing.com/the-case-for-marketing-attribution/\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions.\n"}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-73", "title": "Data Engineering Weekly", "content": "Another exciting take from Benn on the current state of the analytical ecosystem. As an industry, there is a consensus on the ELT approach. There are well-established tools and practices to support the ELT practices. However, the analytical and BI tools track the problems in different directions.\u00a0\nMy take on this,\u00a0\nI believe the last-mile data consumption is going to remain. Unlike the ELT system, the BI system does have a human-in-the-loop. It might take one form to another, but it is here to stay\u2014any human-in-the-loop problems inherently\u00a0wicked problem\u00a0in nature. Just like we switching from AOL -> Yahoo Messenger -> MySpace -> Orkut -> Facebook -> WhatsApp, it is a never-ending process.\u00a0\nThe blog is an exciting overview of the three lakehouse systems, Apache Hudi, Apache Iceberg, and Delta Lake. The author narrates various features of these lakehouse systems and how they support time travel queries. I'm looking forward to the Delete operation follow-up blog.\nhttps://dacort.dev/posts/modern-data-lake-storage-layers/\nHere is the talk for the same.\nStanford HAI (Human-Centered Artificial Intelligence) writes an exciting blog on data-centric AI.\ndevelopers must turn their attention toward the data side of AI research, says James Zou, assistant professor of biomedical data science at Stanford University and member of the Stanford Institute for Human-Centered Artificial Intelligence. \u201cOne of the best ways to improve algorithms\u2019 trustworthiness is to improve the data that goes into training and evaluating the algorithm,\u201d he says\nhttps://hai.stanford.edu/news/data-centric-ai-ai-models-are-only-good-their-data-pipeline\nAll the Stanford HAI data-centric AI virtual workshops are available on Youtube.\nOne of the burning questions from all the data team is,\nHow do you build an analytics platform that is flexible enough to let people explore and answer their questions, while at the same time making sure there are guardrails in place so that similar-sounding metrics that are calculated slightly differently don\u2019t compete with each other, causing a loss of hard-earned trust in the data team?\nThe author wrote an exciting retrospect blog on their data team mission and accomplishment.\nNetflix engineering is one of those companies seen running large-scale real-time data infrastructure. The author writes an exciting blog narrating the four generations of Netflix's real-time data infrastructure.\nhttps://zhenzhongxu.com/the-four-innovation-phases-of-netflixs-trillions-scale-real-time-data-infrastructure-2370938d7f01\nResearchers project the data warehouse market to grow 34% each year until it reaches $39b in 2026. The author makes a well-articulated case where data warehouses will become the core of modern companies.\nData certification is a standard approach in many data-driven companies to streamline the business metrics and build trust in data. The author writes an excellent blog on six steps to implementing a data certification program.\nhttps://barrmoses.medium.com/stop-treating-your-data-engineer-like-a-data-catalog-14ed3eacf646\nCounting is the most complex problem in data engineering; in fact, that is the only problem we are all trying to solve other than moving data from one S3 bucket to another. - The hard truth of data engineering that no one wants to hear :-) \nHow long does Monzo's customer service staff spend doing a given task? A simple counting query, isn't it? Monzo bank narrates their experience and the complexity of handling the time data.\nhttps://monzo.com/blog/2022/02/04/how-we-validated-our-handling-time-data\nVimeo writes an exciting blog on its adoption of dbt and how it compares to our previous workflow. I like how the author focused on developer workflow rather than comparing the functionality of the tools. The pain points around having the Airflow Jinja template for SQL pipeline and dbt solving it are a great read.\nhttps://medium.com/vimeo-engineering-blog/dbt-development-at-vimeo-fe1ad9eb212\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/the-curious-case-of-aws-emr-pricing", "title": "Data Engineering Weekly", "content": "Vladimir Prus wrote an interesting\u00a0article on the motivation of their team to move Apache Spark's workload to Kubernetes. One particular quote in the article made me think about the EMR pricing model.\nThe first goal was cost reduction. With AWS EMR you pay for the EC2 instances and for the EMR itself. You can use spot instances to reduce EC2 costs, but then the EMR surcharge can add 50% to the total bill.\nBefore that, a brief overview of EMR from my perspective. Amazon EMR is a cloud big data platform for running data processing jobs, interactive SQL queries, and machine learning (ML) applications using open-source analytics frameworks such as Apache Spark, Apache Hive, and Presto. AWS EMR is perhaps the most successful version of the now-retired\u00a0Apache Ambari\u00a0project.\u00a0\nAWS continuously bundles open source solutions like Apache Spark, Apache HBase, Hive, Tensorflow, and Hudi & Apache Iceberg recently. Here is the list of open-source tools available as part of EMR.\n\u00a0The 50% EMR surcharge cost is simply because AWS EMR charges\u00a0per instance EMR license cost!!! A 50% surcharge for a service that bundles the open-source frameworks was a surprise. I started to research feature offerings from EMR that prompt these 50% license cost.\u00a0\nThe EMR File System (EMRFS) is an implementation of HDFS that all Amazon EMR clusters use for reading and writing regular files from Amazon EMR directly to Amazon S3. It is a workaround to support a consistent view of data on top of S3. However, S3 announced strong read-after-write consistency, which eliminates the need for EMRFS consistent view.\nReference: https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-plan-consistent-view.html\nPerhaps, but for auto-scaling to work, one needs to pay for the AWS CloudWatch. Further, if auto-scaling is the reason, is it fair to say disabling the auto-scaling waives off the 50 % EMR surcharge?\nSimilar to EMR, AWS does offer services on top of Kubernetes [EKS] and Kafka [MSK]. Let's compare how does the pricing look like for these services.\nYou pay $0.10 per hour for each Amazon EKS cluster that you create. You can use a single EKS cluster to run multiple applications by taking advantage of Kubernetes namespaces and IAM security policies. \nHmm, it is not per instance model rather than a flat fee per cluster.\nReference: https://aws.amazon.com/eks/pricing/\nAWS quotes MSK instances as \u201cKafka instances.\u201d The Kafka instances pricing is significantly higher than the respective EC2 instance pricing. The significant advantage thing to note here is \nYou do not pay for Apache ZooKeeper nodes that Amazon MSK provisions for you or for data transfer that occurs between brokers or between Apache ZooKeeper nodes and brokers within your clusters.\nEssentially you\u2019re getting the replication traffic for free in Kafka.\nReference: https://aws.amazon.com/msk/pricing/\nThe AWS EMR pricing model made me curious how the competitive cloud providers structure their offerings similar to EMR.\nGoogle Dataproc charges a flat hourly rate of $0.010. The pricing model looks similar to AWS EKS.\nThe Dataproc pricing formula is: $0.010 * # of vCPUs * hourly duration.\nReference: https://cloud.google.com/dataproc/pricing\nNo surprise!!! Azure HDInsight follows the same pricing strategy as EMR. Is it possible Azure simply mimics the AWS EMR pricing model?\nReference: https://azure.microsoft.com/en-us/pricing/details/hdinsight/\nI looked over again on EMR and HDInsight features. I\u2019m still trying to figure out the additional feature offering from AWS that demands per instance EMR license cost. \nI guess that is the question I leave to the readers. Please comment or reply if you\u2019ve more understanding behind the AWS EMR pricing model.\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-72", "title": "Data Engineering Weekly", "content": "\nIs the data org a service organization? It doesn\u2019t need to be, but what we can learn from the service operations. The article is an exciting read for the data team and any teams that build the internal platform to support the business growth.   \nMy highlight of the blog is\nData teams should remember that as well. We often chase big projects, like launching a new testing platform, building a new pricing forecast model, or refactoring core financial metrics. But the value of this work doesn\u2019t discount the value of analytical maintenance.\nContext switching across multiple tools hamper developer productivity. It is one of my takes on the modern data stack.\nLinkedIn writes an exciting blog about a similar problem and unifies the data science, data exploration, and business analytics workflow.\nhttps://engineering.linkedin.com/blog/2022/darwin--data-science-and-artificial-intelligence-workbench-at-li\nUber writes an exciting article about data compression algorithms comparing Snappy, GZip, and ZSTD. ZSTD is a clear winner on the optimal compression and vCore second savings. The column deletion support in parquet and multiple column reordering is informative.\nThe experiment is for one table and one partition, with four columns containing the type of UUID as string, timestamps as BIGINT, and lat/long as double, which we sort in different orders. The results show that data size is affected by ordering. Eventually, we see a 32% drop in the data size from no sorting to 4 columns sorting with the proper order (UUID, time_ms, latitude, longitude)\nhttps://eng.uber.com/cost-efficiency-big-data/\nPresto-Raptor is a shared-nothing storage engine for Presto. Presto-Raptor is an exciting tool we attempted to use almost four years back but left without documentation and momentum. It is exciting to see a new evaluation of Raptor architecture to RapterX by using Alluxio as an underlying data locality engine.\nhttps://prestodb.io/blog/2022/01/28/avoid-data-silos-in-presto-in-meta.html\nThe blog highlights the current state of Graph Intelligence and How and why the best companies are adopting Graph Visual Analytics, Graph AI, and Graph Neural Networks. The highlight of the blog\nYou don\u2019t need a graph database: none of Meituan\u2019s 30 GNNs use one.\nhttps://gradientflow.com/what-is-graph-intelligence\nThe blog is one year old, but it is a common confusion for many companies about the ownership of tracking the events. The blog builds a strong case for why the product managers should own the tracking plan.\nhttps://iterative.ly/blog/tracking-plan-ownership\nMany OLAP engines started with the root from the ads serving analytics, where the events were primarily immutable. The upsert (mutability) is often an afterthought, making it harder to adapt for business process analytics. Singular writes a blog on their workaround to make Apache Druid support upsert. It is one reason why I like the Apache Pinot design with the row-level upsert support that fits well for the business process analytics.\nhttps://singular-engineering-blog.medium.com/achieving-fast-upserts-for-apache-druid-db6c33fba466\nCould we predict experiment outcomes without even running an experiment? Pinterest writes about the Offline Replay Experimentation Framework, where the framework simulates the performance of new ideas entirely offline based on historical data.\nhttps://medium.com/pinterest-engineering/experiment-without-the-wait-speeding-up-the-iteration-cycle-with-offline-replay-experimentation-7a4a95fa674b\nHead-of-line blocking is always challenging while building queueing systems. Uber writes about solving head-of-line patterns with Kafka multi-thread Consumer Proxy with Out-of-Order Commit support. Zendesk writes about its Job Queue system named Event Job Distributor by introducing SQS as a proxy layer.\nhttps://medium.com/zendesk-engineering/building-reliability-into-uncertain-event-delivery-a09db0750ef9\nWorkday writes about multi-tenant reusable models serving infrastructure and various sharding strategy options to scale the infrastructure. An interesting approach is the bin-packed shared model with cost functions applied on each tenant/ model.\nhttps://medium.com/workday-engineering/scaling-multi-tenanted-machine-learning-applications-on-kubernetes-3f744ae543e2\nJoom writes about the current state of running Apache Spark in Kubernetes and the lesson learned along the way. The blog is an interesting fact to think about.\nWith AWS EMR, you pay for the EC2 instances and the EMR itself. You can use spot instances to reduce EC2 costs, but the EMR surcharge can add 50% to the total bill.\nI get that EMR support for Auto Scale, but other than that, it is simply a package of open-source systems. I still don't understand why AWS adds this \"Amazon EMR Price\" to the EC2 price?.. For EKS, you pay $0.10 per hour for each Amazon EKS cluster you create.\nhttps://medium.com/@vladimir.prus/spark-on-kubernetes-in-2022-32458999e831\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-71", "title": "Data Engineering Weekly", "content": "Kudos to Data Talk Club for running the data engineering Zoom camp. All the videos were published on Youtube.\nhttps://www.youtube.com/playlist?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb\nYou can find the code for the Zoom camp here,\nhttps://github.com/DataTalksClub/data-engineering-zoomcamp\nWe\u2019ve seen the LakeHouse vs. DataWarehouse (aren\u2019t they the same?) benchmark studies a couple of months back by Databricks, Snowflake, and again Databricks. It is interesting to see Snowflake announce the support for Apache Iceberg as an external table format support. \nI\u2019ve seen a pattern where raw product data often sit on S3. The data move close to the systems like Snowflake as the data get aggregated. Presto traditionally played the role of the federated query engine. It is interesting to see Snowflake stepping into it. Coincidently, AWS EMR announced support for Apache Iceberg on the same day.!! \nSnowflake Announcement:\nhttps://www.snowflake.com/blog/expanding-the-data-cloud-with-apache-iceberg/\nAWS announcementhttps://aws.amazon.com/about-aws/whats-new/2022/01/amazon-emr-supports-apache-iceberg/\nStaying on the LakeHouse architecture, Apache Hudi writes about change data capture with Debezium and Apache Hudi. The support for the \u201cincremental view\u201d (Merge on Read) makes Hudi a perfect system for Change Data Capture use cases. \nhttps://hudi.apache.org/blog/2022/01/14/change-data-capture-with-debezium-and-apache-hudi/\nThis is an excellent summarization of what happened in Apache Hudi 2021.\nhttps://hudi.apache.org/blog/2022/01/06/apache-hudi-2021-a-year-in-review\nUnderstanding the funnel of the business process flow is vital for a business. Measuring things is hard, but data helps enrich our understanding of what is going on. Amplify Partners writes an excellent blog on Sales metrics for self-service business models, sales-assisted business models, and product-qualified leads for potential upselling.\nhttps://amplifypartners.com/company-building/sales-metrics-101/\nData is a critical differentiator for a company among its competitors. As a result, we see increased adoption or talk about democratizing the data across the organization. The current answer to the quest is more documentation & cataloging. But is this enough? Is there anything we can learn from consumer media about information sharing? The authors compare news sites, Wikipedia, Yelp & Google.\nhttps://towardsdatascience.com/good-data-citizenship-doesnt-work-265f13a37fa5\nWe need to look at the social news feed industry and the information pushed to the end-users rather than polling. \nSpotify writes about ML Home, the internal user interface for Spotify\u2019s Machine Learning Platform. The blog focuses on product lessons learned along the way in the quest to entrench Spotify\u2019s ML ecosystem.\nhttps://engineering.atspotify.com/2022/01/19/product-lessons-from-ml-home-spotifys-one-stop-shop-for-machine-learning/\nOne of the significant features of Apache Pinot is the ability to define an indexing strategy for each column. The talk gives excellent insights on how text search indexing works in Apache Pinot.\nTwitter writes a quick note on its ongoing effort to invest in privacy-enhancing tech and the partnership with openmined.org.\nhttps://blog.twitter.com/engineering/en_us/topics/insights/2022/investing-in-privacy-enhancing-tech-to-advance-transparency-in-ML\nSquare writes about implementing DAG level ACL support for Apache Airflow. The blog discusses various auth support available in Apache Airflow and the implementation of REMOTE_USER mode.\nhttps://developer.squareup.com/blog/secure-apache-airflow-using-customer-security-manager/\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-70", "title": "Data Engineering Weekly", "content": "RudderStack\u00a0Provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools.\nLet\u2019s start this week\u2019s edition with some excellent SQL tips.\nAnalytics engineering practices are becoming standard across the analytics world. The author summarizes why we should treat SQL as code and the need for a version control system for analytics.\nhttps://blog.devgenius.io/why-google-treats-sql-like-code-and-you-should-too-53f97925037e\nContinuing on the analytical engineering topic, the author writes about some of the fundamental tools you\u2019ll require and any technical job. It's nice to see SQL top there as required, not just for analytics but for all technical jobs. \nhttps://vickiboykis.com/2022/01/09/git-sql-cli/\nYou can see a similar sentiment in this tweet.\nEfficient feedback with the auto-remediation system can save many on-call hours. Netflix writes about the regex rule engine to diagnose the most common batch and real-time system errors.\nhttps://netflixtechblog.com/auto-diagnosis-and-remediation-in-netflix-data-platform-5bcc52d853d1\nLinkedIn writes about its algorithmic fairness and explainability design to measure and mitigate unfair bias at scale. The Fairness training toolkit and the continuous feedback look to measure the success of the Fair model analyzer are exciting reads.\nhttps://engineering.linkedin.com/blog/2022/a-closer-look-at-how-linkedin-integrates-fairness-into-its-ai-pr\nWe've seen an increasing pattern of adopting declarative DSL patterns for end-to-end feature engineering in real-time and batch mode. Airbnb has written about its declarative feature engineering system in the past. DoorDash writes an exciting blog describing Fabricator, its declarative feature engineering framework.\nhttps://doordash.engineering/2022/01/11/introducing-fabricator-a-declarative-feature-engineering-framework/\nSupport for broad integration patterns like push-pull, and most importantly, analytics on top of the metadata is vital for the modern metadata platform. Answering questions like \"show me all the datasets that contain PII, accessed directly or indirectly via lineage within the last three months\" are vital to gain insight into the data management system. Metaphor writes about DataHub and how it supports the modern metadata platform capabilities.\nhttps://metaphor.io/blog/the-modern-metadata-platform\nJoin RudderStack and Avo for a live webinar on January 27 @ 9am PT to learn how you can increase your event data quality and streamline your behavioral data pipelines.\nhttps://www.avo.app/event-driven-infrastructure-webinar\nAirflow is one breakthrough system that brings code as a pipeline pattern to data engineering. Since then, orchestration engines like Prefect & Dagster have taken the concept to the next level with Airflow learning. The author takes some of the pain points of running Airflow and compares it with Dagster & Prefect.\nhttps://towardsdatascience.com/airflow-prefect-and-dagster-an-inside-look-6074781c9b77\nI shared my thoughts on the DAG model of data pipeline is obsolete here.\nTrip.com writes about its expereince switching from ClickHouse to StarRocks for their real-time analytical database. TIL about Star rocks, and seems like an exciting system. I\u2019m continuing to hear more performance issues with ClickHouse, and am curious to know folks experience with ClickHouse. Please DM @data_weekly if you\u2019re using ClickHouse in production.\nhttps://starrocks.medium.com/trip-com-starrocks-efficiently-supports-high-concurrent-queries-dramatically-reduces-labor-and-1e1921dd6bf8\nLastly, I enjoyed attending CIDR 2022 last week, and was delighted to see the latest research on the data ecosystem. All the CIDR talks and papers published here\nhttp://cidrdb.org/cidr2022/program.html\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-69", "title": "Data Engineering Weekly", "content": "RudderStack\u00a0Provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools.\nDevoted Health shares its experience running dbt for a year. The two-phase commit strategy to publish datasets, wrapper tooling to handle authentication, unit testing framework on top of dbt are some exciting reads. The blog is an excellent reminder that any tool integrated well with the developer workflow multiplies its effectiveness.\nhttps://tech.devoted.com/one-year-of-dbt-b2e8474841ca\nThe article captures the trend on the commoditization of data infrastructure complexity. Any successful technology should move from niche to commoditized to survive longer-term. \nHowever, it is vital to remember What Goes Around Comes Around in software engineering. In the past, companies used to run their own CRM, HR system in-house, and tools like Microsoft SSIS, Informatica successfully commoditized the ETL until the underlying business model changed to the SaaS model. With the likes of Fivetran and Airbyte, we are just reinventing SSIS. We don't know what the underlying business model changes in the next decade, so long live ETL.\nhttps://groupby1.substack.com/p/data-engineering\nWhat Goes Around Comes Around Paper. [Must read in case you missed it]\nThis article is an excellent checklist before starting your data mesh journey. The author highlights the need for organizational maturity before taking the data mesh approach since the principles require a strong foundation & tooling.\nhttps://medium.com/google-cloud/10-reasons-why-you-should-not-adopt-data-mesh-7a0b045ea40f\nThe blog is an excellent analysis of the data engineers ratio in an organization and how the organization's engineering culture impacts the hiring pattern. It is interesting to see platform/ marketplace companies hire more data engineers than B2B companies.\nhttps://mikkeldengsoe.substack.com/p/data-to-engineers\nHalodoc writes an excellent overview of its data platform 2.0, focusing on the LakeHouse architecture. The blog narrates some of the key takeaways from implementing Apache Hudi, a configuration-driven approach to onboarding new tables. Kudos for including the end-to-end reference architecture diagram. \nhttps://blogs.halodoc.io/lake-house-architecture-halodoc-data-platform-2-0/amp/\nJoin RudderStack and Avo for a live webinar on January 27 @ 9am PT to learn how you can increase your event data quality and streamline your behavioral data pipelines.\nhttps://www.avo.app/event-driven-infrastructure-webinar\nPicnic writes about its migration story from AWS Kinesis to Confluent Cloud. The prime motivation behind the move seems to be to have a longer retention time and adopt the broad Kafka ecosystem. Interestingly, Kinesis can't extend its hot data retention for more than seven days!!\nhttps://blog.picnic.nl/picnic-analytics-platform-migration-from-aws-kinesis-to-confluent-cloud-adb06601c78\nPayPal writes about ML-driven sales pipeline management. The lightweight two-layer ensemble classifier framework as a solution to progressive prediction problems is an exciting read.\nhttps://medium.com/paypal-tech/sales-pipeline-management-with-machine-learning-15398bab913b\nYouzan writes an in-depth overview of their migration of data orchestration engine from Airflow to Apache Dolphine. The article contains an excellent comparison of Airflow and Dolphin regarding scalability and high availability.\nhttps://medium.com/@ApacheDolphinScheduler/from-airflow-to-apache-dolphinscheduler-the-evolution-of-scheduling-system-on-youzan-big-data-ec897f310f91\nI firmly believe that native indexing support for semi-structured data is a must-have feature in modern data warehouse systems. It is exciting to see Google BigQuery announce native support for semi-structured data.\nhttps://cloud.google.com/blog/products/data-analytics/bigquery-now-natively-supports-semi-structured-data\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-68", "title": "Data Engineering Weekly", "content": "RudderStack\u00a0Provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools.\nA comprehensive overview of databases in 2021, from the dominance of PostgreSQL to the Database vendor fights over performance benchmark results. It is an exciting time for cloud databases where companies like ClickHouse Inc, StartTree, Imply & Single Store collectively raised around $480M in 2021.\nhttps://ottertune.com/blog/2021-databases-retrospective/\nA collection of exciting notes in case you missed the dbt coalesce 2021\nDay 1: https://www.shipyardapp.com/blog/dbt-coalesce-2021-day-1-takeaways/\nDay 2: https://www.shipyardapp.com/blog/dbt-coalesce-2021-day-2-takeaways/\nDay 3: https://www.shipyardapp.com/blog/dbt-coalesce-2021-day-3-takeaways/\nA comprehensive overview of the ML platform across the companies. It is not so surprising to see most of the platform developed in-house. I hope the year 2022 pave the way to democratize and simplify MLOps. \nhttps://towardsdatascience.com/lessons-on-ml-platforms-from-netflix-doordash-spotify-and-more-f455400115c7\nJob orchestration and scheduling are the core parts of data engineering. In this InfoQ talk, Netflix narrates its data pipeline scheduler design and lessons learned from operating large-scale pipelines.\nhttps://www.infoq.com/presentations/netflix-big-data-orchestrator/\nData analytics goes through its hierarchy of needs, from descriptive to predictive analytics to prescriptive action. Shopify shares its journey on handling pandemic data and the data science maturity model. \nhttps://shopifyengineering.myshopify.com/blogs/engineering/shopify-unique-data-science-hierarchy-of-needs\nJoin The Data Stack Show Live for a special panel with experts from Databricks, dbt, Fivetran, Essence VC, and Hinge. The panel will look at the modern stack from all angles and discuss the future of data tooling.\nhttps://rudderstack.com/video-library/the-data-stack-show-live-what-is-the-modern-data-stack/\nWe often underestimate the lead time to train a new engineer to an internal tool, which is a significant push for companies to adopt open standards/ open source systems. Etsy writes about its journey through the ML platform redesign on Google Cloud.\nhttps://codeascraft.com/2021/12/21/redesigning-etsys-machine-learning-platform/\nThe flexibility of Notebooks also brings the challenge of disconnected infrastructure in an organization. Twitter narrates the tools that helped simplify notebook lifecycle management and integrated development environments.\nhttps://blog.twitter.com/engineering/en_us/topics/infrastructure/2021/advancing-jupyter-notebooks-at-twitter---part-1--a-first-class-d\nWhat are the top boot camps and Universities for Data Scientists? The author did an excellent data-scientific way to figure this out!! It is interesting to see Udacity on top of traditional universities, and no doubt it is the top Bootcamp for data science.\nhttps://python.plainenglish.io/find-the-top-bootcamps-for-data-professionals-from-over-5k-profiles-92c38b10ddb4\nData lakes have come a long way, and supporting transactions is now an essential characteristic of Lakehouse design. The author writes an exciting overview of different patterns of concurrency control and Apache Hudi's support for it.\nhttps://www.linkedin.com/pulse/lakehouse-concurrency-controls-we-too-optimistic-vinoth-chandar/\nYotpo writes about its CDC pipeline using Kafka & Debezium for email services. Domain event sourcing is increasingly adopting the transactional outbox pattern. IMAO and Debezium is an underrated system, yet the critical open source solution in data engineering. It deserves much more limelight than what it gets now.\nhttps://medium.com/yotpoengineering/scheduling-millions-of-messages-with-kafka-debezium-6d1a105160c\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-67", "title": "Data Engineering Weekly", "content": "RudderStack\u00a0Provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools.\nAirbnb writes a third-part series on automating data production at scale, focusing on CDC pipelines. Automated data privacy management is critical to GDPR & California Consumer Privacy Act. Airbnb walkthrough automation & alerting are in place with its data production service. TIL: Thrift & Protobuf does support custom annotations.\nhttps://medium.com/airbnb-engineering/automating-data-protection-at-scale-part-3-34e592c45d46\nPart 1: https://medium.com/airbnb-engineering/automating-data-protection-at-scale-part-1-c74909328e08\nPart 2: https://medium.com/airbnb-engineering/automating-data-protection-at-scale-part-2-c2b8d2068216\nMost real-world datasets have a time component, and forecasting the future can unlock significant value. Google writes about Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting details the Temporal Fusion Transformer (TFT), an attention-based DNN model for multi-horizon forecasting.\nhttps://ai.googleblog.com/2021/12/interpretable-deep-learning-for-time.html\nThe increased specialization of data engineering opens a lot of innovations on effectively utilizing data at scale. The author captures a timeline of the data engineering practices from the data warehouses of Kimball to the metrics store model. If the metrics store gains mass adoption, I presume we will see a new family of specialized metrics databases similar to Prometheus or InfluxDB.\nhttps://towardsdatascience.com/a-brief-history-of-the-metrics-store-28208ec8f6f1\nData Visualization is the interface between insights consumers & producers. Human perception heavily influences the interpretation of data visualization. Sometimes, the insight producer dumps all the visualization in front of the audience and leaves the human interpretation to play its parts. The author recommends a set of curated processes to tell data stories meaningfully.\nhttps://medium.com/data-science-at-microsoft/anatomy-of-a-chart-9e420dc8495b\nStaying with data visualization, colors significantly shape perception. The author writes about the best practices for choosing color combinations and talks about the Viz Palette, a tool to pick and optimize colors in and out of JavaScript.\nhttps://medium.com/@Elijah_Meeks/viz-palette-for-data-visualization-color-8e678d996077\nJoin The Data Stack Show Live for a special panel with experts from Databricks, dbt, Fivetran, Essence VC, and Hinge. The panel will look at the modern stack from all angles and discuss the future of data tooling.\nhttps://rudderstack.com/video-library/the-data-stack-show-live-what-is-the-modern-data-stack/\nConfluent writes about what could go wrong with the Kafka outage and best practices to handle the failures. The blog contains some exciting techniques on how Kafka producers can gracefully handle failure. The usage of fsync vs. the likes of the async disk API is an exciting read.\nhttps://www.confluent.io/blog/how-to-survive-a-kafka-outage/\nContinuing on the Kafka infrastructure story, Yelp writes about its Kafka architecture on Kubernetes. The blog writes an overview of Yelp's usage of CruiseControl to automate the Kafka operations, and I highly recommend using it in production to reduce the operational toll.\nhttps://engineeringblog.yelp.com/2021/12/kafka-on-paasta-part-one.html\nModern data warehouses build on multiple data sources and diverse data producers and consumers. As complexity grows, the need for standardization of ownership, alerting, testing & quality plays a significant role in establishing trust in the data platform. KeepTruckin shares its experience of how the standardized tooling & metadata saved their data org.\nhttps://medium.com/keeptruckin-eng/how-metadata-saved-our-data-organization-cab3335eb4ae\nIt is evident that establishing standardization around data asset management tooling greatly helps the data organization, but how does one start to think about it. The author establishes the case for thinking of analytics tools as a product to bring integrity to the data platform.\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-66", "title": "Data Engineering Weekly", "content": "RudderStack\u00a0Provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools.\nThere is a lot of exciting stuff to catch up on in DataEngineeringWeekly this week, especially since we took off last week.\nCoalesce - the analytics engineering conference is a delight to watch this week. dbt as a metric layer is an exciting evolution and is the key highlight of the conference. If you missed the conference, you can watch all the replays here.\nhttps://coalesce.getdbt.com/replays/\nOpen-source Data Stack conference is another exciting data conference focused on the modern data stack in the open-source world. The author writes an excellent summary of the conference.\nhttps://jameskle.com/writes/open-source-data-stack-2021\nYou can catch up on all the replays of the open source data stack conference 2021 here.\nhttps://www.opensourcedatastack.com/stage/events\nAWS published the top announcements from the AWS re: Invent 2021 conference. The top announcements from the data engineering perspective are,\nAWS lake formation cell-Level Security\nAWS data exchange for third-party APIs\nAWS Redshift serverless(?)\nSageMaker Studio Lab, a free service to learn and experiment with ML\nThe availability of new storage-optimized EC2 instances (Im4gn & Is4gen)\u00a0\nhttps://aws.amazon.com/blogs/aws/top-announcements-of-aws-reinvent-2021/\nLinkedIn shares its analytical stack transition story from Teradata data warehouse systems to open source big data technologies. The analytical stack includes 1400+ datasets, 900+ data flows, and 2100+ users. The migration strategy with improving the data model is an exciting read.\nhttps://engineering.linkedin.com/blog/2021/evolving-linkedin-s-analytics-tech-stack\nThough the title says the top data books, the shortlisted books focus on data visualization or Tableau platform. Nonetheless, it is great to read data visualization books. \nhttps://www.tableau.com/about/blog/2021/12/andy-cotgreave-top-data-books-2021\nJoin The Data Stack Show Live for a special panel with experts from Databricks, dbt, Fivetran, Essence VC, and Hinge. The panel will look at the modern stack from all angles and discuss the future of data tooling.\nhttps://rudderstack.com/video-library/the-data-stack-show-live-what-is-the-modern-data-stack/\nErik Bernhardsson writes an exciting prediction on cloud vendors' trends builds a case on top of the success of Snowflake over Redshift. It is undoubtedly true in the analytical world where AWS solutions always package and sell open-source tools but never go beyond simplifying the developer workflow. A couple of interesting predictions to highlight,\nKubernetes will be some weird thing people loved for five years, just like Hadoop was from 2009-2013, but the world will move on.\nYAML will be something old jaded developers bring up after a few drinks. You know it's time to wrap up at the party at that point.\nhttps://erikbern.com/2021/11/30/storm-in-the-stratosphere-how-the-cloud-will-be-reshuffled.html\nA streaming sliding window with a finite interval is a go-to ML metrics monitoring strategy. The threshold, window size, and alerts are still defined manually for each metric. The author argues why this procedure to evaluate ML on streams of data is broken, highlighting representation differences, varying sample size & delayed feedback on the sliding window.\nhttps://www.shreya-shankar.com/rethinking-ml-monitoring-1/\nMicrosoft announces the release of SynapseML (previously MMLSpark), an open-source library that simplifies the creation of massively scalable machine learning (ML) pipelines. With SynapseML, developers can build scalable and intelligent systems for solving challenges in domains such as Anomaly Detection, Computer Vision, Deep Learning, Text analytics, etc.\nhttps://www.microsoft.com/en-us/research/blog/synapseml-a-simple-multilingual-and-massively-parallel-machine-learning-library/\nMonzo writes about its journey to bring column-level lineage to track and understand scope changes across the data warehouse and automatically detect unused columns. TIL about ZetaSQL, which helps to parse & analyze BigQuery Sql, and looking forward to playing around with it.\nhttps://medium.com/data-monzo/mapping-our-data-journey-with-column-lineage-56209c00606d\nPayPal writes about Rule Execution Framework to manage a centralized rule configuration system to manage data quality rules & rulesets. The adoption of SQL to write complex data validation rules and the workflow focused on the domain owners to define the data quality rules are exciting.\nhttps://medium.com/paypal-tech/building-data-quality-into-the-enterprise-data-lake-9dec305c3757\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-a-year-in", "title": "Data Engineering Weekly", "content": "RudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools. Sign up today to get 5M events/month for free through the end of 2022. \nJoin The Data Stack Show Live for a special panel with experts from Databricks, dbt, Fivetran, Essence VC, and Hinge. The panel will look at the modern stack from all angles and discuss the future of data tooling. \nhttps://rudderstack.com/video-library/the-data-stack-show-live-what-is-the-modern-data-stack/\nYes, this week's edition is slightly different from the usual. As we are nearing the end of 2021, I wanted to step back and review the milestones the data engineering weekly reached and some insights from the analysis.\u00a0\nYou can see the complete analysis & code here\nhttps://share.streamlit.io/ananthdurai/dataengweekly-analysis/main/dataengweekly_analysis/app.py\nGithub: https://github.com/ananthdurai/dataengweekly-analysis\n\nThe Data Engineering Weekly came a long way this year. We got an impressive growth rate of 293% increase in the number of subscribers.\ud83c\udf89\ud83e\udd73\ud83c\udf88 Thank you all for your kind support, reading & sharing the newsletter.\nIn 2021, We created a GitHub repo for our readers to contribute articles and share their views about the latest thing happening in data engineering. If you have not done before, here is the repo to contribute your articles.\nhttps://github.com/ananthdurai/dataengineeringweekly\nData Engineering Weekly also got our title sponsorship from Rudderstack and Link sponsorship from MonteCarloData. Thank you both, Rudderstack & MonteCarloData.\nTowards the end of 2020, Data Engineering Weekly published Back To The Future: Data Engineering Trends 2020 & Beyond: The lookback of the latest development in data engineering 2020 & thoughts on 2021 and beyond. If you\u2019ve not read it, here is the link\nI recently gave a talk on the emerging trends at the Crunch Conf - 2021. Here are the slides of the talk.\nhttps://speakerdeck.com/vananth22/back-to-the-future-emerging-trends-in-data-engineering\n\u00a0Here are the top 3 trends summary that we talked about in the data engineering newsletter.\nMetadata management will become mainstream. The data lineage, quality, and discovery tools will merge into a unified data management platform.\nData Mesh principles will get adopted more and drive a unified data management platform.\nLakehouse systems like Hudi, Iceberg, and Deltalake will significantly shape the data engineering architecture.\nAs we are approaching the end of 2021, let\u2019s step back and see what happened to the predictions?\u00a0\nThe idea is simple, Let\u2019s take a look at all the articles shared in data engineering weekly and run through some simple N-Gram analysis to see if we can discover any insights. The N-Gram analysis runs on three-part of the content.\nThe purpose is to find which company publishes the most data engineering articles.\u00a0\nThe articles often contain the keywords as part of the URLs. The NGram analysis on URL leads to some exciting trend insights.\nLastly, we crawled all the blog content published to understand the keyword trends.\nThanks to the open-source libraries, text analytics became much simpler to run through. The libraries I used are\nYAKE! is a lightweight, unsupervised automatic keyword extraction method based on statistical text extracted from single documents to select the most critical keywords.\u00a0\nGitHub: https://github.com/LIAAD/yake\nStreamlit.io is the fastest way to build and share data apps. Streamlit turns data scripts into shareable web apps in minutes. All in Python, and we host & share it for free!!! I hosted the data engineering analytics result in streamlit!!! If you've not tried streamlit.io, I highly recommend trying it out.\u00a0\naiostream provides a collection of stream operators that can combine to create asynchronous pipelines of operations. It tremendously reduces the boilerplate code of writing async code in Python and helps to increase the productivity of the analytics since it is a time-consuming task to run each link scrapping sequentially.\u00a0\nGitHub: https://github.com/vxgmichel/aiostream\nThe 1-gram & 2-gram analytics shows mostly the technology & tools the data engineering teams are using. Note: You can see the complete analysis here https://share.streamlit.io/ananthdurai/dataengweekly-analysis/main/dataengweekly_analysis/app.py\nWe've seen several new companies coming in data discovery & metadata management. There were eight editions in the 2020 data engineering edition featuring stories about companies adopting data discovery solutions, but in 2021 it is limited to 3 editions. Data Engineering Weekly published an metadata edition 2020. My interpretation is that the data discovery reaches  \"Late majority\" in the adoption curve. Collibra raised $250M fundingDoubling the valuation in one year is a classic sign of the industry's maturity.\nWe have indeed seen companies as Adventa & Intuit write about their adoption story of Data Mesh principles. We've seen the principles redefined and adopted from the individual implementor's perspective. The lack of tooling & standards makes the data mesh principles a loosely held idealistic view than a breakthrough in data engineering. So I would give this still on the \"Early Adopters\" stage.\nOne of the (not) surprising findings from the N-Gram analysis is the number of mentions of the modern data stack. The modern data stack is a collection of cloud-hosted data platforms that run on cloud databases like Snowflake, Redshift & BigQuery. Though we have seen companies like Adobe, Uber & Netflix talk about its adoption of Iceberg & Apache Hudi, the adoption of modern data stack shows companies more prefer commercial solutions. We've seen commercialization of LakeHouse infrastructure in 2021 with companies like Tabular for Iceberg, the EMR offering of Apache Hudi, and the benchmark street fights between Snowflake & Databricks.\nI would say LakeHouse is still in the \"Innovator Phase\" and still, a long way to go.\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions.\n"}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-65", "title": "Data Engineering Weekly", "content": "RudderStack\u00a0Provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools.\nAttending AWS re: Invent this year? If so, Join data leaders at AWS re: Invent for an exclusive networking and happy hour at TAO in the Venetian Hotel & Casino on 12/1/2021. Enjoy conversations, hors d'oeuvres, drinks, and music at this exclusive data event hosted by Monte Carlo and Trifacta.\nJoin us at AWS re: Invent\nI have always been a big fan of what FreeCodeCamp.org is doing to create free CS educational content. If you're starting to learn Data Analytics with Python, this is a fantastic course to start.\nMeta (Facebook) published videos of its recent data observability summit.  I've not watched all videos and looking forward to watching data and ML observability in the public cloud & \"Catch me if you can\": Keeping up with ML in production.\nhttps://m.facebook.com/watch/9445547199/490224945331402\nNetflix published the fifth post in a multi-part series on how Netflix uses A/B tests to inform decisions and continuously innovate its products. The fifth part focuses on how Netflix uses the test results to support decision-making in a complex business environment.\nhttps://netflixtechblog.com/building-confidence-in-a-decision-8705834e6fd8\nSpotify shared a two-part post on its ML adoption story & lesson learned to build personalized content on its Homepage. The blog is an exciting narration of thinking through converting a rule-based application into ML-driven.\nPart 1: https://engineering.atspotify.com/2021/11/15/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-i/\nPart 2: https://engineering.atspotify.com/2021/11/18/the-rise-and-lessons-learned-of-ml-models-to-personalize-content-on-home-part-ii/\nThe code we write fundamentally, the reflection of the human thought process, and human bias in the system are harmful by-products. Being dependent on existing data tends to privilege what systems are already in place. Vimeo writes an exciting blog on that line on how its search & recommendation team approaches uncover the bias in its ML models.\nhttps://medium.com/vimeo-engineering-blog/uncovering-bias-in-search-and-recommendations-751b01d1c874\nJoin The Data Stack Show Live for a special panel with experts from Databricks, dbt, Fivetran, Essence VC, and Hinge. The panel will look at the modern stack from all angles and discuss the future of data tooling.\nhttps://rudderstack.com/video-library/the-data-stack-show-live-what-is-the-modern-data-stack/\nPinterest writes about its internal pub-sub system called MemQ, born out of learning from operating Kafka. The system design of the pluggable replicator storage layer is the highlight of the design. The key takeaways on operating Kafka is a must-read.\nNot every dataset needs a sub-second latency service. Latency and cost should be inversely proportional (lower latency should cost more)\nA PubSub system's storage and serving components must be separated to enable independent scalability based on resources.\nOrdering on reading instead of writing provides the required flexibility for specific consumer use cases (different applications can have different for the same dataset)\nStrict partition ordering is not necessary at Pinterest in most cases and often leads to scalability challenges.\nRebalancing in Kafka is expensive, often results in performance degradation, and harms customers on a saturated cluster.\nRunning custom replication in a cloud environment is expensive.\nhttps://medium.com/pinterest-engineering/memq-an-efficient-scalable-cloud-native-pubsub-system-4402695dd4e7\nPayPal writes about its performance benchmark on improving the throughput of its Kafka cluster. The performance gain from switching java GC from CMS to G1GC is an interesting takeaway.\nhttps://medium.com/paypal-tech/kafka-consumer-benchmarking-c726fbe4000\nSubscribing to a real-time CDC pipeline to get the update in a scalable way is powerful. Confluent writes about how ksqlDB supports efficiently subscribe to real-time SQL queries. However, the lack of support for the group by partition by & window expression is a disappointment.\nhttps://www.confluent.io/blog/push-queries-v2-with-ksqldb-scalable-sql-query-subscriptions/\nFinally, the blog narrates the practical implementation of Type 1 & Type 2 slowly changing dimensions with dbt.\nhttps://servian.dev/modelling-type-1-2-slowly-changing-dimensions-with-dbt-1b80078f290a\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-64", "title": "Data Engineering Weekly", "content": "RudderStack\u00a0Provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools.\nThrilled to announce the release of O'Reilly's first-ever book on data quality, Data Quality Fundamentals: A Practitioner's Guide to Building More Trustworthy Data Pipelines! In this book, the Data Observability category creators make the business case for data trust and explain how data leaders can tackle data quality at scale by leveraging best practices and technologies used by some of the world\u2019s most innovative companies.\nDownload Your Free Copy\nRecent work on building multi-cloud data identity & access management allowed revisiting this space. The opinion poll shows Apache Ranger is the widely adopted solution, and the cloud provider's solution is second to Apache Ranger.\n\nSasS applications emerging from business process solutions to full-suite data workflow engines provide lower cost & faster distribution to run a business effectively. The article raises an interesting question. Does the role of Data Warehouse changing to a backend of data?!! The following tweet also echoes a similar thought on the role of SaaS applications in modern data engineering. It would be interesting to see this trend and how it shapes the data warehouse systems as we know of today. \nhttps://pchase.substack.com/p/thenewbackend\nConfluent writes about its adoption story of Apache Druid for its Cloud Metrics API services. The scalability challenges, hardware choices, and compaction strategies are an exciting read.\nhttps://www.confluent.io/blog/scaling-apache-druid-for-real-time-cloud-analytics-at-confluent/\nExpedia shares its high-level overview of real-time user analytics infrastructure. The blog narrates a good refresher for Apache Cassandra with some trivia quizzes!!!\nhttps://medium.com/expedia-group-tech/apache-cassandra-for-real-time-user-analytics-at-expedia-group-4b612bac05a7\nFeature engineering is one of the most significant challenges in applied machine learning. Flyte makes it easy to create concurrent, scalable, and maintainable workflows for machine learning and data processing. Feast provides the feature registry, an online feature serving system, and Flyte can engineer the features. The blog narrates how two systems complement each other and the interoperability among them.\nhttps://betterprogramming.pub/bring-ml-close-to-data-using-feast-and-flyte-bd0cb5608678\nThe team at RudderStack provides a detailed breakdown of their data stack. The write-up includes details on how they eat their own dog food using bi-directional RudderStack pipelines to connect the entire stack, so they can extract full value from every component.\nhttps://rudderstack.com/blog/rudderstacks-data-stack-deep-dive\nCoinbase writes about its adoption story of AWS MSK and the benefits it provides from Kafka security service (KSS), tooling & Kafka connect service. Coinbase reduced the end-to-end streaming pipeline latency by 95% when switching from Kinesis (~ 200 msec) to Kafka (< 10 msec).\nhttps://blog.coinbase.com/how-we-scaled-data-streaming-at-coinbase-using-aws-msk-4595f171266c\nPolicyGenius writes about its data warehouse system built on Google Cloud & Airflow. It is exciting to see the Google sheet is an important data source. The data classification on stages of data lifecycle as the Source data, Foundational view, Unified view & the Reporting view is a refreshing take on the pipeline classification.\nhttps://medium.com/policygenius-stories/building-a-data-warehouse-on-google-cloud-platform-that-scales-with-the-business-2b07f7c7292e\nScentbird writes some limitations with AWS Redshift & Glue-based data warehouse solution and its migration journey to Snowflake. The narration around Glue limitations is exciting, and I presume these limitations will apply to most of the no-code UI-based ETL engines.\nhttps://medium.com/@Not4j/scentbird-analytics-2-0-migrate-from-redshift-to-snowflake-redesign-etl-process-e79611723a90\nhttps://github.com/erika-e/dbt-tips\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-63", "title": "Data Engineering Weekly", "content": "RudderStack\u00a0Provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools.\nFormer CEO of Snowflake Bob Muglia joins a packed lineup of data leaders pioneering the technologies & processes shaping data engineering, along with First Chief Data Scientist of the U.S., founder of the Data Mesh, and many more!\nClick To Get Your Free Ticket For All Data Engineering Weekly Readers\nThe blog is a  comprehensive narration of the recent trends in data engineering and how the modern data stack is reshaping data engineering. As noted in the blog, the critical trend to watch,\nwe've seen the Spark ecosystem increasingly become a database through the rise of SparkSQL. Meaning not only are the databases increasingly good at supporting ETL workloads, but some ETL systems are also increasingly good at acting as a database.\nMy two cents, the explosion of the modern data stack brings \"best of the breed.\" solution. However, the core developer workflow still revolves around the ELT systems. It will be an exciting trend to watch how ETL systems emerge to provide integrated data experience.\nhttps://preset.io/blog/reshaping-data-engineering/\nHow do you measure the success of the analytical practices? The fact that the feedback loop for a business decision is long and often can't assess the counterfactual, is it still makes sense to measure the analytical practice by the outcome? \n\"Time To Insight\" is the north star metric to measure the success of a data engineering team. The blog narrates how the \"Time To Make Decision\" metric measures analytical practice on the same line.\nhttps://benn.substack.com/p/method-for-measuring-analytical-work\nOne of the challenges of data infrastructure is to balance the query performance and the cost. Twitter writes an exciting blog narrating how machine learning-driven optimization is on top of Presto to optimize resource usage.\nhttps://blog.twitter.com/engineering/en_us/topics/insights/2021/forecasting-sql-query-resource-usage-with-machine-learning\nSQL is a powerful language that allows us to express complex questions of our data with ease. How does SQL adopt not only the data at rest but also for the streaming data? The article narrates how the push vs. pull query execution changes the query complexity from O(number of records in input table) vs. O(rate of table change).\nhttps://www.confluent.io/blog/databases-meet-stream-processing-the-future-of-sql/\nThe modern data stack predominately focused on the concept of a LakeHouse architecture. It takes the best attributes from traditional data warehouses and runs on platforms with data lake storage architectures. On following Confluent's thoughts on streaming SQL, the author raised great questions on the role of Fog computing in the modern data platform.\nhttps://ryanwgross.medium.com/designing-data-platforms-to-harness-the-power-of-fog-computing-cf7dc29050b1\nJoin leaders from Snowflake, Mammoth Growth, RudderStack, and Mixpanel to learn why the most sophisticated teams architect their data stacks around the data warehouse.\nhttps://rudderstack.com/video-library/the-modern-data-stack-is-warehouse-first\nStarTree writes about why Pinot is fast, explaining various indexing & multi-model support. JSON indexing to support semi-structured data analysis, aggregation optimization using star tree indexing are some of the highlights to read.\nPart 1: https://www.startree.ai/blogs/what-makes-apache-pinot-fast-chapter-1/\nPart 2: https://www.startree.ai/blogs/what-makes-apache-pinot-fast-chapter-ii/\nQonto shares its experience scaling Airflow on Kubernetes. The pod template files to optimize the resource consumption for the sensor & task operators, monitoring the lifecycle of a task & cluster elasticity are some of the exciting reads.\nhttps://medium.com/qonto-way/scaling-airflow-on-kubernetes-lessons-learned-a0d3d0417fc1\nMeltwater writes about its journey to adopt the data lake from a single database for the reporting solution. The cost comparison matrix is a fascinating study that shows S3 + Athena is 6X cost-efficient than the RDS solution.\nhttps://underthehood.meltwater.com/blog/2021/11/05/our-journey-from-database-to-data-lake/\nA great read on practical tips on reducing the Kafka infrastructure cost, focus AWS instance type, compression, rake aware consumers to fetch data from closest replica, cluster rebalancing & cluster tuning configurations.\nhttps://leevs.dev/kafka-cost-reduction/\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-62", "title": "Data Engineering Weekly", "content": "RudderStack\u00a0Provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools.\nHear from data leaders pioneering the technologies & processes shaping data engineering. Featuring First Chief Data Scientist of the U.S., founder of the Data Mesh, and many more!\nClick To Get Your Free Ticket For All Data Engineering Weekly Readers\nThe success of any developer framework depends on how efficiently the tool integrates with the developer workflow. Netflix writes about open source Metaflow G.U.I. for monitoring and operating its full-stack framework for data science.\nhttps://netflixtechblog.com/open-sourcing-a-monitoring-gui-for-metaflow-75ff465f0d60\nThe article gives a good overview of Netflix's Metaflow, demonstrating the scaling and cloud integration support of Metaflow with the A.W.S. step function.\nhttps://towardsdatascience.com/how-netflix-metaflow-helped-us-build-real-world-machine-learning-services-9ab9a97cdf33\nPresto is known for interactive queries against data warehouses, but it has evolved into a unified SQL engine on open data lake analytics for interactive and batch workloads. Apache Spark execution engine with Presto is an exciting development to bring one SQL for batch & interactive workload.\nhttps://prestodb.io/blog/2021/10/26/Scaling-with-Presto-on-Spark.html\nReliable data infrastructure is critical for a faster \u201ctime-to-insight\u201d for analytical queries. Shopify writes about its approach to benchmarking Trino infrastructure. The Key lessons section highlighting\nA solid statistics foundation is crucial.\nMany nuances of an environment can unintentionally influence results\nEnsure you gather all the relevant data\nThe principles are essential for operating any data-driven infrastructure.\u00a0\nhttps://shopifyengineering.myshopify.com/blogs/engineering/faster-trino-query-execution-verification-benchmarking-profiling\nAccess to the third-party data to correlate with the business metrics is vital to understanding the business's external influence. \"Data Sharing\" from cloud datawarehouse is increasingly popular, as is the ETL & Reverse-ETL tooling. I wrote about the data exchange pattern in the past.\nFollowing Snowflake data exchange, Redshift announces the A.W.S. data exchange for Redshift. It is an exciting phase to watch marketplaces build on top of it.\nhttps://www.infoq.com/news/2021/10/aws-dax-amazon-redshift-preview/\nJoin leaders from Snowflake, Mammoth Growth, RudderStack, and Mixpanel to learn why the most sophisticated teams architect their data stacks around the data warehouse.\nhttps://rudderstack.com/video-library/the-modern-data-stack-is-warehouse-first\nPayPal writes about its Machine Learning model CI/CD pipeline and shadow platform to meet the regulatory requirements of ML/DL models tested in a shadow pipeline before deploying in production. The end-to-end workflow of CI/CD & shadow platform handling temporally aware features is an exciting read.\nhttps://medium.com/paypal-tech/machine-learning-model-ci-cd-and-shadow-platform-8c4f44998c78\nGroupon writes the second part of the blog about its loader framework Pinion to ingest the event to Delta Lake. The blog narrates how the loader framework performs data validation, compaction, auditing to support data governance, multi-stage ingestion strategy.\nhttps://medium.com/groupon-eng/pinion-the-load-framework-part-2-e6a47586e7be\nThe measurable impact is critical to iterate and improve the efficiency of a platform. Microsoft data science writes an exciting blog on measuring the impact of data science with P.U.G.E.T. (product/ problem definition, Users and customer segments, Goals, and metrics, Efficient and measurable strategy, Trade-offs).\nhttps://medium.com/data-science-at-microsoft/measuring-impact-in-data-science-part-1-6ef9712bcbea\nThe data workload is increasingly adopting a shared execution environment and the talk from Nextdoor highlights the impact of load balancing & resource sharing on inference service's performance.\nhttps://engblog.nextdoor.com/running-ml-inference-services-in-shared-hosting-environments-6176b39bc9b7\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-61", "title": "Data Engineering Weekly", "content": "RudderStack\u00a0Provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools.\nThrilled to announce the release of O'Reilly's first-ever book on data quality, Data Quality Fundamentals: A Practitioner's Guide to Building More Trustworthy Data Pipelines! In this book, the Data Observability category creators make the business case for data trust and explain how data leaders can tackle data quality at scale by leveraging best practices and technologies used by some of the world\u2019s most innovative companies.\nDownload Your Free Copy\nAnother great write-up from Benn Stancil on the future of operational analytics narrates why analytics is the experience. The narration on how the dashboard failing analogy is an exciting read that closely resembles the typical second system syndrome.\nhttps://benn.substack.com/p/the-future-of-operational-analytics\nAirbnb\u2019s Minerva metrics layer, and the recent Looker & Tableau partnership triggered some exciting conversation on transformation layer vs. metrics layer. The author narrates the paradigm shift in the transformation layer and how the transformation layer & metrics layer complement each other.\nhttps://robertyi.substack.com/p/signaling-a-tectonic-shift-in-the\nThe conversation is an excellent recap of the current state of data engineering and what the future holds with the fast-changing data tooling landscape. The narration on scalability & cost optimization, consensus & change management in a distributed ownership is an exciting read.\nhttps://www.montecarlodata.com/the-future-of-the-data-engineer/\nMonzo writes about an overview of its data infrastructure on Google Cloud. The usage of dbt and the wrapper tooling on top of dbt to speed up the execution is an exciting read. It is evident from the blog that one of the most significant challenges of data engineering is the ownership and the contract between the producer & consumers.\nhttps://medium.com/data-monzo/an-introduction-to-monzos-data-stack-827ae531bc99\nData versioning is the essence of data pipelines. The authors narrate what data versioning is and three patterns to approach the data versioning.\nKimbal model - SCD pattern\nA daily snapshot of the dimension table - functional pattern\nCDC pipeline - event sourcing\nhttps://medium.com/@petrica.leuca/what-is-data-versioning-and-3-ways-to-implement-it-4b6377bbdf93\nJoin leaders from Snowflake, Mammoth Growth, RudderStack, and Mixpanel to learn why the most sophisticated teams architect their data stacks around the data warehouse.\nhttps://rudderstack.com/video-library/the-modern-data-stack-is-warehouse-first\nTwitter writes about its journey on adopting the Kappa architecture pattern and the reasoning for moving away from the Lambda architecture pattern. The blog is an exciting read for the scalability challenges while maintaining the same view for real-time and batch analytics.\nhttps://blog.twitter.com/engineering/en_us/topics/infrastructure/2021/processing-billions-of-events-in-real-time-at-twitter-\nConsumer offset monitoring is critical for operating the streaming applications on top of Apache Kafka. Uber writes about uGroup, a Kafka consumer management framework.\nhttps://eng.uber.com/introducing-ugroup-ubers-consumer-management-framework/\nLinkedIn writes about MagnetA push-based shuffle is an implementation of shuffle where the shuffle blocks are pushed to the remote shuffle services from the mapper tasks in the past. The blog narrates an overview of the push-based shuffle, and now it is available as part of Spark 3.2 open source release.\nhttps://engineering.linkedin.com/blog/2021/push-based-shuffle-in-apache-spark\nApache Iceberg is an open table format for large analytic datasets. Debezium writes about how the Debezium server can add a new sink connector for creating the Apache Iceberg consumers to capture change data stream.\nhttps://debezium.io/blog/2021/10/20/using-debezium-create-data-lake-with-apache-iceberg/\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions.\n"}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-60", "title": "Data Engineering Weekly", "content": "RudderStack\u00a0Provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools.\nHear from data leaders pioneering the technologies & processes shaping data engineering. Featuring First Chief Data Scientist of the U.S., founder of the Data Mesh, and many more!\nClick To Get Your Free Ticket For All Data Engineering Weekly Readers\nCOVID-19 pandemic has had a profound impact on daily life. Google A.I. discusses the recent paper A prospective evaluation of AI-augmented epidemiology to forecast COVID-19 in the USA and Japan. Though the learned transition with the available data is novel, the author acknowledges that a lack of reliable, high-quality public data is significant.\nhttps://ai.googleblog.com/2021/10/an-ml-based-framework-for-covid-19.html\nUdemy writes about its journey to build an event tracking system. The discussion around buy vs. build, protobuf vs. Avro, Avro schema annotations are exciting reads.\nhttps://medium.com/udemy-engineering/designing-the-new-event-tracking-system-at-udemy-a45e502216fd\nPinterest writes about efficient Yarn resource management for its batch processing platform. The blog is an exciting case study of data-driven system design compared to the auto-scaling of computing instances.\nhttps://medium.com/pinterest-engineering/efficient-resource-management-at-pinterests-batch-processing-platform-61512ad98a95\nOpenMetadata is an open-source project building Schema First and API First Metadata Standard. A Single place to Discover, Collaborate and Get your data right.\nReviewer: Sriharsha Chintalapani\nhttps://blog.open-metadata.org/announcing-openmetadata-20399b816e60\nNow you can submit your reviews here https://github.com/ananthdurai/dataengineeringweekly.\nSalesforce writes about its usage of Trino as an ETL engine. Trino certainly has some shortcomings in ETL, such as lack of mid-query fault tolerance and limited expressive power; there are also some highly underrated advantages to using Trino for ETL. The author narrates techniques to overcome some of the shortcomings of Trino as an ETL engine.\nhttps://engineering.salesforce.com/how-to-etl-at-petabyte-scale-with-trino-5fe8ac134e36\nHere\u2019s an interesting case study on how machine learning can directly impact the bottom line. RudderStack writes an outline of how app developers, Torpedo Labs, use BigQuery ML to identify high-value mobile game players who are dangerously close to churning.\nhttps://rudderstack.com/blog/churn-prediction-with-bigqueryml\nStitch Fix writes about Hamilton, a microframework for dataframe generation. Hamilton efficiently solving the complexity of the chain of dataframe transformation on each column. Instead of having Data Scientists write code that they subsequently execute in a massive procedural tangle, Hamilton utilizes how the function is defined to create a DAG and execute it for Data Scientists.\nhttps://multithreaded.stitchfix.com/blog/2021/10/14/functions-dags-hamilton/\nSlowly changing dimension and incremental data processing are the 90% of data pipeline workload pattern. AWS writes how to handle slowly changing dimensions (SCD) in Redshift with best practices and anti-patterns.\nhttps://aws.amazon.com/blogs/big-data/implement-a-slowly-changing-dimension-in-amazon-redshift/\nExcited to see in the upcoming Apache Spark 3.2, we add \u201csession windows\u201d as new supported types of windows, which works for both streaming and batch queries. The blog walkthrough how to add a session window on event time.\nhttps://databricks.com/blog/2021/10/12/native-support-of-session-window-in-spark-structured-streaming.html\nHomeToGo writes about its adoption of dbt into the data infrastructure and dbt integration with Apache Airflow. The layered approach of metrics computations on top of the dbt model, testing the dbt model with GreatExpectations, is exciting to read.\nhttps://engineering.hometogo.com/dbt-at-hometogo-ece067987267\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-59", "title": "Data Engineering Weekly", "content": "RudderStack\u00a0Provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools.\nHear from data leaders pioneering the technologies & processes shaping data engineering. Featuring First Chief Data Scientist of the U.S., founder of the Data Mesh, and many more!\nClick To Get Your Free Ticket For All Data Engineering Weekly Readers\nPossibly one of the good walkthroughs of data mesh implementation experience shared from Flexport. The logical division of the transaction and analytical layers, focusing on modeling, not the technology, domain interoperability is a first-order concern, are some of the key takeaways from the talk.\nA great read of practical recommendations if you start thinking of switching your career towards business analyst. My favorite suggestion, \nExamine websites or apps you regularly use, especially those with a customer journey or flow (e.g., buying something). Assess what works well or what doesn\u2019t.\nhttps://medium.com/ft-product-technology/so-you-want-to-be-a-business-analyst-fc28596411f5\nWe can measure the effectiveness of a team by the number of clean migration projects executed. I used \"clean migration\" because, as the author says \nIn most migrations, getting 90% done isn't good enough, even getting 100% done is not good enough; you have to kill something old for a migration to be successful.\nhttps://towardsdatascience.com/3-steps-for-a-successful-data-migration-9de8e7f1671c\nThe blog narrates two Jupyter rendering engines, nbconvert and Voil\u00e0, with the highlights of D\u00e9j\u00e0vu utility, which specifies new default values for several options to mimic Voil\u00e0's behavior for hiding input cells and prompt numbers.\nhttps://blog.jupyter.org/looking-at-notebooks-from-a-new-perspective-bfd06797f188\nPayPal writes about the implementation of Dione, an open-source indexing library that creates a \"shadow\" table with the selected key values. Avro B-Tree implementation further optimizes the indexing to support single-row fetch tasks.\nhttps://medium.com/paypal-tech/paypal-introduces-dione-an-open-source-spark-indexing-library-783e12800585\nHere\u2019s an interesting case study on how machine learning can directly impact the bottom line. RudderStack writes an outline of how app developers, Torpedo Labs, use BigQuery ML to identify high-value mobile game players who are dangerously close to churning.\nhttps://rudderstack.com/blog/churn-prediction-with-bigqueryml\nLinkedIn writes about its approach to build explainable AI systems. The blog defines transparency in AI as,\nAI system behavior and its related components are understandable, explainable, and interpretable.\nThe blog narrates implementing a homegrown system called Intellige: A user-facing model explainer for narrative explanations.\nPaper: Intellige: A User-Facing Model Explainer for Narrative Explanations\nhttps://engineering.linkedin.com/blog/2021/transparent-and-explainable-AI-systems\nNetflix writes the third part of the A/B testing series highlighting what an A/B test is and how Netflix decision-makers using A/B testing. The third part of the multi-part post narrates interpreting A/B test results highlighting false positives and statistical significance.\nhttps://netflixtechblog.com/interpreting-a-b-test-results-false-positives-and-statistical-significance-c1522d0db27a\nPrevious Parts:\nPart 1: Decision Making at Netflix\nPart 2: What is an A/B test\nZalando writes an exciting blog post discussing the usage of the probabilistic data structure for feature storage. The benchmark results the traditional key-value storages using 15 GB of data vs. the probabilistic data structure uses 470 MN for 1.762 bill data points.\nhttps://engineering.zalando.com/posts/2021/10/space-efficient-machine-learning-feature-stores-using-probabilistic-data-structures.html\nDatabricks announces that the pandas API will be part of the Apache Spark\u2122 3.2 release by merging the Koalas onto PySpark. The blog follows the successive iterations on the pandas API, such as 90% API compatibility coverage, more type-hints, performance improvements, and stabilization.\nhttps://databricks.com/blog/2021/10/04/pandas-api-on-upcoming-apache-spark-3-2.html\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-58", "title": "Data Engineering Weekly", "content": "RudderStack\u00a0Provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools.\nHear from data leaders pioneering the technologies & processes shaping data engineering. Featuring First Chief Data Scientist of the U.S., founder of the Data Mesh, and many more!\nClick To Get Your Free Ticket For All Data Engineering Weekly Readers\nO'Reilly published a comprehensive salary report for data & AI professionals. The sad trend continues where Women's salaries were sharply lower than men's salaries, averaging $126,000 annually, or 84% of the average salary for men ($150,000). Python, SQL & JavaScript are the top 3 most popular programming languages for data & AI.\nhttps://www.oreilly.com/radar/2021-data-ai-salary-survey/\nSoftware-driven industrialization is moving from process-based workflow to an ML/AI-driven workflow. But how do you build a machine learning team? And what does this mean for software companies? The author walks through how to start a machine learning team, hiring & tracking the impact.\nhttps://medium.com/iconiq-growth/make-machine-learning-work-for-your-company-a-primer-f68ad0b1cd40\nMatt Turck published a comprehensive list of Machine Learning, AI & Data (MAD!!!) landscape. One interesting fact that I noticed in the landscape is that Jupiter notebooks still own the collaboration space. Data Engineering is inherently social & collaborative work across the org, and I can see this collaboration space still wide open.\nhttps://mattturck.com/data2021/\nPinterest writes about its high available ads real-time streaming services on Apache Flink & Kafka stream. The hot-hot primary & standby pipeline for each service is an exciting design to read.\nhttps://medium.com/pinterest-engineering/ensuring-high-availability-of-ads-realtime-streaming-services-ea3889420490\nLinkedIn writes about distributed tier merge in building offline search index using Apache Spark. The migration from MapReduce to Spark & distributed tier merge improved the build time by 40% across the product!!\nhttps://engineering.linkedin.com/blog/2021/distributed-tier-merge\nHere\u2019s an interesting case study on how machine learning can directly impact the bottom line. RudderStack writes an outline of how app developers, Torpedo Labs, use BigQuery ML to identify high-value mobile game players who are dangerously close to churning.\nhttps://rudderstack.com/blog/churn-prediction-with-bigqueryml\nDoorDash writes an exciting blog narrating its migration of Airflow from a single instance infrastructure to KubernetesPodOperators. The blog states the higher memory availability of the Airflow scheduler after offloading the operator workloads to Kubernetes.\nhttps://doordash.engineering/2021/09/28/how-to-run-apache-airflow-on-kubernetes-at-scale/\nAirflow poking sensor implementation is a resource-intensive operator that will keep running until the specified condition is satisfied. Airbnb writes about the impact of smart sensors on its Airflow infrastructure. With deduplication, it reduces 40% of the load from the Hive meta store.\nhttps://medium.com/airbnb-engineering/the-airflow-smart-sensor-service-221f96227bcb\nStoryblocks writes about adopting the Blue-Green ETL model with Airflow on its Redshift data warehouse. The load and swap in the mutable pipeline is always a challenge, and it's great to see the Blue-Green deployment pattern adoption.\nhttps://medium.com/storyblocks-engineering/blue-green-etls-with-airflow-task-groups-71c36d120c2e\nData pipeline on top of the external, uncontrolled datasets can be challenging. Wealthfront writes about its data quality approach following persisting the raw data, transforms to a confirmed schema and validate, and handles the anomalies.\nhttps://eng.wealthfront.com/2021/09/28/automating-data-quality-checks-on-external-data/\nTeads published helpful tips and tools to manage BigQuery to resolve slow-running queries and improve slot usage and table size. The BqVisualiser looks like an exciting tool to visualize and optimize the query performance.ff\nhttps://medium.com/teads-engineering/managing-a-bigquery-data-warehouse-at-scale-e6ec9a8406b2\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-57", "title": "Data Engineering Weekly", "content": "RudderStack\u00a0Provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools.\nHear from data leaders pioneering the technologies & processes shaping data engineering. Featuring First Chief Data Scientist of the U.S., founder of the Data Mesh, and many more!\nClick To Get Your Free Ticket For All Data Engineering Weekly Readers\nVicki Boykis writes exciting storytelling of the current state of the ML engineer on embracing the reality of building production-ready Machine Learning insights. The highlight of the blog \nMachine learning systems are new. We\u2019re still in the steam-powered days of machine learning, yet machine learning is not simply machine learning. It is, at this stage, more engineering than simply machine learning. We\u2019re building more and more on older systems, abstracting away complexity, and in the process creating newer and newer levels of it that we now have to manage and hold in our heads. Many of the algorithms have been written. Much of the work we do, both in machine learning and in development today in general, will be glue work and vendor work.\nhttp://veekaybee.github.io/2021/09/23/enlightenment/\nFlink\u2019s checkpoint support for a two-phase commit protocol enables exactly-once semantics in Apache Flink. Uber writes about extending Flink\u2019s exactly-once semantics with Apache Pinot\u2019s upsert support to achieve end-to-end pipeline\u2019s exactly-once semantics.\nhttps://eng.uber.com/real-time-exactly-once-ad-event-processing/\nAirbnb writes the third part of the data consistency at scale, talking about Minerva, its metrics infrastructure. The approach of metrics-centric instead of the traditional BI approach of table-centric is an exciting one to read.\nhttps://medium.com/airbnb-engineering/how-airbnb-enables-consistent-data-consumption-at-scale-1c0b6a8b9206\n Lift and ship an infrastructure is always challenging with many unknowns. Microsoft writes an exciting blog on how it is using ML to bring observability to the migration process.\nhttps://medium.com/data-science-at-microsoft/how-we-used-ml-and-heuristic-data-labeling-to-help-customers-with-their-cloud-migration-d3af7ff020fc\nPayPal writes about data processing performance and cost comparison on running the analytics query on BigQuery vs. GCS. It is interesting to see the trends on more analytical workload moving to the cloud from big internet companies.\nhttps://medium.com/paypal-tech/comparing-bigquery-processing-and-spark-dataproc-4c90c10e31ac\nHere\u2019s an interesting case study on how machine learning can directly impact the bottom line. RudderStack writes an outline of how app developers, Torpedo Labs, use BigQuery ML to identify high-value mobile game players who are dangerously close to churning.\nhttps://rudderstack.com/blog/churn-prediction-with-bigqueryml\nAdevinta writes about its journey to adopt data as a  product and the infrastructure changes around it that lead 13% increase in weekly querying users. I\u2019m curious to see the learnings from each data as a product has its repository of code isolated from the rest approach.\nhttps://medium.com/adevinta-tech-blog/treating-data-as-a-product-at-adevinta-c1dce5d394c5\nJames Le shared comprehensive notes from the recent Tecton apply(meetup) 2021. The rule-based data profiling from GE, ML software hierarchy, and interactive ML are exciting reads.\nhttps://data-notes.co/what-i-learned-from-attending-tecton-apply-meetup-2021-9e6a89d1e72d\nA great experience sharing blog on the journey from software engineering to Machine Learning engineering, highlighting the learning process and what books and courses can't teach.\nhttps://medium.com/paypal-tech/a-journey-from-software-to-machine-learning-engineer-at-izettle-49192ce3a758\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-56", "title": "Data Engineering Weekly", "content": "RudderStack\u00a0Provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools.\nHear from data leaders pioneering the technologies & processes shaping data engineering. Featuring First Chief Data Scientist of the U.S., founder of the Data Mesh, and many more!\nClick To Get Your Free Ticket For All Data Engineering Weekly Readers\nY Combinator\u2014an incubator of both startups and the Silicon Valley zeitgeist\u2014funded 15 analytics, data engineering, and AI and ML companies. In 2021, they funded 100. Does the modern data stack bring too many tools to the table to solve the data problem? Benn Stancil is discussing data OS.\nhttps://benn.substack.com/p/the-data-os\nUC Berkeley published its spring 2021 data engineering course slides and resources. It is excellent learning material for data engineering practitioners.\nhttps://cal-data-eng.github.io/\nData protection and privacy monitoring is a critical aspect of the data management platform. It is the most challenging aspect of data management since it can travel through multiple data storages, making it harder to keep track of manually. Airbnb writes about Madoka, a metadata system for data protection that maintains the security and privacy-related metadata for all data assets on the Airbnb platform.\nhttps://medium.com/airbnb-engineering/automating-data-protection-at-scale-part-1-c74909328e08\nFunnel analysis is a critical analytical feature from click tracking events. Uber writes an exciting blog about YAML generators, followed by a simple UI workflow engine to develop funnel analysis. It triggers an interesting data pipeline debate, no-code or code-only data pipeline. IMO, the answer is to know your audience and their workflow to make them productive.\nhttps://eng.uber.com/streamlining-mobile-data-workflow-process/\nIntuit writes about a general overview of its data infrastructure, emphasizing that lack of standardization can lead to fragmentation and islands of computing. The blog narrates Intuit's developer portal and UI-driven pipeline lifecycle management platform.\nhttps://medium.com/intuit-engineering/a-paved-road-for-data-pipelines-779004143e41\nHere\u2019s an interesting case study on how machine learning can directly impact the bottom line. RudderStack writes an outline of how app developers, Torpedo Labs, use BigQuery ML to identify high-value mobile game players who are dangerously close to churning.\nhttps://rudderstack.com/blog/churn-prediction-with-bigqueryml\nSelf-serving diagnostic tooling is a vital part of the data platform for democratizing the adoption. Pinterest writes about Dr. Squirrel, a Flink logs aggregator to perform job health checks, flag unhealthy jobs explicitly, and provide root cause analysis and actionable steps to help fix the issues.\nhttps://medium.com/pinterest-engineering/faster-flink-adoption-with-self-service-diagnosis-tool-at-pinterest-50a07143f444\nCruise control is one of my favorite tools to operate Apache Kafka at scale. Cloudera writes an exciting blog giving an overview of Cruise Control and its use cases. \nhttps://blog.cloudera.com/operating-apache-kafka-with-cruise-control/\n It is always challenging to integrate Airflow as a task dependency system with Dbt, a model-dependent system. AutoTrader writes an exciting blog about its DbtTaskGenerator to auto-generate Airflow DAGs using Dbt's manifest files.\nhttps://engineering.autotrader.co.uk/2021/09/15/auto-generated-airflow-dag-for-dbt.html\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-55", "title": "Data Engineering Weekly", "content": "RudderStack\u00a0Provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools.\nHear from data leaders pioneering the technologies & processes shaping data engineering. Featuring First Chief Data Scientist of the U.S., founder of the Data Mesh, and many more!\t\nRegister Today\nAWS writes a deep dive on PartiQL, a SQL 92 compatible query language that runs queries against structured, semi-structured, and unstructured data. PartiQL idea of a logical type system data modeling on top of formats like JSON & Parquet and the support for dynamic typing is an exciting space to watch. Though AWS data services rapidly adopting PartiQL, how far it can gain momentum in the open-source community against the likes of Apache Calcite is yet to be seen.\nhttps://aws.amazon.com/blogs/database/a-partiql-deep-dive-understanding-the-language-bringing-sql-queries-to-aws-non-relational-database-services/\nPeople don't make decisions based on data; they make the decision based on the story.!!!\nData storytelling is a vital aspect of data analytics that increases collaboration and informed decisions. The culture and workflow of collaborative story building on top of data are the critical ingredients for efficient business ops. The author writes an exciting blog narrating the workflow of sharing data insights at Netlify.\nhttps://locallyoptimistic.com/post/share-your-data-insights-to-engage-your-colleagues/\nPinterest shared a 3 part blog post on its journey with Apache Druid. The blog narrates the shortcoming of the Apache HBase infrastructure, instance optimization based on tiered request pattern, secondary key pruning, and bloom filter index on real-time segments.\nhttps://medium.com/pinterest-engineering/pinterests-analytics-as-a-platform-on-druid-part-1-of-3-9043776b7b76\nhttps://medium.com/pinterest-engineering/pinterests-analytics-as-a-platform-on-druid-part-2-of-3-e63d5280a1a9\nhttps://medium.com/pinterest-engineering/pinterests-analytics-as-a-platform-on-druid-part-3-of-3-579406ffa374\nConfluent writes about its end-to-end data durability monitoring infrastructure for Apache Kafka. The data integrity check focuses on the system state change operations to detect the integrity instead of data scrubbing is an elegant integrity check approach.\nhttps://www.confluent.io/blog/how-confluent-cloud-protects-kafka-data-integrity-for-eight-trillion-messages-per-day/\nFAIR framework for good data management and stewardship for scientific data initially introduced in a 2016 article in  Nature, with \u201clong-term care of valuable digital assets\u201d at the core of it. Databricks writes an exciting blog on how lakehouse architecture empowering the FAIR framework. The blog introduced me to the FAIR principle, and it is an exciting article to read.\nhttps://databricks.com/blog/2021/09/07/implementing-more-effective-fair-scientific-data-management-with-a-lakehouse.html\nNature Article on FAIR principle\nHere\u2019s an interesting case study on how machine learning can directly impact the bottom line. RudderStack writes an outline of how app developers, Torpedo Labs, use BigQuery ML to identify high-value mobile game players who are dangerously close to churning.\nhttps://rudderstack.com/blog/churn-prediction-with-bigqueryml\nIt is always refreshing to read the backstory of a successful open-source system and how it starts from a simple beginning and grows over time. StarTree shared one of the success stories of how Apache Pinot starts from a simple beginning at LinkedIn and grows with the adoption at Uber.\nhttps://www.startree.ai/blogs/launching-at-linkedin-the-story-of-apache-pinot/\nUber writes about real-time analytics systems with Redis, AWS Fargate & Dash framework evaluation from the long polling ingestion to event-driven model. It is the first story I read about Uber's usage of AWS and sounds like an interesting development. Earlier Dropbox shared its analytical stack migration to AWS, Twitter ads analytical stack migration to Google Cloud.\nhttps://eng.uber.com/streaming-real-time-analytics/\nSnowflake shared its Apache Airflow migration from EC2 instances to KubernetesPodExecutors to scale DAG growth. The blog adds best practices of Airflow health monitoring & alerting practices. It is sad to see the Airflow operational challenges remain the same even after years!!!\nhttps://www.snowflake.com/blog/migrating-airflow-from-amazon-ec2-to-kubernetes/\nSlack (like) platform plays a significant role in data ops and application monitoring to bridge the workflow between humans and machines. CapitalOne writes an exciting blog that narrates using Apache Airflow and Slack Bot to monitor ElasticSearch.\nhttps://medium.com/capital-one-tech/automate-application-monitoring-with-slack-9e4e498652a3\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions.\n\n"}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-54", "title": "Data Engineering Weekly", "content": "RudderStack\u00a0Provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools.\nHear from data leaders pioneering the technologies & processes shaping data engineering. Featuring First Chief Data Scientist of the U.S., founder of the Data Mesh, and many more!\nClick To Get Your Free Ticket For All Data Engineering Weekly Readers\nLet\u2019s start this week with a fun meme that shows the importance of data quality and contract-driven data asset creation. \nUber writes an exciting blog about the tuning Flink streaming platform. The blog narrates the business cases for real-time analytics with geospatial and temporal analysis and focuses on Network, CPU, and memory optimization strategies.\nhttps://eng.uber.com/building-scalable-streaming-pipelines/\nksqlDB is the streaming SQL engine for Kafka that enables stream processing tasks using SQL statements. Confluent writes a guide to the ksqlDB internal architecture discussing how stream joins, stateful & stateless computing & fault tolerance works.\nhttps://www.confluent.io/blog/ksqldb-architecture-and-advanced-features/\nData Science at Microsoft discusses the role and significance of ML program management and the role of the program manager in the end-to-end lifecycle of the data product. As the scale and the breadth of the ML application adoption grow, the Technical Program Manager for Machine Learning is an exciting job profile that will grow in the coming years.\nPart 1: https://medium.com/data-science-at-microsoft/ml-program-management-at-scale-part-1-of-3-4816a99ad1bd\nPart 2: https://medium.com/data-science-at-microsoft/ml-program-management-at-scale-part-2-of-2-3ab2cc54f36f\nGreat Expectations writes a three-part series on maximizing the productivity of the analytics team, focusing on the debugability of the dashboards, reducing the technical debt on the data pipeline, and the role of Great Expectations in the data engineering process.\nhttps://greatexpectations.io/blog/maximizing-productivity-of-analytics-teams-pt1/\nhttps://greatexpectations.io/blog/maximizing-productivity-of-analytics-teams-pt2/\nhttps://greatexpectations.io/blog/maximizing-productivity-of-analytics-teams-pt3/\nShopify writes a five-step guideline article on building ML products. The first three-step guidelines focus on asking the necessity of the ML model rather than simple heuristic algorithms. The blog reemphasizes that simplicity is the best ML model strategy.\nhttps://shopifyengineering.myshopify.com/blogs/engineering/building-business-machine-learning-models\nHere\u2019s an interesting case study on how machine learning can directly impact the bottom line. RudderStack writes an outline of how app developers, Torpedo Labs, use BigQuery ML to identify high-value mobile game players who are dangerously close to churning.\nhttps://rudderstack.com/blog/churn-prediction-with-bigqueryml\nProduct data analytics is the core of lean product development. Uber writes an exciting blog on its rider app using such metrics-driven product development. The blog narrates the data acquisition lifecycle from mobile devices across different OS versions, emphasizes the importance of log standardization, anomaly detection, and data quality standards.\nhttps://eng.uber.com/how-data-shapes-the-uber-rider-app/\nDoorDash writes about the cost-driven optimization techniques it uses in the pipeline to optimize Snowflake usage. The optimization techniques focus on deprecating unused ETL jobs, favoring incremental ETL processing over bulk processing, reducing the number of projections in the SQL queries, clustering keys, and maximize the Snowflake native function usage.\nhttps://doordash.engineering/2021/06/22/overcoming-rapid-growth-challenges-for-datasets-in-snowflake/\nAnomaly detection is a critical functionality in data engineering for reliable metrics, yet it is no short of challenges to implement and run at scale. The author narrates how CueObserve, an open-source metrics monitoring system, is solving anomaly detection at scale.\nhttps://towardsdatascience.com/running-timeseries-anomaly-detection-at-scale-on-sql-data-4407eb3d3bd3\nPicnic adopted data vault modeling techniques for its data warehouses. Continue to adapt the data vault modeling technique, Picnic open sources diepvries a simple python library that automates the data loading process for Data Vault and avoids the maintenance of repetitive SQL queries for ETL jobs.\nhttps://blog.picnic.nl/releasing-diepvries-a-data-vault-framework-for-python-3f01a5d46f84\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-53", "title": "Data Engineering Weekly", "content": "RudderStack\u00a0Provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools.\nHear from data leaders pioneering the technologies & processes shaping data engineering. Featuring First Chief Data Scientist of the U.S., founder of the Data Mesh, and many more!\nClick To Get Your Free Ticket For All Data Engineering Weekly Readers\nBenn Stancil writes another exciting blog highlighting the missing focus on the modern data experience. The growth of modern data engineering tooling focus on a specific part of data engineering leads to data practitioners' isolated and inconsistent experience. An integrated data platform experience that can connect the modern and past data tools greatly accelerates data-driven culture.\nhttps://benn.substack.com/p/the-modern-data-experience\nThere are several advantages of adopting a decentralized schema-on-read data lake approach. However, it can leads to inconsistency in the naming of the schema. A \"server\" column can be named as \"Machine\" or \"Host\" or \"instance\" in other tables. Finding column relationships is a complex task historically solved by sampling the data or finding the unique value matching. Microsoft lab writes an exciting paper that uses SQL query logs to find the relationship.\nPaper: https://www.microsoft.com/en-us/research/publication/discovering-related-data-at-scale/\nTalk:\nThe emerging cloud-native data platforms, collectively known as the \"modern data stack,\" simplify entry barriers to data analytics. The author walks through the developments on the modern data stack and bright side of \"Modern Data Stack V2\", focusing on AI, Data Sharing, Data Governance, Streaming & Application Serving.\nhttps://medium.com/@jordan_volz/five-predictions-for-the-future-of-the-modern-data-stack-435b4e911413\nInfoQ released 2021 AI/ML/Data Engineering trends as a CHASM model. The top highlights are the Deep learning frameworks moved from innovators to early adopters and AutoML picking momentum. I've not come across any business process automation with digital assistance, so finding the digital assistance frameworks at the Early Adopters stage is a bit of a surprise.\nhttps://www.infoq.com/articles/ai-ml-data-engineering-trends-2021/\nWe can associate the growth of modern data stacks and SQL reclaiming the throne of data engineering. The blog is an excellent overview of why SQL is back now and why it is a perfect tool for data engineering?\nhttps://www.trifacta.com/blog/sql-for-elt-and-cloud-data-engineering/\nHere\u2019s an interesting case study on how machine learning can directly impact the bottom line. RudderStack writes an outline of how app developers, Torpedo Labs, use BigQuery ML to identify high-value mobile game players who are dangerously close to churning.\nhttps://rudderstack.com/blog/churn-prediction-with-bigqueryml\nSlack writes its data lineage journey highlighting lineage ingestion and consumption part of it. The Notification service out of the lineage data is an excellent reminder that the potential of the lineage exponentially increases when we start integrating it into the data practitioner's workflow.\nhttps://slack.engineering/data-lineage-at-slack/\nI am an application developer. Why should I care about data engineering? \nI've been asked this question in one of the data engineering talks. I thought a bit and responded without much conviction that, \nEvery engineer is a data engineer /practitioner. \nThe blog from Gusto is an exciting read on growth engineering practices with the AARRR metrics framework, and I still stand by my statement :-). \nhttps://engineering.gusto.com/what-is-growth-engineering/\nUDFs bring uniformity and consistency to the data pipeline's business logic; however, few cloud providers support it, and there are no standards for defining the UDF. LinkedIn attempted to solve this problem with Transport: Towards Logical Independence Using Translatable Portable UDFs.\nThe author raises an excellent question on the role of UDFs in the modern data platform and the importance of UDF standardizations.\nhttps://sisudata.com/blog/cloud-analytics-platforms\nNubank writes about its process of scaling data analytics with software engineering practices. The blog is an exciting reminder on focusing on structured dataset creations, collaboration & knowledge sharing, and the lifecycle management of the datasets.\nhttps://building.nubank.com.br/scaling-data-analytics-with-software-engineering-best-practices/\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-52", "title": "Data Engineering Weekly", "content": "Welcome to the 52nd edition of the data engineering newsletter. This week's release is a new set of articles that focus on a few thoughts on data mesh discussion, Pedram Navid's the last thing I'll ever say about the data mesh, LakeFs's what can replace Hive meta store, Apache Hudi's platform overview, Continual's Is Data-First AI the Next Big Thing? Open Lineage's data quality Open Lineage facets, Airflow & GreatExpectations, RudderStack's Why It's Hard for Engineering to Support Marketing, Uber's data platform cost optimization, SQLGlot, a Python SQL parser & transpiler for SparkSQL, Hive, Presto & Trino, and the catalog of AI/DL university lecture.\nRudderStack\u00a0provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools.\nLast week Data Twitter had some interesting discussions around Data Mesh. I share a few of my thoughts on the data mesh discussion here. \nAlmost five years ago, I met Dave (not the real name) at one of the tech conferences. Dave is the principal architect for one of the largest retail banks in Canada. We were exchanging common challenges in the data ingestion, observability and data silo. I'm fascinated and explains how the ingestion framework I worked on has in-build observability and scalable architecture. Dave responds to me with a smiling face, \"Ananth, do you think a large bank with the capital at disposal can't buy one of these systems. I don't have a data silo, but a people silo. People hold on to their data as a negotiation tool, so all data problem becomes a trade-off resulting inefficient workaround. How will you fix the people silo and free the data?\"\nThe people silo problem is still valid in most organizations. IMO I don't see any scalability issue with a monolithic architecture where storage and compute can scale independently. The multi-tenant centralized storage with a clear separation of concern can scale with proper tooling. DBT is solving the silo with the domain view structure, but the instrumentation part is still challenging. \nI genuinely believe a concept like Data Mesh and domain ownership much-needed one to validate data systems similar to the CAP theorem for distributed systems. CAP theorem is not perfect either, but it is good enough to validate. Any vague and misleading concept leads to multiple interpretations that will only result in chaotic culture.\nThe author writes an excellent reflection article on Data Mesh about the understanding and confusion that requires clarifications. \nPedram Navid: The Last Thing I'll Ever Say About the Data Mesh\nOn a positive development on this, Data Mesh book now offers code for limited free access to the O'Reilly platform for a bit with a new chapter every month.\nI believe the goal of Data Mesh is to spread the democratize data accessibility and break organization silo. A reference architecture for data mesh and a clear demonstration of how domain ownership brings accountability can simplify the concept by miles.\nHive meta store is a critical component in the interception of all query engines path provides a virtualization layer between the storage and compute. LakeFs write an exciting article on how Hive meta store sustained last ten years while the Hadoop popularity declined. The article predicts the possible components that can succeed the Hive meta store.\nhttps://lakefs.io/hive-metastore-why-its-still-here-and-what-can-replace-it/\nApache Hudi pioneered the serverless transactional layer for event logs that significantly shape the data infrastructure. The article gives an in-depth overview of Apache Hudi's building blocks and future roadmap aligning with its founding principle.\nhttps://hudi.apache.org/blog/2021/07/21/streaming-data-lake-platform/\nContinual writes about the evolution of ML platforms from collaboration-centric to model-based to data-centric platforms. The blog is an exciting read on how one generation platform abstraction leads the next-generation platform and democratization of ML/AI engineering in the last ten years.\nhttps://continual.ai/post/is-data-first-ai-the-next-big-thing\nThe data quality defines the success of a data-driven organization.\nThe blog is an excellent reminder of why no data is better than bad data. The article narrates the traceability of data quality with OpenLineage Facets integration with Airflow & Great Expectations.\nhttps://openlineage.io/blog/dataquality_expectations_facet/\nEngineers and marketers don\u2019t [often] get along, and the tension between these teams isn't fabricated. It's based on conflicting approaches that naturally present alignment challenges. RudderStack writes a thoughtful analysis of the contentious relationship and hints at a solution.\nhttps://rudderstack.com/blog/why-it-s-hard-for-engineering-to-support-marketing\nAn ever-growing data generation adds pressure on the cost of operations to the data infrastructure. Cost optimization is a critical architectural constraint in modern data infrastructure. Uber writes its experience on optimizing cost on data storage, computing & querying layer. S3 tiered storage provides similar optimization for AWS on the storage.\nhttps://eng.uber.com/cost-efficient-big-data-platform/\nPresto/ Trino is an excellent query engine for the exploration stage of analysis but not providing sufficient fault tolerance like Spark SQL/Hive for the production pipeline. It is a painful task to convert SQL from one engine to another. I recently came across SQLGlot with the promise of automating it. I've not tested it, but I'm excited about this tool.\nhttps://github.com/tobymao/sqlglot\nI recently came across this catalog of advanced ML/ DL university lectures. Kudos to the list's curator, and I hope it will benefit the data engineering weekly community.\nhttps://docs.google.com/spreadsheets/d/1KYJ9Z8f76WZGYpT2E5sjr5gL-O35Lpjm-SMmU00fplk/htmlview\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-51", "title": "Data Engineering Weekly", "content": "Welcome to the 51st edition of the data engineering newsletter. This week's release is a new set of articles that focus on Uber's operational excellence in the data quality experience, Airbnb's \"Wall Framework\" to prevent data bugs, Tiffany Jachja's first three weeks as a data engineering manager, Hurb.com's data platform architecture, RudderStack's churn prediction with BigQueryML, Disney Streaming's Voidbox-Docker on YARN, AWS's expiring S3 object based on Last Access Date, and High Scalability's evolution of search engine architecture.\nRudderStack\u00a0provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools.\nPoor data quality not only leads to a degraded machine learning model but also requires a lot of laborious manual effort to investigate and refill. Uber writes about its Unified Data Quality Platform(UDQ) that automatically detects data quality issues. The approach to generate automatic test cases from the past learning and metadata fields emphasis the more significant role of data lineage and metadata-driven workflow.\nhttps://eng.uber.com/operational-excellence-data-quality/\nOn a similar data quality journey of Uber, Airbnb writes about Wall Framework, its abstraction on top of Airflow where users can add data quality check as part of the Airflow DAG. Wall framework is a config-driven approach that provides the most common DQ checks & anomaly detection as a service.\nhttps://medium.com/airbnb-engineering/how-airbnb-built-wall-to-prevent-data-bugs-ad1b081d6e8f\nThe author shared the first three weeks of experience as a data engineering manager. It is a good read for any new data engineering manager from aligning the team on a joint mission, clear distinction of roles & responsibility.\nhttps://tiffanyjachja.medium.com/my-first-three-weeks-a-data-engineering-manager-8b0be08da7a5\nHurb.com, one of the major OTAs in Latin America, writes about an overview of its data infrastructure. The article is a great reference architecture for a Google cloud platform with the adoption of Google dataflow & BigQuery. The exciting part of the article where the author discusses the choice of data visualization engine, how per-user billing preventing them from democratizing the data, and the choice of Metabase to address the issue.\n\nhttps://medium.com/hurb-engineering/data-platform-architecture-at-hurb-com-8c472c051fa2\nHere\u2019s an interesting case study on how machine learning can directly impact the bottom line. RudderStack writes an outline of how app developers, Torpedo Labs, use BigQuery ML to identify high-value mobile game players who are dangerously close to churning.\nhttps://rudderstack.com/blog/churn-prediction-with-bigqueryml\nDisney Streaming writes about Voidbox, which enables any application encapsulated in docker image running on YARN cluster along with MapReduce and Spark. Voidbox supports Docker container-based DAG(Directed Acyclic Graph) tasks in execution is an exciting approach where Voidbox can encapsulate each step of the data pipeline as a Docker run.\nhttps://medium.com/disney-streaming/voidbox-docker-on-yarn-e1b9f3a789ec\nS3 is a widely used system for building data lakes, websites, mobile applications, and enterprise applications even though S3 tiered storage can bring down the storage cost, but not be without the performance hit while accessing the tiered storage. AWS writes a reference architecture to delete the S3 objects based on Last Access Date using the S3 server access log & S3 inventory.\nhttps://aws.amazon.com/blogs/architecture/expiring-amazon-s3-objects-based-on-last-accessed-date-to-decrease-costs/\nSearch engine plays a vital role in information retrieval, which is the critical function of data engineering. The article evaluates some of the critical milestones of the search engine architecture, and the challenges those architecture style faces today.\nhttp://highscalability.com/blog/2021/8/2/evolution-of-search-engines-architecture-algolia-new-search.html\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-50", "title": "Data Engineering Weekly", "content": "Welcome to the 50th edition of the data engineering newsletter. This week's release is a new set of articles that focus on Benn Stancil\u2019s the third rail, BVP\u2019s data infrastructure roadmap, ValidIO\u2019s DBT and analytical engineering, Zalando\u2019s using knowledge graph to accelerate master data management, RudderStack\u2019s real-time personalization with Redis, Anna Geller\u2019s top 10 common pitfalls of data modeling, Zoba\u2019s scaling more cities with Airflow, and DevianArt\u2019s CDC journey.\nRudderStack\u00a0provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools.\nBenn Stancil writes an insightful view on the analyst role in an organization, the importance of analyst as a business decision support system, how the engineering skills overshadow the analyst function, and the lack of practical training brings the gap in the industry.\nhttps://benn.substack.com/p/third-rail\nBessemer Ventures writes the guideline for its investment strategy for the data infrastructure. The blog has a comprehensive overview of the data infrastructure trends, the adoption of the cloud, and the future direction.  \nhttps://www.bvp.com/atlas/roadmap-data-infrastructure\nThe rise of the modern data stack is expanding the scope of analytical engineering. The article narrates how DBT catalyzes the change and discusses the evolution of data warehouses & data lakes.\nhttps://medium.com/validio/dbt-and-the-analytics-engineer-whats-the-hype-about-907eb86c4938\nThe adoption of data lakes pushes master data management to the backseat partly because the \"schema-on-write\" MDM systems go against the \"schema-on-read\" design principles. At the same time, the adoption of microservices architecture is prone to yield inconsistent entity states. Zalando writes an exciting blog on how it tackles the MDM challenges with a graph database.\nhttps://engineering.zalando.com/posts/2021/07/knowledge-graph-master-data-mdm.html\nNailing personalization can mean increasing revenue by 15%, but technical challenges keep many companies stuck using basic methods. RudderStack writes a step-by-step guide on designing and implementing a real-time personalization engine using Redis and RudderStack.\nhttps://rudderstack.com/blog/real-time-personalization-with-redis-and-rudderstack\nData modeling is the most challenging problem in data. The author narrates the most common pitfalls of data modeling, from treating data modeling as the one-of task to blindly following data modeling rules.\nhttps://betterprogramming.pub/10-common-mistakes-when-building-analytical-data-models-814c763d1b70\nZoba, the on-demand forecasting and optimization tool for mobility, writes about using Airflow to expand mobility to the newer markets. The workflow optimization to sync bootstrapping and periodic sync using Airflow's TaskFlow API is a good design reference for Airflow.\nhttps://medium.com/zoba-blog/scaling-zoba-to-more-cities-using-airflow-1524f1941ffb\nDevianArt writes about its journey of adopting CDC infrastructure. Often the CDC narrations focusing on the choice of the CDC frameworks. The author did an excellent job focusing on describing the overall complexity of the ecosystem and troubleshooting.\nhttps://www.wix.engineering/post/change-data-capture-at-deviantart\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-49", "title": "Data Engineering Weekly", "content": "Welcome to the 49th edition of the data engineering newsletter. This week's release is a new set of articles that focus on Netflix's designing better ML systems learnings, James Serra's take on centralized vs. decentralized ownership, Uber's containerizing Apache Hadoop, LinkedIn's journey from the daily dashboard to enterprise-grade data pipeline, Alibaba Cloud's CDC analysis with Apache Flink & Apache Iceberg, RudderStack's why its harder for engineers to support marketing, Uber's geospatial indexing adoption with Apache Pinot, Salesforce's data pipeline with Kotlin, Pinterest's near real-time indexing for Apache HBase, and Grab's processing ETL tasks with Ratchet.\nRudderStack\u00a0provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools.\nAirbnb hosts its first virtual tech talk focusing on data quality Wednesday, July 28th, 12:00 PM-1:00 PM PST. SignUp here\nhttps://journeytowardhighqualitydata.splashthat.com/\nNetflix shares its design principles on building the recommender ML infrastructure. The article unbundles the three core parts of the orchestration engine from Netflix's Metaflow,\u00a0\nThe user DAG code defines what needs to execute\nThe job scheduler defines how the code will execute\nCompute infrastructure that defines where the code will execute\nhttps://www.infoq.com/presentations/designing-ml-systems-netflix/\nThe data as a product is a robust design thought introduced from the data mesh principles. Yet, there is still some confusion around the feasibility of adopting the data mesh principles, mainly because of the lack of toolings.\nThe author raised some valid concerns & constraints on adopting the data mesh's decentralized ownership, and I tend to agree with a few of them. Are we collectively underestimating the complexity of the data engineering, or is that an idea ahead of time since the tooling is not ready? Nonetheless, it is great to see the Data Mesh principles pushing the boundaries of the data engineering toolings.\nhttps://www.jamesserra.com/archive/2021/07/data-mesh-centralized-ownership-vs-decentralized-ownership/\nUber writes about its experience on the instability of running a mutable infrastructure and the experience of adopting immutable containerized Apache Hadoop infrastructure. The implementation of pre-fetching the docker image to reduce the bootstrap failures, Kerberos integration, and the complexity analysis on adopting the internal service mesh vs. DNS solutions is an informative read.\nhttps://eng.uber.com/hadoop-container-blog/\nThe Daily Executive Dashboard (DED) dashboards contain critical growth, engagement, and success metrics that indicate the health of a company. LinkedIn writes an exciting blog that narrates its executive dashboard pipeline journey from the incubation of Microstrategy -> Teradata -> integration with LinkedIn's data infrastructure stack.\nhttps://engineering.linkedin.com/blog/2021/from-daily-dashboards-to-enterprise-grade-data-pipelines\nThe real-time analytics on the change data capture events are critical for business operations. The blog narrates the historical approach of analyzing the CDC events by various systems like HBase, Kudu, Hive incremental tables, Spark Delta, and narrates the reasoning to adopt Apache Iceberg + Flink solution.\nhttps://www.alibabacloud.com/blog/how-to-analyze-cdc-data-in-iceberg-data-lake-using-flink_597838\nEngineers and marketers don\u2019t [often] get along, and the tension between these teams isn't fabricated. It's based on conflicting approaches that naturally present alignment challenges. RudderStack writes a thoughtful analysis of the contentious relationship and hints at a solution.\nhttps://rudderstack.com/blog/why-it-s-hard-for-engineering-to-support-marketing\nUber writes about the criticality of real-time geospatial analytics for its business and how it uses Apache Pinot's geospatial indexing based on Uber's H3 indexing system helped to solve some of the business cases for Uber Eats. The article narrates how Pinot's geospatial indexing support helped solve the scalability issue with the previous Cassandra-based solution, from 120 db calls to 1.\nhttps://eng.uber.com/orders-near-you/\nSalesforce writes about its choice of adopting Kotlin for building the data pipeline. The null pointer safety,  presence of the data classes to reduce the boilerplate codes, flexible branching expression, and the fact that it seamlessly integrates with java to utilize the java library ecosystems are some of the exciting features in Kotlin as a data pipeline language.\nhttps://engineering.salesforce.com/building-data-pipelines-using-kotlin-2d70edc0297c\nThe lack of seamless secondary indexing support is one of the design constraints of adopting Apache HBase. Pinterest writes about Ixia, its internal generic search interface on top of HBase to provide near-real-time secondary indexing support.\nhttps://medium.com/pinterest-engineering/building-scalable-near-real-time-indexing-on-hbase-7b5eeb411888\nGrab writes about its Lending platform adoption of Ratchet library for performing data pipeline & ETL tasks in Go. It's exciting to see a couple of articles sharing their experience building data pipelines in Kotlin & Go, diverging from the usual Python, Java, or Scala.\nhttps://engineering.grab.com/processing-etl-tasks-with-ratchet\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-48", "title": "Data Engineering Weekly", "content": "Welcome to the 48th edition of the data engineering newsletter. This week's release is a new set of articles that focus on Pedram Navid\u2019s for SQL, Benn Stancil\u2019s analytics is at a crossroad, Continual\u2019s the future of modern data stack, NuBank's Scaling data analytics with software engineering best practices, DoorDash's indexing infrastructure with Apache Kafka and Elasticsearch, Pinterest's unified Flink datasource, PayPal's introduction to dataFu-Spark, Pinterest's interactive querying with Apache Spark.\nRudderStack provides data pipelines that make it easy to collect data from every application, website, and SaaS platform, then activate it in your warehouse and business tools.\nAirbnb is hosting its first virtual tech talk focusing on data quality Wednesday,  July 28th, 12:00 PM-1:00 PM PST. SignUp here\nhttps://journeytowardhighqualitydata.splashthat.com/\nLast week we saw  Jamie Brandon's manifesto against SQL. SQL's problems boil down to its inexpressiveness, incompressibility, and non-porousness. Pedram Navid writes a well-thought article for SQL, saying it is not a concern in most cases, and when it comes to composability, tools like dbt have helped bridge that gap bringing the power of jinja templating to SQL. The author raised some valid questions: all these arguments against SQL result from an almost class divide between \"Software Engineering\" and \"Data People.\"?\nhttps://pedram.substack.com/p/for-sql\nBeen Stancil writes beautifully summarized thoughts on For SQL, Against SQL, and the data team a short story by start by asking, \"The world is full of great analysts. Will we have the courage to go looking for them?.\" The author rightly points out that the most challenging and most essential problems analysts work on aren't technical or even mathematical, highlighting the challenges for analytical engineering.\nhttps://benn.substack.com/p/analytics-is-at-a-crossroads\nThe blog triggered some exciting and much-needed debate on Twitter, and Josh Wills summarized it beautifully. \nThe data infrastructure came a long way from the in-house Hadoop clusters to increasingly adopting the cloud-native solution. The article narrates the emerging focus area on top of the modern data stack, such as AI, data sharing, data governance, streaming, and application serving.\nhttps://continual.ai/post/the-future-of-the-modern-data-stack\nSelf-serving data analytics is the primary goal of an organization to scale data usage and remove the bottleneck from the data team. A well-defined process and tools to enable the process are essential for self-serving analytics. NuBank writes an exciting article sharing its self-serving path on scaling data analytics with software engineering best practices.\nhttps://building.nubank.com.br/scaling-data-analytics-with-software-engineering-best-practices/\nNailing personalization can mean increasing revenue by 15%, but technical challenges keep many companies stuck using basic methods. RudderStack writes a step-by-step guide on designing and implementing a real-time personalization engine using Redis and RudderStack.\nhttps://rudderstack.com/blog/real-time-personalization-with-redis-and-rudderstack\nDoorDash writes about its search indexing infrastructure built on Apache Kafka, Apache Flink, and Elasticsearch. The adoption of incremental indexing to support both the CDC and ETL data, the Assembler design to connect with ETL DB, and windowed API lookup to enrich the entities are some of the highlight design strategies in the indexing infrastructure.\nhttps://doordash.engineering/2021/07/14/open-source-search-indexing/\nPinterest writes about its streaming infrastructure, Xenon, focusing on a unified Flink data source approach to combine Kafka and data on S3 that abstracts the complexity of data storage from the consumer yet deliver all the streaming guarantees. The article captures the trend in the data infrastructure that closes the gap between batch processing and stream processing.\nhttps://medium.com/pinterest-engineering/unified-flink-source-at-pinterest-streaming-data-processing-c9d4e89f2ed6\nApache DataFu\u2122 is a collection of libraries for working with large-scale data in Hadoop. It provides a well-testing solution to common big data processing problems like data deduplication and skewed joins etc. PayPal writes about DataFu integration with Spark with the example of finding the most recent updates in a record, skewed joins, join with range, counting distinct values, and calling python code from scala.\nhttps://medium.com/paypal-tech/introducing-datafu-spark-ba67faf1933a\nThough Presto remains the most popular query engine choice for quick interactive querying with limited resource requirements, we often end up requiring Hive or Spark SQL to query extensive data for ad-hoc exploration. Pinterest shares its experience of building Spark SQL as an interactive query engine using Apache Livy and Remote Spark Context.\nhttps://medium.com/pinterest-engineering/interactive-querying-with-apache-spark-sql-at-pinterest-2a3eaf60ac1b\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-47", "title": "Data Engineering Weekly", "content": "Welcome to the 47th edition of the data engineering newsletter. This week's release is a new set of articles that focus on Erik Bernhardsson's building a data team, Jamie Brandon's Against SQL, Benn Stancil's self-serve is a feeling, Saxo Bank's enabling data discovery, OpenLineage's backfilling Airflow DAGs using Marquez, Shopify's tuning Trino queries, DoorDash's modularize the recommendation engine, Uber's tuning ML models, FinancialTimes' learning from rapid experiments, and OLAP != OLAP Cubes.\nThe blog is possibly one of the best narrations highlighting the real-world complexity of data engineering. The blog is walking through various stages of data team as an organization grows and can be a catalyst to bring data-driven culture.\nhttps://erikbern.com/2021/07/07/the-data-team-a-short-story.html\nSQL is the lingua franca for databases, but how does this stand with other general-purpose languages? The author takes a fresh perspective on SQL, discussing some of the shortcomings and discuss the possibility of what is after SQL looks.\nhttps://scattered-thoughts.net/writing/against-sql/\nEvery organization loves to talk about self-serve analytics, but the definition of self-serve after vague. Is it a bot answering all business questions that is self-serving? The author narrates all aspects of self-serving and emphasizes the importance of chase the self-serve experience that makes the data team and the data consumers feel most at home.\nhttps://benn.substack.com/p/self-serve-is-a-feeling\nBackfilling is a vital aspect of the data pipeline to fix the computing or produce a newer version. In a typical functional data engineering, backfilling can have a cascading downstream effect. Though systems like Airflow does provide backfilling capabilities out-of-the-box, the scope is limited to DAG definition. Marquez writes an exciting blog that narrates how to use the Marquez lineage API to trigger end-to-end backfilling.\nhttps://openlineage.io/blog/backfilling-airflow-dags-using-marquez/\nSaxo Bank writes about its data infrastructure with an in-house central data management application, \"Data Workbench.\" powered by Data Hub and Great Expectations. The blog narrates the data inconsistency issues resulting from inconsistent naming and the Saxo Bank's approach with the data glossary feature.\nhttps://medium.com/datahub-project/enabling-data-discovery-in-a-data-mesh-the-saxo-journey-451b06969c8f\nShopify writes about its experience in tuning the Trino query infrastructure. The workload-specific Trino clusters, analysis on the coordinator node congestion, limit the number of drivers per query to preventing the compute starvation are some of the exciting reads.\nhttps://shopifyengineering.myshopify.com/blogs/engineering/faster-trino-query-execution-infrastructure\nDoordash writes about its experience applying pipeline design patterns to the explore page to improve the modularization. The blog is an exciting read on the pipeline approach to decoupling retrieval and ranking to efficiently solve the information retrieval problem.\nhttps://doordash.engineering/2021/07/07/pipeline-design-pattern-recommendation/\nCreating and maintaining a high-performing model is an iterative process. Uber writes about its ML platform Michelangelo and the support for iterative tuning and one-off comprehensive tuning of ML models.\nhttps://eng.uber.com/tuning-model-performance/\nThe fast-changing content platform brings challenges to run through A/B testing. The Financial Times writes about its lesson learned from adopting the rapid experiments strategy.\nhttps://medium.com/ft-product-technology/6-lessons-from-rapid-experimentation-at-the-financial-times-19524ea36040\nOLAP and OLAP Cubes are often confused, where OLAP specifies the access pattern, and the OLAP Cubes specify the data structure. The blog is walking through the distinction of the two terms.\nhttps://www.holistics.io/blog/olap-is-not-olap-cube/\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-46", "title": "Data Engineering Weekly", "content": "Welcome to the 46th edition of the data engineering newsletter. This week's release is a new set of articles that focus on Pinterest's trusting metrics, Uber's CI/CD for MLOps, Julien's leveraging DBT as a data modeling tool, Facebook's time-series analytics, Pinterest's partial deserialization on thrift, DoorDash's supply-demand ML platform, Swiggy's predicting two-wheeler travel distance, and the story of DBT met Materialize.\nData accuracy is strategically fundamental for the business to make data-driven decisions. The certified data metrics are the standard practice in many companies to bring trust in data. Pinterest writes an exciting post explaining how simple counting can be a complicated task and the metrics certification process works at Pinterest.\nhttps://medium.com/pinterest-engineering/trusting-metrics-at-pinterest-ed76307e10a0\nThe rapid adoption of ML as a core part of feature development also brings significant operational challenges known as MLOps. Uber writes an exciting blog on the evolution of its CI features, including the dynamic model reloading, auto-shading & auto-expiration of the model for an efficient MLOps continuous integration system.\nhttps://eng.uber.com/continuous-integration-deployment-ml/\nThe blog reflects one year with DBT, answering questions on whether we can use DBT as a data modeling tool. The author narrates the pros and cons of DBT, from model features & documentation to testing strategy.\nhttps://medium.com/analytics-and-data/leveraging-dbt-as-a-data-modeling-tool-b3caf78f4a3a\nFacebook open source a Python library for generic time-series analysis Kats. Kats supports forecasting, time-series pattern detection, feature extraction & embedding, and time-series event simulators.\nhttps://engineering.fb.com/2021/06/21/open-source/kats/\nGithub: https://github.com/facebookresearch/Kats\nThe structured event stream processing brings challenges to data modeling. Often the event structure ends up a complex nested structure, and the consumers need to process only a subset of events most of the time. Serialization & deserialization is compute-intensive for the downstream consumers. Pinterest writes an existing blog on how it implemented partial deserialization on thrift to process the events efficiently.\nhttps://medium.com/pinterest-engineering/improving-data-processing-efficiency-using-partial-deserialization-of-thrift-16bc3a4a38b4\nDoorDash writes about its ML-driven approach for its Supply-Demand system that reduces the cancelation and delivery time. The blog is a classic reference design of matching the product requirement to system capabilities for an efficient operation.\nhttps://doordash.engineering/2021/06/29/managing-supply-and-demand-balance-through-machine-learning/\nSwiggy's data science team shares its system design on predicting the two-wheeler distance travel distance that uses the synthesized ground truth distance as labels and historical features to build an ML model. The blog that compares the existing distance computing model with the ML approach is an exciting read.\nhttps://bytes.swiggy.com/learning-to-predict-two-wheeler-travel-distance-752d836d741d\nAs the data volume increases, the processing pattern tends to move towards real-time processing rather than batch processing. DBT writes about the new adopter for Materialize, a SQL platform for processing stream data. The streaming data warehouse is an exciting space to watch.\nhttps://blog.getdbt.com/dbt-materialize-streaming-to-a-dbt-project-near-you/\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-45", "title": "Data Engineering Weekly", "content": "Welcome to the 45th edition of the data engineering newsletter. This week's release is a new set of articles that focus on Rohan Goel's ultimate repo of data discovery solutions, NYT's tracking Covid-19 from hundreds of sources, Adevinta's Building a data mesh to support an ecosystem of data products, Nvidia's what is synthetic data, Adobe's migrating to Apache Iceberg, Confluent's consistency & completeness in Kafka stream, Databricks Photon public preview announcement, StarTree's introduction to geospatial queries in Pinot, LinkedIn's text analytics in Pinot, and Myntra's Apache Spark optimization using Sparklens.\nThe data discovery system is a critical infrastructure in data engineering, and there are growing startups to solve the discovery problem. The ultimate repo of data discovery solutions is an excellent work that captures the current data discovery solutions. Thanks, Rohan, for sharing it.\nhttps://www.notion.so/The-Ultimate-Repo-of-Data-Discovery-Solutions-149b0ea2a2ed401d84f2b71681c5a369\nData Engineering Weekly wrote a metadata edition that captures the timeline of data discovery systems development in previous editions. \nhttps://www.dataengineeringweekly.com/p/data-engineering-weekly-21-metadata\nNYT shared their experience developing a covid-19 application where the tracking work began with a single spreadsheet to more than 9.98 million programmatic requests for Covid-19 data from websites worldwide. It's fascinating to read about the scrapper development where the counties and cities source website frequently changed during the Covid crisis. The blog raised an important point: public data is not open data unless it is well-maintained, documented, and queryable APIs.\nhttps://open.nytimes.com/tracking-covid-19-from-hundreds-of-sources-one-extracted-record-at-a-time-dd8cbd31f9b4\nAdevinta writes an exciting blog about its journey towards data mesh architecture and what worked and didn't work. The learning focused on SQL access, Dataset as a Product & Domain data is an excellent blueprint of implementing data mesh architecture. \nIt is exciting to read the dataset classification on the \"Core Datasets\" & \"Domain Dataset.\" approach from Adevinta.  It is essential to recognize while implementing the Decentralized Data Ownership that data is inherently social, and a standalone domain adds no value unless it can connect with other domains.\nhttps://medium.com/adevinta-tech-blog/building-a-data-mesh-to-support-an-ecosystem-of-data-products-at-adevinta-4c057d06824d\nSynthetic data is annotated information that computer simulations or algorithms generate as an alternative to real-world data. The importance of synthetic data comes as AI pioneer Andrew Ng calling for a broad shift to a more data-centric approach to machine learning. The blog narrates the history of synthetic data and comparing it with the augmented and anonymized data.\nhttps://blogs.nvidia.com/blog/2021/06/08/what-is-synthetic-data/\nAdobe shared its experience migrating to Apache Iceberg for faster data access and reducing the dependency on catalogs. In addition, the blog narrates the pros & cons of in-place upgrade vs. shadow migration, and the decision matrix to decide on the migration strategy is a practice that one can adapt in any migration projects.\nhttps://medium.com/adobetech/migrating-to-apache-iceberg-at-adobe-experience-platform-40fa80f8b8de\nIt is a vital feature of a stream processing engine to guarantee that it can recover from failures to a consistent state. Thus, the final results will not contain duplicates or lose any data &\u00a0completeness and\u00a0do not generate incomplete, partial outputs as final results even when input stream records may arrive out of order. Confluent writes an exciting blog that narrates how Kafka Stream guarantees such stream processing semantics on consistency and completeness.\nhttps://www.confluent.io/blog/rethinking-distributed-stream-processing-in-kafka/\nDatabricks announced a brand new query engine, Photon, to run SQL & Spark SQL queries on top of the Delta Lake. Photon does not yet support all Spark features; a single query can run partially in Photon and partially in Spark!!!.  It would be an exciting paper to read how the optimizer work when the task is preferred over Photon vs. Spark SQL, and most importantly, how the data serialization works!!!\nhttps://databricks.com/blog/2021/06/17/announcing-photon-public-preview-the-next-generation-query-engine-on-the-databricks-lakehouse-platform.html\nOne of the exciting features that I liked about Pinot is the customizable indexes for each dimension and provide interactive analytics in real-time. StarTree Data shared how one can run Geospatial analytical using Apache Pinot.\nLinkedIn shared a similar usage of Pinot, narrating how Linkedin runs text analytics using Pinot.\nhttps://medium.com/apache-pinot-developer-blog/introduction-to-geospatial-queries-in-apache-pinot-b63e2362e2a9\nhttps://engineering.linkedin.com/blog/2021/text-analytics-on-linkedin-talent-insights-using-apache-pinot\nSparklens is a profiling and performance prediction tool for Spark with a built-in Spark Scheduler simulator. It helps identify the bottlenecks that a Spark application is facing and provides us with critical path time. Myntra shared its experience in utilizing Sparklens to optimize the Apache Spark jobs.\nhttps://medium.com/myntra-engineering/optimisation-using-sparklens-59477440bdd8\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-44", "title": "Data Engineering Weekly", "content": "Welcome to the 44th edition of the data engineering newsletter. This week's release is a new set of articles that focus on Chris Riccomini's What the heck is Data Mesh, Atlan's the rise of metadata lake, Google's data cascading in AI, Uber's evolution of data science workbench, Benchling's version control data aquarium, Shopify's deleting the undeletable, Expedia's self-serving business intelligence, eBay's optimizing analytical processing pipeline, Capital One's end-to-end ML model, Yelp's modernizing business data index, and Ashley Melanson's how DBT can transform your data pipeline.\nData mesh is a widely discussed data engineering principle, and there were many exciting discussions around it. The concept of \"data as a product\" compelling, and many successful internet companies adopted it in the past with great success. The debate on how the data mesh principle encapsulates the data as a product & decentralized ownership is an exciting space to watch. The author shared some insightful views on data mesh principles.\nhttps://cnr.sh/essays/what-the-heck-data-mesh\nA couple of interesting Twitter threads on the article very educational on data mesh principles.\n\nModern business operations increasingly depend on data to derive their business. As the data takes the central role in the business operation, the number of stakeholders interacting with the data is more diverse than ever. In this increasingly diverse data world, metadata holds the key to the elusive promised land. Is it a time to think about metadata lake? The blog narrates the role of metadata lake in the modern data stack.\nhttps://towardsdatascience.com/the-rise-of-the-metadata-lake-1e95127594de\nData is a foundational aspect of machine learning (ML) that can impact ML systems' performance, fairness, robustness, and scalability. Paradoxically, while building ML models are often highly prioritized, the work related to data is often the least prioritized aspect. The blog summarizes the recent ACM paper Everyone wants to do the model work, not the data work: Data Cascades in High-Stakes AI, and discuss how to address the data cascading effects.\nhttps://ai.googleblog.com/2021/06/data-cascades-in-machine-learning.html\nUber writes about the evolution of its data science workbench, narrating the efficient scheduling, easier Apache Spark integration with the workspace, and package dependency management. The three key learning in the blog is educational read.\nBuild for the experts, design for the less technical users\nDon\u2019t stop at building what\u2019s known; empower people to look for the unknown\nCreate communities with both data scientists and non-data scientists\nhttps://eng.uber.com/evolution-ds-workbench/\nBenchling writes about its evolution of data infrastructure from a legacy warehouse to a continuous data pipeline tuned to increase the analyst velocity. The discussion around the challenges of implementing continuous integration, how data infrastructure is different from a traditional web application, and how Snowflake's zero-copy data clone helped achieve continuous data integration is an exciting read.\nhttps://benchling.engineering/building-a-version-controlled-data-aquarium-976d17fbdd20\nDeleting the data is one of the most complicated problems in data engineering. Often the data infrastructure misses the dependency graph, field-level context on PII data. Shopify writes an exciting blog highlighting the challenges of PII data and talks about its schematization system, obfuscation & enrichment strategy.\nhttps://shopifyengineering.myshopify.com/blogs/engineering/managing-pii-shopify-scale\nThe Lakehouse design provides a delicate balance between the complicated data warehouses and inconsistent data lake systems. Expedia writes about their adoption of Lakehouses, the extension of lakehouse to domain-specific DataLakeMart, and OLAP systems.\nhttps://medium.com/expedia-group-tech/powering-self-service-business-intelligence-across-expedia-group-e3d029a7d1f6\nTuning a data pipeline requires a layered approach to achieve SLA timelines. eBay writes about the various layers to consider while tuning the Spark jobs, such as system level, the process, table optimization, SQL optimization & the Apache Spark job config parameter tuning. The structured debugging approach is a delight to read, and this is the one spot the data infrastructure needs a lot of attention, from manual tuning to automated pipeline tuning.\nhttps://tech.ebayinc.com/engineering/optimizing-analytics-data-processing-on-ebays-new-open-source-based-platform/\nThe main advantage of machine learning over traditional software engineering is that it allows one to build a component that performs a task by training a model from data, which removes the need for a human to precisely perform the task. Why can't we adopt end-to-end ML rather than part of the tasks? Capital One writes about the pros & cons of adopting an end-to-end ML model and the challenges ahead to reach the promising land of the end-to-end ML model.\nhttps://medium.com/capital-one-tech/end-to-end-models-for-complex-ai-tasks-8c34080145cd\nServing the computed metrics to the end-user in an acceptable latency is critical for an enriched user experience. Yelp writes about its journey of business data indexing system that queried the MySQL table to stream-based CDC system that leverages Kafka, Flink, Apache Beam & Cassandra.\nhttps://engineeringblog.yelp.com/2021/06/modernizing-business-data-indexing.html\nI would be surprised if you've not heard or played around with DBT by now. If you've not done so far, the author did a great write-up breaking down the components of DBT.\nhttps://ashleymellz.medium.com/open-source-spotlight-how-dbt-can-transform-your-data-analytics-pipeline-c54cf9516cdf\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-43", "title": "Data Engineering Weekly", "content": "Welcome to the 43rd edition of the data engineering newsletter. This week's release is a new set of articles that focus on Pedram\u2019s building the modern data team, Nvidia\u2019s explainable AP, LinkedIn\u2019s update on responsible AI, Airbnb\u2019s metrics computation at scale, Wrike TechClub\u2019s data quality roadmap, Shopify\u2019s In-Context analytics, Spotify\u2019s visual analytics, Groupon\u2019s Airflow adoption, Mapbox\u2019s migration from Airflow to Dagster, Databricks top 10 announcement from Data + AI summit.\nModern data toolings like DBT maturing can process and manage complex pipelines; however, building the modern data team remains challenging. In addition, prioritizing what the team should work on holds the key to minimizing the dysfunction of a team. In the blog, the author shares the views on data as a product, the good & bad of agile & scrum adoption for the data team.\nhttps://pedram.substack.com/p/modern-data-team\nAI got adopted across industries as part of the core decision-making frameworks, from radiology, credit check to public policymaking. Hence Explainable AI (XAI) is a vital aspect of AI development. What is XAI? How does it work? Nvidia writes an exciting blog introducing XAI.\nhttps://blogs.nvidia.com/blog/2021/05/24/what-is-explainable-ai/\nOn a similar line, LinkedIn talks about an update on responsible AI and how it embedded the principles in the design and engineering process. LinkedIn's responsible AI follows Microsoft's responsible AI principles, discusses AI fairness, privacy, and future roadmap.\nhttps://engineering.linkedin.com/blog/2021/responsible-ai-update\nAirbnb writes about the second part of the Minerva platform to standardize metrics computation at scale. It's an exciting system design read with a declarative SDK to manage datasets, data versioning to maintain metric consistency, self-healing pipeline with batched backfilling, and data quality integrations.\nhttps://medium.com/airbnb-engineering/airbnb-metric-computation-with-minerva-part-2-9afe6695b486\nData quality is a vital aspect of data engineering, and many companies talked about their internal implementation and data quality approach. However, how does one should start the journey of data quality? How does the roadmap look like, and what is the consequence of lacking certain engineering practices? The blog is an excellent narration of the data quality roadmap and reference articles to support data quality efforts.\nPart-1: https://medium.com/wriketechclub/data-quality-roadmap-part-i-61332d5be7a\nPart-2: https://medium.com/wriketechclub/data-quality-roadmap-part-ii-case-studies-614e85906178\nThough the dashboard visualization is a great way to get data into the customer's hand, integrating the analytics into the workflow brings much power to data products. Shopify writes an exciting blog on how it approaches the in-context analytics experience by metrics-driven product design.\nhttps://shopifyengineering.myshopify.com/blogs/engineering/shopify-in-context-analytics\nVisualization is a quick and meaningful way to interpret the data, and the visualization tools often quick to start but hard to master. Spotify writes an exciting blog on how hiring an expert visualization engineer to build core dashboards and templates & guides to standardize the dashboards improves the quality of data analytics.\nhttps://medium.com/spotify-insights/visual-analytics-at-spotify-3d4221d8686\nGroupon writes about its usage of Apache Airflow, and the decision to move away from cron scheduler. The blogs contains a comprehensive functional comparison chart among Apache Airflow, Oozie, Azkaban, and cron schedulers. \nhttps://medium.com/groupon-eng/managing-billions-of-data-points-evolution-of-workflow-management-at-groupon-dab000a3440d\nMapbox shared their migration journey from Airflow to Dagster with the claim that Dagster reduced the core process time from days or weeks to 1-2 hours.!!! The blog narrates Dagster\u2019s Airflow compatibility to do incremental migration, Dagster\u2019s tooling support for testing & local development.\nhttps://medium.com/dagster-io/incrementally-adopting-dagster-at-mapbox-b635b1118594\nDatabricks writes a quick recap of the top 10 announcements from Data + AI summit. Delta sharing, an open protocol to share data securely, data catalog, and Kolas merge into Apache Spark are some of the exciting development to watch in the near future.\nhttps://databricks.com/blog/2021/06/04/dont-miss-these-top-10-announcements-from-data-ai-summit.html\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/omicron-paradigm-architectural-patterns", "title": "Data Engineering Weekly", "content": "Many enterprises are increasingly relying on vertical & horizontal SAAS applications to operate their business. The modern enterprise depends on SAAS applications for all business operation touchpoints from customer relationship management, marketing & demand generations, human resource management, finance and accounting, content management, support ticket management, supply chain management, inventory management, and digital & multi-channel selling.\nThe specialized SAAS applications provide significant vertical business insights to a business to operate efficiently. Salesforce strategically has a better understanding of the CRM domain, where Zendesk has a better understanding of the customer feedback domain. The contextual data enrichment, such as sending the support ticket information from Zendesk to Salesforce or attach the product data to the support tickets, can significantly magnify the SAAS applications\u2019 ability to provide much deeper business insights.\nSeamless integration among the specialized SAAS applications can exponentially increase the operational excellence of a business. Hence, there is a hidden infinite logistic of the data from one SAAS application to another. I call it the\u00a0Omicron Paradigm.\n\nSome exciting patterns are emerging from different corners of the industry to streamline the Omicron paradigm. We can broadly classify the Omicron architectural patterns into three patterns.\nContributor pattern\nProxy pattern\nOrchestrator pattern\nThe contributor pattern is the most widely adopted Omicron pattern. The pattern is the continuation of the traditional ETL model, where the data ingested from different data sources to one centralized data lake and flowing to various other destinations. Thus, you can conceptually imagine the SAAS applications as an extension of the traditional data mart.\n\n\nThe contributor pattern\u2019s advantage is that the data ingestion tools like\u00a0Fivetran,\u00a0Rudderstack, and\u00a0Airbyte\u00a0integrate with most SAAS applications. In addition, the reverse ingestion from the data lake to the SAAS applications is gaining traction with\u00a0Census\u00a0and\u00a0HighTouch.\nThe commoditization of storage by the cloud infrastructure accelerated the contributor pattern; simultaneously, it brings significant data management challenges. The data ingestion frameworks can abstract the infrastructure to ingest the data. Still, the data management, quality, correctness, and integrity of the data fall into the internal data platform team. The contributor pattern\u2019s success depends on highly skilled data practitioners, a challenging skill set to acquire for an enterprise.\nThe proxy pattern is the logical next step for the cloud data platform\u2019s evolution by providing a data management layer abstraction on top of the cloud data warehouse which acts as a proxy to manage the data logistic across the SAAS applications.\n\n\nThe critical difference between the proxy pattern and the contributor pattern is the cloud data platform manages the data ingestion and simplifies the data management practices. As a result, the enterprise data warehouse simply another SAAS application without building a complicated data management platform and SAAS integrations.\u00a0Snowflake\u2019s data-sharing\u00a0platform and\u00a0Databricks data-sharing\u00a0platform are some of the trends following the proxy pattern.\nThe proxy pattern reduces the data management complication and enables an enterprise to set their business operations fast & efficiently. Data is a critical competitive asset of any enterprise. How the cloud data platform can provide data portability and platform portability are challenges while adopting the proxy pattern.\nThe orchestrator pattern encapsulates the recent trend from the SAAS products to provide the Data As A Service (DaaS). The recent acquisitions from\u00a0Atlassian(ChartIO),\u00a0Google (Looker)&\u00a0Salesforce(Tableau)\u00a0amplifies the SAAS DaaS offering. Simultaneously, the advancement in federated query engines like Presto/ Trino, the domain ownership & the data mesh principle, and the No-ETL vital contributors to the orchestrator pattern.\n\n\nIn the orchestrator pattern, A cloud-native distributed data platform enables federated data access across different SAAS applications. For instance, some work in the\u00a0Trino community integrates the Salesforce JDBC connector\u00a0with Trino.\nThe Orchestrator pattern significantly simplifies the data management by pushing the complexity to the SAAS applications. As a result, the SAAS applications have better domain understanding and can solve the data management problem for once than each enterprise solving independently. However, the lack of standard protocol to access the SAAS platforms and the federated query engine\u2019s efficiency in joining multiple data sources are challenges in adopting the orchestrator pattern.\nThe Omicron paradigm is an exciting development in enterprise data management. There is no one silver bullet architectural style for success here, as each architecture has its maturity model. Data integration & data management remains the challenge for any modern enterprise. Commoditization of data management by embracing any one of the Omicron Architecture will shape the data technologies in the future.\nAll the logos are the property of their respective owners and are used here for discussion purposes only. Links are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers\u2019 opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-42", "title": "Data Engineering Weekly", "content": "Welcome to the 42nd edition of the data engineering newsletter. This week's release is a new set of articles that focus on Benn's analytics is a mess, Earnest Research's experience with SQLFluff, eBay's new optimized Spark SQL Engine, Dropbox's optimizing payment with Machine Learning, Microsoft's how seasonality impact metrics, Lyft's end-to-end ML platform, Razorpay's Druid infrastructure, Astronomer's Airflow & Ray integration, Anna's take on how Slack shared channel can improve data quality, and confluent's Kafka summit recap.\nWhen we look at companies with mature data practices, we only see the final, stable metrics and dashboards. However, simple metrics like \"What is the unique user count for this week\"? the definition of unique can have multiple answers, and make no mistake, they all more or less correct. Are metrics real? Are we creating an analytical mess with multiple definitions of metrics? The author narrates how it's not only normal, but it's also necessary.\nhttps://benn.substack.com/p/analytics-is-a-mess\nSQL deserves linter more than ever, and I 100% agree. In this blog post, Earnest Research talks about its experience and effectiveness of SQLFluff, an open-source linter tool for SQL.\nhttps://towardsdatascience.com/sqlfluff-the-linter-for-modern-sql-8f89bd2e9117\nSQLFluff Github: https://github.com/sqlfluff/sqlfluff\neBay writes about its optimized SQL engine for interactive analysis. eBay effectively using the spark's thrift server on Yarn with workload isolation using Yarn queue. The usage of bloom filter indexing, transparent data caching strategy, bucketing improvements, and parquet read optimization are some of the exiting read.\nhttps://tech.ebayinc.com/engineering/explore-ebays-new-optimized-spark-sql-engine-for-interactive-analysis/\nOne of the challenges of the subscription business model is to manage the subscription renewal process efficiently to reduce involuntary churn. Dropbox writes an exciting case study of how it applied ML techniques in the renewal process to increase the retention rate.\nhttps://dropbox.tech/machine-learning/optimizing-payments-with-machine-learning\nThe seasonality such as weekends, holidays are the critical factors to accommodate in the exploratory data analytics before interpreting the analysis. The blog narrates the walk through the impact of seasonality in analysis and discusses how to handle it. It might be overcomplicated or not fully necessary to get a formula to \u201cnormalize\u201d such data. However, it might be helpful to track such seasonality to understand better how your business is doing.\nhttps://medium.com/data-science-at-microsoft/how-weekends-can-impact-seasonality-and-metrics-db223bd9738a\nFlyte is the workflow automation platform for complex, mission-Critical Data and ML processes at scale. The blog narrates a general overview of Flyte, integration with data catalog, and extensibility of the platform.\nhttps://medium.com/acing-ai/lyfts-end-to-end-ml-platform-e4498fb1c089\nRazorpay writes about its journey of adopting Apache Druid from Apache Kylin & Spark for multi-dimensional analysis. The blog narrates some of the cluster tunings of Druid, how it improves the performance of the data platform,  and some of the challenges such as auto-scaling Druid's middle manager, enhance analytics on complex data types.\nhttps://medium.com/@birendra.sahu_77409/how-razorpay-uses-druid-for-seamless-analytics-and-product-insights-364c01b87f1e\nRay is a Python-first cluster computing framework that allows Python code, with complex libraries or packages, to be distributed and run on clusters of infinite size. In this blog, Astronomer writes about Airflow integration with Ray using the task flow API and narrates how it uses Ray's in-memory object storage to pass data between the tasks instead of Airflow's traditional XCom approach.\nhttps://www.astronomer.io/blog/airflow-ray-data-science-story\nIntegrating the data quality process with the developer workflow and monitoring process is a critical aspect of a data platform's success. The author discusses one such process of integrating data quality alerting and monitoring with Slack and the business process to ensure high data quality standards.\nhttps://towardsdatascience.com/how-a-shared-slack-channel-can-improve-your-data-quality-e62a4c2a0936\nConfluent writes about a recap of the recent Kafka summit - Europe 2021. Some exciting talks on data mesh foundation, a deep dive on Zookeeper-less Kafka, and the importance of schema registry & structured streaming.\nhttps://www.confluent.io/blog/highlights-from-kafka-summit-europe-2021/\nData Mesh Talk:\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-41", "title": "Data Engineering Weekly", "content": "Welcome to the 41st edition of the data engineering newsletter. This week's release is a new set of articles that focus on Airbnb\u2019s track & measure growth marketing, Dagster takes on Airflow vs. Dagster, NewYorkTimes data privacy tooling, Lyft\u2019s ML model infrastructure on Kubernetes, Uber\u2019s Orbit a time series forecasting library, LinkedIn\u2019s Greykite time series forecasting library, GameChanger\u2019s moving data out of Kafka, GroupOn\u2019s SCD framework Pinion, eBay\u2019s vendor to in-house analytical infrastructure migration, and Pinterest optimized Kafka mirror maker.\nAirbnb writes about its unified tracking measurement system to support marketing campaigns by introducing C-parameter tracking and a system for analytics and growth evaluation. In addition, the blog narrates some of the drawbacks of UTM tracking and why it chooses a custom tracking system.\nhttps://medium.com/airbnb-engineering/how-does-airbnb-track-and-measure-growth-marketing-15ee4ce55c5d\nDagster writes an exciting blog comparing Dagster with Airflow in various lifecycles of a data pipeline development on developing & testing, Deploy & execute and monitor & observe. The metadata-rich,  parameterizable functions\u2013\u2013called\u00a0solids,\u00a0separation of computing and IO, support for Adhoc executions, process isolation with a clear separation of user process and system process, and flexible event-based scheduling are some of the exciting features to explore in Dagster.\u00a0\nhttps://dagster.io/blog/dagster-airflow\nThe privacy policy and GDPR compliance can be challenging for consumer applications, given that there are more than 100 privacy laws by various countries. NYT writes an exciting blog on handling various privacy laws dynamically by its homegrown system called PURR (Privacy, Users, Rules, and Regulations).\nhttps://open.nytimes.com/how-we-manage-new-york-times-readers-data-privacy-d39627d79a64\nLyft writes about its ML model infrastructure on Kubernetes focuses on various ML model development functions, model development, running the training & batch prediction jobs, and model user dashboard for previous model versions & job performances. The design focus on fast iterations, no restriction on supported modeling libraries and their versions, and enabling the system to be accessed programmatically are some of the exciting system design read.\nhttps://eng.lyft.com/lyftlearn-ml-model-training-infrastructure-built-on-kubernetes-aef8218842bb\nUber as a marketplace business, forecasting is a vital aspect to solve the business problems. Uber writes about its open-source time-series library, Orbit, a Python package for Bayesian time series forecasting and inference which provides an intuitive initialize-fit-predict interface for time series tasks and uses probabilistic programming languages under the hood.\nhttps://eng.uber.com/orbit/\nPaper: Orbit - Probabilistic Forecast with Exponential Smoothing\nA similar approach to Uber, To support LinkedIn\u2019s forecasting needs, LinkedIn developed & open-sourced the Greykite Python library. Greykite contains a simple modeling interface that facilitates data exploration and model tuning. The Silverkite algorithm, which is the flagship algorithm of the Greykite library, works well on time series with (potentially time-varying) trends and seasonality, repeated events/holidays, and short-range effects.\nhttps://engineering.linkedin.com/blog/2021/greykite--a-flexible--intuitive--and-fast-forecasting-library\nPaper: A flexible forecasting model for production systems\nGamechanger writes about Tangent, its Kafka to S3 pipeline, and some of the learning while trying to adopt opensource systems such as Kafka Connect, Secor & Gobblin. The focus on monitoring approaches and the integration of terraforming generic autoscaling policies are exciting to read.\nhttps://tech.gc.com/from-pipeline-to-beyond/\nGroupon writes about Pinion, an abstraction over the\u00a0Delta lake APIs for S3 and spark-snowflake connector for Snowflake\u00a0to do SCD type 1,2 & 3 operations in the respective target system. The configuration-driven, plug & play approach to handle the slowly changing dimension to increase the developer productivity is an exciting read on improving the data pipeline efficiency.\u00a0\nhttps://medium.com/groupon-eng/pinion-the-load-framework-79cc1d8bff55\nA data infrastructure at its core requires supporting two primary functions, a  scalable batch & real-time computation and fast, interactive query & analytics. eBay writes about the challenges it faced with vendor solutions on the growing need for data governance & reliability and various customization on the opensource systems to move from the vendor solution to an open ecosystem.\nhttps://tech.ebayinc.com/engineering/from-vendor-to-in-house-how-ebay-reimagined-its-analytics-landscape/\nKafka MirrorMaker widely used replicate traffic among different Kafka clusters spread across multiple regions. Pinterest writes about its Shallow Mirror, an optimized Kafka Mirror Maker, the scalability challenges as the adoption grows, and some of its optimization to improve the Kafka mirror maker performance.\nhttps://medium.com/pinterest-engineering/shallow-mirror-f543b14bb25\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-40", "title": "Data Engineering Weekly", "content": "Welcome to the 40th edition of the data engineering newsletter. This week's release is a new set of articles that focus on\u00a0Airbnb\u2019s metrics consistency at scale, Google\u2019s logica, Shopify\u2019s guide to exploratory analysis, Uber\u2019s automating merchant live monitoring in real-time, SoundCloud\u2019s the journey of the corpus,  Jupyter notebook on the terminal, and Apache Spark 3.1 features.\nLinkedIn published the LinkedIn Big Data Summit agenda is a half-day workshop-style event that focuses on the intersection of AI, Cloud, and Big Data. The conference is open for everyone to attend.\nhttps://thelinkedinbigdatasummit.splashthat.com/\nAirbnb writes about its analytical journey, sharing a few growing pains and introducing Minerva, Airbnb's metrics infrastructure. It's exciting to read Minerva's simplified denormalization process, flexible backfill, comprehensive data management policy support, and integration with the data discovery system.\nhttps://medium.com/airbnb-engineering/how-airbnb-achieved-metric-consistency-at-scale-f23cc53dea70\nOne of the shortcomings of SQL, it is not flexible enough to test and develop reusable components. Google open-source Logica extends classical Logic programming syntax to solve SQL problems using the syntax of mathematical propositional logic rather than the natural English language.\nhttps://opensource.googleblog.com/2021/04/logica-organizing-your-data-queries.html\nExploratory data analysis (EDA) is a critical tool in every data scientist\u2019s kit, and the results are invaluable for answering critical business questions. Shopify shared some of the essential tips for an effective EDA, highlighting the importance of understanding the missing values, categorizing the data, distribution nature of the data, data correlation, and outlier data.\nhttps://shopifyengineering.myshopify.com/blogs/engineering/conducting-exploratory-data-analysis\nIntuit writes about its holistic approach to secure the data lake. The journey from manual to automated data discovery and classification, encryption by default, focus on dataset ownership are the key highlights.\nhttps://medium.com/intuit-engineering/safeguarding-data-in-the-data-lake-intuits-holistic-approach-1109bbbae2cb\nUber writes about Charon, its internal framework for controlling the demand at the merchant level through the enforcement of real-time rules. The high-level architecture is an exciting read with Presto & Pinot at the core of the rule engine integrated with Hive & Kafka.\nhttps://eng.uber.com/charon/\nSoundCloud writes its journey migrating from Redshift to BigQuery with the project Corpus to create a single centralized source of truth for SoundCloud's most relevant data. It's an exciting read on the mission-driven approach focusing on quality, compliance, timeliness, usability, efficiency & maintainability, and the approaches to adhere to the principles.\nhttps://developers.soundcloud.com/blog/the-journey-of-corpus\nJupyter notebook on terminal!!! The blog walkthrough on how to install with examples.\nhttps://blog.jupyter.org/nbterm-jupyter-notebooks-in-the-terminal-6a2b55d08b70\nDatabricks writes the highlights of Spark 3.1 releases introducing the new streaming table API, support for stream-stream joins, and structured streaming UI improvements.\nhttps://databricks.com/blog/2021/04/27/whats-new-in-apache-spark-3-1-release-for-structured-streaming.html\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-39", "title": "Data Engineering Weekly", "content": "Welcome to the 39th edition of the data engineering newsletter. This week's release is a new set of articles that focus on WePay's Offline to Online Data Pipeline, Why is self-serving still a problem?, ABN AMRO's data ingestion architecture, InfoQ's Data Gateway, Intuit's SuperGlue, Lyft's Data Science teams remote work, NuBanks' distributing the data teams, Adobe's one-stop anomaly detection shop, and subscribing to a SQL query changes.\nThe fraud detection system requires offline computations or complex analysis performed by users integrated and propagated to an online serving system to fight fraud. WePay writes about its Fraud detection infrastructure on top of the Google Cloud Platform.\nhttps://wecode.wepay.com/posts/an-offline-to-online-data-pipeline-at-wepay\nSelf-serve analytics is a north star architecture for any data infrastructure, but what is self-serve analytics? The author made some excellent narratives on self-serve analytics and emphasis why opinionated simplicity is better than indifferent optionality.\nhttps://benn.substack.com/p/self-serve-still-a-problem\nABM AMRO writes about its digital integration and access layer architecture with the concept of data providers and the consumers. How does the integration and interoperability work between the data provider and the consumers? The blog narrates Digital Integration and Access Layer(DIAL) and the principles behind the architecture.\nhttps://piethein.medium.com/abn-amros-data-integration-architecture-3d266a59fbdd\nModern distributed application architectures created the need for API Gateways and helped popularize API Management and Service Mesh technologies. The growing adoption of data mesh increases the domain-centric ownership similar to Microservices. The blog narrates the current landscape of data gateway and various technologies available to establish data gateway.\nhttps://www.infoq.com/articles/data-gateways-cloud-native/\nIntuit open sources SuperGlue, its lineage-tracking tool built to help visualize data propagation through complex pipelines composed of tables, jobs, and reports. The SQL parsing to understand the lineage, dependency recommendations, anomaly detection, and personalization are some of the exciting features of SuperGlue.\nhttps://towardsdatascience.com/superglue-journey-of-lineage-data-observability-data-pipelines-23ffb2990b30\nGithub: https://github.com/intuit/superglue\nLyft's data scientist team shared some of their experience working during this pandemic. The article contains some great learning on the tools and skills that helped Lyft's data science team.\nhttps://eng.lyft.com/a-day-in-the-life-of-a-lyft-data-scientist-ffe6651f138b\nOne of the critical KPI for a data team is its impact on the organization's data-driven culture. As it scales, a data platform does not only need to adapt its technology stack but also the people organization around it. NuBank writes an exciting blog sharing their learning on building the data team.\nhttps://building.nubank.com.br/distributing-the-data-team-to-boost-innovation-reliably/\nAdobe\u2019s security intelligence team open sources OSAS, a security intelligence toolset to detect security anomalies. \nhttps://medium.com/adobetech/introducing-the-one-stop-anomaly-shop-osas-c27581ee1bd3\nGitHub: https://github.com/adobe/OSAS\nPaper: https://www.insticc.org/Primoris/Resources/PaperPdf.ashx?idPaper=7424dHp8E4k=\nAn interesting Hacker News thread on the topic of subscribing to the incremental materialize views. There are some excellent comparisons of RethinDB, Materialize, and Postgres' proposals for incremental view maintenance.\nhttps://news.ycombinator.com/item?id=26901352\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-38", "title": "Data Engineering Weekly", "content": "Welcome to the 38th edition of the data engineering newsletter. This week's release is a new set of articles that focus onOrphaned Analytics, Macro Trends in the tech, a comprehensive guide on data quality, SQL engine benchmarking, Lake file format comparison, Iceberg\u2019s ACID transaction support, cloud cost management, forecasting algorithms, Apache Pinot\u2019s theta sketches, and layering DBT project\nDoes the number of developed ML models is an indicator of a company's analytics prowess and maturity? What is the cost of orphaned analytics? The author narrates the cost of orphaned analytics, representing a significant operational and regulatory risk and walkthrough the role of Hypothesis Development Canvas to Prevent Orphaned Analytics.\nhttps://www.datasciencecentral.com/profiles/blogs/orphaned-analytics-the-great-destroyers-of-economic-value\nThoughtWorks writes about the macro trends in the technology industry. The fall and rise of SQL and mainstream machine learning trends are an exciting read.\nhttps://www.thoughtworks.com/insights/blog/macro-trends-technology-industry-april-2021\nData quality shows the extent to which data meets users\u2019 standards of excellence or expectations. In a comprehensive guide for data quality, the author narrates data quality dimensions, including Accuracy, Completeness, Timeliness, Consistency, and Uniqueness, and the formulas for metrics calculation.\nhttps://towardsdatascience.com/a-comprehensive-framework-for-data-quality-management-b110a0465e83\nExplorium writes an informative benchmark comparing Redshift, Trino & Presto. Redshift Spectrum outperforming Trino & Presto adjusted to the cost is an interesting finding.\nhttps://medium.com/explorium-ai/benchmarking-sql-engines-for-data-serving-prestodb-trino-and-redshift-1c5f16d6e5da\nLakeFS writes an exciting blog comparing the lake formats Hudi, Iceberg, and Delta Lake on their platform compatibility, performance & throughput, and concurrency. The recommendations are, If you are also already a Databricks customer, Delta Engine brings significant improvements. If your primary pain points are managing huge tables on an object store (more than 10k partitions), Iceberg works excellent. If you use various query engines and require flexibility for managing mutating datasets, Hudi does the job.\nhttps://lakefs.io/hudi-iceberg-and-delta-lake-data-lake-table-formats-compared/\nThe write amplification increases while concurrent process trying upsert at the same time. Adobe writes about Tombstone, its internal implementation of row-level upsert operation on Iceberg to handle more than 10B rows reprocessing every day\nhttps://medium.com/adobetech/iceberg-series-acid-transactions-at-scale-on-the-data-lake-in-adobe-experience-platform-f3e8fe0cef01\nCloud cost optimization is one of the vital aspects of platform engineering, and Airbnb's cost data foundation shares its learnings from building a pipeline, defining metrics, and designing visualizations.\nhttps://medium.com/airbnb-engineering/achieving-insights-and-savings-with-cost-data-ec9a49fd74bc\nMicrosoft writes the second part of the time series forecasting series, focusing on selecting the algorithms. The blog narrates a univariate forecasting engine and evaluation metrics to measure the predictions.\nhttps://medium.com/data-science-at-microsoft/time-series-forecasting-part-2-of-3-selecting-algorithms-11b6635f61bb\nLinkedIn writes about Apache Pinot's  Theta-Sketches set intersection cardinality estimation to solve the audience-reach estimation problem in production. This new solution alleviated the existing problem of data staleness by reducing data size (by approximately 80%) and capping the data size growth from superlinear to sub-linear.\nhttps://engineering.linkedin.com/blog/2021/pinot-and-theta-sketches\nHow to layer a DBT project? The author narrates why the layering is vital in data infrastructure and a complete description of the root layer, logic layer, dimension & activity layer, and reporting layer.\nhttps://mitchellsilv-79772.medium.com/layering-your-data-warehouse-f3da41a337e5\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-37", "title": "Data Engineering Weekly", "content": "Welcome to the 37th edition of the data engineering newsletter. This week's release is a new set of articles that focus on Data Council's popular OSS data projects, Uber's real-time infrastructure, Introduction to Pinot's upsert, Facebook's self-supervised forecasting, Databricks' forecasting using Spark & Prophet, Data engineering practices at Wikimedia, Yelp's data infrastructure, Salesforce's strongly consistent global secondary index for HBase, AutoTraders' event tracking validation, Monte Carlo Data's root cause analysis for data engineers, and AlayaLabs' production data pipeline.\nData Council published 2021 popular data engineering open source. It's no surprise DBT is leading the list. A notable thing to see is three orchestration engines (Airflow, Dagster & Prefect) in the top 6 list, which shows the orchestration space still lacks consolidation. One vital takeaway from the survey is that data preparation is the hardest part of data analytics on any scale.\nhttps://petesoder.medium.com/what-are-the-most-popular-oss-data-projects-of-2021-84ef021bb5a2\nThe conversation gives an excellent overview of Wikimedia's data infrastructure. There is a good highlight of the challenges of collecting data from the edge network, principle-based metrics definition than profit-based, and privacy. One awakening moment for me in the conversation, we have sophisticated data computation and management frameworks, and none of them treat data privacy as a first-class citizen.\nhttps://www.speedwins.tech/posts/some-words-with-nuria-ruiz\nUber writes an exciting paper summarizing its real-time infrastructure with Apache Kafka, Apache Flink, Apache Pinot & Presto as a foundational technology stack. The Kafka consumer proxy, the logical separation of Kafka topics, Auto-scaling Flink applications, Pinot's upsert feature, and Pinot integration with the rest of the data ecosystems are some of the exciting read.\nhttps://arxiv.org/pdf/2104.00087.pdf\nPinot is an immutable data store, which means that there is no genuine concept of upsert as you stream data into it from Kafka. The blog summarizes the need for upsert support and how it differs from the traditional database upserts.\nhttps://medium.com/apache-pinot-developer-blog/introduction-to-upserts-in-apache-pinot-987c12149d93\nForecasting is one of the core data science and machine learning tasks. Providing fast, reliable, and accurate forecasting results with large amounts of time series data is vital for a business operation. Facebook writes about its framework, SSL-HPT, that takes time-series features as inputs and produces optimal hyperparameters in less time without sacrificing accuracy.\nhttps://ai.facebook.com/blog/large-scale-forecasting-self-supervised-learning-framework-for-hyper-parameter-tuning/\nFacebook's Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. Databricks writes about training hundreds of time series forecasting models in parallel with Facebook Prophet and Spark.\nhttps://databricks.com/blog/2021/04/06/fine-grained-time-series-forecasting-at-scale-with-facebook-prophet-and-apache-spark-updated-for-spark-3.html\nYelp writes about its data lake sink connector that enables integrating analytical events. The data pipe CLI tool, Pipeline Studio web UI & schema management tooling, highlights building self-service data ingestion.\nhttps://engineeringblog.yelp.com/2021/04/powering-messaging-enabledness-with-yelps-data-infrastructure.html\nSecondary indexing, which enables efficient queries on non-primary key fields, is central in many use cases. Apache HBase's ability to read random, real-time read/write access comes with the cost that the access pattern depends on the key. Salesforce writes about how Apache Phoenix supports a strongly consistent global secondary index. The design approach of handling immutable (the secondary index column is immutable) and mutable(the secondary index column is mutable) is an exciting read.\nPart 1: https://engineering.salesforce.com/the-design-of-strongly-consistent-global-secondary-indexes-in-apache-phoenix-part-1-90b90bda4210\nPart 2: https://engineering.salesforce.com/the-design-of-strongly-consistent-global-secondary-indexes-in-apache-phoenix-part-2-392c57ec6633\nOne of the challenging parts of the event instrumentation is how to build automated test suites to ensure that new releases of their websites, mobile apps, and server-side applications do not break tracking. Autotrader writes an exciting article on how it automated the Snowplow event validation with the Cypress test suite.\nhttps://engineering.autotrader.co.uk/2021/04/09/cypress-snowplow-micro-blog.html\nDebugging and Root Cause Analysis of a distributed system is very challenging, and the data pipeline is no exception. Standard techniques like error messages, reading the code, or unit & integration tests often misleading. The blog narrates the manual steps required for one such root cause analysis in a data pipeline and emphasizes the need for automation for a faster resolution.\nhttps://towardsdatascience.com/root-cause-analysis-for-data-engineers-782c02351697\nAlyaLabs writes about its data infrastructure using Snowflake, S3, Airflow & Looker and how it converts the prototyping from Jupiter Notebook to a continuous data pipeline.\nhttps://medium.com/alayalabs/from-jupyter-notebooks-to-production-data-pipelines-our-framework-for-delivering-data-projects-6b8f41643520\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-36", "title": "Data Engineering Weekly", "content": "Welcome to the 36th edition of the data engineering newsletter. This week's release is a new set of articles that focus on PayPal's thread on customer churn problem, Pinterest's opensource Querybook, Capital One's from batch to real-time CDC journey, QuantumBlack's thoughts on data engineering role, Pinterest's Flink infrastructure on detecting image similarity, Shopify's building smart search products, Microsoft's introduction to the time series forecasting, Confluent's first glimpse on Kafka without Zookeeper, Fathom's website analytics infrastructure, Financial Times trending topic infrastructure, Picnic's data warehouse journey, Data Mechanics Apache Spark 3.1 release with Kubernetes support GA, Groupon's customizing Airflow UI.\nThis week, let's start with an insightful thread from PayPal on how it solved a customer churn problem from first principles. The thread is a blueprint of how to run your data analytics process.\nAd-hoc analytics is the first step for building a data analytics product. The need for ad-hoc analytics evolved from a simple SQL editor to an integrated workflow engine\u2014Pinterest opensource Querybook with enhanced visualization, collaboration, and scheduling feature as a hub for data analytics.\nhttps://medium.com/pinterest-engineering/open-sourcing-querybook-pinterests-collaborative-big-data-hub-ba2605558883\nChange Data Capture and event sourcing is the vital component of data infrastructure. Capital One writes about introducing the event sourcing & CDC and an excellent comparison between Debezium and AWS Data migration service.\nhttps://medium.com/capital-one-tech/the-journey-from-batch-to-real-time-with-change-data-capture-c598e56146be\nToday\u2019s data engineers are responsible for unlocking data science and analytics in an organization and building well-curated, accessible data foundations. Responsibilities have increased, and expectations are higher than they were even five years ago.\nThe article is an exciting summary of the emerging importance of Data Engineering and the need for the organization for growing data engineering skills.\nhttps://quantumblack.medium.com/data-engineerings-role-is-scaling-beyond-scope-and-that-should-be-celebrated-ca9fa1cb8cbb\nPinterest writes about its near real-time infrastructure to detect image similarity. The article narrates the design of Flink stream-stream join, LSH (Locality Sensitive Hashing) lookup, and the graph storage need for storing the identified cluster to the member list. Pinterest's approach to propagate the debugging data through the Flink operator is an exciting read on the complex pipeline's operability, which one can adapt to any stream processing pipeline.\nhttps://medium.com/pinterest-engineering/detecting-image-similarity-in-near-real-time-using-apache-flink-723ce072b7d2\nSearch is a core functionality of most business applications, and it is one of the vital applications of a data product. How to continuously validate the search algorithms? Shopify narrates a three-step approach from collecting the data to evaluating online and offline metrics.\nhttps://shopifyengineering.myshopify.com/blogs/engineering/evaluating-search-algorithms\nTime series forecasting operates in a well-defined problem space and expands across different domains. Producing high-quality forecasts is not an easy problem. Microsoft wrote an exciting blog on time series forecasting fundamentals and summarized a few popular Python forecasting packages to get started.\nhttps://medium.com/data-science-at-microsoft/time-series-forecasting-part-1-of-3-understanding-the-fundamentals-13b52eda3e5\nReference Book:\nThe article reference Forecasting: Principles and Practice for time series forecasting principles.\nApache Kafka community started replacing Zookeeper with a self-managed metadata quorum, and the community potentially gets early access in the upcoming 2.8 release. Confluent writes about how the quorum control works if you opt for Kafka and scaling up & down the Kafka cluster.\nhttps://www.confluent.io/blog/kafka-without-zookeeper-a-sneak-peek/\nKIP-500 An informative read about Kafka as a quorum design.\nFathom Engineering writes about its analytical database journey from MySQL to SingleStore (MemSQL). The article narrates the scalability challenges with MySQL as an analytical DB and the evaluation process of Elasticsearch, Timescale DB, Rockset & ClickHouse. The article is an excellent reminder of how important to have the documentation well-written and easy to understand.\nhttps://usefathom.com/blog/worlds-fastest-analytics\nFinancial Times writes about its trending topic prediction infrastructure and how it helps journalists write more relevant stories. Slack's integration as part of the prediction workflow to send signals to the stakeholders is an exciting design and a good reminder about incorporating the business workflow as part of the prediction system.\nhttps://medium.com/ft-product-technology/predicting-ft-trending-topics-7eda85ece727\nPicnic data team writes about their five-year journey of its data warehouse system. There are many exciting lessons on store time in UTC, the importance of follow up on the stop-gap solutions, start with a low-risk tech stack and scale up as you grow, document the data catalog early, minimize the number of tooling.\nhttps://blog.picnic.nl/how-we-built-our-lakeless-data-warehouse-38178f6cee12\nWith the Apache Spark 3.1 release in March 2021, the Spark on Kubernetes project is officially declared production-ready and Generally Available. The blog narrates the Apache Spark Kubernetes support journey from version 2.4 to 3.1. The blog highlights some of the key enhancements on Spark 3.0, such as handling graceful executor decommission, supporting the NFS volume option (now it's much simpler to integrate EFS), and stage-level scheduling.\nhttps://www.datamechanics.co/blog-post/apache-spark-3-1-release-spark-on-kubernetes-is-now-ga\nThe ability to customize Airflow UI with additional task KPI can significantly improve the data team's productivity. Groupon writes an exciting blog on how it did the same with additional KPI with the code example.\nhttps://medium.com/groupon-eng/how-to-add-custom-kpis-to-airflow-ac09eb1bf3e1\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-35", "title": "Data Engineering Weekly", "content": "Welcome to the 35th edition of the data engineering newsletter. This week's release is a new set of articles that focus on DeepLearningAI's model-centric to data-centric AI, FreeCodeCamp's MLOps explained, Salesforce's building successful AI platform, Shopify's CDC journey, Strava's scaling data culture, New York Times SQL interview process, Tiquets taming data dependency with DBT, executing shuffle without MapReduce in Ray, Adaltas file format's data size comparison, Working with nested data structure in PySpark, DataCamp's open-source auto-dependency ingestion project ViewForm.\n80% of the ML workload is data preparation and management, yet 99% of papers published focus on AI research, and 1% focus on data. The talk narrates ML lifecycle's importance and why it requires from model-centric to data-centric makes data quality a systematic & reliable process.\nThe enterprises are increasingly embedding ML-enabled decision automation across business verticles. The reliability of the ML applications became mainstream, and the rise of MLOps takes the mainstage. The article walks through different stages of the MLOps and the skills required to develop ML products.\nhttps://www.freecodecamp.org/news/what-is-mlops-machine-learning-operations-explained/\nPaper:\nThe article quoted an interesting paper read: Hidden Technical Debt in Machine Learning Systems\nHow to plan about building an AI platform? The blog narrates the founding blocks of a successful platform. It emphasizes the importance of end-to-end user experience, having the right mix of domain and technical expertise, effective communication channels, faster research to production, uniformity, privacy & trust.\nhttps://engineering.salesforce.com/building-a-successful-enterprise-ai-platform-197a3c4d8b60\nShopify writes an exciting blog about its change data capture journey from periodic batch query polling to continuous change data capture using Debeizium & Kafka Connect. The blog narrates the technical challenges with pull-based change data capture and lessons learned from the CDC platform, such as schema changes and handling large records.\nhttps://shopify.engineering/capturing-every-change-shopify-sharded-monolith\nOne of the data engineering team's vital responsibilities is to drive the data culture across the organization. The blog narrates the importance of focusing on the data pipeline's bottleneck to accelerate the data journey to minimize the people scale problems.\nhttps://fivetran.com/blog/scaling-data-culture-is-a-marathon-not-a-sprint\nNYT writes an exciting blog about the pros and cons of whiteboarding vs. online coding vs. take-home interview formats for the data analyst workload. NYT adoption of the hybrid model interview process is an interesting approach to read.\nhttps://open.nytimes.com/an-update-to-our-sql-interviews-cf39dafeddcf\nThe model-based dependency much powerful when it comes to the data pipeline workload, and DBT made it a default dependency mode. The blog narrates the challenges of maintaining views without tools like DBT, how DBT simplified the problem, and some of the pain points running DBT in production.\nhttps://medium.com/tiqets-tech/taming-the-dependency-hell-with-dbt-2491771a11be\nRay provides a simple primitive for building and running distributed applications. A distributed data computation algorithms rely on efficient data shuffling. The blog narrates how Ray simplifies the data shuffling without the need for MapReduce frameworks.\nhttps://medium.com/distributed-computing-with-ray/executing-a-distributed-shuffle-without-a-mapreduce-system-d5856379426c\nObject storage becomes the default persistence layer for the data lake, and choosing an efficient file format is equally important. The blog did excellent work on comparing various file formats and concludes ORC provides a much effective storage optimization.\nhttps://medium.com/adaltas/storage-size-and-generation-time-in-popular-file-formats-48a23190c1da\nThe nested data structures are the norm of data analytics and help minimize the need to build complex normalization forms. Arrays and Maps are the common nested structures. The author walks through how to handle nested data types in PySpark.\nhttps://anindyacs.medium.com/working-with-nested-data-types-7d1228c09903\nIn a complex data pipeline, finding all the upstream dependency is a tedious job that often results in hacky code search. The data lineage can mitigate the findability of the dependency yet requires multiple navigations. ViewForm takes an exciting approach to automatically generate the internal and external dependency for the task as a code-gen for Airflow.\nhttps://medium.com/datacamp-engineering/viewflow-fe07353fa068\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-34", "title": "Data Engineering Weekly", "content": "Welcome to the 34th edition of the data engineering newsletter. This week's release is a new set of articles that focus on Google\u2019s massively parallel graph computation, Uber\u2019s data journey, Hyperight\u2019s is data mesh right for your organization, Lyft\u2019s ML feature infrastructure, Flyte joins LF Data & AI, PayPal\u2019s secure data movement, Data pipeline @ Samsara, Gousto data teams\u2019 best of 2020, Cloudflare\u2019s anomaly detection, Instacart\u2019s take on large-scale labeling, Dagster 0.11 release note, and why Kafka is fast.\nGraph computation is widely used for various data science purposes, from ranking web pages by popularity and mapping out social networks. Google AI discusses MapReduce's limitations in graph processing and introduces Adaptive Massively Parallel Computation Model using a distributed hash table.\nhttps://ai.googleblog.com/2021/03/massively-parallel-graph-computation.html\nPapers:\nParallel graph algorithms in constant adaptive rounds: theory meets practice\nMassively Parallel Computation via Remote Memory Access\nUnconditional Lower Bounds for Adaptive Massively Parallel Computation\nUber writes an exciting blog on the challenges of operating a data platform at scale. Self-serving analytics is a north star dream of many businesses. However, it also brings multiple challenges such as data duplication, data discovery issues, disconnected tooling, logging inconsistency, lack of process, and lack of SLA and ownership.\u00a0\nThe blog narrates how Uber is solving the problem by adapting the fundamental data platform principles.\nData as Code\nData is Owned\nData Quality is Known for each dataset.\u00a0\nAccelerate data productivity with data tools optimized for collaboration.\nOrganize the data with local data ownership\nhttps://eng.uber.com/ubers-journey-toward-better-data-culture-from-first-principles/\nDoes Data Mesh make sense for all types of organizations? The captures the collective thoughts on data mesh principles on when to apply them and the future outlook of data mesh and DataOps.\nhttps://read.hyperight.com/is-data-mesh-right-for-your-organisation/\nA vital requirement for the ML model's feature computation needs to be made available via batch queries for model training and via low-latency online inference. Lyft writes about its feature service consist of feature definition, feature ingestion & processing, and retrieval.\nhttps://eng.lyft.com/ml-feature-serving-infrastructure-at-lyft-d30bf2d3c32a\nContinuing on Lyft\u2019s ML feature serving infrastructure, Flyte, the core platform for orchestrating the machine learning job, joins the Data & AI chapter of the Linux Foundation.\nhttps://eng.lyft.com/flyte-joins-lf-ai-data-48c9b4b60eec\nPaypal writes an exciting article on the challenges of secure data movement across data centers. The article narrates how it uses Apache Gobblin, Kerberos, and KMS to handle secure transfer, encryption at rest, and the prevention of unauthorized & unauthenticated access.\nhttps://medium.com/paypal-tech/how-paypal-moves-secure-and-encrypted-data-across-security-zones-10010c1788ce\nSamsara writes about its data pipeline infrastructure builds with a data transformation DSL and AWS step function. One of the complicated challenges of a data pipeline that depends on the tasks than the model (data) requires significant engineering effort to resolve duplications. Samsara narrates an exciting read on how it handles the task dependency and deduplication of the tasks using DynamoDB to store the data transformation metadata.\nhttps://medium.com/samsara-engineering/data-pipelines-samsara-64596dbc2137\nGousto writes an excellent summary highlighting some of the data teams\u2019 projects 2020, design choices, and decision factors. I wish every team publishes their yearly summary as a guide.\nhttps://medium.com/gousto-engineering-techbrunch/gousto-data-team-best-of-2020-8a731837ace2\nCloudflare writes about anomaly detection for bot management using Redis, Kafka, and ClickHouse. The blog narrates the overall architecture, the adoption of microservices, and the Redis performance tuning.\nhttps://blog.cloudflare.com/lessons-learned-from-scaling-up-cloudflare-anomaly-detection-platform/\nData collections often require human labeling to annotate the datasets. Crowdsourcing has emerged as one of the possible ways to collect labels at scale. Instacart writes a \u201cPre-flight Checklist\u201d of tasks for implementing large-scale crowdsourcing tasks.\nhttps://tech.instacart.com/7-steps-to-get-started-with-large-scale-labeling-1a1eb2bf8141\nDagster released version 0.11.0, codenamed \u201cLucky Star,\u201d with MySQL backend support, better backfill management, and experimental support for data lineage.\nhttps://github.com/dagster-io/dagster/releases/tag/0.11.0\nThe author narrates some of Kafka's foundational design principles and demonstrates why it becomes the central nerve of data processing and management.\nhttps://medium.com/swlh/why-kafka-is-so-fast-bde0d987cd03\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-33", "title": "Data Engineering Weekly", "content": "Welcome to the 33rd edition of the data engineering newsletter. This week's release is a new set of articles that focus on Michael Stonebraker\u2019s Top 10 Big Data Blunders, Stanford University\u2019s AI index report 2021, Maxime\u2019s The future of the Business Intelligence is open source, Mehdi\u2019s data engineering skills report, Apache Airflow survey 2020, DataMinded\u2019s things to consider for Argo Workflow, Spotify\u2019s new experimentation strategy, LightUp\u2019s hidden data outages, Confluent\u2019s real-time analytics with Kafka & Pinot, Pinterest\u2019s Flink deployment framework, AWS\u2019s new feature on Hudi, and Trino\u2019s new window function enrichments.\nMichael Stonebraker: Top 10 Big Data Blunders\nSome of the recent articles and conversations around data modeling remind me of Michael Stonebraker's talk about the top 10 big data mistakes. It is an excellent talk to watch/ re-watch.\nStanford University: The AI Index Report - Measuring trends in Artificial Intelligence\nStanford University published AI Index Report for 2021, focusing on AI development in the USA. It\u2019s an exciting read, and the top 9 takeaways are,\n\u201cDrugs, Cancer, Molecular, Drug Discovery\u201d received the greatest amount of private AI investment in 2020, with more than USD 13.8 billion, 4.5 times higher than 2019.\nIn 2019, 65% of graduating North American PhDs in AI went into the industry\u2014up from 44.4% in 2010\nAI systems can now compose text, audio, and images to a sufficiently high standard.\nThe diversity challenge - In 2019, 45% of new U.S. resident AI Ph.D. graduates were white\u2014by comparison, 2.4% were African American, and 3.2% were Hispanic.\nChina overtakes the US in AI journal citations.\nThe majority of the US AI Ph.D. grads are from abroad\u2014and they\u2019re staying in the US.\nSurveillance technologies are fast, cheap, and increasingly ubiquitous.\nAI ethics lacks benchmarks, and consensus remains a challenge.\nAI gained attention in congress: The 116th Congress is the most AI-focused congressional session in history. The number of mentions of AI in congressional record more than triple that of the 115th Congress.\nhttps://aiindex.stanford.edu/report/\nMaxime Beauchemin: The Future of Business Intelligence is Open Source\nThe open-source databases and data processing ecosystem revolutionized software development. The author raised an interesting question: When it comes to the BI platform, Why is it mostly closed source?\nhttps://maximebeauchemin.medium.com/the-future-of-business-intelligence-is-open-source-9b654595773a\nMehdi Ouazza: What are the most requested technical skills in the data job market? Insights from 35k+ data jobs ads\nIt is an insightful hack to understand the skills in demand in data engineering. SQL & Python the top skill to develop if you're into data science or data engineering. The author's take on Python over Scala for data engineering resonates well with the Spark ecosystem's current development.\nhttps://medium.datadriveninvestor.com/what-are-the-most-requested-technical-skills-in-the-data-job-market-insights-from-35k-datajobs-ads-d8642555f89e\nApache Airflow: Airflow survey 2020\nApache Airflow published the 2020 Airflow survey result. Some of the exciting trends to highlight\n13.79 adoption of the general developer community outside the data engineers.\n85% of people using Airflow like/ very likely recommends Airflow.\nAirflow local executor popular than the Kubernetes executor\nSlack & Github is a go-to place for technical questions, 2X higher than StackOverflow!!\nhttps://airflow.apache.org/blog/airflow-survey-2020/\nDataMinded: What to consider before choosing Argo Workflow?\nArgo Workflows is an open-source container-native workflow engine for orchestrating parallel jobs on Kubernetes. The blog narrates the basic workflow using Argo and the pros and cons of Argo Workflow from the data engineering perspective.\nhttps://medium.com/datamindedbe/what-to-consider-before-choosing-argo-workflow-54f6067307a8\nSpotify: Spotify\u2019s New Experimentation Coordination Strategy\nSpotify wrote about its new experimentation coordination strategy and migrated the experimentation platform to using Bucket Reuse for all experiments. The narration on handling exclusive and nonexclusive experiments and the concept of paths exciting to read.\nhttps://engineering.atspotify.com/2021/03/10/spotifys-new-experimentation-coordination-strategy/\nLightUp: Your Data Keeps Breaking Silently: Isolated Incidents or a New Category of Problems\nLightUp writes an exciting two-part blog on the new category of the data outage termed \"The Hidden Data Outages.\". The blog narrates some of the case studies where the hidden data outages cause significant business loss and the call for a dedicated data monitoring platform. \n\nhttps://blog.lightup.ai/your-data-keeps-breaking-silently-isolated-incidents-or-a-new-category-of-problems-part-2-2d3979f27a21\nhttps://blog.lightup.ai/your-data-keeps-breaking-silently-isolated-incidents-or-a-new-category-of-problems-part-1-2e397df2c36c\nConfluent: Under the Hood of Real-Time Analytics with Apache Kafka and Pinot\nApache Pinot is a distributed analytics data store rapidly becoming the go-to solution for building real-time analytical applications at scale. The blog narrates how the real-time ingestion from Kafka to Apache Pinot works and the internal implementation of mutable vs. immutable segments, query processing & memory management.\nhttps://www.confluent.io/blog/real-time-analytics-with-kafka-and-pinot/\nPinterest: Pinterest Flink Deployment Framework\nPinterest writes about its Flink deployment framework and the integration with the CI/ CD pipeline. The blog narrates some of the best practices, such as job deduplication, state preservation before deploying a new version, and focusing on its reversibility.\nhttps://medium.com/pinterest-engineering/pinterest-flink-deployment-framework-512c6cd4a1b7\nAWS: New features from Apache Hudi available in Amazon EMR\nAWS highlighted the new feature improvements in the Apache Hudi available part of the AWS ecosystem. The ability to convert the existing parquet files to the Hudi format, seamless integration with AWS database migration services are some of the standout features. Redshift Spectrum\u2019s ability to query the Apache Hudi dataset is an exciting trend to watch. \nhttps://aws.amazon.com/blogs/big-data/new-features-from-apache-hudi-available-in-amazon-emr/\nTrino: Introducing new window features\nThe SQL window function is a vital feature for analytics queries. Trino writes about its new improvements in supporting the window functions with the full support for the Range frame type, supporting the Group frame type, and adding the windowing as part of the WHERE clause.\nhttps://trino.io/blog/2021/03/10/introducing-new-window-features.html\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions.\n"}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-32", "title": "Data Engineering Weekly", "content": "Welcome to the 32nd edition of the data engineering newsletter. This week's release is a new set of articles that focus on Picnic\u2019s Data Vault modeling, Mihaileric\u2019s why we need more data engineers, Microsoft\u2019s onboarding data scientist checklist, Netflix\u2019s data movement with Google Services, Redpoint Venture\u2019s data feedback loop with SAAS applications, DoorDash\u2019s declarative real-time feature engineering, Uber\u2019s applying ML for internal auditing, Pinterest\u2019s ML techniques to fight misinformation, Monte Carlo\u2019s new data quality rules, and Anna Anisienia\u2019s take on Airflow task group design.\nLet\u2019s start this week with some fun but also the sad reality of the data engineering journey.\nPicnic: Data vault - new weaponry in your data science toolkit\nThe emerging cloud datawarehouse and the structured data approach bring back the importance of data modeling techniques like data vault and the Kimball methodologies. Picnic writes an exciting read on how it uses these data modeling techniques on top of Snowflake to empower historical data access, time-traveling through historical data, integrate with the real-time pipeline. \nhttps://blog.picnic.nl/data-vault-new-weaponry-in-your-data-science-toolkit-e21278c9c60a\n LinkedIn | Twitter\nmihaileric.com: We Don't Need Data Scientists, We Need Data Engineers\nHow do the data practitioners' (data & ML engineering) jobs distributed across the companies are interesting to understand the data domain's emerging pattern. Though the Author analyzed a small set of YC startups, the underlying observation is worth noting. The modern ML frameworks like Tensorflow, PyTorch industrialized machine learning, but the data collection, cleaning & labeling remains unindustrialized and requires manual work for the most part.\nhttps://www.mihaileric.com/posts/we-need-data-engineers-not-data-scientists/\nNetflix: Data movement for Google services at Netflix\nThe business operations use multiple SAAS tools to operate a business unit effectively. It brings many challenges like data access control, lineage tracking, and integration with other business operations. Netflix writes an exciting blog post highlighting how it tackles the challenges using a proxy service for Google workspace apps integrations.\nhttps://netflixtechblog.medium.com/data-movement-for-google-services-at-netflix-9a77ca69f7c4\nRedpoint Ventures: The Feedback Loops in Data that Will Change SaaS Architecture\nAs we noticed in Netflix's Google workspace integration journey, It's an increasingly common pattern for an enterprise to contribute and leverage data from SAAS applications to meet the business goals. The author captures the feedback loop of data flowing across the SAAS applications. It is an exciting space to watch.\nhttps://www.linkedin.com/pulse/feedback-loops-data-change-saas-architecture-tomasz-tunguz/\nDoorDash: Building Riviera: A Declarative Real-Time Feature Engineering Framework\nML models play a significant role in improving the users' experience. As a result, an efficient feature engineering framework is a critical part of the ML infrastructure.DoorDash writes an exciting blog that narrates the importance of having a near-realtime feature store to enrich the customer experience and how the Flink-as-a-service platform helps to fulfill the mission.\nhttps://doordash.engineering/2021/03/04/building-a-declarative-real-time-feature-engineering-framework/\nUber: Applying Machine Learning in Internal Audit with Sparsely Labeled Data\nAs machine learning continues to evolve, transforming the various industries, it touches. Uber narrates one such transformation on how ML helps its internal auditing system, answering questions such as how many Agents per country, number of transactions, total cash paid, evolution over the past three years. It's no surprise to notice the data availability and data labeling mentioned as the most significant challenge rather than ML model development.\nhttps://eng.uber.com/ml-internal-audit/\nPinterest: How Pinterest fights misinformation, hate speech, and self-harm content with machine learning\nProviding a safe and secure experience from health misinformation to hate speech, self-harm, and graphic violence is a significant challenge for social platforms. Pinterest narrates the ML-driven architecture that empowers the system to detect unsafe content before it\u2019s reported automatically.\nhttps://medium.com/pinterest-engineering/how-pinterest-fights-misinformation-hate-speech-and-self-harm-content-with-machine-learning-1806b73b40ef\nMonte Carlo: The New Rules of Data Quality\nHistorically the data quality checks focused on a siloed, data producer-driven testing, which is essentially equivalent to unit testing. Is a unit test is enough for data testing? The blog narrates some of the principles to follow to engineer data quality and empathize the data quality is a collective responsibility.\nhttps://towardsdatascience.com/the-new-rules-of-data-quality-5e4fdecb9618\nAnna Anisienia: TaskFlow API in Apache Airflow 2.0 \u2014 Should You Use It?\nA data pipeline is more than a unit of execution and often requires sharing its state for the downstream jobs for providing composable pipeline. The blog narrated some of the task group design's pros and cons and the practical implication and raised some interesting points on data transformation vs. orchestration.\nhttps://towardsdatascience.com/taskflow-api-in-apache-airflow-2-0-should-you-use-it-d6cc4913c24c\nMicrosoft: Onboarding to a data science team\nMicrosoft writes an exciting blog on a typical checklist for onboarding to a data science job. Though the blog focuses on the data science job, the checklist applies to data engineers since data engineers often require multiple stakeholder communication.\nhttps://medium.com/data-science-at-microsoft/onboarding-to-a-data-science-team-2b735dae464\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-31", "title": "Data Engineering Weekly", "content": "Welcome to the 31st edition of the data engineering newsletter. This week's release is a new set of articles that focus on Redpoint Ventures Reverse ETL, JP Morgan\u2019s data mesh implementation, DBT\u2019s modern data stack, ValidIO\u2019s ML & Data trends 2021, Airbnb\u2019s visualizing data timeline, Pinterest\u2019s lesson learned from running Kafka at scale, Confluent\u2019s 42 things to do once Zookeeper is gone, LinkedIn\u2019s solving data integration problem with Apache Gobblin, Facebook\u2019s mitigating the effect of silent data corruption, Reddit\u2019s scaling reporting system, and LinkedIn\u2019s GraphQL implementation of DataHub. \nRedpoint Ventures: Reverse ETL \u2014 A Primer\nOver the last decade, the cloud and SAAS products changed the way business operates. In a modern business, the customer data spread across many SAAS vendors. A single source of truth is a myth in the modern data infrastructure. I often call this a \"Truth In Motion.\" I shared a similar thought a year back.\nOn the same line, the blog narrates \"Reverse ETL.,\" where the data are flowing from the internal data warehouse to SAAS providers like Salesforce, Zendesk & Intercom. It is an exciting space to watch as the success depends on how the SAAS vendors simplify the ingress and produce cost-effective time to value the customer's data.\nhttps://medium.com/memory-leak/reverse-etl-a-primer-4e6694dcc7fb\nJPMorgan Chase: Implementing a Data Mesh Architecture at JPMC\nJPMC talked about the team's thoughts and implementation strategy of adopting the data mesh principles. The talk is a good narrative of structuring the data mesh principles, publishing taxonomy, and the need for a pragmatic compromise while adopting the data mesh principles.\nhttps://www.dremio.com/subsurface/implementing-a-data-mesh-architecture-at-jpmc\nDBT: The Modern Data Stack: Past, Present, and Future\nHow does the data engineering world look like beyond the Hadoop ecosystem? The blog from DBT gives a comprehensive overview of the modern data stack, starting from the introduction of Redshift and its impact on the data warehouse. The blog narrates the challenges ahead reminds this space is wide open for innovation over the next decades.\nhttps://blog-getdbt-com.cdn.ampproject.org/c/s/blog.getdbt.com/future-of-the-modern-data-stack/amp/\nVALIDIO: ML & Data Trends: Wrapping up 2020 and looking into 2021 & beyond\nThe blog narrates how the underlying data infrastructure influences the ML development in line with the recent trends on \"Reverse ETL\" and the modern cloud-native data stack. The blog also reiterates we are still in the early stages of MLOps, Data Quality tooling, and unified data architecture on the path to industrialization ML development.\nhttps://medium.com/validio/ml-data-trends-wrapping-up-2020-and-looking-into-2021-beyond-b3ff1eadc211\nAirbnb: Visualizing Data Timeliness at Airbnb\nCommitment, Consistency & Clarity in the data pipeline are the core principles to build trust in data to empower a data-driven culture. Airbnb writes an exciting blog about SLA Tracker and how it took a data-driven approach to debug the data pipeline to improve efficiency.\nhttps://medium.com/airbnb-engineering/visualizing-data-timeliness-at-airbnb-ee638fdf4710\nConfluent/ Pinterest: Lessons Learned from Running Apache Kafka at Scale at Pinterest\nPinterest writes its lessons learned from running Apache Kafka at scale. Broker replacement, partition rebalancing, and cost control are the common challenges running Kafka at scale, and the blog narrates how automation can help run the tasks. The Pinterest Orion is an exciting project to watch.\nhttps://www.confluent.io/blog/running-kafka-at-scale-at-pinterest/\nConfluent: 42 Things You Can Stop Doing Once ZooKeeper Is Gone from Apache Kafka\nConfluent writes about the advantages of removing the Zookeeper dependency can improve the Kafka infrastructure with performance, capacity planning, operations, and monitoring. The KIP-500 RFC on replacing Zookeeper with a self-managed quorum is an exciting read.\nhttps://www.confluent.io/blog/42-ways-zookeeper-removal-improves-kafka/\nLinkedIn: Solving the data integration variety problem at scale, with Gobblin\nThe growing niche SAAS applications add complexity to the data ingestions to the data warehouse system. LinkedIn writes about Apache Gobblin's unique approach to building data integration at scale. Instead of relying upon per source connectors, the multi-stage protocol & message format architecture seems an elegant solution for a complex problem.\nhttps://engineering.linkedin.com/blog/2021/data-integration-library\nFacebook: Mitigating the effects of silent data corruption at scale\nIn a large-scale infrastructure, files usually compressed when they are not being read and decompressed when a request to read the file. What happens when the decompression fails? How often the failure? Facebook writes an exciting blog about its paper. silent data corruption at scale.\nhttps://engineering.fb.com/2021/02/23/data-infrastructure/silent-data-corruption/\nReddit: Scaling Reporting at Reddit\nReddit writes about its journey on scaling the reporting platform from Redis to Apache Druid. The blog discusses the broader limitations of adopting the key-value storage for serving the analytics, the overhead on the application development, and operation issues with unknown bugs.\nhttps://redditblog.com/2021/02/26/scaling-reporting-at-reddit/\nLinkedIn: DataHub Project Updates (February 2021 Edition)\nOne of the challenges of adopting a modern data stack is that it is isolated towards dashboarding and reporting use cases. It is refreshing to read that the recent LinkedIn DataHub release focuses on adopting GraphQL to ease the integration with broader infrastructure components.\nhttps://medium.com/datahub-project/linkedin-datahub-project-updates-february-2021-edition-338d2c6021f0\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-30", "title": "Data Engineering Weekly", "content": "Welcome to the 30th edition of the data engineering newsletter. This week's release is a new set of articles that focus on Uber\u2019s schema-agnostic log analytics platform, Google\u2019s opensource model search system, Intuit\u2019s Data Mesh strategy, Salesforce\u2019s secure data intelligence platform, Netflix\u2019s composable data pipeline, BrightWind\u2019s wind analytical data hub, Apache Pinot\u2019s star tree indexing, Squarespace\u2019s A/B testing platform, Snowflake vs. Redshift comparison, and overview of the modern analytical stack. \nUber: Fast and Reliable Schema-Agnostic Log Analytics Platform\nElasticsearch provides a dynamic schema inference to improve the performance of log indexing. The dynamic type inference often leads to type conflict errors, which drops the offending errors. The indexing and operational cost spike up the cloud cost to maintain the log search engine. Uber writes about schema-agnostic data model log search build on top of ClickHouse. The type-specific schema design is an elegant design for a complicated need for a log search engine.\nhttps://eng.uber.com/logging/\nGoogle: Introducing Model Search: An Open Source Platform for Finding Optimal ML Models\nIn recent years, AutoML algorithms have emerged to help researchers find the right neural network automatically without the need for manual experimentation. To extend access to AutoML solutions, Google open-sourced Model Search,A platform that helps researchers develop the best ML models efficiently and automatically built on top of Tensorflow.\nhttps://ai.googleblog.com/2021/02/introducing-model-search-open-source.html\nIntuit: Intuit\u2019s Data Mesh Strategy\nThe Data Mesh principles based on Domain-Driven Design, which emphasizes ownership and accountability of singularly focused data as a product approach, resonates well with many large-scale data teams. Intuit writes an exciting blog about its learning, vision, and strategy to adopt data mesh principles.\nhttps://medium.com/intuit-engineering/intuits-data-mesh-strategy-778e3edaa017\nLinkedIn: LinkedIn Sales Insights: Quality data foundations for smarter sales planning\nData quality defines the success and failure of a data platform. \nLinkedIn writes about its Sales analytical platform focusing on how data quality is critical for its success. The blog narrates the importance of data quality as a key SLI and measuring data consistency, completeness, and freshness.\nhttps://engineering.linkedin.com/blog/2021/linkedin-sales-insights--quality-data-foundations-for-smarter-sa\nSalesforce: Building a Secured Data Intelligence Platform\nData Privacy and Security is a vital aspect of the data infrastructure. The security-driven infrastructure design is essential for a data infrastructure that handles the business's most sensitive information. Salesforce writes an exciting blog on data platform security design throughout the data lifecycle from encryption keys, In-transit encryption, authentication & access control, multi-tenancy, and third-party access.\nhttps://engineering.salesforce.com/building-a-secured-data-intelligence-platform-ba85411a0c1b\nNetflix: Netflix Data Mesh - Composable Data Processing\nNetflix talked about its composable data processing pipeline connecting various Netflix\u2019s contention production studios. The challenges around self-serve data processing and the complication of integrating index-based schema evolution system (Iceberg) and name-based schema evolution system (Avro) is an exciting talk to watch. \nPlease note the Data Mesh is an overloaded term here, and not to confuse with the Data Mesh principles :-)\nBrightHub: BrightWind\u2019s wind resource data hub\nModern internet companies have the luxury of running in a \"controlled data production environment\" to a large extend yet struggle to deal with data quality and accessibility. A data platform's challenge exponentially multiplies when the data produced either manually or non/ semi-connected devices. BrightWind, the wind & solar analytical system, writes about its experience building data infrastructure.\nPart 1: BrightWind\u2019s wind resource data hub\nPart 2: Handling difficult wind resource data\nPart 3: Ingesting daily data files\nPart 4: Using S3 for spiky time series data ingestion\nApache Pinot: Star-Tree Index: Space-Time Trade-Off in OLAP\nThe multi-model/ multi-index databases are an exciting phase to watch. The Apache Pinot's talk on CMU's vaccination series is an exciting talk about the internals of how Pinot effectively using the start tree indexing for providing predictable response time for latency-sensitive applications.\nSquarespace: How We Reimagined A/B Testing at Squarespace\nTools drive the process, and process drives the engineering culture. \nSquarespace writes an exciting read on how a unified experimentation framework drives fostering the culture of experimentation. The blog narrates the standardization on experimentation assignment, analytics, and statistical approach that drives the platform unification.\nhttps://engineering.squarespace.com/blog/2021/how-we-reimagined-ab-testing-at-squarespace\nGitConnected: Snowflake vs Redshift RA3 \u2014 The need for (more than just) speed\nThe separation of storage and compute becomes the defacto architectural pattern for MPP engines. Presto, Snowflake long adopted the pattern, and Redshift joined the club with RA3 clusters with a competitive pricing model. The blog is an excellent comparison of Snowflake vs. Redshift in terms of performance, usage cost, idle cost, scaling, workload isolation, and automated data masking.\nhttps://levelup.gitconnected.com/snowflake-vs-redshift-ra3-the-need-for-more-than-just-speed-52e954242715\nTechnically.dev: What Your Data Team Is Using: The Analytics Stack\nThe art and science of data infrastructure are all about stitching diverse yet related toolings to work together :-). A few seemingly small differences can cause big headaches when it comes to interoperability. The blog is an excellent summarization of various analytical toolings categorized by where data comes from, where data goes, how data moves around, how data gets ready, and how data gets used.\nhttps://technically.dev/posts/what-your-data-team-is-using\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-29", "title": "Data Engineering Weekly", "content": "Welcome to the 29th edition of the data engineering newsletter. This week's release is a new set of articles that focus on Google\u2019s research paper on Data Cascades in High-Stakes AI, Fiddler Labs debugging ML model performance, Monte Carlo\u2019s Data Observability Using SQL, Airbnb\u2019s Superset adoption, Apache Kylin\u2019s Evolution of Precomputation, Spotify\u2019s Sorted Merge Bucket implementation, Doordash\u2019s effective data science communication, Funding Societies Data Governance journey, QueryClick\u2019s Self-Serve analytical journey, and Databricks Delta Lake 0.8.\nGoogle: \"Everyone wants to do the model work, not the data work\" - Data Cascades in High-Stakes AI\nData quality has an enormous effect on the results and efficiency of AI. Data quality carries an elevated significance in high-stakes AI due to its heightened downstream impact, impacting predictions like cancer detection, wildlife poaching, and loan allocations. For instance, poor data practices reduced IBM\u2019s cancer treatment AI accuracy and led to Google Flu Trends missing the flu peak by 140%. \nWhat We Can Learn From the Epic Failure of Google Flu Trends\nGoogle research published a report on data practices in high-stakes AI from interviews with 53 AI practitioners in India, East and West African countries, and the USA. The paper captures the Data cascading effect causing adverse, downstream effects from data issues, resulting in negative social impact. \nOne of the disrupting read to know 92% of AI practitioners reported experiencing one or more, and 45.3% reported two or more cascades in a given project. I highly encourage the data engineers to read the report. I believe there is a potential social enterprise opportunity.\nhttps://research.google/pubs/pub49953/\nFiddler Labs: Debug Machine Learning model performance issue\nThe Twitter thread is an exciting read, where the author shared the experience working on Facebook's newsfeed ranking platform on debugging the machine learning model performance. The thread emphasized most Machine Learning model performance issues due to data pipeline issues and the importance of explainable AI.\nMonte Carlo: Data Observability in Practice Using SQL\nThe previous two articles talked about the importance of data quality and the impact of inadequate data pipeline observability. How can we establish the most simplistic data pipeline monitoring? The databases traditionally added constraints part of DDL to ensure integrity. The modern data pipeline requires much more options than simple constraints. Monto Carlo writes an exciting two-part blog narrating how one can use SQL to measure critical data pipeline reliability.\nhttps://www.montecarlodata.com/data-observability-in-practice-using-sql-1/\nhttps://towardsdatascience.com/data-observability-in-practice-using-sql-part-ii-schema-lineage-5ca6c8f4f56a\nAirbnb: Supercharging Apache Superset\nAirbnb writes about its Apache Superset adoption growth and performance improvement strategy. It's impressive to see Airbnb's data ecosystem now comprises more than 100,000 tables and virtual datasets backing over 200,000 charts and 14,000 dashboards. The predictive cache warm-up, domain sharding for high concurrency, and query rate-limiting are exciting to read on dashboard performance optimization strategies.\nhttps://medium.com/airbnb-engineering/supercharging-apache-superset-b1a2393278bd\nApache Kylin: The Evolution of Precomputation Technology and its Role in Data Analytics\nPrecomputation is a common technique used in information retrieval and analysis, including index, materialized view, OLAP cube, and more. The blog narrates the evolution of pre-computation, the future of pre-computation, and the role of AI & automation technology shaping the pre-computation. Airbnb applied a similar strategy in the previous article on supercharging Apache Superset.\nhttps://www.infoq.com/articles/evolution-precomputation-technology-data-analytics/\nSpotify: How Spotify Optimized the Largest Dataflow Job Ever for Wrapped 2020\nData skew and shuffle are the two curse of data processing. Spotify writes an exciting post on high impact usage of Sorted Merge Bucket (SMB) join to optimize its data pipeline. The SortedBucketSink, SortedBuketSource, and the filehandle iterator's usage remind me of Slack\u2019s batch search infrastructure implementation, and it is great to see a framework abstraction for SMB implementation.\nhttps://engineering.atspotify.com/2021/02/11/how-spotify-optimized-the-largest-dataflow-job-ever-for-wrapped-2020/\nDoordash: How to Drive Effective Data Science Communication with Cross-Functional Teams\nThe data analytics team's vital responsibility is to communicate actionable insights to key stakeholders, not just identify and measure them. Clear communication to the key stakeholders ensures clear strategic direction and actionable business insight. Doordash's analytical team writes an exciting post emphasize the need for an established communication framework and detail some of the best practices it follows.\nhttps://doordash.engineering/2021/02/11/how-to-drive-effective-data-science-communication/\nFunding Societies: Data governance journey at SEA\u2019s largest digital P2P lending platform\nComprehensive data governance and data management are essential for a financial system, not only for business growth but also for strict regulatory requirements. The Funding society writes an in-depth narration of its data governance journey from executive buy-in, define data governance policy, Data & access management policy, and data domain driven design.\nhttps://medium.com/fsmk-engineering/data-governance-journey-at-seas-largest-digital-p2p-lending-platform-part-1-7a7e8f07b7f\nhttps://medium.com/fsmk-engineering/data-governance-journey-at-seas-largest-digital-p2p-lending-platform-part-2-ebaa098b6acf\nQueryClick: Our (Bumpy) Road To Self Service Analytics | QueryClick\nSelf-Serving analytical infrastructure is a north star system design for any data infrastructure systems. It requires cultural and technological changes, which architecture should account for. On a similar line, QueryClick shares its self-serving analytics journey.\nhttps://medium.com/queryclick-tech-blog/queryclicks-bumpy-road-to-self-service-analytics-664a154de6a2\nDatabricks: Automatically Evolve Your Nested Column Schema, Stream From a Delta Table Version, and Check Your Constraints\nDatabricks writes about some of the key features released part of the Delta Lake 0.8 version. It's exciting to read some of the new features like the schema evolution support nested column schema with auto-merge support, support for constraints, and the ability to time travel delta stream from a specific version.\nhttps://databricks.com/blog/2021/02/10/automatically-evolve-your-nested-column-schema-stream-from-a-delta-table-version-and-check-your-constraints.html\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-28", "title": "Data Engineering Weekly", "content": "Welcome to the 28th edition of the data engineering newsletter. This week's release is a new set of articles that focus on Google\u2019s ML for computer architecture, Microsoft\u2019s PyTorch vs. TensorFlow, Capital One\u2019s Time travel offline ML evaluation frameworks, Alibaba Cloud\u2019s Data Lake introduction, PayPal\u2019s Next-Gen data movement framework, Apache Pinot\u2019s integration story with Presto, Gradient Flow\u2019s growing importance of Metadata, Metadata Day 2020 overview, Monte Carlo Data\u2019s data pipeline SLA, and TDD with Apache Airflow. \nGoogle: Machine Learning for Computer Architecture\nThe custom accelerators like Google TPU and Edge CPU significantly advanced the ML workloads. The hardware accelerator ecosystem must continue to innovate in architecture design and acclimate to rapidly evolving ML models and applications to sustain these advances. Google AI writes about blending ML into the\u00a0high-level\u00a0system specification and architectural design stage, a pivotal contributing factor to the chip's overall performance.\nhttps://ai.googleblog.com/2021/02/machine-learning-for-computer.html\nMicrosoft: A tale of two frameworks: PyTorch vs. TensorFlow\nTensorFlow and PyTorch are the two most popular Machine Learning framework. Microsft writes a comparison article that illustrates the differences between PyTorch and TensorFlow by focusing on creating and training two simple models, mainly how to use dynamic subclassed models with the Module API from PyTorch 1.x and the Module API from TensorFlow 2.x.\nhttps://medium.com/data-science-at-microsoft/a-tale-of-two-frameworks-pytorch-vs-tensorflow-f73a975e733d\nCapital One: Time Travel is Real-Building Offline Evaluation Frameworks\nIn a typical system design, multiple services act on an entity to change the state. For an offline ML model evaluation, Adding the temporal view of all the systems' data and processing to identify the state of customer interactions over time with time travel function is challenging. Capital One writes an exciting blog post discussing some of the challenges of building such a system with a high-level reference architecture.\nhttps://medium.com/capital-one-tech/time-travel-is-real-building-offline-evaluation-frameworks-a78103613ef9\nAlibaba Cloud: Data Lake: Concepts, Characteristics, Architecture, and Case Studies\nAlibaba Cloud writes an excellent overview about Data Lake. The blog is an exciting summary of what is a data lake? What are the characteristics of a data lake? The data architectural pattern differences between Lambda and Kappa architectures, Comparing the commercially available data lake solutions, and a case study from Huwai Data Lake system design.\nhttps://alibaba-cloud.medium.com/data-lake-concepts-characteristics-architecture-and-case-studies-28be1b265624\nPayPal: Next-Gen Data Movement Platform at PayPal\nData that moves is alive and valuable. At rest, data is dead. PayPal writes its journey to build the Next-Generation of Data Movement Platform. The design principles behind the PayPals\u00a0Risk\u00a0Analytical\u00a0Dynamic\u00a0Datasets(RAAD) pipeline build on top of Apache Gobblin and Apache Airflow is an exciting read about a self-serving unified data platform.\nhttps://medium.com/paypal-engineering/next-gen-data-movement-platform-at-paypal-100f70a7a6b\nApache Pinot: Real-time Analytics with Presto and Apache Pinot\nApache Pinot writes a two-part post about Pinot integration with Presto. The blog narrates various design choices, the trade-off between latency and flexibility, and discusses Pinot's aggregator pushdown implementation with significant performance improvement.\nhttps://medium.com/apache-pinot-developer-blog/real-time-analytics-with-presto-and-apache-pinot-part-i-cc672caea307\nhttps://medium.com/apache-pinot-developer-blog/real-time-analytics-with-presto-and-apache-pinot-part-ii-3d09ff937713\nGradient Flow: The Growing Importance of Metadata Management Systems\nMetadata management is the critical feature of data infrastructure. We've seen several technology companies developed internal metadata management systems and shared the challenges that led them to focus on metadata, including Airbnb's Data portal, Netflix's Metacat, Uber's Databook, LinkedIn's Datahub, Lyft's Amundsen, WeWork's Marquez, Spotify's Lexikon. Gradient Flow writes an exciting blog about the importance of metadata, the current architectural pattern for metadata management, and various vendors for the metadata landscape.\nhttps://gradientflow.com/the-growing-importance-of-metadata-management-systems/\nKnowledge Technologies: Review: Metadata Day 2020\nLinkedIn organized the metadata day 2020 last December as a general forum to discuss the current trend in metadata management. Data Engineering Weekly wrote the chronological order of metadata management systems by various companies. Continuing to echo the metadata day's impact, the author writes an exciting summary of the metadata day 2020.\nhttps://medium.com/knowledge-technologies/review-metadata-day-2020-e38c28c4cf1a\nData Engineering Weekly\u2019s Metadata Day Special Edition:\nhttps://www.dataengineeringweekly.com/p/data-engineering-weekly-21-metadata\nMonte Carlo Data: How to Make Your Data Pipelines More Reliable with SLAs\nSLA, SLO, SLI are widely used to measure the reliability of the services. Slack, for instance, provides customer credit if the SLA breaches below 99.99%. The author narrates how a data pipeline can successfully adopt a similar measure to improve the data reliability and minimize data downtime.\nhttps://towardsdatascience.com/how-to-make-your-data-pipelines-more-reliable-with-slas-b5eec928e906\nIn case you missed it, I gave a talk about operating data pipeline on Airflow @Slack a couple of years back contains some best practices on data pipeline reliability.\nMarcos Marx: How to develop data pipeline in Airflow through TDD (test-driven development)\nContinuous integration and testing are a vital part of improving the productivity of developing a data pipeline. One of Apache Airflow's critical attributes of success is writing and testing a data pipeline programmatically. The author writes an exciting blog walking through the steps to enable the Test-Driven-Development on data pipeline using Apache Airflow.\nhttps://blog.magrathealabs.com/how-to-develop-data-pipeline-in-airflow-through-tdd-test-driven-development-c3333439f358  \nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions.\n\n\n\n\n\n"}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-27", "title": "Data Engineering Weekly", "content": "Welcome to the 27th edition of the data engineering newsletter. This week's release is a new set of articles that focus on decentralized content moderation, Kafka as a database, Snowflake's External Table, Dagster 0.10.0, Uber's real-time data intelligence platform, Dropbox's Superset adoption, Cloudflare's data center operations using Airflow, Apache Kudi's clustering, Trainline's data lake.\nMartin Kleppmann: Decentralized content moderation\nJanuary 2021 is a happening month, brings a lot of debate over censorship and content moderation by social media. People gossip and spread misinformation over the centuries, but the impact is limited to a local context. Twitter's and Facebook created a Cerebro for misinformation. The author summarizes the need to rethink content moderation from a centralized, subjective moderation to democratic, decentralized content moderation. It is an exciting space to watch how data infrastructure can evolve to improve content moderation.\nhttps://martin.kleppmann.com/2021/01/13/decentralised-content-moderation.html\nFacebook\u2019s Fighting abuse @scale 2019 conference contains some exciting talks on the same.\nhttps://engineering.fb.com/2019/12/13/security/fighting-abuse-scale-2019/\nDavid Xiang: Kafka As A Database? Yes Or No\nApache Kafka plays a vital component in modern infrastructure. Is Kafka a database? It is a hot debate that shapes the future of streaming technology. The author summarizes the merits and demerits of treating Kafka as a database. One of Kafka's conventional arguments is that it supports the read/ write separation of concerns with write-once/ multi-model read pattern. Simultaneously, maintaining data integrity and multi-model materialization is not cheap and can further complicate the system design. Nonetheless, it is exciting to watch the evolution of streaming databases.\nhttps://davidxiang.com/2021/01/10/kafka-as-a-database/\nSnowflake: External Tables Are Now Generally Available On Snowflake\nThe cloud storage services like AWS S3, Azure Data Lake Storage, or Google Cloud are the popular choice for data lake systems. Snowflake, the famous cloud data warehouse, introduced external tables that enable Snowflake to query cloud data storage. Snowflake also supports streaming ingestion for the external datasets similar to Apache Hudi & Delta Lake. Presto played the federated query engine role to unify querying data lake and cloud data warehouse systems, and it is a significant development from Snowflake to provide the native implementation.\nhttps://www.snowflake.com/blog/external-tables-are-now-generally-available-on-snowflake/\nDagster: Dagster 0.10.0: The Edge of Glory\nDagster released version 0.10.0, codenamed \"The Edge of Glory.\" It's exciting to see Dagster's focus on native scheduler instead of relying on the cron or Kubernetes, supporting the sensors, tight integration with Kubernetes, and I/O manager abstraction to simplify the dev & testing phase of the pipeline development.\nhttps://dagster.io/blog/dagster-0-10-0-the-edge-of-glory\nUber: Uber\u2019s Real-time Data Intelligence Platform At Scale: Improving Gairos Scalability/Reliability\nUber writes about Gairos, its real-time data processing, storage, and querying platform to facilitate streamlined and efficient data exploration at scale. The total size of queryable data served by Gairos is 1,500+TB, and the number of production pipelines is over 30. The total number of records is more than 4.5 trillion, and the total number of clusters is over 20. Over 1 million events flow into Gairos every second. The Gairos Optimization Engine is an exciting implementation to self-tune Elasticsearch & ingestion pipeline.\nhttps://eng.uber.com/gairos-scalability/\nDropbox: Why we chose Apache Superset as our data exploration platform\nApache Superset is now the top-level Apache project. Dropbox writes about why it chooses Apache Superset over competitive visualization frameworks like redash, mode & periscope.\nhttps://dropbox.tech/application/why-we-chose-apache-superset-as-our-data-exploration-platform\nCloudflare: Automating data center expansions with Airflow\nThe infrastructure operations and maintenance tasks are often scheduled as a cron job. However, cron has its limitation, and the orchestration engines like Airflow provides much more efficient scheduler for non-time-sensitive tasks. Cloudflare writes an excellent blog on the same of using Apache Airflow for data center operations.\nhttps://blog.cloudflare.com/automating-data-center-expansions-with-airflow/\nApache Hudi: Optimize Data Lake layout using Clustering in Apache Hudi\nThe small file is a classic problem in data infrastructure and inherent impact on query performance. Apache Hudi introduced a pluggable clustering architecture to handle the small files and colocated related data to improve query efficiency.\nhttps://medium.com/apache-hudi-blogs/optimize-data-lake-layout-using-clustering-in-apache-hudi-9fe151999c69\nTrainline: Building a data lake: from batch to real-time using Kafka\nTimeline writes about its data pipeline evolution. It's exciting to see a similar data ingestion maturity model from API integration to batch processing to real-time data ingestion systems.\nhttps://engineering.thetrainline.com/building-a-data-lake-from-batch-to-real-time-using-kafka-67272041b124\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-26", "title": "Data Engineering Weekly", "content": "Welcome to the 26th edition of the data engineering newsletter. This week's release is a new set of articles that focus on Unusual Venture's data lineage forum, Uber's metrics standardization journey, Adobe's Apache Iceberg usage, Databricks talk on Lakehouse, Pinterest's realtime search engine, Intuit's take on the data lake, Microsoft's take on cost management, and Grab's realtime workflow engine.\nUnusual Ventures: Unusual Roundtable Takeaways: Data Lineage and its Role in Data Unification\nThe Unusual Ventures writes an excellent summary of data lineage and how essential the lineage democratize the data. I had a great time with the folks at Unusual Ventures talking about the data lineage and am excited about its growth.\nhttps://www.unusual.vc/post/unusual-roundtable-takeaways-data-lineage-and-its-role-in-data-unification\nUber: The Journey Towards Metric Standardization\nHow many of us in a meeting where multiple versions of unique users mentioned as business metrics?!! Uber wrote an exciting blog discussing the consequence of data democratization and the importance of metrics standardization.  uMetric, Uber's internal unified metric platform that powers the full lifecycle of a metric from the definition, discovery, planning, computation, and quality to consumers, is an excellent case study for balancing standards and productivity.\nhttps://eng.uber.com/umetric/\nAdobe: Taking Query Optimizations to the Next Level with Iceberg\nData Inconsistency, scalability issues with the metadata stores, and inefficient data access where partition pruning can give only minimal optimization are some of the significant pain points Apache Iceberg designed to fix. Adobe writes an exciting blog about optimization for Iceberg, discussing vectorized reading, Nested schema pruning & predicate pushdown, Manifest tooling, and snapshot expiration.\nhttps://medium.com/adobetech/taking-query-optimizations-to-the-next-level-with-iceberg-6c968b83cd6f\nDatabricks@CIDR DB 2021 Conf: Lakehouse: A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics\nDatabricks presented the emerging lakehouse platforms at the CIDR conference. The talk is an excellent summarization of lakehouse platforms like Databricks Delta, Apache Iceberg, Apache Hudi, and Hive ACID. The Data Engineering Weekly\u2019s one prediction is, \nThe Lakehouse platforms will define the next generation data architecture. \nIt is great to see Adobe\u2019s case study and Databricks CIDR talk on the same.\nPaper: http://cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf\nPinterest: Manas Realtime \u2014 Enabling changes to be searchable in a blink of an eye\nThe flush interval is one of the most significant pain points while using Lucene based search engines. The constant tuning it requires to balance the latency and throughput for the real-time indexing requires significant benchmarking. Pinterest writes about Manas, an internal search infrastructure, and balances the batch indexing and the real-time indexing. It's great to see emerging designs coming up in the search infrastructure, which is often underserved compares to the OLTP engine developments.\nhttps://medium.com/pinterest-engineering/manas-realtime-enabling-changes-to-be-searchable-in-a-blink-of-an-eye-36acc3506843\nIntuit: Is Your Data Lake More like a Used Book Store or a Public Library\nThe data catalog and the data discovery is the differentiator between productive and efficient data infrastructure vs. confused and inefficient data infra. Intuit writes an excellent analogy of used book stores and the public library to demonstrate the efficient data infrastructure strategy with data catalog systems.\nThe Data Engineering Weekly used a similar analogy to demonstrate the importance of metadata management systems. It's great to see some exciting, successful case studies emerging.\nhttps://medium.com/intuit-engineering/is-your-data-lake-more-like-a-used-book-store-or-a-public-library-f444ef6a1798\nData Engineering Weekly\u2019s discussion about Data Mesh\nData Mesh Simplified: A Reflection Of My Thoughts On Data Mesh\nMicrosoft: Your analytics platform has gone rogue, Part 1: Unforeseen costs\nThe modern data processing engines can process petabytes of data in parallel, and the cloud platforms enable the ease of scalability. It also brings the challenge of infrastructure cost management as a core engineering skill. Microsoft writes about uncontrolled costs and strategies to account while architecting and managing the data at scale.\nhttps://medium.com/data-science-at-microsoft/your-analytics-platform-gone-rogue-part-1-unforeseen-costs-b9dd437ff53\nGrab: Trident - Real-time event processing at scale\nGrab writes about Trident, it's in-house real-time IFTTT engine, which processes events and operates business mechanisms on a massive scale. The blog is an exciting read on workload distribution, datastore scaling, and query optimization strategy\nhttps://engineering.grab.com/trident-real-time-event-processing-at-scale\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions.\n"}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-25", "title": "Data Engineering Weekly", "content": "Welcome to the 25th edition of the data engineering newsletter. This week's release is a new set of articles that focus on Kleiner Perkins's future of computing and data infrastructure, LinkedIn's fast ingestion with Gobblin, Intuit's data journey, AWS's PyDeequ, Alibaba Cloud's Flink infra with 4 billion events per sec, Expedia's ML deployment pattern, Delta lake vs. Hudi, handling late-arriving dimensions, entity resolution for big data, Airflow 2.0 and Debezium year-in-review 2020.\nKleiner Perkins: Looking ahead to the future of computing and data infrastructure\nKleiner Perkins writes an excellent blog about the future of computing and data infrastructure. The cloud data warehouse, serverless architecture, workflow as a (No)code movement, and the lack of an end-to-end solution to optimize the ML infrastructure value chain are some of the exciting trends to watch. The author's take on data security enforces the importance of a metadata management system. \nThe (data/ security) breaches showing up in the news on a near-weekly basis all seem to be rooted in the same problem - a lack of awareness of what data an organization has, where it is, and who has access to it.\nhttps://www.kleinerperkins.com/perspectives/a-2020-perspective/\nYou can read Data Engineering Weekly\u2019s take on data infrastructure trends here.\nhttps://www.dataengineeringweekly.com/p/back-to-the-future-data-engineering\nLinkedIn: FastIngest: Low-latency Gobblin with Apache Iceberg and ORC format\nLinkedIn writes about the evolution of Apache Gobblin from a batch ingestion framework to a fast ingestion framework minimizing the ingestion latency from 45 minutes to less than 5 minutes. The blog narrates how Gobblin uses Apache Iceberg to guarantee read/ write isolation, the tradeoff with ORC format encoding, and continuous data publishing is an exciting read. Yarn's choice for resource management and scheduling is interesting, and looking forward to reading more on how the Gobblin replanner evolves from stop-the-world rebalance to dynamic rebalance.\nhttps://engineering.linkedin.com/blog/2021/fastingest-low-latency-gobblin\nIntuit: The Intuit Data Journey\nClean migration is a sign of effective engineering, and Intuit writes one such clean migration of its data infrastructure from on-premise to the cloud-native. The blog emphasizes data infrastructure fundamentals, such as to treat data as a product, and focuses on data quality, availability, performance, security & cost-effectiveness. It's exciting to read the challenges ahead of Intuit's data platform and the focus on the data mesh approach.\nhttps://medium.com/intuit-engineering/the-intuit-data-journey-d50e644ed279\nAWS: Testing data quality at scale with PyDeequ\nAWS introduced Deequ, a data quality library, in early 2019. Deequ is used internally at Amazon to verify the quality of many large production datasets. Dataset producers can add and edit data quality constraints. The system computes data quality metrics regularly (with every new version of a dataset), verifies constraints defined by dataset producers, and publishes datasets to consumers in case of success.\nAs an evolution of Deequ AWS open source PyDeequ, a python wrapper on top of Deequ can be integrated with PySpark to define and run the test cases.\nhttps://aws.amazon.com/blogs/big-data/testing-data-quality-at-scale-with-pydeequ/\nAlibaba Cloud: Four Billion Records per Second! What is Behind Alibaba Double 11 \u2014 Flink Stream-Batch Unification Practice during Double 11 for the Very First Time\nAlibaba writes a great success story on Apache Flink's scalability and the effectiveness of stream-batch unification. During the last Double 11 Global Shopping Festival, Apache flink pipeline processed an impressive four billion records per second. The data volume also reached an incredible seven TB per second. The Flink-based stream-batch unification has successfully withstood strict tests in terms of stability, performance, and efficiency in Alibaba's core data service scenarios. This article shares the practice experience and reviews the evolvement of stream and batch unification within Alibaba's core data services.\nhttps://alibaba-cloud.medium.com/four-billion-records-per-second-f8eeabce934d\nExpedia: Accelerate Machine Learning with the Optimal Deployment Pattern\nExpedia writes about ML model deployment patterns, narrating some of the significant challenges operating the ML model in production and how they differ from the traditional back-end systems. The blog is an exciting read about various deployment patterns and each deployment model's pros & cons.\nhttps://medium.com/expedia-group-tech/accelerate-machine-learning-with-the-optimal-deployment-pattern-501ddb9dc7ad\nLu Jiaqi: The ACID table storage layer- thorough conceptual comparisons between Delta Lake and Apache Hudi\nThe support for ACID on top of the object storage is a significant development in 2020. The blog narrates the data lake approach's drawback and compares the ACID support between Databricks Delta Lake and Apache Hudi.\nhttps://h164654156465.medium.com/the-acid-table-storage-layer-thorough-conceptual-comparisons-between-delta-lake-and-apache-hudi-3fd124e0bb0\nDatabricks: Handling Late Arriving Dimensions Using a Reconciliation Pattern\nProcessing facts and dimensions are the core of data engineering. In a typical event sourcing, the producer publishes the facts and dimensions in different streams. The blog narrates some of the design challenges with the late-arriving dimensions, especially with the fast/rapidly changing dimensions(RCD), and how reconciliation pattern helps to solve it.\nhttps://databricks.com/blog/2020/12/15/handling-late-arriving-dimensions-using-a-reconciliation-pattern.html\nACM Computing Survey/ The morning paper: An overview of end-to-end entity resolution for big data\nOne of the most critical tasks for improving data quality and increasing data analytics's reliability is Entity Resolution (ER), aiming to identify different descriptions that refer to the same real-world entity. The paper narrates an end-to-end view of ER workflows for Big Data, critically reviews the pros and cons of existing methods, and concludes with the leading open research directions. \nhttps://dl.acm.org/doi/abs/10.1145/3418896\nThe morning paper provides excellent summarization of the paper.\nhttps://blog.acolyer.org/2020/12/14/entity-resolution/\nDataband: Airflow 2.0 and Why We Are Excited at Databand\nAirflow version 2.0 is a significant milestone release for the Airflow community. Databand shares a similar excitement narrates two significant features released with Airflow 2.0; the Decorator Flows & Scheduler performance.\nhttps://medium.com/databand-ai/airflow-2-0-and-why-we-are-excited-at-databand-b26605a3b9f4\nDebezium: Debezium in 2020 -- The Recap!\nDebezium, the defacto open-source distributed platform for change data capture, publishes the year-in-review 2020. The blog post contains a rich consolidation of some exciting articles about the adoption of Debezium.\nhttps://debezium.io/blog/2021/01/06/debezium-2020-recap/\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-24", "title": "Data Engineering Weekly", "content": "Welcome to the 24th edition of the data engineering newsletter. This week's release is a new set of articles that focus on Netflix's data warehouse storage optimization, Adobe's high throughput ingestion with Iceberg, Uber's Kafka disaster recovery, complexity & consideration for Real-time infrastructure, Allegro's marketing data infrastructure, Apache Pinot & ClickHouse year-in-review, Confluera's Apache Pinot adoption, BuzzFeed's data infrastructure with BigQuery, Apache Beam's data frame API & PrestoSQL is now Trino.\nNetflix: Optimizing data warehouse storage\nNetflix data warehouse contains hundreds of Petabytes of data stored in AWS S3, and it is growing every day. Optimizing S3 storage layout optimizations can yield faster query time, cheaper downstream processing, and increased developer productivity. Netflix writes about AutoOptimize that automates storage optimization techniques like merge, sort, and compaction.\nhttps://netflixtechblog.com/optimizing-data-warehouse-storage-7b94a48fdcbe\nAdobe: High Throughput Ingestion with Iceberg\nAdobe's Experience platform writes the second part of its adoption of Apache Iceberg. The Experience platform API supports direct ingestion of batch files from the clients. Though it improves their clients' efficiency, it brings the classic small files and high-concurrent write problem. The blog is an exciting read about the buffered write approach to mitigate small files and high-concurrent writing problems and improvement done on Apache Iceberg version-hint.txt file.\nhttps://medium.com/adobetech/high-throughput-ingestion-with-iceberg-ccf7877a413f\nIceberg at Adobe: https://medium.com/adobetech/iceberg-at-adobe-88cf1950e866\nUber: Disaster Recovery for Multi-Region Kafka at Uber\nUber has one of the largest deployments of Apache Kafka in the world, processing trillions of messages and multiple petabytes of data per day. In this blog, Uber writes about the architecture behind the disaster recovery for multi-region Kafka. Uber adopted an active-active cluster pattern where the producer publishes messages locally to regional clusters. Then the messages from regional clusters are replicated to Aggregate Clusters to provide a global view. The active-active and the active-passive event consumption pattern on the consumer side is an exciting read.\nhttps://eng.uber.com/kafka/\nHacking Analytics: Real-time Data Pipelines \u2014 Complexities & Considerations\nThe real-time infrastructure reduces the latency to analytical insight and makes the business process more agile. Simultaneously, the real-time infrastructure brings multiple other challenges from data ingestion, data processing, storage, and the serving of real-time insights. The article is an excellent read that summarizes the overall landscape of the real-time infrastructure components.\nhttps://medium.com/analytics-and-data/real-time-data-pipelines-complexities-considerations-eecad520b70b\nAllegro: Big data marketing. The story of how the technology behind Allegro marketing works.\nAllegro writes about its data infrastructure behind the marketing feeds that integrates with the Google Merchant Center and Facebook ads. The journey from one Spark job per feed to the second generation feed generation framework & self-healing system is an exciting read.\nhttps://allegro.tech/2020/12/bigdata-marketing.html\nApache Pinot: year-in-review 2020 and the roadmap for 2021\n2020 is an exciting year for the Apache Pinot community with some excellent query optimization, support for JSON querying, and upsert support.\nApache Pinot community published year-in-review 2020 and the roadmap for 2021.\nConfluera: Real-time Security Insights: Apache Pinot at Confluera\nStaying with Apache Pinot, Confluera, a threat-detection and response platform, writes about Apache Pinot's adoption. The blog published a primary benchmark comparing the Apache Druid and Apache Pinot that shows Pinot performance gain with the aggregator queries.\nhttps://medium.com/confluera-engineering/real-time-security-insights-apache-pinot-at-confluera-a6e5f401ff02\nBuzzFeed: More Data, More Problems\nBuzzFeed writes about the migration to Google BigQuery and how it scales the data operations. One of the challenges with BigQuery calculates the number of slots required by each query based on its complexity and amount of data scanned. Inefficient or large queries will take a longer time to execute and potentially block or slow other concurrent queries due to the number of slots it requires. BuzzFeed writes about how the adoption of data modeling techniques and materialization helped to optimize the queries.\nhttps://tech.buzzfeed.com/more-data-more-problems-3c585b8bc84d\nApache Beam: DataFrame API Preview now Available!\nApache Beam introduced the Beam SQL in early 2019. On that journey to simplifying the API with more familiar patterns, Apache Beam introduced DataFrame API aims to compatible with the well-known Pandas DataFrame API.\nhttps://beam.apache.org/blog/dataframe-api-preview-available/\nClickHouse: The ClickHouse Community in 2020\nClickHouse published year-in-review 2020, with the origin story from Yandex, community overview, how to contribute, and the adoption story from Cloudflare.\nhttps://clickhouse.tech/blog/en/2020/the-clickhouse-community/\nTrino: We\u2019re rebranding PrestoSQL as Trino\nPrestoSQL rebrands as Trino after Facebook's copyright claim with PrestoDB. Trino published the commit trends comparing Trino and PrestoDB. It's sad to see one of the vital opensource in the modern data infrastructure splits in mindshare, and I hope both Trino and PrestoDB complement each other.\nhttps://trino.io/blog/2020/12/27/announcing-trino.html\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions.\n"}, {"url": "https://www.dataengineeringweekly.com/p/back-to-the-future-data-engineering", "title": "Data Engineering Weekly", "content": "Welcome to the 23rd edition of data engineering weekly. This week's edition is a yearend special edition where we will take a more in-depth look at the trends and emerging patterns in data engineering 2020. I divided the trends into the following categories.\nData Infrastructure\nData Architecture\nData Management\nI hope you enjoy the data engineering trends 2020, and please share your thoughts in the comments.\nI know this is a long article with so many links and references. TL;DR If I had to make a top 3 prediction for 2021 and beyond, here are they.\nMetadata management will become mainstream. The data lineage, data quality, and data discovery tools will merge into a unified data management platform.\nData Mesh principles will get adopted more and drive a unified data management platform.\nLakehouse systems like Hudi, Iceberg, Deltalake will play a significant role in shaping the data engineering architecture.\nNow, let\u2019s go deep into each category and see the trends and predictions. Happy reading.\nIn 2020, We saw the cloud platforms continue adopting the open-source data infrastructure solutions\u2014the adoption growing from AWS's EMR, Google Cloud data proc, and Azure HDInsight to the recent AWS managed Airflow.\nIntroducing Amazon Managed Workflows for Apache Airflow (MWAA)\nThough opinions differ on cloud platforms packaging the opensource, the cloud-managed infrastructure certainly carry many advantages for the consumers to quickly adopt complex infrastructure and focus on the business problems.\nThe rise of serverless architecture particularly very interesting trend in data engineering. The article summarizes the serverless data ops trend\nDawn of DataOps: Can We Build a 100% Serverless ETL Following CI/CD Principles?\nGoogle Cloud launched a Google Cloud Workflow as a serverless orchestration engine.\nGet to know Workflows, Google Cloud\u2019s serverless orchestration engine\nIn 2021, It is an exciting space to watch how managed data infrastructure and the rise of serverless computing merge. \nAt the beginning of 2010, tightly coupled computing and storage is a strategy to run large scale data processing engines. 2019 is when the industry finally declared the old way of thinking data processing no longer working and acknowledge the cloud datawarehouse system is the way to go.\nHadoop is Dead. Long live Hadoop.\nIn 2020, Snowflake's successful IPO reassured the cloud datawarehouse systems are the future. The S3 strong read-after-write consistency guarantee is a significant step in adopting object storage for the cloud datawarehouse system, if not already.\nAmazon S3 Update \u2013 Strong Read-After-Write Consistency\nThe cloud datawarehouse system will continue to dominate and increase the adoption in 2021 and beyond. It will be interesting to watch how the cloud datawarehouse systems are tightly integrating with the data management systems.\nThe cloud datawarehouse systems and the managed data infrastructure adds pressure on optimizing the cost of operating the datawarehouse systems. Netflix writes about cost optimization strategies for its data warehouse system.\nByte Down: Making Netflix\u2019s Data Infrastructure Cost-Effective\nAt the same time, the GPU accelerated workload can provide a strategic business advantage. Pinterest and NVIDIA shared how Pinterest using GPU acceleration for visual search.\nPinterest Trains Visual Search Faster with Optimized Architecture on NVIDIA GPUs \nI added cost optimization as a separate section since cost optimization is often an afterthought. The unpredictability of the object storage engines egress and storage cost, handling cold vs. hot data & the need for specialized hardware for a specific workload will be the norm of 2021 and beyond. \nAlluxio is one solution that I am aware of providing tiered data processing capabilities, though not tuned for cost optimization.\nAccelerate Spark and Hive Jobs on AWS S3 by 10x with Alluxio as a Tiered Storage Solution\nIt will be interesting to see how data processing frameworks like Spark, Flink adopting cost optimization as the first class optimization model, cache frequently used datasets and aware of specialized workloads. \nThe separation of computing and storage and the scalable object storage like S3 increased the adoption of data lake principles in early 2019. One of the challenges remains to adopt object storage on the lack of transaction guarantees. The support for ACID transactions, data versioning, auditing, indexing, caching, and query optimization are vital characteristics to build large scale data systems. \nIn 2020, We noticed the emerging lakehouse frameworks like DataBricks Delta Lake, Apache Hudi, and Apache Iceberg.\nLakehouse: A New Generation of Open Platforms that Unify Data Warehousing and Advanced Analytics\nAdobe shared its Iceberg adoption story Iceberg at Adobe\nUber writes about its journey with Apache Hudi, and EMR now offers Hudi part of EMR\nBuilding a Large-scale Transactional Data Lake at Uber Using Apache Hudi\nApply record level changes from relational databases to Amazon S3 data lake using Apache Hudi on Amazon EMR and AWS Database Migration Service\nIcerbeg\u2019s version 2 to support row level upsert is another interesting development to watch in 2021.\nThe Lakehouse systems continue to mature and will play a major role in shaping the data engineering architecture. It will be interesting to watch how lakehouse complement or compete with the likes of Snowflake and Redshift.\nManaging the real-time and batch computing and providing one integrated dataset view remains the main challenge in data processing.\nPinterest writes about some of the complication of Lambda Architecture and its migration journey to the Kappa architecture\nPinterest Visual Signals Infrastructure: Evolution from Lambda to Kappa Architecture\nLinkedIn took an interesting approach of Lambda-Less model\nFrom Lambda to Lambda-less: Lessons learned\nThere is no real-time vs. batch, it is all about the window that we process, but that is easier to say than the reality. In 2021 and beyond, I hope we will have a more innovative solution in this space.\nApache Beam is an excellent attempt to bring the model closer. The development of Spark Streaming and the recent Apache Flink\u2019s\u00a0batch computing support\u00a0are some of the trends to watch.\nReal-time computing and insights are critical for many businesses. Event sourcing is a well-established design pattern, and that brings the question of the decade. Can we join streams and compute business metrics or feed everything into OLAP databases and query it?\nConfluent writes about the KSQL materialization process.\nHow Real-Time Materialized Views Work with ksqlDB, Animated\nMaterialize writes about joins in detail\nJoins in Materialize\nOn the OLAP engine space Druid, Click House and Pinot adding multiple OLAP features and improves the operational efficiency. Apache Pinot is an impressive OLAP engine gaining momentum in 2020. Uber shared its experience operating Apache Pinot at scale.\nOperating Apache Pinot @ Uber Scale\nThough streaming SQL engines and OLAP engines solve similar problems, I think there is a fundamental difference. Streaming SQL engines are good for pre-defined analytics, write once, and run workloads continuously. OLAP engines are good for interactive analytics when analytical queries are unknown while building the datasets.\nIn 2021 and beyond I expect tighter integration among the Streaming SQL like KSQL and OLAP engines like Pinot.\nThe poor data quality costs an estimated $3.1 trillion per year in the USA alone, equating to 16.5% of the GDP.!! The data quality is critical for developing a data pipeline, and your ML model is as efficient as the quality of the data.\nWhy data quality is key to successful ML Ops\nWe\u2019ve seen both Microsoft and Airbnb writes about how data quality effort improved its org decision-making process.\nPartnering for data quality\nData Quality at Airbnb - Part 1 \u2014 Rebuilding at Scale\nData Quality at Airbnb - Part 2 \u2014 A New Gold Standard\nWe have seen multiple tools and systems emerged on Data Quality, and this is a pretty good summarization f the data quality ecosystem.\nData Quality \u2014 A Primer\n One of the most remarkable trends of 2020 in data engineering is the emerging tooling and infrastructure to manage metadata at scale. I shared some of my thoughts about the importance of metadata in the past.\nIn 2020, we have seen many great articles from companies across the industry that shared their data discovery and metadata management. Data Engineering Weekly dedicated a week\u2019s edition to focus on metadata management.\nData Engineering Weekly #21: Metadata Edition\nLinkedIn organized\u00a0Metadata Day 2020 - Metaspeak Meetup\u00a0as an attempt to unify people working on metadata management. Datakin announced the\u00a0Open Lineage initiative\u00a0to standardize the data lineage and the discovery effects.\nI\u2019ve included the data quality and the metadata management in the same section for a reason. In 2020 we saw isolated solutions to solve data lineage, data quality, and data discovery. Data Pipeline is a complex inter-dependent creation process of one dataset from another. Data lineage and data quality are two tightly coupled metadata systems that power the data discovery system.\nIn 2021 and beyond, I expect all three problem spaces to merge and emerge as one unified data management platform that can provide data quality, lineage, and discovery service out of the box.\nIn 2020, Data Mesh emerged as de-facto principles for scale data management as the organization grows. Thoughtworks writes about the data mesh principles in the past.\nHow to Move Beyond a Monolithic Data Lake to a Distributed Data Mesh\nData Mesh Principles and Logical Architecture \nWe saw number of companies started to adopt the data mesh principles and wrote about it.\nData Mesh in Practice - How Europe\u2019s Leading Online Platform For Fashion Goes Beyond Data Lake\nData Mesh @ Yelp - 2019\nIs Data Mesh principle to all levels of organization? I wrote a simplified explanation for the data mesh.\nData Mesh Simplified: A Reflection Of My Thoughts On Data Mesh\nThe Data Domain Ownership narrated in the Data Mesh principle the scalable approach for data management at scale. \nIn 2021 we will see accelerated adoption of the data mesh principles, and it will further push the vision of one integrated data management system.\nI added DBT as a trend on its own. Still, the fundamental pattern behind the success of DBT is that the industry comes to appreciate and embrace SQL as the best data abstraction for most of the data engineering workload. The success of DBT is also primarily driven by the success of the cloud datawarehouse systems and the emerging data lake 3.0 systems.\nI tweeted sometime back the significant advantage of DBT as a data processing orchestrator.\nOn the same line here are some of the articles shares their experience with DBT.\nWhy is dbt so important?\nMaking your dbt models more useful with Census\nUnderstanding the scopes of dbt tags\nHow to Build a Production Grade Workflow with SQL Modelling\nIn 2021, I expect the trends to continue, and we will see the likes of Databricks, AWS launch their version of DBT or adopt it. The general purpose data orchestration engines like Airflow, Dagster, and Prefect already integrated well with DBT.\nDagster and dbt: Better Together\nIt will be interesting to see if the general-purpose orchestration engines come with their DBT version. You may not need Airflow\u2026. yet shows how to build the data pipeline without Airflow, and Building a Scalable Analytics Architecture with Airflow and dbt makes me think is it worth to go through all the hacks to make it work.\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-22", "title": "Data Engineering Weekly", "content": "Welcome to the 22nd edition of the data engineering newsletter. This week's release is a new set of articles that focus on Datakin\u2019s OpenLineage, LinkedIn\u2019s metadata day, Microsoft\u2019s metadata management, the dead of data catalog, Alibaba\u2019s real-time data warehouse, Uber\u2019s no-code workflow, Line\u2019s self-serving compute, Slack\u2019s react logging lib, LinkedIn\u2019s Corel, Netflix\u2019s ML for content decision making, and Intuit\u2019s ML platform. \nDatakin: Introducing OpenLineage\n2020 is the year we have seen the rise of metadata management. You can read about the chronological order of data management development here. Building on the momentum and unifying the data lineage effort, Datakin and other leading opensource data lineage and the orchestration services Airflow, Amundsen, Datahub, dbt, Egeria, Great Expectations, Iceberg, Marquez, Pandas, Parquet, Prefect, Spark, and Superset announce open lineage initiative.\nhttps://datakin.com/2020/12/18/introducing-openlineage/\nSlides: https://www2.slideshare.net/julienledem/open-core-summit-observability-for-data-pipelines-with-openlineage\nLinkedIn: Metadata Day 2020 - Metaspeak Meetup\nLinkedin organized the Metadata Day 2020 on Dec-14th. The meetup video is now available on YouTube.\nhttps://www.youtube.com/channel/UCDoVCT4j6QmKCnNmmNoWtBw\nMicrosoft: Partnering for metadata management\nMetadata management\u00a0is concerned with information that is\u00a0not\u00a0the data itself, but rather is\u00a0about\u00a0the data. Microsoft\u2019s Azure data science team narrates its metadata management journey from an internal Azure Knowledge Graph to the adoption of Azure Purview.\nhttps://medium.com/data-science-at-microsoft/partnering-for-metadata-management-277733911d03\nMonte Carlo: Data Catalogs Are Dead; Long Live Data Discovery\nOrganizations in the past have relied on data catalogs to power data governance. But is that enough? Knowing where your data lives and who has access to it is fundamental to understanding its impact on your business. The article is an exciting read on where the data catalog fails and the need for adopting data discovery services.\nhttps://towardsdatascience.com/data-catalogs-are-dead-long-live-data-discovery-a0dc8d02bd34\nAlibaba Cloud: Evolution of the Real-time Data Warehouses of the Alibaba Search and Recommendation Data Platform\nAlibaba Search and Recommendation Data Warehouse Platform writes about its real-time data warehouse architecture that supports multiple e-commerce businesses, such as Taobao (Alibaba Group), Taobao Special Edition (Taobao C2M), and Eleme. The blog is an exciting read about the journey of real-time infrastructure, some of the shortfalls of Apache HBase, and the adoption of homegrown Hologres.\nhttps://alibaba-cloud.medium.com/evolution-of-the-real-time-data-warehouses-of-the-alibaba-search-and-recommendation-data-platform-fdb5292a01e2 \nAlibaba Hologres Paper:  https://kai-zeng.github.io/papers/hologres.pdf\nUber: No Code Workflow Orchestrator for Building Batch & Streaming Pipelines at Scale\nApache Airflow reimagines programmatically to orchestrate the data pipeline. The commoditization of computing and storage made the organizations to adopt data at all levels of the business. It also brings challenges on how to empower everyone in the organization to create the data pipeline. Uber writes an exciting blog on how the team got inspired by the No Code systems builds uWorc, a simple drag and drop interface that can manage the entire life cycle of a batch or streaming pipeline without writing a single line of code.\nhttps://eng.uber.com/no-code-workflow-orchestrator/\nLine: Introducing Frey: LINE\u2019s new self-service batch ingestion system\nContinuing on the self-service data processing systems trend, Line writes about its self-serving batch ingestion service Frey. Frey integrated with Airflow and provided a UI interface for the users to eliminate the learning curve. Once a user's job is created and deployed, the users can get all the information such as execution status and logs and perform operations such as backfill and rerun.\nhttps://engineering.linecorp.com/en/blog/introducing-frey-lines-new-self-service-batch-ingestion-system/\nSlack: Creating a React Analytics Logging Library\nThe domain events instrumentation is the most critical part of building data products. The instrumentation often manual and impacts the developer productivity. Slack writes an excellent blog on how it built the client-side react logging library and improved developer productivity.\nhttps://slack.engineering/creating-a-react-analytics-logging-library-2/\nLinkedIn: Coral: A SQL translation, analysis, and rewrite engine for modern data lakehouses\nThe Big Data computation infrastructure is continuously evolving. The industry came a long way from Map Reduce to Hive, Pig, Spark, and Presto. The evolution also brings interoperability issues among the computation frameworks. LinkedIn developed Dali Catalog to abstract the interoperability complexity and provided a unified data view. LinkedIn writes about Corel, its open-source SQL translation, analysis, and rewrite engine that integrates with Dali and enables Dali view portability across execution engines like Presto, Spark, and Pig.\nhttps://engineering.linkedin.com/blog/2020/coral\nNetflix: Supporting content decision makers with machine learning\nNetflix is pioneering content creation at an unprecedented scale. The commissioning of a series or film is a creative decision. How to use ML to predict and support the creative process? In this post, Netflix writes about how machine learning and statistical modeling can help creative decision-makers tackle these questions on a global scale.\nhttps://netflixtechblog.com/supporting-content-decision-makers-with-machine-learning-995b7b76006f\nP.S: The blog is an exciting read for me personally since it was one of the on-site interview questions for me at Slack :-)\nIntuit: Accelerating AI @Intuit With Feature Pipelines and Store\nOperating an ML pipeline in production and dealing with complex infrastructure like AWS and stream technologies such as Kafka, Spark Streaming, Flink, etc., is hard. An efficient abstraction of the ML lifecycle management system can accelerate business innovation. Intuit writes about the feature engineering and feature store part of its ML platform, narrates some of the core features.\nhttps://www.linkedin.com/pulse/accelerating-ai-intuit-feature-pipelines-store-simarpal-khaira/\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-21-metadata", "title": "Data Engineering Weekly", "content": "Welcome to the 21st edition of the data engineering newsletter. The 21st edition of the newsletter focuses on the recent breakthroughs in metadata management. I believe the next big set of challenges in data engineering is all about efficient data management. \nData without metadata is like writing a dictionary without meaning.\nOn this note, Linkedin is organizing the Metadata day 2020 https://metadataday2020.splashthat.com/ to unify the industry thoughts around metadata management. If you\u2019ve not registered it, please follow the link and register for it. \nAs a small contribution to Linkedin's efforts, I'm dedicating this week's newsletter as the metadata edition, capturing the timeline of various metadata management systems from Netflix, Lyft, Uber, Airbnb, Linkedin, Datakin, Paypal, Spotify, Shopify, and Facebook.\nMay 2015: Apache Atlas Joins Apache Incubator\nApache Atlas project from then Hortonworks joins Apache Incubator project focusing on providing open metadata management and governance capabilities for organizations to build a catalog of their data assets, classify and govern these assets and provide collaboration capabilities around these data assets for data scientists, analysts, and the data governance team. Apache Atlas graduated as a top-level Apache project in June-2017. IBM writes an excellent article on the role of Apache Atlas in the open ecosystem.\nhttps://www.ibmbigdatahub.com/blog/insightout-role-apache-atlas-open-metadata-ecosystem\nhttps://atlas.apache.org/\nMarch 2016: Open Sourcing WhereHows: A Data Discovery and Lineage Portal - LinkedIn\nLinkedIn writes about WhereHow, a project of the LinkedIn Data team, works by creating a central repository and portal for the processes, people, and knowledge around the most crucial element of any big data system: the data itself. At the blog publication time, WhereHow already carried an impressive 50 thousand datasets, 14 thousand comments, 35 million job executions, and related lineage information.\nhttps://engineering.linkedin.com/blog/2016/03/open-sourcing-wherehows--a-data-discovery-and-lineage-portal\nMarch 2017: Democratizing Data at Airbnb - Airbnb\nAirbnb developed DataPortal to democratize data and empower Airbnb employees to be data-informed by aiding with data exploration, discovery, and trust. The article is an excellent read detailing the fragmented data landscape and data modeling techniques for the data discovery tooling.\nhttps://medium.com/airbnb-engineering/democratizing-data-at-airbnb-852d76c51770\nJune 2018: Metacat: Making Big Data Discoverable and Meaningful at Netflix - Netflix\nNetflix wrote about Metacat, a system that acts as a federated metadata access layer for all data stores. A centralized service for various compute engines could use to access the different data sets. Metacat adopted an interesting architectural pattern where the respective metadata stores are still the source of truth for schema metadata, and Metacat does not materialize it in its storage.\nhttps://netflixtechblog.com/metacat-making-big-data-discoverable-and-meaningful-at-netflix-56fb36a53520\nAuguest 2018: Databook: Turning Big Data into Knowledge with Metadata at Uber - Uber\nUber wrote about its Databook journey from static HTML files uploaded regularly with a dynamic, easy to navigate UI. The blog narrates the choice between event-based metadata collection vs. scheduled collection, data modeling strategies, and search engine support.\nhttps://eng.uber.com/databook/\nNovember 2018: Marquez: A Metadata Service for Data Abstraction, Data Lineage, and Event-based Triggers - WeWork\nDatakin talked about Marquez, an open-source metadata service developed and released by WeWork. Marquez follows a centralized data storage model with a REST API interface to ingest the data, and a MetadataUI for dataset discovery, connecting multiple datasets and exploring their dependency graph.\nhttps://www.datacouncil.ai/talks/marquez-a-metadata-service-for-data-abstraction-data-lineage-and-event-based-triggers\nhttps://marquezproject.github.io/marquez/\nApril 2019: Amundsen \u2014 Lyft\u2019s data discovery & metadata engine - Lyft\nLyft wrote about Amundsen, a data discovery system builds on top of the metadata services. The blog narrates the increasing complexity of data growth and how it impacts productivity and compliance. The blog is an excellent read that focuses on the user experience perspective instead of the technology design.\nhttps://eng.lyft.com/amundsen-lyfts-data-discovery-metadata-engine-62d27254fbb9\nOctober 2019: Open Sourcing Amundsen: A Data Discovery And Metadata Platform - Lyft\nLyft open-sourced Amundsen and writes in detail about the architecture that powers the data discovery engine. The blog compares the pull vs. push model for ingesting the metadata and how it is beneficial to the pull model. Amundsen consists of a generic data ingestion framework DataBuilder, a frontend service, a Metadata service to handle requests from the frontend, and a search service backed by ElasticSearch.\nhttps://eng.lyft.com/open-sourcing-amundsen-a-data-discovery-and-metadata-platform-2282bb436234\nFebruary 2020: Open sourcing DataHub: LinkedIn\u2019s metadata search and discovery platform - LinkedIn\nLinkedIn open-sourced DataHub, its metadata search and discovery platform, and wrote about the journey from WhereHow to DataHub. The blog narrates the difficulty in developing the opensource first generic framework and how DataHub developed tooling and support to open source contributions.\nhttps://engineering.linkedin.com/blog/2020/open-sourcing-datahub--linkedins-metadata-search-and-discovery-p \nMarch 2020: How We Improved Data Discovery for Data Scientists at Spotify - Spotify\nSpotify wrote about Lexicon, a data discovery service to improve the data discovery experience for data scientists. The discovery focuses on personalization, such as finding the popular dataset across the organization, finding relevant datasets for the team, and suggesting that everyone should be aware.\nNote: The Spotify engineering blog down by the time I write this newsletter. \nhttps://labs.spotify.com/2020/02/27/how-we-improved-data-discovery-for-data-scientists-at-spotify/\nJune 2020: Marquez Joins LF AI as New Incubation Project\nMarquez joined LF AI Foundation\u00a0as an incubation project.\nhttps://lfaidata.foundation/blog/2020/06/16/marquez-joins-lf-ai-as-new-incubation-project/\nJuly 2020: How We\u2019re Solving Data Discovery Challenges at Shopify - Shopify\nShopify wrote about Artifact, its data discovery, and data management tool to increase productivity, provide greater accessibility to data, and allow for a higher level of data governance. The blog narrates the challenges of building a data discovery service, from acquiring metadata to transforming, modeling, and applying to make it easier for consumption.\nhttps://shopify.engineering/solving-data-discovery-challenges-shopify\nAuguest 2020: Amundsen Joins LF AI as New Incubation Project\nAlmost an year after opensourcing Amundsen joins the LF AI foundation. \nhttps://lfaidata.foundation/blog/2020/08/11/amundsen-joins-lf-ai-as-new-incubation-project/\nOctober 2020: Nemo: Data discovery at Facebook - Facebook\nFacebook wrote about Nemo, its data discovery engine. Nemo has two major components, indexing, and serving, with a front end on top of the serving section. Indexing is in turn divided into bulk indexing, which happens daily, and instant indexing, which updates the index immediately. For Serving, Nemo is particularly interested in adopting a spaCy-based NLP library that performs text parsing and ML approach for post-processing.\nhttps://engineering.fb.com/2020/10/09/data-infrastructure/nemo/\nNovember 2020: Turning Metadata Into Insights with Databook - Uber\nUber wrote about a reflection of its experience running Databook and evolution it over time. The blog narrates the importance of well-structured, well-managed metadata, a centralized metadata system that focuses on the user experience, and an extendable data model.\nhttps://eng.uber.com/metadata-insights-databook/\nDecember 2020: DataHub: Popular metadata architectures explained - LinkedIn\nLinkedin wrote about DataHub, its third generation evaluation on the learning from WhereHow. The blog narrates the rich learning from the first generation data discovery tooling to the third generation approach. The third generation DataHub adopted the log-oriented metadata sourcing approach and strongly typed domain-oriented metadata models. The adoption of Pegasus schema (PDL) by DataHub\u2019s Generalized Metadata Architecture is an exciting read. Uber\u2019s Databook adopted Dragon, a similar data schema modeling technique.\nhttps://engineering.linkedin.com/blog/2020/datahub-popular-metadata-architectures-explained\nDecember 2020: The journey of metadata at PayPal - Paypal\nPaypal wrote about the evolution of Universal Data Catalog(UDC), starting 2017 from its incubation. The blog narrates how UDC's growth helped Paypal to deprecate several duplicate infrastructures and why Paypal adopted the pull model to source metadata.\nhttps://medium.com/paypal-engineering/the-journey-of-metadata-at-paypal-c374ac66e2e6\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions.\n\n"}, {"url": "https://www.dataengineeringweekly.com/p/data-mesh-simplified-a-reflection", "title": "Data Engineering Weekly", "content": "Data Mesh is a set of data engineering principles coined by Zhamak Dehghani from ThoughtWorks. I highly recommend reading How to Move Beyond a Monolithic Data Lake to a Distributed Data Mesh and Data Mesh Principles and Logical Architecture. \nData Mesh influenced by domain-driven design, emphasizing the importance of data ownership and shared tooling to generate, curate, and democratize data. Data Mesh principles getting good adaption in some of the leading organizations. Yelp talked about its data mesh journey, So does Netflix and Zalando. \nThough the literacy around data mesh maturing, I see a few confusions around Data Mesh in a few blogs. \nNow the fundamental question you may ask. Why Data Mesh and Why now? To understand Data Mesh, we need to understand the current state of the data engineering world. It may not directly apply to your organization, but most of the data infrastructure remains in this sad state. \nImagine you are writing a dictionary with only the words with no meaning for it. On top of it, you shuffle the words randomly and publish the dictionary without any index for it, and hire high-paid data engineers and analysts to decode the dictionary. \nIt is the current state of the data infrastructure. The modern data infrastructure has sophisticated systems like Kafka, Spark, and the ability to emit and process events at a petabyte-scale. Yet, the data generation process we follow is equivalent to writing a dictionary without meaning.\nA data lake is a centralized repository that allows you to store all your structured and unstructured data at any scale. You can store your data as-is without having to structure the data. \nAs the definition suggests, Data Lake focuses on centralized data storage to break the organization's data silo. The central repository removes entry barriers to integrate and analyze from various data sources in an organization. \nHowever, as the data lake grows, the complexity of the data management grows. \nThe producer of the data generates data and sends it to the data lake.\nThe consumers down the line have no domain understanding of the producer and struggle to understand the data lake data.\nThe consumers then connect with the data producer to understand the data. At that point, the producer side's domain expertise depends on human knowledge that may or may not available. \nAs Data Lake grows, it becomes a technical debt rather than a strategic advantage. \nData Mesh is an enterprise data platform principle that converges the principles from Distributed Domain Driven Architecture, Self-serve Platform Design, and Thinking Data as a Product. \nData Mesh focuses on treating data as a product. The feature team that works on the product feature has the domain understanding of the data, not the data's consumer. Data Mesh pushes the data ownership responsibility to the feature team to create, attach metadata, catalog the data, and store it for easier consumption. \nThe data curation and the cataloging of the data at the data creation phase bring more visibility to the data and make it easier for consumption. The process also eliminates human knowledge silo and truly democratize the data. It helps the data consumers not worry about the data discovery and focuses on producing value from the data.\nData Lake is like a reporter writing an article for the New York Times. The reporter goes and interviews related people to write a story, fact checks it, and delivers a reporter narration to the readers. \nData Mesh is like writing a book for O'Reilly or similar publications. The publication provides a foundational infrastructure for all the authors. The authors write their views, add index, and glossary for the book and deliver their narration to the readers.\nData Mesh sounds very cool, but there is always a catch. This is a great Twitter thread summarizing the challenges in adoption.\nAs I mentioned, If we blindly adopt Data Mesh principles without the proper tooling, it can easily lead us to the good old org data silo problem. As mentioned in the below Tweet, no tool can fix the problem.\nThe following threads narrate the observation from the industry and how to adapt the Data Mesh principles.\nData Mesh is not a technology or a storage solution instead of a set of principles to streamline its data management. As Gwen, Sriram, Kishore, and Vinoth pointed out, it is an invisible structure in most organizations and requires proper tooling to enable the Data Mesh principles.\nThe analogy of evolving monolithic architecture to microservices architecture fits well with the Data Mesh principles. If you are starting up, a monolithic architecture may work well for you. As you grow, focus on building tools to label, catalog, organize, and search your data, leading to the adoption of the Data Mesh principles.\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this blog are my own and do not represent current, former, or future employers' opinions. \nLeave a comment\nShare\n"}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-20", "title": "Data Engineering Weekly", "content": "Welcome to the 20th edition of the data engineering newsletter. This week's release is a new set of articles that focus on S3 strong read-on-writes consistency, Apache Pinot 0.6.0 release, ThoughtWorks thoughts on Data Mesh principles, Adobe\u2019s experience with Iceberg, Linkedin\u2019s journey from Lambda to Lambda-less architecture, The Financial Times data platform journey, Shopify\u2019s SQL workflow modeling, The data producer-consumer problem, Picnic\u2019s data engineering, Teads Spark 3.0 migration, and the rise of serverless orchestration engines. \nA big announcement this week in AWS re-invent is S3 now writes strong read-after-write consistency for the GET, PUT, and LIST operation, as well as operations that change object tags, ACLs, or metadata. Netflix writes in the past how the lack of strong read-after-write consistency causing significant business issues. S3Mper, EMRFS, and S3Guard are some of the frameworks trying to fix the S3 consistency issue. I hope all the nightmare over now, and safely delete all hacks!!!\nhttps://aws.amazon.com/blogs/aws/amazon-s3-update-strong-read-after-write-consistency/ \nApache Pinot releases version 0.6 this week. The support for upsert operation, an optimized Apache Spark connector, and tiered storage support are some of the exciting features to watch.\nhttps://medium.com/apache-pinot-developer-blog/announcing-apache-pinot-0-6-0-66bdfb6acf5e\nThoughtworks writes a followup article about Data Mesh Principles and the Logical Architecture. The previous article on How to Move Beyond a Monolithic Data Lake to a Distributed Data Mesh would be a highly recommended read if you missed it. I believe Domain ownership and the Data as a Product are the future of democratizing the organization's data.\nhttps://martinfowler.com/articles/data-mesh-principles.html\nData Mesh is a great idea, but one should adopt it with caution. Here is an excellent Twitter thread on the same.\nAdobe\u2019s experience platform data lake currently processing ~1 million batches per day, which equates roughly to 13TB of data and 32 billion events. Data management at scale brings unique challenges of data reliability, read reliability, and scalability. Adobe writes an excellent post with an overview of the data lake and the effective usage of Apache Iceberg to manages the data lake.\nhttps://medium.com/adobetech/iceberg-at-adobe-88cf1950e866\nThe Lambda architecture has become a popular architectural style that promises both speed and accuracy in data processing using a hybrid approach of both batch processing and stream processing methods. But it also has some drawbacks, such as complexity and additional development/operational overheads. LinkedIn writes an excellent post on some of the lessons learned in operating this system in the Lambda architecture, the decisions made in transitioning to Lambda-less.\nhttps://engineering.linkedin.com/blog/2020/lambda-to-lambda-less-architecture\nThe Financial Times, one of the world\u2019s leading business news organizations, has been around for more than 130 years. Financial Times writes an excellent article narrates its data platform journey from 2008. It\u2019s an exciting read to see the evaluation from the external provider/ SQL Server to real-time analytics with EKS infrastructure.\nhttps://medium.com/ft-product-technology/financial-times-data-platform-from-zero-to-hero-143156bffb1d\nShopify narrates how it builds a production-grade workflow with SQL modeling on top of DBT. SQL withstand as the best abstract for ad-hoc exploration and analytical thinking tool. Shopify embraces the principle builds a testing and documentation framework for SQL workflow.\nhttps://shopify.engineering/build-production-grade-workflow-sql-modelling\nThe article nailed some of the unsolved problems with modern data management platforms, the gap between the dataset producer and the consumer on data discovery, trust, and data governance. A great read that narrates the importance of data discovery tools similar to Amundsen.\nhttps://mark-grover.medium.com/production-consumption-gap-75080253244f\nData Engineering is often considered as the \u201cbackend of the backend\u201d in a few organizations. Picnic writes about business challenges that Picnic\u2019s data engineering team solve together with other teams. It\u2019s exciting to see how Picnic efficiently integrated data engineering practice to its core business processes such as supply chain management, delivery, and online store.\n https://blog.picnic.nl/the-data-engineers-role-in-the-future-of-groceries-74656881a3d6\nThe serverless workflow adoption is increasing as the complexity of operating systems such as Apache Airflow requires specialized skills. Both AWS & Google offer Airflow as a service and offer alternative serverless workflow engines like AWS step function and Google Workflow. Google Cloud writes about its workflow, a serverless orchestration engine in comparison with Airflow.\nhttps://cloud.google.com/blog/products/application-development/get-to-know-google-cloud-workflows\nSpark 3.0 introduced some exciting optimizations such as Dynamically coalescing shuffle partitions, Dynamically switching join strategies, and Dynamically optimizing skew joins. Teads writes about its journey to upgrade Spark 3.0 and share the observation of how Spark 3.0 improves performance compares to Spark 2.X.\nhttps://medium.com/teads-engineering/updating-to-spark-3-0-in-production-f0b98aa2014d\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-19", "title": "Data Engineering Weekly", "content": "Welcome to the 19th edition of the data engineering newsletter. This week's release is a new set of articles that focus on Data Quality part-2 at Airbnb, Dynamic Data Testing, Medium story on how counting is a hard problem, Opinionated view on AWS managed Apache Airflow, Challenges in Deploying ML application by the University of Cambridge, Tableau\u2019s Snowflake migration, data platform at Bukalapak, and T3GO experience with Apache Hudi and Aluxio.\nAirbnb writes the second part of its data quality effort dealing with the certified data products. The blog narrates the data quality\u2019s multi-dimensional problem of Accuracy, Availability, Consistency, Cost Efficiency, Usability, and Timeliness. \nhttps://medium.com/airbnb-engineering/data-quality-at-airbnb-870d03080469\nPart 1 of Data Quality at Airbnb\nData Quality is essential for building data infrastructure, but how to build the data quality framework? Dynamic data testing is an excellent article that narrates key design constraints and a maturity framework from static data testing to dynamic data testing.\nhttps://medium.com/anomalo-hq/dynamic-data-testing-f831435dba90\nCounting is the hardest problem in computer science. Twitter famously admitted users count error in one of the earning call. Medium has gone through a similar problem where a recent wave of users has questioned the validity of the displayed number of followers on their profiles. Medium narrates its experience dealing with the counting error and how a backfill changes the narrative. \nhttps://medium.engineering/counting-your-followers-facbfafe45d9\nChallenges in Deploying Machine Learning: a Survey of Case Studies is an excellent paper to read from the University of Cambridge that narrates the challenges of deploying the ML model from data management to model deployment. \nhttps://arxiv.org/abs/2011.09926\nAWS announced managed Apache Airflow as a service this week. I\u2019m not entirely sure how I feel about it. AWS keeps simply packaging open-source solutions without any contributions back. However, there is not much innovation from AWS in this space. Redshift\u2019s failure to innovate leads to Snowflake\u2019s successful IPO. Should AWS focus on innovating its offering rather than simply packaging open-source solutions?\nhttps://aws.amazon.com/blogs/aws/introducing-amazon-managed-workflows-for-apache-airflow-mwaa/\nTableau writes it\u2019s on-prem to Snowflake cloud data warehouse migration experience and some of the key constraints before the migration. \nhttps://www.tableau.com/blog/2020/11/our-prem-cloud-database-migration-collaborative-effort\nGrab writes an excellent blog on its experience building the Fare calculator system. The CQRS architectural pattern triggers the ongoing domain events vs. CDC argument for event sourcing and exciting space to watch in data engineering. \nhttps://engineering.grab.com/democratizing-fare-storage-at-scale-using-event-sourcing\nOperating data infrastructure requires a significant engineering effect. Bukalapak, an Indonesian E-Commerce company, wrote about its experience running an in-house data infrastructure, the operational complexity, and migration to the cloud. I enjoyed reading the blog, which openly admits the engineering challenges and pivoting to a much simpler solution.\nhttps://medium.com/bukalapak-data/data-platform-transformation-at-bukalapak-1085865a5c86\nT3GO, a Chinese smart travel platform, writes its experience using Apache Hudi, Aluxio, and Alibaba OSS. Apache Hudi becomes a popular data lake solution and combining with Aluxio accelerates the cloud workload while accessing the data. The benchmark comparing HDFS with OSS & Aluxio looks impressive.\nhttps://www.alluxio.io/blog/building-high-performance-data-lake-using-apache-hudi-and-alluxio-at-t3go/\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-18", "title": "Data Engineering Weekly", "content": "Welcome to the 18th edition of the data engineering newsletter. This week's release is a new set of articles that focus on Microsoft\u2019s ML model governance, Google\u2019s MinDiff , Slack\u2019s Airflow migration, Doordash\u2019s scaling ML feature store, Data science methodology at Atlassian, Pinterest\u2019s journey from Lambda to Kappa architecture, What is next step for data management? What is data mesh? How is data pipeline framework & data ingestion framework landscape looks like? and experience with the Oracle Cloud for Data Engineering.\nPredictive modeling is critical for today's business. As a result, governing the Machine Learning Model, including controlling access, testing, validating, logging changes and access, and tracing model results is critical. Microsoft writes an excellent read about Machine Learning Model governance at scale with development lifecycle, roles, and responsibility.\nhttps://medium.com/data-science-at-microsoft/machine-learning-model-governance-at-scale-26c9a2dc15b5\nGoogle announced the release of\u00a0MinDiff, a new regularization technique available in the\u00a0TF Model Remediation library\u00a0for effectively and efficiently mitigating unfair biases when training ML models. It\u2019s exciting to see more tooling coming along on overcoming unfair biases in ML.\nhttps://ai.googleblog.com/2020/11/mitigating-unfair-bias-in-ml-models.html\nThe efficiency of an infrastructure team is defined by the number of clean migration it can do. Slack's data infrastructure team has done it twice with the Airflow and writes about how it does Python 3 migration. It is an exciting read in terms of excellence in software engineering practices.\nhttps://slack.engineering/migrating-slack-airflow-to-python-3-without-disruption/\nAs the ML development becomes mainstream in a company, the amount of feature data can grow to billions of records with millions actively retrieved during model inference under low latency constraints. Doordash narrates the challenges of operating a large scale feature store and optimizes Redis for scalability.\nhttps://doordash.engineering/2020/11/19/building-a-gigascale-ml-feature-store-with-redis/\nHow to build a data science methodology that works for your team? Atlassian narrates its experience running the data science process and various methodologies for building a data science team.\nhttps://medium.com/atlassiandata/build-a-data-science-methodology-7633935dc644\nPinterest writes it's the evolution from Lambda to Kappa Architecture for the visual signal infrastructure. The Lambda architecture inherently suffers from high latency, challenging to debug with two different business logic execution paths. The blog narrates about how granular retries are a significant advantage while adopting Kappa architecture.\nhttps://medium.com/pinterest-engineering/pinterest-visual-signals-infrastructure-evolution-from-lambda-to-kappa-architecture-f8f58b127d98\nBeyond the Database, and Beyond the Stream Processor: What's the Next Step for Data Management? The article is an excellent summarization of stream processing vs database tables duality. The article narrates the fundamental different from stream & database with the access pattern, and the need for rethinking what is database means in coming years.\nhttps://www.infoq.com/articles/whats-the-next-step-for-data-management/\nData mesh is the paradigm shift in developing data products. The concept is still evolving and adopted as the complexity of data increases. ThoughtWorks writes an excellent article on Data Mesh detailing it's not about tech, but it's about ownership and communication.\nhttps://www.thoughtworks.com/insights/blog/data-mesh-its-not-about-tech-its-about-ownership-and-communication\nThe popularity of cloud databases like Snowflake increases the adoption of cloud data integration services like Fivetran. The blog post compares the current landscape of opensource data integration services Singer, Airbyte, Pipelinewise & Meltano. It is exciting to watch Airbyte gaining momentum in the landscape.\nhttps://towardsdatascience.com/the-state-of-open-source-data-integration-and-etl-d2f2e8733e2a\nThe data pipeline frameworks are the central nervous system of building data applications. The author narrates the current state of the data pipeline by comparing Luigi, Airflow & Kubeflow. Dagster & Prefect are the other two exciting frameworks emerging in the data pipeline landscape.\nhttps://dav009.medium.com/current-state-of-data-pipelines-frameworks-november-2020-4cceefb1cb14\nWhat is the status of Data Engineering at Oracle Cloud? The author narrates experience with the Oracle Cloud in comparison with GCP. The verdict If you ignore the research time, from pre-setup to deployment and then post-setup, all the process takes about 2 hours to complete. Just for the sake of comparison, deploying a Dataproc cluster (GCP\u2019s equivalent) takes on average 2 minutes!!!\nhttps://medium.com/@danielapetruzalek/one-week-using-oracle-cloud-for-data-engineering-7933e35e4af2\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-17", "title": "Data Engineering Weekly", "content": "Welcome to the 17th edition of the data engineering newsletter. This week's release is a new set of articles that focus on Databook at Uber, Flink Forward recap, Linkedin's new JVM ML Lib, PyTorch & MlFlow integration, A Data Engineer's take of Postgres, ClickHouse Vs. Redshift, and Picnic's data warehouse design.\u00a0\nUber continues to build on this strategy and write an excellent blog about Databook, Uber\u2019s in-house platform which surfaces and manages the metadata related to various data entities such as datasets, internal dashboards, business metrics, and more.\nhttps://eng.uber.com/metadata-insights-databook/\nPython is a popular language of choice for writing ML applications, where data pipeline mostly of the JVM languages. Linkedin writes about its opensource ML library for Java, Dagli, to build an integrated data pipeline and reduce the technical debt.\nhttps://engineering.linkedin.com/blog/2020/open-sourcing-dagli\nVerverica writes about a recap of the recent Flink Forward Global 2020 virtual conference. The evaluation of Flink SQL as a standard for unified stream and batch processing is an exciting trend to watch out for.\nhttps://www.ververica.com/blog/flink-forward-global-2020-recap\nPyTorch writes about its integration with MLFlow as a step toward enabling an end-to-end exploration of the production platform. The blog narrates some of the essential requirements for MlOps and what is next for PyTorch to enable MlOps.\nhttps://medium.com/pytorch/mlflow-and-pytorch-where-cutting-edge-ai-meets-mlops-1985cf8aa789\nClickHouse OLAP engine largely flying under the radar compares to popular OLAP alternatives such as Apache Druid and Apache Pinot. The blog post is an excellent narration of ClickHouse capabilities, especially the window functions and a performance comparison with Redshift.\nhttp://brandonharris.io/redshift-clickhouse-time-series/\nEvent sourcing is a critical yet not widely discussed area in data engineering. Source and standardize the events across different devices without impacting the client's performance, developer velocity, and security still a challenge. Walmart writes an excellent article about Walmart's development process for sourcing and preparing the event-driven data for analysis.\nhttps://medium.com/walmartglobaltech/preparing-event-driven-data-for-analysis-3010da7416d7\nAs a Data Engineer, I wish Postgres could offer these features an excellent refreshing read about a data engineer's take on a database. Though the blog focuses on Postgres, the features like Temporal View, Incremental View Maintenance, and In-memory tables are fantastic features to have in any data warehouse storage.\nhttps://medium.com/analytics-and-data/as-a-data-engineer-i-wish-postgres-could-offer-these-features-d964588b69de\nPicnic writes a detailed and blog about its data warehouse system. The simplified infrastructure focuses on the data quality, trust in the data, and data democratization is a great read. The blog narrates an exciting take on the dimensional modeling techniques, with the blend of Data Vault and the Kimball Methodology.\nhttps://blog.picnic.nl/picnics-lakeless-data-warehouse-8ec02801d50b\nWhat happens if an accidental removal of data on S3? the deletion can happen accidentally or manually, and the regeneration of the dataset is often expensive pipeline orchestration. Fandom writes an excellent blog on disaster recovery for the S3 datasets.\nhttps://medium.com/fandom-engineering/aws-s3-disaster-recovery-using-versioning-and-objects-metadata-6b9f56ac8858\nFresenius medical care writes about its data lake infrastructure. Jupiter Notebook, Presto, S3 with Parquet remains the standard choices and widespread in data democratization.\nhttps://drdirk.medium.com/data-lake-architecture-at-fresenius-medical-care-f826536f09fe\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-16", "title": "Data Engineering Weekly", "content": "Welcome to the 16th edition of the data engineering newsletter. This week's release is a new set of articles that focus on Data quality at Airbnb, Data version control tools, Spotify experimentation framework, Uber\u2019s proposal to add remote shuffle for Spark, Data pipeline\u2019s health with Great Expectation, Dagster-DBT integrations, and why documentation matters for ML.\nAirbnb writes an excellent post about data quality and how to structure the organization to maximize data engineering efficiency. The blog highlights the startup\u2019s common mistake of not focusing on data quality and its impact while scaling the organization.\nhttps://medium.com/airbnb-engineering/data-quality-at-airbnb-e582465f3ef7\nThe functional programming model is widely adopted in data engineering. One of the key characteristics of a pure function is it should always return the same output, given the same input. Data versioning is one of the common techniques in data engineering to adopt a pure function strategy. The article compares the currently available data versioning frameworks.\nhttps://dagshub.com/blog/data-version-control-tools/\nSpotify writes about the second part of its Experimentation Framework. The article narrates how users are assigned to experiments, analyze results, and ensure test integrity.\nhttps://engineering.atspotify.com/2020/11/02/spotifys-new-experimentation-platform-part-2/\nData shuffling is the performance bottleneck for most of the Spark jobs. The throughput of the network IO improving over the period of time, Uber writes about its proposal to build an external shuffle service. The external shuffle service accepts shuffle data from the shuffle writer, persists, and sends it to the shuffle reader. I wonder if Kafka can emerge as a shuffle service provider.\nhttps://github.com/uber/RemoteShuffleService/blob/master/docs/server-high-level-design.md\nGithub writes about how to integrate Great Expectation with Github's actions. Although the cost of running the test against the production data and the obvious security issue still a concern, the concept sounds very exciting. \nhttps://github.blog/2020-10-01-keeping-your-data-pipelines-healthy-with-the-great-expectations-github-action/\nDagster is growing rapidly with some amazing new features in the data orchestration layer. DBT and Dagster integration is an exciting read.\nhttps://dagster.io/blog/dagster-dbt\nDBT tags are an impressive feature addition to the pipeline. The tags can be added for a table-level or a column level, and the test suites can be executed for the associated tags. The article narrates how to use DBT tags.\nhttps://yu-ishikawa.medium.com/understanding-the-scopes-of-dbt-tags-691d0286f3aa\nAllegro Tech writes about its Python framework for data processing pipelines on GCP, BigFlow. BigFlow is a set of the toolkit developed by Allegro tech for data processing in the Google cloud.\nhttps://allegro.tech/2020/11/bigflow-a-python-framework-for-data-processing-on-gcp.html\nWho created the data? What is in the data? When was the data created? How to access the data? Data discoverability is the key aspect of data engineering to find all these questions. Documentation and adding metadata to the datasets are essential for reliable data infrastructure. The article is an excellent narration of why documentation is important for Machine Learning.\nhttps://medium.com/df-foundation/why-data-documentation-matters-for-machine-learning-d2265b76fe\nCapacity planning is a critical part of scaling the system. Cloudflare writes about how it monitors the disk size and bytes consumed to enrich capacity planning for ClickHouse.\nhttps://blog.cloudflare.com/clickhouse-capacity-estimation-framework/\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-15", "title": "Data Engineering Weekly", "content": "Welcome to the 15th edition of the data engineering newsletter. This week's release is a new set of articles that focus on how to structure data org? Bulldozer from Netflix, Lime's data catalog, How autonomous racecar crashed? Is it time for Decision Scientists?. A recap of RecSys and Spotify's experimentation framework.\nHow to structure the data team in a company? It is a question for any growing companies in the industry. The article is an excellent summarization of various modes the data team can operate. The article compares the centralized model, embedded model, full-stack model, pods, and chapter model.\nhttps://medium.com/snaptravel/how-should-our-company-structure-our-data-team-e71f6846024d\nThe data-driven applications feedback the learning from the data infrastructure to the business process applications. The datawarehouse infrastructure tuned for a large volume of data than serve latency-sensitive applications. It requires the data from the data warehouse to a global, low-latency, and highly-reliable key-value store. Netflix writes about its self-serve data platform that moves data efficiently from data warehouse tables to key-value stores in batches Bulldozer.\nhttps://netflixtechblog.com/bulldozer-batch-data-moving-from-data-warehouse-to-online-key-value-stores-41bac13863f8\nLime writes about how and why it builds the data catalog service, comparing the buy vs. build approach. Every data infrastructure is unique, and the legacy components make the adoption of standard data catalog services, and the article demonstrates the same.\nhttps://medium.com/lime-eng/why-and-how-we-built-a-data-catalog-at-lime-6cb79419b7e2\nThe data processing tooling is growing in two separate ecosystems. Data engineering relies on JVM frameworks, where ML engineering relies on Python frameworks. Cylon is an exciting tool trying to unify the ecosystem with API developed on Java & Python and C++'s core implementation. The benchmark claims it is 12X performant than Apache Spark. Apache Spark, Flink, and Beam are trying to achieve a unified interface oneway or other, and this space continues to be an exciting development to watch.\nhttps://supun-kamburugamuve.medium.com/cylon-library-for-fast-scalable-data-engineering-bf74742fe5d1\n\u201cHere's Why That Autonomous Race Car Crashed Straight Into a Wall\u201d is a good reminder of how data quality is critical while building data-driven applications. Low data quality is a debt to a system. Even worst, it can cost people life.\nhttps://www.thedrive.com/news/37366/why-that-autonomous-race-car-crashed-straight-into-a-wall\nIs it a time for the data scientist to decision scientist? GoJek writes about how it thinks about data science and various functions and skill mapping for the analyst, data scientist, and decision scientist.\nhttps://blog.gojekengineering.com/decision-scientists-at-gojek-the-who-what-why-960d7d27b0d0\nSpotify writes about its experimentation platform journey. The article is an excellent read. It demonstrates the pattern of challenges in developing an experimentation platform, such as reducing time to process the metrics, reducing the data volume, improving custom metrics generation, and automation experiment distribution.\nhttps://engineering.atspotify.com/2020/10/29/spotifys-new-experimentation-platform-part-1/\nEtsy writes about its personalized search infrastructure. The challenges and considerations are a good reminder of personalized recommendations like the latency, context before serving the personalization, cold start, and privacy concerns.\nhttps://codeascraft.com/2020/10/29/bringing-personalized-search-to-etsy/\nMattermost, an open-source messaging and collaboration platform, writes about its data stack. The Apache Airflow, Looker, Snowflake, and DBT slowly becoming the standard tech stack for business analytics.\nhttps://rudderstack.medium.com/mattermosts-data-stack-explained-how-they-leverage-unlimited-data-for-customer-analytics-5eac78288932\nCriteo writes a great summary of RecSys 2020. It is good to read in case you missed following this year\u2019s RecSys conf.\nhttps://medium.com/criteo-labs/highlights-of-recsys-2020-2a07690e0d8c\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-14", "title": "Data Engineering Weekly", "content": "Welcome to the 14th edition of the data engineering newsletter. This week's release is a new set of articles that focus on data quality at Microsoft, Operating Pinot at Uber, data science at Hulu & Trivago, Data lake at Grofers, Notebook from Yelp, Spark shuffle optimizer from Linkedin, Testing the SQL by SoundCloud, Running Airflow on Kubernetes, and Flink SQL platform from Ververica.\nThe poor data quality costs an estimated $3.1 trillion per year in the USA alone, equating to 16.5% of the GDP. Reliably generating high-quality datasets to calculate business metrics is of the utmost importance in data engineering. Microsoft writes an exciting blog on the same describing its DataCop, a high scale data quality auditor.\nhttps://medium.com/data-science-at-microsoft/partnering-for-data-quality-dc9123557f8b\nWhale! \ud83d\udc33 The stupidly simple data discovery tool is an interesting take on the data discovery tooling. The author shared the Airbnb data portal experiences, and the startup data frame looks exciting to watch.\nhttps://medium.com/df-foundation/meet-whale-the-stupidly-simple-data-discovery-tool-9f847c004b47\nLinkedIn writes about Magnet: A scalable and performant shuffle architecture for Apache Spark. The blog narrates the current reliability, efficiency, and scalability challenges. The shuffle push is an exciting take to optimize Apache Spark.\nhttps://engineering.linkedin.com/blog/2020/introducing-magnet\nApache Pinot is a low latency, high throughput rich analytical engine. Uber writes about its experience running Apache Pinot at Uber\u2019s scale.\nhttps://eng.uber.com/operating-apache-pinot/\nJupyter notebooks allow us to do ad hoc development interactively and analyze data with visualization support. As the business critically of Jupytor notebook increases, it is important to reproduce the output of the notebook. Yelp writes about Folium that enabling reproducible Notebooks.\nhttps://engineeringblog.yelp.com/2020/10/introducing-folium-enabling-reproducible-notebooks-at-yelp.html\nWhat is the function of a data science team at Hulu? How do they approach a data science problem? What are the interesting challenges ahead? The blog narrates the landscape of Hulu\u2019s data science approaches.\nhttps://medium.com/hulu-tech-blog/data-science-at-hulu-an-overview-bbc8c9b52a24\nGrofers is one of the biggest online grocery delivery company in India. Grofers writes about its evaluation of data lake. It's an exciting read, particularly with the honest take on the mistakes, lessons learned, and Apache Hudu and CDC approaches' adoption.\nhttps://lambda.grofers.com/origins-of-data-lake-at-grofers-6c011f94b86c\nOn continuing with Apache Hudi, As the change data capture becomes a mainstream sourcing mechanism for the data lake, the need for row-level upsert gains momentum. AWS writes a blog post on how it does the row-level changes with the Apache Hudi tables.\nhttps://aws.amazon.com/blogs/big-data/apply-record-level-changes-from-relational-databases-to-amazon-s3-data-lake-using-apache-hudi-on-amazon-emr-and-aws-database-migration-service/ \nThe untested code is a legacy code, but the data pipelines built on SQL never tested. Great Expectations and DBT are steps in that direction for overall data quality, yet ingesting test data is still the challenge. SoundCloud writes its attempt to fix the SQL testing challenge.\nhttps://developers.soundcloud.com/blog/testing-sql-for-bigquery\nAirflow on Kubernetes is becoming the standard for running the Airflow infrastructure. The blog post narrates three ways to run Airflow on Kubernetes, Using the KubernetesPodOperator, Using the KubernetesExecutor, and Using the KEDA with Airflow.\nhttps://fullstaq.com/blog/three-ways-to-run-airflow-on-kubernetes/\nThe rise of the SQL in streaming workload is inevitable. Ververica announces the general availability of Flink SQL in the Ververica platform.\nhttps://www.ververica.com/blog/ververica-platform-2.3-an-end-to-end-platform-for-flink-sql\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-13", "title": "Data Engineering Weekly", "content": "Welcome to the 13th edition of the data engineering newsletter. This week's release is a new set of articles that focus on the success stories and the importance of Kafka, data quality tools landscape, MLOps tools landscape, schema evolution, and airflow DAG best practices from Twitter, Grab, Climate Corp, and Confluent.\nSchema evolution is a fundamental aspect of data management and, consequently, data governance. Applications tend to evolve, and together with them, their internal data definitions need to change. The article narrates different schema compatibility that focuses on Avro and the Confluent schema registry.\nhttps://medium.com/data-rocks/schema-evolution-is-not-that-complex-b7cf7eb567ac\nData quality is critical for running high functional business operations. The high-quality datasets produce high accurate model predication that gives strategic advantages to the business operations. The article is a pretty good overview of various players in the data quality space.\nhttps://medium.com/memory-leak/data-quality-a-primer-f6a945915511\nThe MLOps goes through a maturity cycle from manual model building to deployment pipeline to CI/CD integration, just like any software maturity model. The article walks through the major cloud providers AWS, Azure, and the Google Cloud ML platform offerings on label the data, experimentation, and industrialisation.\nhttps://towardsdatascience.com/which-cloud-servicer-provider-ml-platform-do-you-need-69ff5d96b7db\nContinuing on the MLOps, The article compares five different MLOps tools available. The comparison focuses on MLflow, Pachyderm, Kubeflow, DataRobot, and Algorithmia.\nhttps://medium.com/better-programming/5-great-mlops-tools-to-launch-your-next-machine-learning-model-3e403d0c97d3\nTwitter's Kafka infrastructure process 150Million events per second. Some mind-blowing stats, 80 Kafka clusters up to 200 brokers per cluster, 40,000 subscribers (Kafka clients), 2000+ Kafka topics. The article is a good walkthrough of Twitter's Kafka adoption.\nhttps://videos.confluent.io/watch/3V7HtAVxvGv2zpcewb5zT3\nhttps://www.slideshare.net/ConfluentInc/twitters-apache-kafka-adoption-journey/ConfluentInc/twitters-apache-kafka-adoption-journey\nGrab shared its experience on optimally scaling Kafka consumer applications that handle 400 billion events per day.\nhttps://engineering.grab.com/optimally-scaling-kafka-consumer-applications\nMachine learning in the agriculture domain is an exciting space to watch, and I hope our time's bright minds start focusing on the growth of civilization instead of ads click baits. Climate Corp shared its experience in building a recommendation engine for the farmers.\nhttps://blog.dominodatalab.com/bringing-ml-to-agriculture/\nAstronomer shared some of the best practices while writing the Airflow DAGs. The article focuses on Idempotent, incremental data processing.\nhttps://www.astronomer.io/guides/dag-best-practices/\nWhat is happening now in my data stream? The materialization is a critical feature of a stream processing engine to support this question. The article is an excellent walkthrough of how KSQLDB handles stream materialization.\nhttps://www.confluent.io/blog/how-real-time-materialized-views-work-with-ksqldb/\nContinuing stream processing, managing the state in a stream processing critical to building distributed stateful applications. Apache Flink is one of the matured stream processing engines in the market, and the blog narrates the internals of the stateful function implementation.\nhttps://flink.apache.org/news/2020/10/13/stateful-serverless-internals.html\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-12", "title": "Data Engineering Weekly", "content": "Welcome to the 12th edition of the data engineering newsletter. This week's release is a new set of articles that focus on Facebook's data discovery engine, a look back on Amundsen, Neilsen's AWS lambda design, building AI assistance, best practices on data engineering from Facebook, Airbnb, Linkedin, Lyft, 1mg, Neilsen.\nFacebook writes about its data discovery engine Nemo. The article is an exciting read. It highlights the challenges of building a data discovery engine, what phrase to search, the relationship among the datasets, and the datasets' ranking. I expect a more machine learning-driven approach in the data discovery space soon.\nhttps://engineering.fb.com/data-infrastructure/nemo/\nContinuing on the data discovery, Amundsen reflects one year as an open-source project. The article reflects Amundsen's pluggable architecture, rich connectors, and fantastic community backing.\nhttps://eng.lyft.com/amundsen-1-year-later-7b60bf28602\nHow can we ensure Data and Code Quality in Data Engineering? The article is an excellent checklist of the top 10 principles to follow, focusing on functional programming principles, documentation, naming convention, and modularization.\nhttps://towardsdatascience.com/10-data-engineering-practices-to-ensure-data-and-code-quality-6224d012d8fb\nLinkedin's \"People You May Know\" is a classic example of building the data-driven network effect to scale the business. The article narrates how the system evolves to handle heterogeneous edges (connection, follow, subscribe models).\nhttps://engineering.linkedin.com/blog/2020/building-a-heterogeneous-social-network-recommendation-system\nSearch is the core business function of Airbnb. The article narrates improving the deep learning ranking for the Airbnb stays. The focus on eliminating bios, cold start for the new listing, eliminating bios of past preference overwhelming the result.\nhttps://medium.com/airbnb-engineering/improving-deep-learning-for-ranking-stays-at-airbnb-959097638bde\nThe growth of Slack-like tools driving the bot/ AI assistance to the mass market. The article narrates the five stages of AI assistance. Simple notification, answers simple FAQs, engages in dialogs, offers a personalized experience, and connects with other AI assistance. The case study to build and deploy the AI Bot is an exciting read.\nhttps://www.infoq.com/articles/build-deploy-ai-assistants/\n1mg, an online platform that provides services for medical diagnostics, consultation, lab tests, and general healthcare, writes about an overview of its data infrastructure. I learned about RudderStack, and it is exciting tools coming in the data sourcing space.\nhttps://medium.com/@RudderStack/1mgs-data-stack-explained-how-they-harness-and-activate-unlimited-real-time-data-8fed74f7b5bf\nFacebooks write about another exciting tool, CG/SQL, that allows developers to write stored procedures in a variant of Transact-SQL (T-SQL) and compile them into C code that uses SQLite SQLite's C API to do the coded operations. It's an exciting approach to brings the type safety to large-scale procedures.\nhttps://engineering.fb.com/open-source/cg-sql/\nNielsen and AWS published a good referential design of using AWS Lambda in the data pipeline. It's impressive usage of the lamdas as a dispatcher worker.\nhttps://aws.amazon.com/blogs/architecture/nielsen-processing-55tb-of-data-per-day-with-aws-lambda/\nHow to Verify your ETL Result? The article is an excellent checklist of various quality checks and good working example integrating with the Airflow DAGs.\nhttps://medium.com/@adiluhungdimas/how-to-verify-your-etl-result-aa4df57b6d9d\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-11", "title": "Data Engineering Weekly", "content": "Welcome to the 11th edition of the data engineering newsletter. This week's release is a new set of articles that focus on data infrastructure trends 2020, seven principles of data ops, data quality, ML transference & performance tuning, and Samza runner for Beam from LinkedIn, Twitter, DoorDash, Airbnb, Shopify, Apache Pinot, Dagster.\nDeveloper productivity and the ability to iterate through the correctness of a job is always challenging. The Airflow test utility took the first step to improve developer productivity. Dagster in this post brought to the next level, describing how to run PySpark in either EMR or Dagster with the mode switch.\u00a0\nhttps://dagster.io/blog/pyspark\nThe seven principles of reliable data pipelines are an excellent read compares with the Google SRE principles. The author narrates the importance of adopting SLO & SLI, reducing the toil, the importance of monitoring the pipeline, and simplicity.\nhttps://medium.com/toro-data-quality/seven-principles-for-reliable-data-pipelines-e82a82810e4f\nThe 2020 data & AI landscape is an excellent read. The author narrates some of the recent trends in the data infrastructure. The shift from Hadoop systems to the cloud warehouses like the snowflake, Google Big query, The gaining momentum for the data lineage and the discovery tools, The second generation orchestration tools Prefect & Dagster the rise of the AIOps are the exciting trends to look.\nhttps://mattturck.com/data2020/\nLinkedIn published the benchmarking results for Samza runner for Apache Beam. It's a good reference article on how to think performance improvement as a continuous process.\nhttps://engineering.linkedin.com/blog/2020/building-a-better-and-faster-beam-samza-runner\nTwitter writes a short and exciting blog about the recent image cropping transparency issue. It is a good reminder that machine learning is not always an answer and lets users choose what they want.\nhttps://blog.twitter.com/en_us/topics/product/2020/transparency-image-cropping.html\nAirbnb writes about how it builds the data platform to conduct Revenue Forecasting at Airbnb. The blog is an excellent narration of some practical challenges with the data infrastructure, supporting multiple query engines, dynamic metrics generation, late arrival data, and maintains SLA.\nhttps://medium.com/@jerry.chu/airbnbs-data-platform-of-revenue-forecasting-2e95a01122e6\nDoordash writes about its recent performance challenges with the search scoring and the ranking model infrastructure. The blog narrates its migration to the internal predication service, emphasizing the importance of the dedicated feature store.\nhttps://doordash.engineering/2020/10/01/integrating-a-scoring-framework-into-a-prediction-service/\nThe critical site-facing analytical applications require high throughput and strict p99th query latency. Apache Pinot is an excellent OLAP engine to serve use facing analytical solutions, and the article narrates the challenges of doing concurrent, low latency SLA queries using Apache Pinot.\nhttps://medium.com/apache-pinot-developer-blog/achieving-99th-percentile-latency-sla-using-apache-pinot-2ba4ce1d9eff\nData quality has been a consistent focus, as it often leads to issues that can go unnoticed for a long time, bring entire pipelines to a halt, and erode stakeholders' trust in the reliability of their analytical insights. Great Expectations writes an excellent narration of how data quality is key to the success of MLOps.\nhttps://medium.com/@expectgreatdata/why-data-quality-is-key-to-successful-ml-ops-a18d6e373ca9\nDescriptive statistics and correlations are data scientists' bread and butter, but they often come with the caveat that\u00a0correlation isn't causation. In this blog post, Shopify narrates different causal inference methods and uses them to build great products.\nhttps://engineering.shopify.com/blogs/engineering/using-quasi-experiments-counterfactuals\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions.\n"}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-10", "title": "Data Engineering Weekly", "content": "Welcome to the 10th edition of the data engineering newsletter. This week's release is a new set of articles that focus on scaling the data platform, ClickHouse vs. Druid, Apache Kafka vs. Pulsar, Apache Spark performance tuning, and the Tensorflow Recommenders from Google, Twitter, Linkedin, eBay, DoorDash, Zendesk & Criteo.\nDoordash writes an excellent blog post on its journey to build the data platform to delight the customer journey. The article is a brilliant reference model to implement data engineering to impact an enterprise.\nhttps://doordash.engineering/2020/09/25/how-doordash-is-scaling-its-data-platform/\nLinkedin writes about the evolution of its experimentation platform, T-REX. It is an excellent read to understand the prehistory of one of the largest experimentation platform and how it evolves from experiment management and delivery system with a UI application; the system gradually evolved into a platform that comprises targeting, dynamic configuration and experiment infrastructure, insight, and reporting pipelines, a notification system, and a seamless UI experience.\nhttps://engineering.linkedin.com/blog/2020/our-evolution-towards-t-rex--the-prehistory-of-experimentation-i\nTwitter recently builds a streaming data logging pipeline for its home timeline prediction system using Apache Kafka and Kafka Streams to replace the existing offline batch pipeline at a massive scale. The blog post narrates customized Kafka Streams join DSL that supports the ML-specific logging pipeline at the Twitter scale.\nhttps://www.confluent.io/blog/how-twitter-built-a-machine-learning-pipeline-with-kafka/\neBay OLAP engine process more than 1 billion OLAP events per second. The legacy system build on top of Druid was found expensive to run. eBay writes about its journey towards migrating to ClickHouse on Kubernetes.\nhttps://tech.ebayinc.com/engineering/ou-online-analytical-processing/\nZendesk writes an excellent post on comparing Apache Kafka with Pulsar. The tiered storage, dynamic scaling, and the growing number of partitions are an essential consideration. The evaluation concluded that though the Pulsar features are exciting, the system's stability still requires attention.\nhttps://medium.com/zendesk-engineering/evaluating-apache-pulsar-92e6ed3fc792\nAirbnb opensource its react visualization library Visx. The primary advantage of Visx to reduce the context switching for the front-end engineers familiar with React and build the custom charting library.\nhttps://medium.com/airbnb-engineering/introducing-visx-from-airbnb-fd6155ac4658\nClickstreams and user activities are at the center stage of our data product lines, yet handling detailed event data processing, especially about timestamps and event order, is challenging. Expedia writes an excellent blog post narrates a strong case of vigilant about the time ordering for the event processing.\nhttps://medium.com/expedia-group-tech/be-vigilant-about-time-order-in-event-based-data-processing-cbfde600dd7d\nArtificial Neural Networks offer significant performance benefits compared to other methodologies, but often at the expense of interpretability. The blog post narrates the case for explainable AI(XAI) to provide more transparency.\nhttps://www.infoq.com/articles/explainable-ai-xai/\nCriteo writes about Apache Spark performance tuning focused on the query compilation. The blog post narrates the difference of RDD's volcano model and Spark SQL's whole stage code generation and sample code to validate the performance.\nhttps://medium.com/criteo-labs/under-the-hood-of-spark-performance-or-why-query-compilation-matters-c084e749be87\nCan We Build a 100% Serverless ETL Following CI/CD Principles? The blog post is an excellent narration of building data pipeline using DBT, Google BigQuery, and Github actions. I'm excited about the direction of commoditizing the data infrastructure.\nhttps://medium.com/swlh/dawn-of-dataops-can-we-build-a-100-serverless-etl-following-ci-cd-principles-3ca587ba1ec0\nThe blog post is an excellent referential narration of building scalable airflow infrastructure on top of Kubernetes, data volume, collecting metrics, and storing the secrets.\nhttps://www.infoq.com/articles/distributed-data-pipelines-apache-airflow/\nThe recommender system, once the flagship area of interest in the ML world getting more commoditized. From recommending movies or restaurants to coordinating fashion accessories and highlighting blog posts and news articles, recommender systems are essential in machine learning. Google introduces\u00a0TensorFlow Recommenders (TFRS), an open-source TensorFlow package that makes building, evaluating, and serving sophisticated recommender models easy.\nhttps://blog.tensorflow.org/2020/09/introducing-tensorflow-recommenders.html\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-9", "title": "Data Engineering Weekly", "content": "Welcome to the 9th edition of the data engineering newsletter. This week's release is a new set of articles that focus on the COVID-19 effect on ML models, combat disinformation, Analytics @ Netflix, data quality libraries, and Apache Pinot from Microsoft, Netflix, Doordash, CapitalOne & Databricks.\nDisinformation is widespread. As a data professional, it's our ethical responsibility to think and prevent disinformation. Deepfake is one of the AI technique can spread disinformation. Microsoft announces Video Authenticator to combat deepfake videos.\nhttps://blogs.microsoft.com/on-the-issues/2020/09/01/disinformation-deepfakes-newsguard-video-authenticator/\nCOVID-19 introduced significant habits and patterns changing in many business applications. The volatility brings a new set of challenges to the machine learning model. DoorDash witnesses an extreme surge and writes a blog post narrates how it retrain the ML model to accommodate the business dynamics changes.\nhttps://doordash.engineering/2020/09/15/retraining-ml-models-covid-19/\nNetflix writes an excellent post on analytics @ Netflix and the purpose of the analytical role at Netflix. The article provides an excellent narrative difference between the data analyst and the data engineer.\nhttps://netflixtechblog.com/analytics-at-netflix-who-we-are-and-what-we-do-7d9c08fe6965\nPeople in data science and engineering are highly connected to the business, solve end-to-end problems, and are directly responsible for improving business outcomes. But what makes this group shine are their differences. They come from lots of backgrounds, which yields different perspectives on how to approach problems. Netflix shares its data science & engineering members' stories and the career path.\nhttps://netflixtechblog.com/how-our-paths-brought-us-to-data-and-netflix-4eced44a6872\nApache Pinot is a real-time distributed datastore, built to deliver scalable real-time analytics with low latency. The blog post narrates how to run real-time climate analysis on Apache Pinot using National Center for Environmental Information (NCEI) dataset.\nhttps://medium.com/apache-pinot-developer-blog/building-a-climate-dashboard-with-apache-pinot-and-superset-d3ee8cb7941d\nA typical analytical development lifecycle goes like Ingest -> Build a model -> Deploy -> Monitor. There is more focus on build the model than deploy, which produces the business value. The article narrates various strategies to deploy the analytical workload.\nhttps://medium.com/datamindedbe/how-to-deploy-analytics-workloads-563279bc9694\nData infrastructure relies on the complex chain of data pipelines. As the data flow through the pipeline, the familiar adage goes, \"Garbage In, Garbage Out.\" Hence the data quality is an integral part of the data pipeline. The blog post compares the top data quality frameworks available, The Tensorflow data validator, Great Expectations, and Deequ.\nhttps://medium.com/datamindedbe/data-quality-libraries-the-right-fit-a6564641dfad\nThe metadata search and discovery of the dataset are critical parts of the data infrastructure to democratize data. The blog post narrates some of the open-source metadata hub tools and deep dive on Linkedin's datahub and Lyft's Amundsen.\nhttps://towardsdatascience.com/a-dive-into-metadata-hub-tools-67259804971f\nTesting the correctness of an async data pipeline is always challenging. The Disney Streaming team writes about the weaver-test, an open-source scala test framework for Kafka & Kinesis streams.\nhttps://medium.com/disney-streaming/testing-asynchronous-pipelines-with-fs2-and-weaver-test-f0ffd37676d\nDwelo writes about its data infrastructure and exciting to see the containerization, DBT, and cloud storage becomes the standard tooling to build data infrastructure.\nhttps://medium.com/dwelo-r-d/data-engineering-at-dwelo-1a68a212cf17\nThe data reveals the hidden truth in social economics. In 2015, the United Nations adopted 17 Sustainable Development Goals (SDGs), which represented a universal call to action to end poverty, protect the planet and ensure that all people enjoy peace and prosperity by 2030. The blog post narrates how enriching the SDG indicator with the open street map geolocation data brings more insights into the average share of the built-up area of open space for public use.\nhttps://www.azavea.com/blog/2020/09/11/openstreetmap-and-the-sustainable-development-goals/\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-8", "title": "Data Engineering Weekly", "content": "Welcome to the 8th edition of the data engineering newsletter. This week's release is a new set of articles that focus on Dagster, Kafka, experimentation platforms from Pinterest, Doordash, Confluent, Eventbrite, and Expedia.\u00a0\nData validation is an integral part of the data pipeline. Dagster writes about its integration with Great Expectation, the fastest-growing open-source data validation, and documentation framework.\nhttps://medium.com/dagster-io/great-expectations-for-dagster-b58d4f45c342\nConfluent writes a post on implementing the message prioritization in Apache Kafka. It\u2019s an important characteristic of Job Scheduler systems. The bucket priority pattern with a Bucket Priority Assigner is an exciting pattern to read.\nhttps://www.confluent.io/blog/prioritize-messages-in-kafka/\nThe SeatGeek opensource its data pipeline framework Druzhba, to extract and load data from various sources.\nhttps://chairnerd.seatgeek.com/druzhba-open-source-release/\nPinterest writes its second part of Project LightHouse to measure Airbnb guest acceptance rates' discrepancies using anonymized demographic data.\nhttps://medium.com/airbnb-engineering/project-lighthouse-part-2-measurement-with-anonymized-data-69fb01eac88\nThe Experimentation Platform is an essential part of rapid product development. Doordash writes about Curie; it's experimentation platform, and the journey from ad-hoc manual analysis to automate the experimentation lifecycle.\nhttps://doordash.engineering/2020/09/09/experimentation-analysis-platform-mvp/\nEventbrite writes about its new feature, building a protest map. It\u2019s an exciting read that narrates the executive buy-in, difficulties in data collection, and the challenges with the recency of the data.\nhttps://www.eventbrite.com/engineering/building-a-protest-map-a-behind-the-scenes-look/\nExpedia writes about Hyperspace\u00a0by Microsoft, an indexing subsystem built on top of Apache Spark, which allows you to create indexes to support ad hoc queries just like a traditional database. It\u2019s an exciting read providing Hyperspace offer index optimization on top of Apache Spark.\nhttps://medium.com/expedia-group-tech/indexing-spark-data-with-microsofts-hyperspace-ec4de4b93ba3\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-7", "title": "Data Engineering Weekly", "content": "Welcome to the seventh edition of the data engineering newsletter. This week's release is a new set of articles that focus on Natural language query processing, Data Ethics, MLOps, Data Orchestration comparison, and A/B testing platform from NVIDIA, Criteo, Netflix, Zendesk, Salesforce, Airbnb, and Facebook.\nDifferential privacy is a mathematically, rigorous framework for quantifying the anonymization of sensitive data. Facebook opensource Opacus, it's a high-speed library for training PyTorch models with differential privacy. It's exciting to see more tooling coming around for the privacy and ethical approach in machine learning community.\nhttps://github.com/pytorch/opacus\nhttps://ai.facebook.com/blog/introducing-opacus-a-high-speed-library-for-training-pytorch-models-with-differential-privacy/\nOn similar Facebook efforts, Airbnb writes about Project Lighthouse, an initiative to measure and combat discrimination when booking or hosting on Airbnb. The blog post focuses on using data that satisfies p-sensitive k-anonymity to calculate acceptance rates by guest perceived race.\nhttps://medium.com/airbnb-engineering/project-lighthouse-part-1-p-sensitive-k-anonymity-c6ee7d79c4f9\nMLOps is a set of best practices for businesses to run AI successfully. MLOps is a relatively new field because the commercial use of AI is itself reasonably new. In this blog post, NVIDIA narrates about MLOps and a few success stories of MLOps adoption.\nhttps://blogs.nvidia.com/blog/2020/09/03/what-is-mlops/\nWhat quantifies as the large-scale machine learning platform. Is it massive hardware, petabytes of data, or the complexity and time constraint? In this blog post, Criteo walkthrough the trade-off of a large-scale machine learning system.\nhttps://medium.com/criteo-labs/the-trade-offs-of-large-scale-machine-learning-71ad0cf7469f\nThe data orchestration is the central nervous system of data infrastructure. Redpoint ventures write an exciting blog about an overview of the data orchestration and the landscape of the tools available.\nhttps://medium.com/memory-leak/data-orchestration-a-primer-56f3ddbb1700\nZendesk narrates about its data catalog story for the enterprise data team and their choice of Apache Atlas.\nhttps://medium.com/zendesk-engineering/data-catalogs-the-luxury-of-choice-c161cd91e713\nApache Flink 1.11 comes with significant changes to the memory model of Flink\u2019s JobManager and configuration options for the Flink clusters. The developers can configure off-heap memory and the metaspace overhead and Total Process memory and Total Flink memory.\nhttps://flink.apache.org/2020/09/01/flink-1.11-memory-management-improvements.html\nIt\u2019s exciting to see more ecosystem starting to build on top of DBT. Census is the data automation platform that syncs your data warehouse with the apps you use. [Unfortunately not open sourced]. Census writes about its data sync tool that syncs with the DBT model.\nhttps://blog.getcensus.com/making-your-dbt-models-more-useful-with-census/\nRazorPay writes about it's Druid adoption to expose high level and business-critical data within the organization through various dashboards, powered by Looker. The journey they took from the batch processing to Apache Kylin to Druid is an exciting read.\nhttps://medium.com/@birendra.sahu_77409/how-razorpay-uses-druid-for-seamless-analytics-and-product-insights-364c01b87f1e\nSalesforce Einstein introduced Photon, a natural language interface to query the database. I tried a few queries, and the joins seem working amazingly.\nhttps://naturalsql.com/\nhttps://blog.einstein.ai/talk-to-your-data-one-model-any-database/\n\nUnlike True Experimentation, the participants were not randomly assigned either the treatment or the control group in the quasi experimentation. Netflix runs quasi-experiments at scale and writes about its Quasimodo tool, which automates some aspects of the scientists' workflow and runs more quasi-experiments in parallel.\nhttps://netflixtechblog.com/key-challenges-with-quasi-experiments-at-netflix-89b4f234b852\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent current, former, or future employers' opinions."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-6", "title": "Data Engineering Weekly", "content": "Welcome to the sixth edition of the data engineering newsletter. This week's release is a new set of articles that focus on Kafka summit recap, Type 2 dimension modeling, securing Presto, handling bios in AI, and ML applications from Shopify, DoorDash, Linkedin & Confluent.\nIt's a Kafka summit week, and more than 33,000 registered users participated in the virtual conference. Confluent writes about the highlights of the summit in a couple of blog posts. The tech deep dive on Kafka Tiered storage and Zookeeper replacement is the major highlight in the Day 1 conference.\nhttps://www.confluent.io/blog/kafka-summit-2020-day-1-recap/\nhttps://www.confluent.io/blog/kafka-summit-2020-session-highlights/\nSlowly changing dimensions are a challenge to the data models, especially in the big data era. In business analytics, we care about not only the current state but also the historical state. Shopify writes an exciting article about Type 2 dimensional modeling, creating these data models using modern ETL toolings like PySpark and dbt (data build tool), and the lessons learned.\nhttps://engineering.shopify.com/blogs/engineering/track-state-type-2-dimensional-models\nPresto becomes an essential component of the data infrastructure. Grab's data team writes about DataGateway, a Presto gateway service to intercept Presto queries, and authenticates its user Access Control List (ACL). The interceptor model is an exciting read compares to the Apache Ranger's plugin model.\nhttps://engineering.grab.com/data-gateway\nDoorDash writes about using a Human-in-the-Loop to Overcome the Cold Start Problem in Menu Item Tagging. Any ML-model based solution faces the cold start problem, where we don\u2019t have enough labeled samples for each class to build a performant model. The blog post is an exciting read narrates the lifecycle of building the human-in-the-loop ML models and why they decided to go with ML generated tagging than embedding model.\nhttps://doordash.engineering/2020/08/28/overcome-the-cold-start-problem-in-menu-item-tagging/\nThe primary concern in the growth of AI is the widespread societal injustice based on human biases reflected both in the data used to train AI models and the models themselves. Linkedin writes about its broader initiative to bring fairness to the AI applications. On the effort, LinkedIn open sources Linkedin Fairness Toolkit (LiFT), a Scala/Spark library that enables the measurement of fairness in large scale machine learning workflows. The LiFT can be deployed in training and scoring workflows to measure biases in training data, evaluate different fairness notions for ML models, and detect statistically significant differences in their performance across different subgroups.\nhttps://engineering.linkedin.com/blog/2020/lift-addressing-bias-in-large-scale-ai-applications\nDatabricks writes about how we can adopt the Databricks platform to quantify the likelihood of customer churn. Though the blog skewed towards the Databricks platform, the lifecycle walkthrough of data preparation, feature engineering, model selection, and the Hyperparameter tuning is an exciting read.\nhttps://databricks.com/blog/2020/08/24/profit-driven-retention-management-with-machine-learning.html\nI came across this Github repo recently, which contains a rich collection of research papers relevant to data science and data engineering.\nhttps://github.com/jarikoi/interesting-papers\nGPU accelerated AI-powered applications breaking new grounds. NVIDIA  leading the GPU research writes about new research, enhanced tools for creators. OpenVDB is the industry-standard library used by VFX studios for simulating water, fire, smoke, clouds, and other effects. It's interesting to read about NVIDIA's NanoVDB adds GPU support for OpenVDB compatible data structure where users can leverage GPUs to accelerate workflows such as ray tracing, filtering, and collision detection while maintaining compatibility with OpenVDB.\nhttps://blogs.nvidia.com/blog/2020/08/25/nvidia-siggraph/\nThe data preparation in geographic data always a challenging task.  The bigger the data, the more problematic is the process, especially when pruning noisy outliers and anomalies. ArcGIS is a geographic information system for working with maps and geographic information. The article narrates with a notebook example of data cleaning for the geographic information and integrates it with ArcGIS.\nhttp://thunderheadxpler.blogspot.com/2020/08/on-machine-learning-in-arcgis-and-data.html\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent the opinions of current, former, or future employers."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-5", "title": "Data Engineering Weekly", "content": "Welcome to the fifth edition of the data engineering newsletter. This week's release is a new set of articles that focus on DBT, Testing, and production deployment of ML infra, ML applications from Reddit, FarFetch, Pinterest, Google Cloud, Lyft, and AI economics.\nDBT is capturing the heart of the data engineers and becomes an essential toolkit for building the data pipeline. I tweeted my thoughts recently on DBT and why it's groundbreaking.\nThe article echoes a similar sentiment. The author narrates some of the drawbacks of SQL or the looseness of the SQL and how DBT addresses these concerns to build complex testable SQL pipelines.\nhttps://highgrowthengineering.substack.com/p/why-is-dbt-so-important- \nThe data workload much different and complicated from the traditional request/ response applications. The data world missed some of the devops practices since it's expensive to maintain a parallel data pipeline. The Dataops terminology is trying to bridge the difference. The article gives a good overview of how Devops and Dataops practices can complement each other.\nhttps://www.infoq.com/articles/dataops-devops-scale-speed/\na16z writes about the challenges of building successful AI companies. The cost of computing, the need for the human in the loop, and the scaling problems are the standout concerns. It also provides opportunities to innovate, and the second part focuses on some of the best practices to develop successful AI applications.\nhttps://a16z.com/2020/02/16/the-new-business-of-ai-and-how-its-different-from-traditional-software/\nhttps://a16z.com/2020/08/12/taming-the-tail-adventures-in-improving-ai-economics/\nIt will always be tempting to think of every data application as an ML-driven application. If you've got a finite input and a known output for a given input, you don't have an ML problem. Capital One writes an interesting article on the same by comparing Pros & Cons of Rule Engines vs. ML models.\nhttps://medium.com/capital-one-tech/a-modern-dilemma-when-to-use-rules-vs-machine-learning-61cc908769b0\nReddit writes about it's reporting services design to monitor the effectiveness of the campaigns. The pragmatic approach of using AWS services and Kubernetes cron services as a scheduler is a good reminder of how to use cloud services effectively.\nhttps://redditblog.com/2020/07/21/building-scheduled-reports-for-ad-campaigns/\nGPS data that we get is often noisy and does not match the real world. Lyte writes about its map-matching algorithm in detail and compares how it overcomes some of the Hidden Markov Model's limitations.\nhttps://eng.lyft.com/a-new-real-time-map-matching-algorithm-at-lyft-da593ab7b006\nFarfetchTech writes about it's an email recommendation engine. The exciting part of the article is not how to generate recommendations, but when to create it. The latest possible time to create the recommendation for an email is to generate it as it opened! Any earlier than that and the recommendation starts getting stale. Any later than that and the email is missing content.\nhttps://www.farfetchtechblog.com/en/blog/post/recommendations-in-emails-pretty-close-to-rocket-surgery/\nPinterest writes about its search and recommendation engine by the skin tone model. It's exciting to see how Pinterest focuses on eliminating biases and diverging datasets to ensure it's an inclusive system. \nhttps://medium.com/pinterest-engineering/powering-inclusive-search-recommendations-with-our-new-visual-skin-tone-model-1d3ba6eeffc7\nChange data capture (CDC) is a term used to refer to a set of techniques for identifying and exposing changes made to a database. Bolt writes about it's CDC pipeline on top of Debezium, Kafka and Kafka Connect.\nhttps://www.confluent.io/blog/how-bolt-adopted-cdc-with-confluent-for-real-time-data-and-analytics/\nTraditionally the data pipelines centered around JVM languages and the data science workloads on Python. Google Cloud writes about the Dataflow Runner v2, a more efficient and portable worker architecture rewritten in C++, based on Apache Beam\u2019s new portability framework. The worker architecture provides the standard feature set across all language-specific SDKs and share bug fixes and performance improvements.\nhttps://cloud.google.com/blog/products/data-analytics/multi-language-sdks-for-building-cloud-pipelines\nThe official Flink docker image download crosses 50 million. Flink on Docker often the preferred deployment model. The article narrates the current state of Flink's docker support and how to get started.\nhttps://flink.apache.org/news/2020/08/20/flink-docker.html\nThe biggest challenge of the ML application is the algorithm but is to put ML systems into production. MLFlow from Databricks, Googles's TFX, Uber's Michelangelo, Facebook's FBLearner Flow, Microsoft's AI Lab, Amazon's Amazon ML, Airbnb's BigHead are some of the systems attempted to streamline running ML applications in production. The article walks through various stages of ML deployment from portability, CI/CD, deployment strategy to monitoring the production system.\nhttps://medium.com/swlh/productionizing-machine-learning-models-bb7f018f8122\nMachine learning systems are trickier to test since we're not explicitly writing the logic of the system. However, automated testing is still an essential tool for the development of high-quality software systems. The article narrates the difference between model evaluation and model testing and how to write model tests.\nhttps://www.jeremyjordan.me/testing-ml/amp/\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent the opinions of current, former, or future employers."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-4", "title": "Data Engineering Weekly", "content": "Welcome to the fourth edition of the data engineering newsletter. This week's release is a new set of articles that focus on data orchestration, ML applications, tuning data workload, and Kafka on Kubernetes. \nAirflow is a huge step forward over loosely coupled cron jobs for running the data pipeline. Dagster, a data-aware, typed, self-describing, logical orchestration graph, takes the data orchestration to the next level by focusing on local development, testable code before production, and Linking data assets to the code that produced them. The focus on data dependencies, not with pure execution dependencies, is a data engineer's dream comes true.\nhttps://medium.com/dagster-io/dagster-the-data-orchestrator-5fe5cadb0dfb\nAmundsen is a data discovery and metadata engine, open-sourced by Lyft joining LF AI Foundation.\nhttps://lfai.foundation/blog/2020/08/11/amundsen-joins-lf-ai-as-new-incubation-project/\nVimeo writes a post on video social analytics infrastructure using Apache Spark. The major challenge around integrating the external API guarded with severe rate limits. The practical usage of micro batching to workaround external API rate limiting and decouple the application logic from API data sourcing is a pragmatic approach and an exciting read.\nhttps://medium.com/vimeo-engineering-blog/video-social-analytics-at-scale-using-apache-spark-5bf34359c9ba\nSlack writes about ML infrastructure to prevent spam invites. The key takeaway is the simplicity of the approach and focuses on the operational aspect of the ML application.\nhttps://slack.engineering/blocking-slack-invite-spam-with-machine-learning/\ufeff\nKoalas is an open-source project which provides a drop-in replacement for pandas that focuses on scalability. Databricks writes a post on how PySpark can effectively work with Koalas.\nhttps://databricks.com/blog/2020/08/11/interoperability-between-koalas-and-apache-spark.html\nEMR is the widely used big data service from AWS. Monitoring Amazon EMR clusters is essential to help detect critical issues with the applications or infrastructure in real-time and identify root causes quickly. AWS writes about hot to integrate EMR metrics with Prometheus and other monitoring ecosystems such as Grafana for dashboarding and AWS SNS to send notification and alerts.\nhttps://aws.amazon.com/blogs/big-data/monitor-and-optimize-analytic-workloads-on-amazon-emr-with-prometheus-and-grafana/\nThe Buy vs. Build on the table when it comes to stream processing considering the complexity of the system. Apache Kafka and AWS Kinesis are the leading competitors when it comes to message brokers. It's (not) surprising that Apache Kafka still years ahead in stream processing.\nhttps://medium.com/flo-engineering/kinesis-vs-kafka-6709c968813\nStrimzi is an open-source CNCF sandbox project that focuses on running Apache Kafka on Kubernetes while providing container images for Apache Kafka itself, Zookeeper, and other components that are part of the Strimzi ecosystem. The blog post narrates how to move the Apache Kafka workload to Kubernetes.\nhttps://developers.redhat.com/blog/2020/08/14/introduction-to-strimzi-apache-kafka-on-kubernetes-kubecon-europe-2020/\nApache Kafka consumers are a single-threaded processing model that follows one partition consumed per thread. The model simplifies the ordering and processing guarantee in processing the stream of events. The downside of the approach, we often underutilize the CPU. Confluent writes a blog post narrates how can we implement Multi-Threaded Message Consumption with the Apache Kafka Consumer and the challenges around it.\nhttps://www.confluent.io/blog/kafka-consumer-multi-threaded-messaging/\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent the opinions of current, former, or future employers."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-3", "title": "Data Engineering Weekly", "content": "Welcome to the third edition of the data engineering newsletter. This week's release is a new set of articles that focus on practical learning, performance tuning, version control, and next-gen GPU stream processing.\u00a0\nI'm excited to read about the GPU-accelerated streaming platform this week. NVIDIA writes about cuStreamz, the first GPU-accelerated streaming data processing library. Written in Python, it built on top of RAPIDS, the GPU-accelerator for data science libraries.\nhttps://medium.com/rapids-ai/gpu-accelerated-stream-processing-with-rapids-f2b725696a61\nContinue on the GPU-accelerated stream processing, Apache Flink 1.11 introduces a new\u00a0External Resource Framework, which allows you to request external resources from the underlying resource management systems (e.g., Kubernetes) and accelerate your workload with those resources. The blog post explains how to integrate the GPU plugin that can help to build an end-to-end real-time AI workflow.\nhttps://flink.apache.org/news/2020/08/06/external-resource.html\nLinkedin writes about learning from Hadoop incidents. All the modern workflow schedulers support retries, but the unstable infrastructure hides the resource cost with this build-in fault tolerance of the system. Though the article focused on HDFS data loss, the theory applies all parts of the data pipeline.\nhttps://engineering.linkedin.com/blog/2020/learnings-from-a-recent-hadoop-incident\nTencent wrote a guest post about it's\u00a0Apache Kafka infrastructure to Handle 10 Trillion+ Messages Per Day. The federated Kafka clusters and the logical topic mapping is emerging as a design pattern to handle large scale Kafka infrastructure. The proxy approach for the consumers is a contrasting approach from the Kafka consumer SDK approach.\u00a0\nhttps://www.confluent.io/blog/tencent-kafka-process-10-trillion-messages-per-day/\neBay writes about Terapeak Research 2.0 platform based on Apache Kafka and Elastic search. The article narrates it's the approach to the fault-tolerant pipeline. The primary, secondary consumer pattern is something new to me.\nhttps://tech.ebayinc.com/engineering/terapeak-research-2-0-making-the-data-processing-pipeline-robust/\nPatterns of Distributed Systems is a refreshing read about the system design. The data infrastructure engineers deal with multiple distributed systems, and the article is an exciting read to approach the design abstractly.\nhttps://martinfowler.com/articles/patterns-of-distributed-systems/\nThe COVID-19 outburst changes the landscape of many businesses and personal life. The Expedia data visualization group writes a fantastic article about how it monitors local restrictions to predict when the customers want to go traveling again and employees' well-being.\nhttps://medium.com/expedia-group-tech/how-expedia-group-is-monitoring-market-recovery-during-covid-19-1ce79e4cf60d\nThe source to destination validation is an essential step in an ETL pipeline. Direct Energy rewrote over 350 SQL Server stored procedures in PySpark as part of on-premises data warehouses to AWS migration. The article narrates Pythagoras, a data reconciliation engine using Amazon EMR and Amazon Athena.\nhttps://aws.amazon.com/blogs/big-data/build-a-distributed-big-data-reconciliation-engine-using-amazon-emr-and-amazon-athena/\nApache Flink writes about Pandas UDF's support for PyFlink. The current version supports only the scalar Pandas UDFs.\nhttps://flink.apache.org/2020/08/04/pyflink-pandas-udf-support-flink.html\nCost optimization becomes mainstream engineering in the cloud infrastructure. Expedia's blog series is an exciting read on optimizing Apache Spark's cost for running the batch workload.\nhttps://medium.com/expedia-group-tech/part-1-cloud-spending-efficiency-guide-for-apache-spark-on-ec2-instances-79ee8814de4e\nhttps://medium.com/expedia-group-tech/part-2-real-world-apache-spark-cost-tuning-examples-42390ee69194\nContinue with the cost optimization,  Amazon EC2 Spot Instances, which enable you to use unused Amazon EC2 computing capacity in the AWS Cloud, offer up to 90% savings over On-Demand Instances. That data may need to be \"shuffled\" to other Amazon EC2 instances to continue processing.  In this article, Qubole writes about FSx for Lustre; a high-performance parallel file system provides a mechanism to offload and eventually access this data in a high-performance shared file system helps reduce costs and improve performance.\nhttps://aws.amazon.com/blogs/storage/how-qubole-optimizes-cost-and-performance-by-managing-shuffle-data/\nMany innovative approaches are coming out for the data lifecycle management. The presentation walks through the major trends in each part of the data life cycle, such as data pipelines, compute engines, data modeling, data products, and data quality. It's missing the data discovery/ data accessibility trends, though.\nhttps://tomtunguz.com/five-data-trends-one-mega-trend-data-lifecycle/\nDBT is gaining much momentum as the leader as an analytics engineering workflow engine. The article walkthrough five reasons why BigQuery users should use DBT. The article focuses on BigQuery, but the reasoning applicable to any SQL databases.\nhttps://medium.com/@yuu.ishikawa/5-reasons-why-bigquery-users-should-use-dbt-144f326c458a \nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent the opinions of current, former, or future employers."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-2", "title": "Data Engineering Weekly", "content": "Welcome to the second edition of the data engineering newsletter. This week's release is a new set of articles that focus on workflow schedulers, data pipeline observability, performance, and cost-effectiveness.\nApache Pinot is gaining momentum as a realtime OLAP system for data engineering needs. In this blog post, Sapient narrates its experience benchmarking Apache Pinot. The ingestion rate cross 120k entries/second on one node is impressive.\nhttps://medium.com/@shounakmk219/tasted-apache-pinot-and-we-loved-it-85f9022c30f7\nNetflix open sourced\u00a0metaflow.org December 2019. Metaflow follows a layered architecture approach to run the data workload, a contrasting approach from a tightly coupled airflow's scheduler architecture. In this post, Netflix explains how the scheduler layer integrated with the AWS step functions.\nhttps://netflixtechblog.com/unbundling-data-science-workflows-with-metaflow-and-aws-step-functions-d454780c6280\nThe Airflow operator represents a single idempotent task. Operators determine what executes when your DAG runs. One of the drawbacks of the operator is that no Airflow does not have explicit inter-operator communication, aka no easy way to pass messages between operators! AIP-31 proposal adopting a functional DAG abstraction to hide the complexity. The following article explains how the functional definition can solve the inter-operator communication.\nhttps://medium.com/databand-ai/aip-31-airflow-functional-dag-definition-b34852a632d0\nPython becomes the de facto language for data science workload. Apache Spark community continually improves the performance of PySpark. Pinterest writes about its data infrastructure to empower their data science workload. The design approach to isolate the Python environment for each workload and the use of SparkMagic is an exciting read.\nhttps://medium.com/pinterest-engineering/empowering-pinterest-data-scientists-and-machine-learning-engineers-with-pyspark-f41b0d1dd1b8\nCost optimization is essential engineering in cloud computing. Netflix writes about its cost optimization platform in this blog post. The automated TTL recommendations only for tables with material cost-saving potentials are the highlight of this post.\nhttps://netflixtechblog.com/byte-down-making-netflixs-data-infrastructure-cost-effective-fee7b3235032\nSquare writes about using Amundsen to support users' privacy. The post narrates the challenges to label columns for the sensitive data and the usage of Google's Cloud data loss prevention tool.\nhttps://developer.squareup.com/blog/using-amundsen-to-support-user-privacy-via-metadata-collection-at-square/\nSpark 3.0 made many improvements with the SparkSQL. The article explains the internals of the Spark SQL execution plan and how to interpret the query plan to optimize the execution.\nhttps://towardsdatascience.com/mastering-query-plans-in-spark-3-0-f4c334663aa4\nStructured Streaming was initially introduced in Apache Spark 2.0. It has proven to be the best platform for building distributed stream processing applications. The article narrates troubleshooting streaming performance using the Spark UI 3.0\nhttps://databricks.com/blog/2020/07/29/a-look-at-the-new-structured-streaming-ui-in-apache-spark-3-0.html\nThe support for running Spark on Kubernetes added with version 2.3, and Spark-on-k8s adoption has been accelerating ever since. The lake of external shuffle service is one of the drawbacks of adopting Spark on Kubernetes. Spark 3.0 added support for soft dynamic allocation to mitigate the issue. The benchmark in the blog shows the performance difference between Spark on K8s and Spark on Yarn narrowing.\nhttps://towardsdatascience.com/performance-of-apache-spark-on-kubernetes-has-caught-up-with-yarn-73730878a792\nThe modern data platform is moving from the traditional data warehouse -> data lake to data mesh. This blog post is blueprint guidance on how to move the data warehouse to the data mesh world. The blog focused on Google cloud offerings, but the concept still applicable to any cloud infrastructure.\nhttps://medium.com/swlh/building-a-data-platform-to-enable-analytics-and-ai-driven-innovation-1bd95e37efb9\nThere has been a growing interest lately among the industry on getting better control over one's data ecosystem and improving its operational efficiency. Following Amundsen (Lyft), DataHub (Linkedin), Databook (Uber), and Metacat (Netflix), Criteo published it's internal data discovery system DataDoc.\nhttps://medium.com/criteo-labs/datadoc-the-criteo-data-observability-platform-2cd826a9a1af\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent the opinions of current, former, or future employers."}, {"url": "https://www.dataengineeringweekly.com/p/data-engineering-weekly-1", "title": "Data Engineering Weekly", "content": "I'm passionate about data engineering and decided to start a newsletter to share what I'm learning every week. I hope the newsletter will act as a medium for distributing knowledge and creates healthy conversations. This week's release is an exciting set of articles that focus on data privacy, data discoverability, and data applications.\nPrivacy often afterthought development in the data world. There are numerous ways one might betray someone's privacy, but they are evident in most everyday situations. The New York Times wrote their thought on data privacy. The post is a good overview of privacy, useful links, and what are the steps NYT is doing in marketing and advertisement on their user's privacy.\nhttps://open.nytimes.com/how-the-new-york-times-thinks-about-your-privacy-bc07d2171531\nThe popularity of microservices adds complexity to enforce data privacy policies over the period. The data often flows through an organization, duplicate multiple times without any accountability. Tracing the data flow and implement security policy is a challenge. Facebook writes about how a scalable data classification system helps to enforce the data policies.\nhttps://engineering.fb.com/security/data-classification-system/\nPoor data quality leads to unusable data. How much can you trust your data is a question in the minds of every data consumers. Thoughtworks wrote an interesting article on the same with an introduction to opensource library deequ from AWS lab.\nhttps://www.thoughtworks.com/insights/blog/how-much-can-you-trust-your-data\n\nThe Spark + AI Summit 2020 ended in the last week of June-2020. In case you missed it, all the slides and the talk available on the summit page. \nhttps://databricks.com/sparkaisummit/north-america-2020/agenda\nThe Klarna data team wrote an excellent summarization of the summit.\nhttps://engineering.klarna.com/highlights-from-spark-ai-summit-2020-for-data-engineers-359211b1eec2\n\n\nData discoverability is an essential aspect of the data infrastructure. The value proportion of a data warehouse system exponentially decreases with a weak data discovery system. The Shopify data team writes about their data discovery system, which is an excellent comprehensive overview of a data discovery design.\nhttps://engineering.shopify.com/blogs/engineering/solving-data-discovery-challenges-shopify\n\n\nCatalog services are an essential metadata engine for data discovery and schema management. Hive meta store, AWS Glue data catalog are some of the catalog services used in data infrastructure. Apache Flink 1.9 added catalog integration, and this blog post is describing how to integrate Apache Flink with the Hive and Postgress based catalog services.\nhttps://flink.apache.org/2020/07/23/catalogs.html\n\n\nThe University of Florida and NVIDIA Tuesday unveiled a plan to build the world's fastest AI supercomputer in academia, delivering 700 petaflops of AI performance.\nhttps://blogs.nvidia.com/blog/2020/07/21/university-of-florida-nvidia-ai-supercomputer/\n\n\nTimeZone is a complicated yet crucial part of data infrastructure. Databricks writes an excellent overview of TimeZone, Dates, and Timestamp with Spark 3.0 \nhttps://databricks.com/blog/2020/07/22/a-comprehensive-look-at-dates-and-timestamps-in-apache-spark-3-0.html\n\n\nPinterest writes shopping intent ML model to drive the shopping upsells Pinterest search. The evolution of the model from the upsell click rate model to the \"long click\" model is an exciting read.\nhttps://medium.com/pinterest-engineering/driving-shopping-upsells-from-pinterest-search-d06329255402\n\n\nWalmart wrote about stream processing with Spring Cloud. Spring Cloud provides stream processing on top of the familiar spring framework. The post gives an introduction to Spring Cloud, a sample application, and how to unit test.\nhttps://medium.com/walmartlabs/streaming-with-spring-cloud-24a001ad307a\n\n\nApache Airflow summit videos now available on Youtube.\n\n\nNoria is a new streaming data-flow system designed to act as a fast storage backend for read-heavy web applications based on this paper from OSDI'18. The thesis presentation by Jon Gjengset on his work on Noria is an educational one.\n\nLinks are provided for informational purposes and do not imply endorsement. All views expressed in this newsletter are my own and do not represent the opinions of current, former, or future employers."}, {"url": "https://www.dataengineeringweekly.com/p/coming-soon", "title": "Data Engineering Weekly", "content": "Welcome to Data Engineering Weekly by me, Ananth Packkildurai. \nSign up now so you don\u2019t miss the first issue.\nIn the meantime, tell your friends!"}]